<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE Publisher PUBLIC "-//Springer-Verlag//DTD A++ V2.4//EN" "http://devel.springer.de/A++/V2.4/DTD/A++V2.4.dtd">
<Publisher>
  <PublisherInfo>
    <PublisherName>Springer-Verlag</PublisherName>
    <PublisherLocation>London</PublisherLocation>
  </PublisherInfo>
  <Journal OutputMedium="All">
    <JournalInfo JournalProductType="ArchiveJournal" NumberingStyle="ContentOnly">
      <JournalID>13735</JournalID>
      <JournalPrintISSN>2192-6611</JournalPrintISSN>
      <JournalElectronicISSN>2192-662X</JournalElectronicISSN>
      <JournalTitle>International Journal of Multimedia Information Retrieval</JournalTitle>
      <JournalAbbreviatedTitle>Int J Multimed Info Retr</JournalAbbreviatedTitle>
      <JournalSubjectGroup>
        <JournalSubject Type="Primary">Computer Science</JournalSubject>
        <JournalSubject Type="Secondary">Information Systems Applications (incl. Internet)</JournalSubject>
        <JournalSubject Type="Secondary">Multimedia Information Systems</JournalSubject>
        <JournalSubject Type="Secondary">Computer Science, general</JournalSubject>
        <JournalSubject Type="Secondary">Image Processing and Computer Vision</JournalSubject>
        <JournalSubject Type="Secondary">Information Storage and Retrieval</JournalSubject>
        <JournalSubject Type="Secondary">Data Mining and Knowledge Discovery</JournalSubject>
        <SubjectCollection Code="Computer Science">SC6</SubjectCollection>
      </JournalSubjectGroup>
    </JournalInfo>
    <Volume OutputMedium="All">
      <VolumeInfo TocLevels="0" VolumeType="Regular">
        <VolumeIDStart>1</VolumeIDStart>
        <VolumeIDEnd>1</VolumeIDEnd>
        <VolumeIssueCount>4</VolumeIssueCount>
      </VolumeInfo>
      <Issue IssueType="Regular" OutputMedium="All">
        <IssueInfo IssueType="Regular" TocLevels="0">
          <IssueIDStart>4</IssueIDStart>
          <IssueIDEnd>4</IssueIDEnd>
          <IssueArticleCount>5</IssueArticleCount>
          <IssueHistory>
            <OnlineDate>
              <Year>2012</Year>
              <Month>10</Month>
              <Day>23</Day>
            </OnlineDate>
            <PrintDate>
              <Year>2012</Year>
              <Month>10</Month>
              <Day>22</Day>
            </PrintDate>
            <CoverDate>
              <Year>2012</Year>
              <Month>12</Month>
            </CoverDate>
            <PricelistYear>2012</PricelistYear>
          </IssueHistory>
          <IssueCopyright>
            <CopyrightHolderName>Springer-Verlag London</CopyrightHolderName>
            <CopyrightYear>2012</CopyrightYear>
          </IssueCopyright>
        </IssueInfo>
        <Article ID="s13735-012-0020-6" OutputMedium="All">
          <ArticleInfo ArticleType="OriginalPaper" ContainsESM="No" Language="En" NumberingStyle="ContentOnly" TocLevels="0">
            <ArticleID>20</ArticleID>
            <ArticleDOI>10.1007/s13735-012-0020-6</ArticleDOI>
            <ArticleSequenceNumber>2</ArticleSequenceNumber>
            <ArticleTitle Language="En" OutputMedium="All">Video concept detection by audio-visual grouplets</ArticleTitle>
            <ArticleCategory>Regular Paper</ArticleCategory>
            <ArticleFirstPage>223</ArticleFirstPage>
            <ArticleLastPage>238</ArticleLastPage>
            <ArticleHistory>
              <RegistrationDate>
                <Year>2012</Year>
                <Month>8</Month>
                <Day>17</Day>
              </RegistrationDate>
              <Received>
                <Year>2012</Year>
                <Month>7</Month>
                <Day>31</Day>
              </Received>
              <Accepted>
                <Year>2012</Year>
                <Month>8</Month>
                <Day>16</Day>
              </Accepted>
              <OnlineDate>
                <Year>2012</Year>
                <Month>9</Month>
                <Day>7</Day>
              </OnlineDate>
            </ArticleHistory>
            <ArticleCopyright>
              <CopyrightHolderName>Springer-Verlag London Limited</CopyrightHolderName>
              <CopyrightYear>2012</CopyrightYear>
            </ArticleCopyright>
            <ArticleGrants Type="Regular">
              <MetadataGrant Grant="OpenAccess"/>
              <AbstractGrant Grant="OpenAccess"/>
              <BodyPDFGrant Grant="OpenAccess"/>
              <BodyHTMLGrant Grant="OpenAccess"/>
              <BibliographyGrant Grant="OpenAccess"/>
              <ESMGrant Grant="OpenAccess"/>
            </ArticleGrants>
          </ArticleInfo>
          <ArticleHeader>
            <AuthorGroup>
              <Author AffiliationIDS="Aff1" CorrespondingAffiliationID="Aff1">
                <AuthorName DisplayOrder="Western">
                  <GivenName>Wei</GivenName>
                  <FamilyName>Jiang</FamilyName>
                </AuthorName>
                <Contact>
                  <Email>wei.jiang@kodak.com</Email>
                </Contact>
              </Author>
              <Author AffiliationIDS="Aff1">
                <AuthorName DisplayOrder="Western">
                  <GivenName>Alexander</GivenName>
                  <GivenName>C.</GivenName>
                  <FamilyName>Loui</FamilyName>
                </AuthorName>
                <Contact>
                  <Email>alexander.loui@kodak.com</Email>
                </Contact>
              </Author>
              <Affiliation ID="Aff1">
                <OrgDivision>Kodak Technology Center</OrgDivision>
                <OrgName>Eastman Kodak Company</OrgName>
                <OrgAddress>
                  <City>Rochester</City>
                  <State>NY</State>
                  <Country Code="US">USA</Country>
                </OrgAddress>
              </Affiliation>
            </AuthorGroup>
            <Abstract ID="Abs1" Language="En" OutputMedium="All">
              <Heading>Abstract</Heading>
              <Para>We investigate general concept classification in unconstrained videos by joint audio-visual analysis. An audio-visual grouplet (AVG) representation is proposed based on analyzing the statistical temporal audio-visual interactions. Each AVG contains a set of audio and visual codewords that are grouped together according to their strong temporal correlations in videos, and the AVG carries unique audio-visual cues to represent the video content. By using the entire AVGs as building elements, video concepts can be more robustly classified than using traditional vocabularies with discrete audio or visual codewords. Specifically, we conduct coarse-level foreground/background separation in both audio and visual channels, and discover four types of AVGs by exploring mixed-and-matched temporal audio-visual correlations among the following factors: visual foreground, visual background, audio foreground, and audio background. All of these types of AVGs provide discriminative audio-visual patterns for classifying various semantic concepts. To effectively use the AVGs for improved concept classification, a distance metric learning algorithm is further developed. Based on the AVG structure, the algorithm uses an iterative quadratic programming formulation to learn the optimal distances between data points according to the large-margin nearest-neighbor setting. Various types of grouplet-based distances can be computed using individual AVGs, and through our distance metric learning algorithm these grouplet-based distances can be aggregated for final classification. We extensively evaluate our method over the large-scale Columbia consumer video set. Experiments demonstrate that the AVG-based audio-visual representation can achieve consistent and significant performance improvements compared wth other state-of-the-art approaches.</Para>
            </Abstract>
            <KeywordGroup Language="En" OutputMedium="All">
              <Heading>Keywords</Heading>
              <Keyword>Video concept detection</Keyword>
              <Keyword>Audio-visual grouplet</Keyword>
            </KeywordGroup>
          </ArticleHeader>
          <Body>
            <Section1 ID="Sec1">
              <Heading>Introduction</Heading>
              <Para>This paper investigates the problem of automatic classification of semantic concepts in generic, unconstrained videos, by joint analysis of audio and visual content. These concepts include general categories, such as scene (e.g., beach), event (e.g., birthday, graduation), location (e.g., playground) and object (e.g., dog, bird). Generic videos are captured in an unrestricted manner, like those videos taken by consumers on YouTube. This is a difficult problem due to the diverse video content as well as the challenging conditions such as uneven lighting, clutter, occlusions, and complicated motions of both objects and camera.</Para>
              <Para>Large efforts have been devoted to classify general concepts in generic videos, such as the TRECVID high-level feature extraction or multimedia event detection [<CitationRef CitationID="CR34">34</CitationRef>], the human action recognition in Hollywood movies [<CitationRef CitationID="CR22">22</CitationRef>], and the Columbia consumer video (CCV) concept detection [<CitationRef CitationID="CR19">19</CitationRef>]. Most previous works classify videos in the same way they classify images, using mainly visual information. Specifically, visual features are extracted from either 2D keyframes or 3D local volumes, and these features are treated as individual static descriptors to train concept classifiers. Among these methods, the ones using the Bag-of-Words (BoW) representation over 2D or 3D local descriptors (e.g., SIFT [<CitationRef CitationID="CR24">24</CitationRef>] or HOG [<CitationRef CitationID="CR8">8</CitationRef>]) are considered state-of-the-art. In a BoW-based approach, local descriptors are vector-quantized against a vocabulary of prototypical descriptors to generate a histogram-like representation.</Para>
              <Para>The importance of incorporating audio information to facilitate semantic concept classification has been discovered by several previous works [<CitationRef CitationID="CR5">5</CitationRef>, <CitationRef CitationID="CR19">19</CitationRef>, <CitationRef CitationID="CR43">43</CitationRef>]. They generally use a multi-modal fusion strategy, i.e., early fusion [<CitationRef CitationID="CR5">5</CitationRef>, <CitationRef CitationID="CR19">19</CitationRef>, <CitationRef CitationID="CR43">43</CitationRef>] to train classifiers with concatenated audio and visual features, or late fusion [<CitationRef CitationID="CR5">5</CitationRef>, <CitationRef CitationID="CR43">43</CitationRef>] to combine judgments from classifiers built over individual modalities. Different from such fusion approaches that avoid studying temporal audio-visual synchrony, the work in [<CitationRef CitationID="CR17">17</CitationRef>] pursues a coarse-level audio-visual synchronization through learning a joint audio-visual codebook based on atomic representations in both audio and visual channels. However, the temporal audio-visual interaction is not explored in previous video concept classification methods. The temporal audio-visual dependencies can reveal unique audio-visual patterns to assist concept classification. For example, as illustrated in Fig. <InternalRef RefID="Fig1">1</InternalRef>, by studying correlations between temporal patterns of visual and audio codewords, we can discover discriminative audio-visual cues, such as the encapsulation of visual basketball patches and audio basketball bouncing sounds for classifying “basketball,” and the encapsulation of visual stadium patches and audio music sounds for classifying “non-music performance”. To the best of our knowledge, such audio-visual cues have not been studied before in previous literature.<Figure Category="Standard" Float="Yes" ID="Fig1">
                  <Caption Language="En">
                    <CaptionNumber>Fig. 1</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>Discovery of audio-visual patterns through temporal audio-visual interactions. <InlineEquation ID="IEq1">
                          <InlineMediaObject>
                            <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq1.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                          </InlineMediaObject>
                          <EquationSource Format="TEX"><![CDATA[$$w^\mathrm{visual}_i$$]]></EquationSource>
                        </InlineEquation> and <InlineEquation ID="IEq2">
                          <InlineMediaObject>
                            <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq2.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                          </InlineMediaObject>
                          <EquationSource Format="TEX"><![CDATA[$$w^\mathrm{audio}_j$$]]></EquationSource>
                        </InlineEquation> are discrete codewords in visual and audio vocabularies, respectively. By analyzing correlations between the temporal histograms of audio and visual codewords, we can discover salient audio-visual cues to represent videos from different concepts. For example, the highly correlated visual basketball patches and audio basketball bouncing sounds provide a unique pattern to classify “basketball.” The correlated visual stadium patches and audio background music are helpful to classify “non-music performance.” In comparison, discrete audio and visual codewords are less discriminative than such audio-visual cues</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <MediaObject ID="MO1">
                    <ImageObject Color="Color" FileRef="MediaObjects/13735_2012_20_Fig1_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                  </MediaObject>
                </Figure>
              </Para>
              <Para>From another perspective, beyond the traditional BoW representation, structured visual features have been recently found to be effective in many computer vision tasks. In addition to the local feature appearance, spatial relations among the local patches are incorporated to increase the robustness of the visual representation. The rationale behind this is that individual local visual patterns tend to be sensitive to variations such as changes of illumination, views, scales, and occlusions. In comparison, a set of co-occurrent local patterns can be less ambiguous. Along this direction, pairwise spatial constraints among local interest points have been used to enhance image registration [<CitationRef CitationID="CR13">13</CitationRef>]; various types of spatial contextual information have been used for object detection [<CitationRef CitationID="CR11">11</CitationRef>, <CitationRef CitationID="CR41">41</CitationRef>] and action classification [<CitationRef CitationID="CR25">25</CitationRef>]; and a grouplet representation has been developed to capture discriminative visual features and their spatial configurations for detecting the human-object-interaction scenes in images [<CitationRef CitationID="CR45">45</CitationRef>].</Para>
              <Para>Motivated by the importance of incorporating audio information to help video concept classification, as well as the success of using structured visual features for image classification, in this paper, we propose an audio-visual grouplet (AVG) representation. Each AVG contains a set of audio and visual codewords that have strong temporal correlations in videos. An audio-visual dictionary can be constructed to classify concepts using AVGs as building blocks. The AVGs capture not only the individual audio and visual features carried by the discrete audio and visual codewords, but also the temporal relations between audio and visual channels. By using the entire AVGs as building elements to represent videos, various concepts can be more robustly classified than using discrete audio and visual codewords. For example, as illustrated in Fig. <InternalRef RefID="Fig2">2</InternalRef>, The AVG that captures the visual bride and audio speech gives a strong audio-visual cue to classify the “wedding ceremony” concept, and the AVG that captures the visual bride and audio dancing music is quite discriminative to classify the “wedding dance” concept.<Figure Category="Standard" Float="Yes" ID="Fig2">
                  <Caption Language="En">
                    <CaptionNumber>Fig. 2</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>An example of AVG-based audio-visual dictionary. Each AVG is composed of a set of audio and visual codewords that have strong temporal correlations in videos. The AVG that captures the visual bride and audio speech (AVG: <InlineEquation ID="IEq3">
                          <InlineMediaObject>
                            <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq3.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                          </InlineMediaObject>
                          <EquationSource Format="TEX"><![CDATA[$$w^\mathrm{visual}_1$$]]></EquationSource>
                        </InlineEquation>, <InlineEquation ID="IEq4">
                          <InlineMediaObject>
                            <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq4.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                          </InlineMediaObject>
                          <EquationSource Format="TEX"><![CDATA[$$w^\mathrm{visual}_2$$]]></EquationSource>
                        </InlineEquation>, <InlineEquation ID="IEq5">
                          <InlineMediaObject>
                            <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq5.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                          </InlineMediaObject>
                          <EquationSource Format="TEX"><![CDATA[$$w^\mathrm{audio}_1$$]]></EquationSource>
                        </InlineEquation>) gives a unique audio-visual cue to classify “wedding ceremony,” and the AVG that captures the visual bride and audio dancing music (AVG: <InlineEquation ID="IEq6">
                          <InlineMediaObject>
                            <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq6.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                          </InlineMediaObject>
                          <EquationSource Format="TEX"><![CDATA[$$w^\mathrm{visual}_1$$]]></EquationSource>
                        </InlineEquation>, <InlineEquation ID="IEq7">
                          <InlineMediaObject>
                            <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq7.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                          </InlineMediaObject>
                          <EquationSource Format="TEX"><![CDATA[$$w^\mathrm{visual}_2$$]]></EquationSource>
                        </InlineEquation>, <InlineEquation ID="IEq8">
                          <InlineMediaObject>
                            <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq8.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                          </InlineMediaObject>
                          <EquationSource Format="TEX"><![CDATA[$$w^\mathrm{audio}_2$$]]></EquationSource>
                        </InlineEquation>) is discriminative to classify “wedding dance.” In comparison, discrete visual or audio codewords can be ambiguous for classification</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <MediaObject ID="MO2">
                    <ImageObject Color="Color" FileRef="MediaObjects/13735_2012_20_Fig2_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                  </MediaObject>
                </Figure>
              </Para>
              <Para>In addition, we develop a distance metric learning algorithm to effectively use the extracted AVGs for classifying concepts. Based on the AVGs, an iterative Quadratic Programming (QP) problem is formulated to learn the optimal distance metric between data points based on the large-margin nearest neighbor (LMNN) setting [<CitationRef CitationID="CR40">40</CitationRef>]. Our distance metric learning framework is quite flexible, where various types of grouplet-based distances can be computed using individual AVGs, and these grouplet-based distances can be fed into the same distance metric learning algorithm for concept classification. Specifically, we propose a grouplet-based distance based on the chi-square distance and word specificity [<CitationRef CitationID="CR26">26</CitationRef>], and through our distance metric learning such a grouplet-based distance can achieve consistent and significant classification performance gain.</Para>
            </Section1>
            <Section1 ID="Sec2">
              <Heading>Overview of our approach</Heading>
              <Para>Figure <InternalRef RefID="Fig3">3</InternalRef> summarizes the framework of our system. We discover four types of AVGs by exploring four types of temporal audio-visual correlations: correlations between visual foreground and audio foreground; correlations between visual background and audio background; correlations between visual foreground and audio background; and correlations between visual background and audio foreground. All of these types of AVGs are useful for video concept classification. For example, as illustrated in Fig. <InternalRef RefID="Fig3">3</InternalRef>, to effectively classify the “birthday” concept, all of the following factors are important: the visual foreground people (e.g., baby and child), the visual background setting (e.g., cake and table), the audio foreground sound (e.g., cheering, birthday song, and hand clapping), and the audio background sound (e.g., music). By studying the temporal audio-visual correlations among these factors, we can identify unique audio-visual patterns that are discriminative for “birthday” classification.<Figure Category="Standard" Float="Yes" ID="Fig3">
                  <Caption Language="En">
                    <CaptionNumber>Fig. 3</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>The overall framework of the proposed joint audio-visual analysis system. The example shows a “birthday” video, where four types of audio-visual patterns are useful for classifying the “birthday” concept: (1) the visual foreground baby with the audio foreground events such as singing the happy birthday song or people cheering, since a major portion of birthday videos have babies or children involved; (2) the visual foreground baby with the audio background music; (3) the visual background setting such as the cake, with the audio foreground singing/cheering; and (4) the visual background cake with the audio background music</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <MediaObject ID="MO3">
                    <ImageObject Color="Color" FileRef="MediaObjects/13735_2012_20_Fig3_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                  </MediaObject>
                </Figure>
              </Para>
              <Para>To enable the exploration of the foreground and background audio-visual correlations, coarse-level separation of the foreground and background is needed in both visual and audio channels. It is worth mentioning that due to the diverse video content and the challenging conditions (e.g., uneven lighting, clutter, occlusions, complicated objects and camera motions, and the unstructured audio sounds with overlapping acoustic sources), precise separation of visual or audio foreground and background is infeasible in generic videos. In addition, exact audio-visual synchronization can be unreliable most of the time. Multiple moving objects usually make sounds together, and often the object making sounds does not synchronically appear in video. To accommodate these issues, different from most previous audio-visual analysis methods [<CitationRef CitationID="CR3">3</CitationRef>, <CitationRef CitationID="CR7">7</CitationRef>, <CitationRef CitationID="CR16">16</CitationRef>, <CitationRef CitationID="CR32">32</CitationRef>] that rely on precisely separated visual foreground objects and/or audio foreground sounds, our proposed approach has the following characteristics.<UnorderedList Mark="Dash">
                  <ItemContent>
                    <Para>We explore statistical temporal audio-visual correlations over a set of videos instead of exact audio-visual synchronization in individual videos. By representing the temporal sequences of visual and audio codewords as multivariate point processes, the statistical pairwise nonparametric Granger causality [<CitationRef CitationID="CR15">15</CitationRef>] between audio and visual codewords is analyzed. Based on the audio-visual causal matrix, salient AVGs are identified, which encapsulate strongly correlated visual and audio codewords as building blocks to classify videos.</Para>
                  </ItemContent>
                  <ItemContent>
                    <Para>We do not pursue precise visual foreground/background separation. We aim to build foreground-oriented and background-oriented visual vocabularies. Specifically, consistent local points are tracked throughout each video. Based on both local motion vectors and spatiotemporal analysis of whole images, the point tracks are separated into foreground tracks and background tracks. Due to the challenging conditions of generic videos, such a separation is not precise. The target is to maintain a majority of foreground (background) tracks so that the constructed visual foreground (background) vocabulary can capture mainly visual foreground (background) information.</Para>
                  </ItemContent>
                  <ItemContent>
                    <Para>Similar to the visual aspect, we aim to build foreground-oriented and background-oriented audio vocabularies, instead of pursuing precisely separated audio foreground or background acoustic sources. In generic videos, the foreground sound events are usually distributed unevenly and sparsely. Therefore, the local representation that focuses on short-term transient sound events [<CitationRef CitationID="CR6">6</CitationRef>] can be used to capture the foreground audio information. Also, the mel-frequency cepstral coefficients (MFCCs) extracted from uniformly spaced audio windows roughly capture the overall information of the environmental sound. Based on the local representation and MFCCs, audio foreground and background vocabularies can be built, respectively.</Para>
                  </ItemContent>
                </UnorderedList>After obtaining various types of AVGs, a distance metric learning algorithm is further developed to effectively use the AVGs for concept classification. Based on the AVGs, we learn the optimal distance metric between data points under the LMNN setting. LMNN is used because of its resemblance to SVMs, i.e., the role of large margin in LMNN is inspired by its role in SVMs, and LMNN should inherit various strengths of SVMs [<CitationRef CitationID="CR33">33</CitationRef>]. Therefore, the final learned distance metric can provide reasonably good performance for SVM concept classifiers.</Para>
              <Para>We extensively evaluate our approach over the large-scale CCV set [<CitationRef CitationID="CR19">19</CitationRef>], containing 9317 consumer videos from YouTube. The consumer videos are captured by ordinary users under uncontrolled challenging conditions, without post-editing. The original audio soundtracks are preserved, which allows us to study legitimate audio-visual interactions. Experiments show that compared with the state-of-the-art multi-modal fusion methods using BoW representations, our AVG-based dictionaries can capture useful audio-visual cues and significantly improve the classification performance.</Para>
            </Section1>
            <Section1 ID="Sec3">
              <Heading>Brief review of related work</Heading>
              <Section2 ID="Sec4">
                <Heading>Audio-visual concept classification</Heading>
                <Para>Audio-visual analysis has been largely studied for speech recognition [<CitationRef CitationID="CR16">16</CitationRef>], speaker identification [<CitationRef CitationID="CR32">32</CitationRef>], and object localization [<CitationRef CitationID="CR4">4</CitationRef>]. For example, with multiple cameras and audio sensors, by using audio spatialization and multi-camera tracking, moving sound sources (e.g., people) can be located. In videos captured by a single sensor, objects are usually located by studying the audio-visual synchronization along the temporal dimension. A common approach, for instance, is to project each of the audio and visual modalities into a 1D subspace and then correlate the 1D representations [<CitationRef CitationID="CR3">3</CitationRef>, <CitationRef CitationID="CR7">7</CitationRef>]. These methods have shown interesting results in analyzing videos in a controlled or simple environment, where good sound source separation and visual foreground/background separation can be obtained. However, they can not be easily applied to generic videos due to the difficulties in both acoustic source separation and visual object detection.</Para>
                <Para>Most existing approaches for general video concept classification exploit the multi-modal fusion strategy instead of using direct correlation or synchronization across audio and visual modalities. For example, early fusion is used [<CitationRef CitationID="CR5">5</CitationRef>, <CitationRef CitationID="CR43">43</CitationRef>] to concatenate features from different modalities into long vectors. This approach usually suffers from the “curse of dimensionality,” as the concatenated multi-modal feature can be very long. Also, it remains an open issue how to construct suitable joint feature vectors comprising features from different modalities with different time scales and different distance metrics. In late fusion, individual classifiers are built for each modality separately, and their judgments are combined to make the final decision. Several combination strategies have been used, such as the majority voting, linear combination, super-kernel nonlinear fusion [<CitationRef CitationID="CR43">43</CitationRef>], or SVM-based meta-classification combination [<CitationRef CitationID="CR23">23</CitationRef>]. However, effective classifier combination remains a basic machine learning problem. Recently, an audio-visual atom (AVA) representation has been developed in [<CitationRef CitationID="CR17">17</CitationRef>]. Visual regions are tracked within short-term video slices to generate visual atoms, and audio energy onsets are located to generate audio atoms. Regional visual features extracted from visual atoms and spectrogram features extracted from audio atoms are concatenated to form the AVA representation. The audio-visual synchrony is found through learning an audio-visual codebook based on the AVAs. However, the temporal audio-visual interaction remains unstudied. As illustrated in Fig. <InternalRef RefID="Fig1">1</InternalRef>, the temporal audio-visual dependencies can reveal unique audio-visual patterns to assist concept classification. In addition, the work of [<CitationRef CitationID="CR17">17</CitationRef>] requires segmenting image frames into visual regions, which is too expensive to be practical.</Para>
              </Section2>
              <Section2 ID="Sec5">
                <Heading>Visual foreground/background separation</Heading>
                <Para>One most commonly used technique for separating foreground moving objects and the static background is background subtraction, where foreground objects are detected as the difference between the current frame and a reference image of the static background [<CitationRef CitationID="CR12">12</CitationRef>]. Various threshold adaptation methods [<CitationRef CitationID="CR1">1</CitationRef>] and adaptive background models [<CitationRef CitationID="CR35">35</CitationRef>] have been developed. However, these approaches require a relatively static camera, small illumination change, simple and stable background scene, and relatively slow object motion. Their performances over generic videos are still not satisfactory.</Para>
                <Para>Motion-based segmentation methods have also been used to separate moving foreground and static background in videos [<CitationRef CitationID="CR21">21</CitationRef>]. The dense optical flow is usually computed to capture pixel-level motions. Due to the sensitivity to large camera/object motion and the computation intensity, such methods cannot be easily applied to generic videos either.</Para>
              </Section2>
              <Section2 ID="Sec6">
                <Heading>Audio source separation</Heading>
                <Para>Real-world audio signals are combinations of a number of independent sound sources, such as various human voices, instrumental sounds, natural sounds, etc. Ideally, one would like to recover each source signal. However, this task is very challenging in generic videos, because only a single audio channel is available, and realistic soundtracks have unrestricted content from an unknown number of unstructured, overlapping acoustic sources.</Para>
                <Para>Early blind audio source separation (BASS) methods separate audio sources that are recorded with multiple microphones [<CitationRef CitationID="CR29">29</CitationRef>]. Later on, several approaches have been developed to separate single-channel audio, such as the factorial HMM methods [<CitationRef CitationID="CR31">31</CitationRef>] and the spectral decomposition methods [<CitationRef CitationID="CR38">38</CitationRef>]. Recently, the visual information has been incorporated to assist BASS [<CitationRef CitationID="CR39">39</CitationRef>], where the audio-video synchrony is used as side information. However, soundtracks studied by these methods are mostly mixtures of human voices or instrumental sounds with very limited background noise. When applied to generic videos, existing BASS methods cannot perform satisfactorily.</Para>
              </Section2>
              <Section2 ID="Sec7">
                <Heading>Distance metric learning</Heading>
                <Para>Distance metric learning is an important machine learning technique of adapting the underlying distance metric according to the available data for improved classification. The most popular distance metric learning algorithms are based on the Mahalanobis distance metric, such as the LMNN [<CitationRef CitationID="CR40">40</CitationRef>] method, the maximally collapsing metric learning approach [<CitationRef CitationID="CR14">14</CitationRef>], the information-theoretic metric learning method [<CitationRef CitationID="CR9">9</CitationRef>], and the semantic preserving BoW method [<CitationRef CitationID="CR42">42</CitationRef>]. However, it is non-trivial to incorporate the grouplet structure into the existing distance metric learning algorithms.</Para>
              </Section2>
            </Section1>
            <Section1 ID="Sec8">
              <Heading>Visual process</Heading>
              <Para>We conduct SIFT point tracking within each video, based on which foreground-oriented and background-oriented temporal visual patterns are generated. The following details the processing stages.</Para>
              <Section2 ID="Sec9">
                <Heading>Excluding bad video segments</Heading>
                <Para>Video shot boundary detection and bad video segment elimination are general preprocessing steps for video analysis. Each raw video is segmented into several parts according to the detected shot boundaries with a single shot in each part. Next, segments with very large camera motion are excluded from analysis. It is worth mentioning that in our case, these steps can actually be skipped, because we process consumer videos that have a single long shot per video, and the SIFT point tracking can automatically exclude bad segments by generating few tracks over such segments. However, we still recommend these preprocessing steps to accommodate a large variety of generic videos.</Para>
              </Section2>
              <Section2 ID="Sec10">
                <Heading>SIFT-based point tracking</Heading>
                <Para>We use Lowe’s 128-dim SIFT descriptor with the DoG interest point detector [<CitationRef CitationID="CR24">24</CitationRef>]. SIFT features are first extracted from a set of uniformly sampled image frames with a sampling rate of 6 fps (frames per second).<Footnote ID="Fn1">
                    <Para>In our experiment, the typical frame rate of videos is 30 fps. Typically we sample 1 frame from every 5 frames.</Para>
                  </Footnote> Then for adjacent image frames, pairs of matching SIFT features are found based on the Euclidean distance of their feature vectors, by also using Lowe’s method to discard ambiguous matches [<CitationRef CitationID="CR24">24</CitationRef>]. After that, along the temporal dimension, the matching pairs are connected into a set of SIFT point tracks, where different point tracks can start from different image frames and last variable lengths. This 6 fps sampling rate is empirically determined by considering both the computation cost and the ability of SIFT matching. In general, increasing the sampling rate will decrease the chance of missing point tracks, with the price of increased computation.</Para>
                <Para>Each SIFT point track is represented by a 136-dim feature vector. This feature vector is composed by a 128-dim SIFT vector concatenated with an 8-dim motion vector. The SIFT vector is the averaged SIFT features of all SIFT points in the track. The motion vector is the averaged histogram of oriented motion (HOM) along the track. That is, for each adjacent matching pair in the track, we compute the speed and direction of the local motion vector. By quantizing the 2D motion space into 8 bins (corresponding to 8 directions), an 8-dim HOM feature is computed where the value over each bin is the averaged speed of the motion vectors from the track moving along this direction.</Para>
              </Section2>
              <Section2 ID="Sec11">
                <Heading>Foreground/background separation</Heading>
                <Para>Once the set of SIFT point tracks are obtained, we separate them as foreground or background with the following two steps, as illustrated in Fig. <InternalRef RefID="Fig4">4</InternalRef>. First, for two adjacent frames <InlineEquation ID="IEq9">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq9.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$I_i$$]]></EquationSource>
                  </InlineEquation> and <InlineEquation ID="IEq10">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq10.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$I_{i+1}$$]]></EquationSource>
                  </InlineEquation>, we roughly separate their matching SIFT pairs into candidate foreground and background pairs based on the motion vectors. Specifically, we group these matching pairs by hierarchical clustering, where the grouping criterion is that pairs within a cluster have roughly the same moving direction and speed. Those SIFT pairs in the biggest cluster are treated as candidate background pairs, and all other pairs are treated as candidate foreground pairs. The rationale is that foreground moving objects usually occupy less than half of the entire screen, and points on the foreground objects do not have a very consistent moving pattern. In comparison, points on the static background generally have consistent motion and this motion is caused by camera motion. This first step can distinguish background tracks fairly well for videos with moderate planar camera motions that occur most commonly in generic videos.<Figure Category="Standard" Float="Yes" ID="Fig4">
                    <Caption Language="En">
                      <CaptionNumber>Fig. 4</CaptionNumber>
                      <CaptionContent>
                        <SimplePara>Example of separating foreground/background SIFT tracks. A rough separation is obtained by analyzing local motion vectors. The result is further refined by spatiotemporal analysis over entire images</SimplePara>
                      </CaptionContent>
                    </Caption>
                    <MediaObject ID="MO4">
                      <ImageObject Color="Color" FileRef="MediaObjects/13735_2012_20_Fig4_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                    </MediaObject>
                  </Figure>
                </Para>
                <Para>In the second step, we further refine the candidate foreground and background SIFT tracks by using the spatiotemporal representation of videos. A spatiotemporal X-ray image representation has been proposed by Akutsu and Tonomura for camera work identification [<CitationRef CitationID="CR2">2</CitationRef>], where the average of each line and each column in successive images are computed. The distribution of the angles of edges in the X-ray images can be matched to camera work models, from which camera motion classification and temporal video segmentation can be obtained directly [<CitationRef CitationID="CR20">20</CitationRef>]. When used alone, such methods cannot generate satisfactory segmentation results in many generic videos where large motions from multiple objects cannot be easily discriminated from the noisy background motion. The performance drops even more for small resolutions, e.g., 320<InlineEquation ID="IEq11">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq11.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\times $$]]></EquationSource>
                  </InlineEquation>240 for most videos in our experiments. Therefore, instead of pursuing precise spatiotemporal object segmentation, we use such a spatiotemporal analysis to refine the candidate foreground and background SIFT tracks. The spatiotemporal image representation is able to capture camera zoom and tilt, which can be used to rectify those candidate tracks that are mistakenly labeled as foreground due to camera zoom and tilt. Figure <InternalRef RefID="Fig4">4</InternalRef> shows an example of visual foreground/background separation by using the above two steps.</Para>
              </Section2>
              <Section2 ID="Sec12">
                <Heading>Vocabularies and feature representations</Heading>
                <Para>Based on the foreground and background SIFT point tracks, we build a visual foreground vocabulary and a visual background vocabulary, respectively. The BoW features can be computed using the vocabularies, which can be used directly to classify concepts. Also, temporal patterns of codeword occurrences can be computed to study correlations between audio and visual signals in Sect. <InternalRef RefID="Sec16">6</InternalRef>.</Para>
                <Para>From Sect. <InternalRef RefID="Sec10">4.2</InternalRef>, each SIFT track is represented by a 136-dim feature vector. All foreground tracks from the training videos are collected together, based on which the hierarchical K-means technique is used to construct a <InlineEquation ID="IEq12">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq12.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$D$$]]></EquationSource>
                  </InlineEquation>-word foreground visual vocabulary <InlineEquation ID="IEq13">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq13.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{f-v}$$]]></EquationSource>
                  </InlineEquation>. Similarly, a <InlineEquation ID="IEq14">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq14.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$D$$]]></EquationSource>
                  </InlineEquation>-word background visual vocabulary <InlineEquation ID="IEq15">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq15.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{b-v}$$]]></EquationSource>
                  </InlineEquation> is constructed with all of the training background tracks. In our experiments, we use relatively large vocabularies, <InlineEquation ID="IEq16">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq16.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$D = 4000$$]]></EquationSource>
                  </InlineEquation>. Based on findings from the previous literature [<CitationRef CitationID="CR44">44</CitationRef>] that when the vocabulary size exceeds 2000 the classification performance tends to saturate, we can alleviate the influence of the vocabulary size on the final classification performance. This size is also a tradeoff between accuracy and computational complexity.</Para>
                <Para>For each video <InlineEquation ID="IEq17">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq17.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$V_j$$]]></EquationSource>
                  </InlineEquation>, all of its foreground SIFT point tracks are matched to the foreground codewords. A soft weighting scheme is used to alleviate the quantization effects [<CitationRef CitationID="CR18">18</CitationRef>], and a <InlineEquation ID="IEq18">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq18.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$D$$]]></EquationSource>
                  </InlineEquation>-dim foreground BoW feature <InlineEquation ID="IEq19">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq19.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf F _j^{f-v}$$]]></EquationSource>
                  </InlineEquation> is generated. Similarly, all of the background SIFT point tracks are matched to the background codewords to generate a <InlineEquation ID="IEq20">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq20.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$D$$]]></EquationSource>
                  </InlineEquation>-dim background BoW feature <InlineEquation ID="IEq21">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq21.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf F _j^{b-v}$$]]></EquationSource>
                  </InlineEquation>. In general, both <InlineEquation ID="IEq22">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq22.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf F _j^{f-v}$$]]></EquationSource>
                  </InlineEquation> and <InlineEquation ID="IEq23">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq23.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf F _j^{b-v}$$]]></EquationSource>
                  </InlineEquation> have their impacts in classifying concepts, e.g., both the foreground people with caps and gowns and the background stadium setting are useful to classify “graduation” videos.</Para>
                <Para>To study the temporal audio-visual interactions, the following histogram feature is computed over time for each of the foreground and background visual vocabularies. Given a video <InlineEquation ID="IEq24">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq24.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$V_j$$]]></EquationSource>
                  </InlineEquation>, we have a set of foreground SIFT point tracks. Each track is labeled to one codeword in vocabulary <InlineEquation ID="IEq25">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq25.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{f-v}$$]]></EquationSource>
                  </InlineEquation> that is closest to the track in the visual feature space. Next, for each frame <InlineEquation ID="IEq26">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq26.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$I_{ji}$$]]></EquationSource>
                  </InlineEquation> in the video, we count the occurring frequency of each foreground codeword labeled to the foreground SIFT point tracks that have a SIFT point falling in this frame, and a <InlineEquation ID="IEq27">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq27.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$D$$]]></EquationSource>
                  </InlineEquation>-dim histogram <InlineEquation ID="IEq28">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq28.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$H^{f-v}_{ji}$$]]></EquationSource>
                  </InlineEquation> can be generated. Similarly, we can generate a <InlineEquation ID="IEq29">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq29.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$D$$]]></EquationSource>
                  </InlineEquation>-dim histogram <InlineEquation ID="IEq30">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq30.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$H^{b-v}_{ji}$$]]></EquationSource>
                  </InlineEquation> for each image frame <InlineEquation ID="IEq31">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq31.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$I_{ji}$$]]></EquationSource>
                  </InlineEquation> based on vocabulary <InlineEquation ID="IEq32">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq32.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{b-v}$$]]></EquationSource>
                  </InlineEquation>. After this computation, for the foreground <InlineEquation ID="IEq33">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq33.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{f-v}$$]]></EquationSource>
                  </InlineEquation> (or background <InlineEquation ID="IEq34">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq34.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{b-v}$$]]></EquationSource>
                  </InlineEquation>), we have a temporal sequence <InlineEquation ID="IEq35">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq35.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\{H^{f-v}_{j1},H^{f-v}_{j2},\ldots \}$$]]></EquationSource>
                  </InlineEquation> (or <InlineEquation ID="IEq36">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq36.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\{H^{b-v}_{j1},H^{b-v}_{j2},\ldots \}$$]]></EquationSource>
                  </InlineEquation>) over each video <InlineEquation ID="IEq37">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq37.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$V_j$$]]></EquationSource>
                  </InlineEquation>.</Para>
              </Section2>
            </Section1>
            <Section1 ID="Sec13">
              <Heading>Audio process</Heading>
              <Para>Instead of pursuing precisely separated audio sound sources, we extract background-oriented and foreground-oriented audio features. The temporal interactions of these features with their visual counterparts can be studied to generate useful audio-visual patterns for concept classification.</Para>
              <Section2 ID="Sec14">
                <Heading>Audio background</Heading>
                <Para>Various descriptors have been developed to represent audio signals in both temporal and spectral domains. Among these features, the MFCCs feature is one of the most popular choices for many different audio recognition systems [<CitationRef CitationID="CR5">5</CitationRef>, <CitationRef CitationID="CR32">32</CitationRef>]. MFCCs represent the shape of the overall spectrum with a few coefficients, and have been shown to work well for both structured sounds (e.g., speech) and unstructured environmental sounds. In soundtracks of generic videos, the foreground sound events (e.g., an occasional dog barking or hand clapping) are distributed unevenly and sparsely. In such a case, the MFCCs extracted from uniformly spaced audio windows capture the overall characteristics of the background environmental sound, since the statistical impact of the sparse foreground sound events is quite small. Therefore, we use the MFCCs as the background audio feature.</Para>
                <Para>For each given video <InlineEquation ID="IEq38">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq38.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$V_j$$]]></EquationSource>
                  </InlineEquation>, we extract the 13-dim MFCCs from the corresponding soundtrack using 25 ms windows with a hop size of 10 ms. Next, we put all of the MFCCs from all training videos together, on top of which the hierarchical K-means technique is used to construct a <InlineEquation ID="IEq39">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq39.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$D$$]]></EquationSource>
                  </InlineEquation>-word background audio vocabulary <InlineEquation ID="IEq40">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq40.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{b-a}$$]]></EquationSource>
                  </InlineEquation>. Similar to visual-based processing, we compute two different histogram-like features based on <InlineEquation ID="IEq41">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq41.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{b-a}$$]]></EquationSource>
                  </InlineEquation>. First, we generate a BoW feature <InlineEquation ID="IEq42">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq42.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf F ^{b-a}_j$$]]></EquationSource>
                  </InlineEquation> for each video <InlineEquation ID="IEq43">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq43.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$V_j$$]]></EquationSource>
                  </InlineEquation> by matching the MFCCs in the video to codewords in the vocabulary and conducting soft weighting. This BoW feature can be used directly for classifying concepts. Second, to study the audio-visual correlation, a temporal audio histogram sequence <InlineEquation ID="IEq44">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq44.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\{H^{b-a}_{j1},H^{b-a}_{j2},\ldots \}$$]]></EquationSource>
                  </InlineEquation> is generated for each video <InlineEquation ID="IEq45">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq45.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$V_j$$]]></EquationSource>
                  </InlineEquation> as follows. Each MFCC vector is labeled to one codeword in the audio background vocabulary <InlineEquation ID="IEq46">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq46.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{b-a}$$]]></EquationSource>
                  </InlineEquation> that is closest to the MFCC vector. Next, for each sampled image frame <InlineEquation ID="IEq47">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq47.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$I_{ji}$$]]></EquationSource>
                  </InlineEquation> in the video, we take a 200 ms window centered on this frame. Then we count the occurring frequency of the codewords labeled to the MFCCs that fall into this window, and a <InlineEquation ID="IEq48">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq48.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$D$$]]></EquationSource>
                  </InlineEquation>-dim histogram <InlineEquation ID="IEq49">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq49.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$H^{b-a}_{ji}$$]]></EquationSource>
                  </InlineEquation> can be generated. This <InlineEquation ID="IEq50">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq50.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$H^{b-a}_{ji}$$]]></EquationSource>
                  </InlineEquation> can be considered as temporally synchronized with the visual-based histograms <InlineEquation ID="IEq51">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq51.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$H^{f-v}_{ji}$$]]></EquationSource>
                  </InlineEquation> or <InlineEquation ID="IEq52">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq52.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$H^{b-v}_{ji}$$]]></EquationSource>
                  </InlineEquation>.</Para>
              </Section2>
              <Section2 ID="Sec15">
                <Heading>Audio foreground</Heading>
                <Para>As mentioned above, the soundtrack of a generic video usually has unevenly and sparsely distributed foreground sound events. To capture such foreground information, local representations that focus on short-term local sound events should be used. In [<CitationRef CitationID="CR6">6</CitationRef>], Cotton et al. have developed a local event-based representation, where a set of salient points in the soundtrack are located based on time-frequency energy analysis and multi-scale spectrum analysis. These salient points contain distinct event onsets, i.e., transient events. By modeling the local temporal structure around each transient event, an audio feature reflecting the foreground of the soundtrack can be computed. In this work, we follow the recipe of [<CitationRef CitationID="CR6">6</CitationRef>] to generate the foreground audio feature.</Para>
                <Para>Specifically, the automatic gain control (AGC) is first applied to equalize the audio energy in both time and frequency domains. Next, the spectrogram of the AGC-equalized signal is taken for a number of different time-frequency tradeoffs, corresponding to window length between 2 and 80 ms. Multiple scales enable the localization of events of different durations. High-magnitude bins in any spectrogram indicate a candidate transient event at the corresponding time. A limit is empirically set on the minimum distance between successive events to produce four events per second on average. A 250 ms window of the audio signal is extracted centered on each transient event time, which captures the temporal structure of the transient event. Within each 250 ms window, a 40-dim spectrogram-based feature is computed for short-term signals over 25 ms windows with 10 ms hops, which results in 23 successive features for each event. These features are concatenated together to form a 920-dim representation for each transient event. After that, PCA is performed over all transient events from all training videos, and the top 20 bases are used to project the original 920-dim event representation to 20 dimensions.</Para>
                <Para>By putting all the projected transient features from all training videos together, the hierarchical K-means technique is used again to construct a <InlineEquation ID="IEq53">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq53.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$D$$]]></EquationSource>
                  </InlineEquation>-word foreground audio vocabulary <InlineEquation ID="IEq54">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq54.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{f-a}$$]]></EquationSource>
                  </InlineEquation>. We also compute two different histogram-like features based on <InlineEquation ID="IEq55">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq55.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{f-a}$$]]></EquationSource>
                  </InlineEquation>. First, we generate a BoW feature <InlineEquation ID="IEq56">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq56.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf F ^{f-a}_j$$]]></EquationSource>
                  </InlineEquation> for each video <InlineEquation ID="IEq57">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq57.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$V_j$$]]></EquationSource>
                  </InlineEquation> by matching the transient features in the video to codewords in the vocabulary and conducting soft weighting. Second, a temporal audio histogram sequence <InlineEquation ID="IEq58">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq58.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\{H^{f-a}_{j1},H^{f-a}_{j2},\ldots \}$$]]></EquationSource>
                  </InlineEquation> is generated for each video <InlineEquation ID="IEq59">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq59.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$V_j$$]]></EquationSource>
                  </InlineEquation> as follows. Each transient event is labeled to one codeword in the audio foreground vocabulary <InlineEquation ID="IEq60">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq60.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{f-a}$$]]></EquationSource>
                  </InlineEquation> that is closest to the transient event feature. Next, for each sampled image frame <InlineEquation ID="IEq61">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq61.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$I_{ji}$$]]></EquationSource>
                  </InlineEquation> in the video, we take a 200 ms window centered on this frame. Then we count the occurring frequency of the codewords labeled to the transient events whose centers fall into this window, and a <InlineEquation ID="IEq62">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq62.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$D$$]]></EquationSource>
                  </InlineEquation>-dim histogram <InlineEquation ID="IEq63">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq63.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$H^{f-a}_{ji}$$]]></EquationSource>
                  </InlineEquation> can be generated. Similar to <InlineEquation ID="IEq64">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq64.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$H^{b-a}_{ji}$$]]></EquationSource>
                  </InlineEquation>, <InlineEquation ID="IEq65">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq65.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$H^{f-a}_{ji}$$]]></EquationSource>
                  </InlineEquation> can be considered as synchronized with <InlineEquation ID="IEq66">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq66.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$H^{f-v}_{ji}$$]]></EquationSource>
                  </InlineEquation> or <InlineEquation ID="IEq67">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq67.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$H^{b-v}_{ji}$$]]></EquationSource>
                  </InlineEquation>.</Para>
              </Section2>
            </Section1>
            <Section1 ID="Sec16">
              <Heading>AVGs from temporal causality</Heading>
              <Para>Recently, Prabhakar et al. [<CitationRef CitationID="CR30">30</CitationRef>] have shown that the sequence of visual codewords produced by a space–time vocabulary representation of a video sequence can be interpreted as a multivariate point process. The pairwise temporal causal relations between visual codewords are computed within a video sequence, and visual codewords are grouped into causal sets. Evaluations over social game videos show promising results that the manually selected causal sets can capture the dyadic interactions. However, the work in [<CitationRef CitationID="CR30">30</CitationRef>] relies on nicely separated foreground objects, and causal sets are manually selected for each individual video. The method cannot be used for general concept classification.</Para>
              <Para>We propose to investigate the temporal causal relations between audio and visual codewords. The rough separation of foreground and background for both temporal SIFT tracks and audio sounds enables a meaningful study of such temporal relations. For the purpose of classifying general concepts in generic videos, all of the following factors have their contributions: foreground visual objects, foreground audio transient events, background visual scenes, and background environmental sounds. Therefore, we explore their mixed-and-matched temporal relations to find salient AVGs that can assist the final classification.</Para>
              <Section2 ID="Sec17">
                <Heading>Point-process representation of codewords</Heading>
                <Para>From the previous sections, for each video <InlineEquation ID="IEq68">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq68.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$V_j$$]]></EquationSource>
                  </InlineEquation>, we have 4 temporal sequences: <InlineEquation ID="IEq69">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq69.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\{H^{f-v}_{j1} ,H^{f-v}_{j2} ,\ldots \}, \{H^{f-a}_{j1} ,H^{f-a}_{j2} ,\ldots \}, \{H^{b-v}_{j1},H^{b-v}_{j2},\ldots \}, \text{ and} \{H^{b-a}_{j1},H^{b-a}_{j2},\ldots \}$$]]></EquationSource>
                  </InlineEquation>, according to vocabularies <InlineEquation ID="IEq70">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq70.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{f-v}, \mathcal V ^{f-a}, \mathcal V ^{b-v}, \text{ and} \mathcal V ^{b-a}$$]]></EquationSource>
                  </InlineEquation>, respectively. For each vocabulary, e.g., the foreground visual vocabulary <InlineEquation ID="IEq71">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq71.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{f-v}$$]]></EquationSource>
                  </InlineEquation>, each codeword <InlineEquation ID="IEq72">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq72.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$w_k$$]]></EquationSource>
                  </InlineEquation> in the vocabulary can be treated as a point process, <InlineEquation ID="IEq73">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq73.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$N^{f-v}_k(t)$$]]></EquationSource>
                  </InlineEquation>, which counts the number of occurrences of <InlineEquation ID="IEq74">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq74.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$w_k$$]]></EquationSource>
                  </InlineEquation> in the interval <InlineEquation ID="IEq75">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq75.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$(0,t]$$]]></EquationSource>
                  </InlineEquation>. The number of occurrences of <InlineEquation ID="IEq76">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq76.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$w_k$$]]></EquationSource>
                  </InlineEquation> in a small interval <InlineEquation ID="IEq77">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq77.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$dt$$]]></EquationSource>
                  </InlineEquation> is <InlineEquation ID="IEq78">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq78.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$dN^{f-v}_k(t) = N^{f-v}_k(t + dt)-N^{f-v}_k(t)$$]]></EquationSource>
                  </InlineEquation>, and <InlineEquation ID="IEq79">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq79.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$E\{dN^{f-v}_k(t)/dt\} = \lambda ^{f-v}_k$$]]></EquationSource>
                  </InlineEquation> is the mean intensity. For theoretical and practical convenience, the zero-mean process is considered, and <InlineEquation ID="IEq80">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq80.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$N^{f-v}_k(t)$$]]></EquationSource>
                  </InlineEquation> is assumed as wide-sense stationary, mixing, and orderly [<CitationRef CitationID="CR27">27</CitationRef>]. Point processes generated by all <InlineEquation ID="IEq81">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq81.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$D$$]]></EquationSource>
                  </InlineEquation> codewords of vocabulary <InlineEquation ID="IEq82">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq82.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{f-v}$$]]></EquationSource>
                  </InlineEquation> form a D-dim multivariate point process <InlineEquation ID="IEq83">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq83.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf{N}^{f-v} (t) = (N^{f-v}_1(t),\ldots ,N^{f-v}_D(t))^T$$]]></EquationSource>
                  </InlineEquation>. Each video <InlineEquation ID="IEq84">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq84.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$V_j$$]]></EquationSource>
                  </InlineEquation> gives one trial of <InlineEquation ID="IEq85">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq85.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf N ^{f-v}(t)$$]]></EquationSource>
                  </InlineEquation> with counting vector <InlineEquation ID="IEq86">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq86.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$(h^{f-v}_{j1} (t), h^{f-v}_{j2} (t), \ldots , h^{f-v}_{jD}(t))^T$$]]></EquationSource>
                  </InlineEquation> , where <InlineEquation ID="IEq87">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq87.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$h^{f-v}_{jk}(t)$$]]></EquationSource>
                  </InlineEquation> is the value over the <InlineEquation ID="IEq88">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq88.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$k$$]]></EquationSource>
                  </InlineEquation>-th bin of the histogram <InlineEquation ID="IEq89">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq89.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$H^{f-v}_{jt}$$]]></EquationSource>
                  </InlineEquation>.</Para>
                <Para>Similarly, D-dim multivariate point processes <InlineEquation ID="IEq90">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq90.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf{N}^{f-a}(t)$$]]></EquationSource>
                  </InlineEquation>, <InlineEquation ID="IEq91">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq91.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf{N}^{b-v}(t)$$]]></EquationSource>
                  </InlineEquation>, and <InlineEquation ID="IEq92">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq92.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf{N}^{b-a}(t)$$]]></EquationSource>
                  </InlineEquation> can be generated for vocabularies <InlineEquation ID="IEq93">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq93.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{f-a}$$]]></EquationSource>
                  </InlineEquation>, <InlineEquation ID="IEq94">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq94.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{b-v}$$]]></EquationSource>
                  </InlineEquation>, and <InlineEquation ID="IEq95">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq95.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{b-a}$$]]></EquationSource>
                  </InlineEquation>, respectively.</Para>
              </Section2>
              <Section2 ID="Sec18">
                <Heading>Temporal causality among codewords</Heading>
                <Para>Granger causality [<CitationRef CitationID="CR15">15</CitationRef>] is a statistical measure based on the concept of time series forecasting, where a time series <InlineEquation ID="IEq96">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq96.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$Y_1$$]]></EquationSource>
                  </InlineEquation> is considered to causally influence a time series <InlineEquation ID="IEq97">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq97.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$Y_2$$]]></EquationSource>
                  </InlineEquation> if predictions of future values of <InlineEquation ID="IEq98">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq98.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$Y_2$$]]></EquationSource>
                  </InlineEquation> based on the joint history of <InlineEquation ID="IEq99">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq99.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$Y_1$$]]></EquationSource>
                  </InlineEquation> and <InlineEquation ID="IEq100">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq100.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$Y_2$$]]></EquationSource>
                  </InlineEquation> are more accurate than predictions based on <InlineEquation ID="IEq101">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq101.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$Y_2$$]]></EquationSource>
                  </InlineEquation> alone. The estimation of Granger causality usually relies on autoregressive models, and for continuous-valued data like electroencephalogram, such model fitting is straightforward.</Para>
                <Para>In [<CitationRef CitationID="CR27">27</CitationRef>], a nonparametric method that bypasses the autoregressive model fitting has been developed to estimate Granger causality for point processes. The theoretical basis lies in the spectral representation of point processes, the factorization of spectral matrices, and the formulation of Granger causality in the spectral domain. In the following, we describe the details of using the method of [<CitationRef CitationID="CR27">27</CitationRef>] to compute the temporal causality between audio and visual codewords. For simplicity, we temporarily omit indexes <InlineEquation ID="IEq102">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq102.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$f-v$$]]></EquationSource>
                  </InlineEquation>, <InlineEquation ID="IEq103">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq103.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$b-v$$]]></EquationSource>
                  </InlineEquation>, <InlineEquation ID="IEq104">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq104.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$f-a$$]]></EquationSource>
                  </InlineEquation>, and <InlineEquation ID="IEq105">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq105.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$b-a$$]]></EquationSource>
                  </InlineEquation>, w.l.o.g., since Granger causality can be computed for any two codewords from any vocabularies.</Para>
                <Section3 ID="Sec19">
                  <Heading>Spectral representation of point processes</Heading>
                  <Para>The pairwise statistical relation between two point processes <InlineEquation ID="IEq106">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq106.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$N_k(t)$$]]></EquationSource>
                    </InlineEquation> and <InlineEquation ID="IEq107">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq107.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$N_l(t)$$]]></EquationSource>
                    </InlineEquation> can be captured by the cross-covariance density function <InlineEquation ID="IEq108">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq108.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$R_{kl}(u)$$]]></EquationSource>
                    </InlineEquation> at lag <InlineEquation ID="IEq109">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq109.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$u$$]]></EquationSource>
                    </InlineEquation>:<Equation ID="Equa1">
                      <MediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_Equa1.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </MediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$\begin{aligned} R_{kl}(u)\!=\!\frac{E\{dN_k(t + u)dN_l(t)\}}{dudt} - I \left[N_k(t) \!=\! N_l(t)\right] \lambda _k\delta (u), \end{aligned}$$]]></EquationSource>
                    </Equation>where <InlineEquation ID="IEq110">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq110.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$\delta (u)$$]]></EquationSource>
                    </InlineEquation> is the classical Kronecker delta function, and <InlineEquation ID="IEq111">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq111.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$I[\cdot ]$$]]></EquationSource>
                    </InlineEquation> is the indicator function. By taking the Fourier transform of <InlineEquation ID="IEq112">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq112.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$R_{kl}(u)$$]]></EquationSource>
                    </InlineEquation>, we obtain the cross-spectrum <InlineEquation ID="IEq113">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq113.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$S_{kl}(f)$$]]></EquationSource>
                    </InlineEquation>. Specifically, the multitaper method [<CitationRef CitationID="CR37">37</CitationRef>] can be used to compute the spectrum, where <InlineEquation ID="IEq114">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq114.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$M$$]]></EquationSource>
                    </InlineEquation> data tapers <InlineEquation ID="IEq115">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq115.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$\{q_m\}^{M}_{m=1}$$]]></EquationSource>
                    </InlineEquation> are applied successively to point process <InlineEquation ID="IEq116">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq116.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$N_k(t)$$]]></EquationSource>
                    </InlineEquation> (with length <InlineEquation ID="IEq117">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq117.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$T$$]]></EquationSource>
                    </InlineEquation>):<Equation ID="Equ1">
                      <EquationNumber>1</EquationNumber>
                      <MediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_Equ1.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </MediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \begin{aligned}&S_{kl}(f)=\frac{1}{2\pi MT}\sum \nolimits _{m=1}^{M}\tilde{N}_k(f,m)\tilde{N}_{l}(f,m)^{*}, \\&\tilde{N}_k(f,m)=\sum \nolimits _{t_p=1}^{T}q_m(t_p)N_k(t_p)\text{ exp}(-2\pi ift_p). \end{aligned} \end{aligned}$$]]></EquationSource>
                    </Equation>The symbol <InlineEquation ID="IEq118">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq118.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$^{*}$$]]></EquationSource>
                    </InlineEquation> is the complex conjugate transpose. Equation (<InternalRef RefID="Equ1">1</InternalRef>) gives an estimation of the cross-spectrum using one realization, and such estimations of multiple realizations are averaged to give the final estimation of the cross-spectrum.</Para>
                </Section3>
                <Section3 ID="Sec20">
                  <Heading>Granger causality in spectral domain</Heading>
                  <Para>For multivariate continuous-valued time series <InlineEquation ID="IEq119">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq119.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$Y_1$$]]></EquationSource>
                    </InlineEquation> and <InlineEquation ID="IEq120">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq120.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$Y_2$$]]></EquationSource>
                    </InlineEquation> with joint autoregressive representations:<Equation ID="Equa2">
                      <MediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_Equa2.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </MediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$\begin{aligned} Y_1(t)&= \sum \nolimits _{p=1}^{\infty }a_{p}Y_1(t-p) + \sum \nolimits _{p=1}^{\infty }b_{p}Y_2(t-p) + \epsilon (t),\\ Y_2(t)&= \sum \nolimits _{p=1}^{\infty }c_{p}Y_2(t-p) + \sum \nolimits _{p=1}^{\infty }d_{p}Y_1(t-p) + \eta (t), \end{aligned}$$]]></EquationSource>
                    </Equation>their noise terms are uncorrelated over time and their contemporaneous covariance matrix is:<Equation ID="Equa3">
                      <MediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_Equa3.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </MediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \Sigma&= \left[ \begin{array}{c@{}c}\Sigma _2&\Upsilon _2\\ \Upsilon _2&\Gamma _2\end{array} \right] , \Sigma _2 = \text{ var}(\epsilon (t)),\Gamma _2 = \text{ var}(\eta (t)),\Upsilon _2 \\&= \text{ cov} (\epsilon (t), \eta (t)). \end{aligned}$$]]></EquationSource>
                    </Equation>We can compute the spectral matrix as [<CitationRef CitationID="CR10">10</CitationRef>]:<Equation ID="Equ2">
                      <EquationNumber>2</EquationNumber>
                      <MediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_Equ2.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </MediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \mathbf S (f)=\left[\begin{array}{c@{~}c}S_{11}(f)&S_{12}(f)\\ S_{21}(f)&S_{22}(f)\end{array}\right]=\mathbf H (f)\Sigma \mathbf H (f)^{*}, \end{aligned}$$]]></EquationSource>
                    </Equation>where <InlineEquation ID="IEq121">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq121.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$\mathbf H (f) = \left[\begin{array}{c@{}c}H_{11}(f)&H_{12}(f)\\ H_{21}(f)&H_{22}(f)\end{array}\right]$$]]></EquationSource>
                    </InlineEquation> is the transfer function depending on coefficients of the autoregressive model. The spectral matrix <InlineEquation ID="IEq122">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq122.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$\mathbf S (f)$$]]></EquationSource>
                    </InlineEquation> of two point processes <InlineEquation ID="IEq123">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq123.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$N_k(t)$$]]></EquationSource>
                    </InlineEquation> and <InlineEquation ID="IEq124">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq124.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$N_l(t)$$]]></EquationSource>
                    </InlineEquation> can be estimated using Eq. (<InternalRef RefID="Equ1">1</InternalRef>). By spectral matrix factorization we can decompose <InlineEquation ID="IEq125">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq125.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$\mathbf S (f)$$]]></EquationSource>
                    </InlineEquation> into a unique corresponding transfer function <InlineEquation ID="IEq126">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq126.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$\tilde{\mathbf{H}}(f)$$]]></EquationSource>
                    </InlineEquation> and noise processes <InlineEquation ID="IEq127">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq127.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$\tilde{\Sigma }_2$$]]></EquationSource>
                    </InlineEquation> and <InlineEquation ID="IEq128">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq128.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$\tilde{\Gamma }_2$$]]></EquationSource>
                    </InlineEquation>. Next, the Granger causality at frequency <InlineEquation ID="IEq129">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq129.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$f$$]]></EquationSource>
                    </InlineEquation> can be estimated according to the algorithm developed in [<CitationRef CitationID="CR10">10</CitationRef>]:<Equation ID="Equ3">
                      <EquationNumber>3</EquationNumber>
                      <MediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_Equ3.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </MediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$\begin{aligned} G_{N_l\rightarrow N_k}(f)&= \text{ ln}\left(\frac{S_{kk}(f)}{\tilde{H}_{kk}(f)\tilde{\Sigma }_2\tilde{H}_{kk}(f)^*}\right),\end{aligned}$$]]></EquationSource>
                    </Equation>
                    <Equation ID="Equ4">
                      <EquationNumber>4</EquationNumber>
                      <MediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_Equ4.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </MediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$\begin{aligned} G_{N_k\rightarrow N_l}(f)&= \text{ ln}\left(\frac{S_{ll}(f)}{\tilde{H}_{ll}(f)\tilde{\Gamma }_2\tilde{H}_{ll}(f)^*}\right). \end{aligned}$$]]></EquationSource>
                    </Equation>The Granger causality scores over all frequencies are then summed together to obtain a single time-domain causal influence, i.e., <InlineEquation ID="IEq130">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq130.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$C_{N_k\rightarrow N_l} = \sum _{f}G_{N_k\rightarrow N_l}(f)$$]]></EquationSource>
                    </InlineEquation>, and <InlineEquation ID="IEq131">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq131.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$C_{N_l\rightarrow N_k} = \sum _{f}G_{N_l\rightarrow N_k}(f)$$]]></EquationSource>
                    </InlineEquation>. In general, <InlineEquation ID="IEq132">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq132.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$C_{N_k\rightarrow N_l} \ne C_{N_l\rightarrow N_k}$$]]></EquationSource>
                    </InlineEquation>, due to the directionality of the causal relations.</Para>
                </Section3>
              </Section2>
              <Section2 ID="Sec21">
                <Heading>AVGs from the causal matrix</Heading>
                <Para>Our target of studying temporal causality between audio and visual codewords is to identify strongly correlated AVGs, where the direction of the relations is usually not important. For example, a dog can start barking at any time during the video, and we would like to find the AVG that contains correlated codewords describing the foreground dog barking sound and the visual dog point tracks. The direction of whether the barking sound is captured before or after the visual tracks is irrelevant. Therefore, for a pair of codewords represented by point processes <InlineEquation ID="IEq144">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq144.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$N_k^{s_k}(t)$$]]></EquationSource>
                  </InlineEquation> and <InlineEquation ID="IEq145">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq145.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$N_l^{s_l}(t)$$]]></EquationSource>
                  </InlineEquation> (where <InlineEquation ID="IEq146">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq146.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$s_k$$]]></EquationSource>
                  </InlineEquation> or <InlineEquation ID="IEq147">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq147.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$s_l$$]]></EquationSource>
                  </InlineEquation> is one of the following <InlineEquation ID="IEq148">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq148.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$f-v$$]]></EquationSource>
                  </InlineEquation>, <InlineEquation ID="IEq149">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq149.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$f-a$$]]></EquationSource>
                  </InlineEquation>, <InlineEquation ID="IEq150">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq150.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$b-v$$]]></EquationSource>
                  </InlineEquation>, and <InlineEquation ID="IEq151">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq151.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$b-a$$]]></EquationSource>
                  </InlineEquation>, indicating the vocabularies the codeword comes from), the nonparametric Granger causality scores from both directions <InlineEquation ID="IEq152">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq152.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$C_{N^{s_k}_k\rightarrow N^{s_l}_l}$$]]></EquationSource>
                  </InlineEquation> and <InlineEquation ID="IEq153">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq153.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$C_{N^{s_l}_l\rightarrow N^{s_k}_k}$$]]></EquationSource>
                  </InlineEquation> are summed together to generate the final similarity between these two codewords:<Equation ID="Equ5">
                    <EquationNumber>5</EquationNumber>
                    <MediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_Equ5.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\begin{aligned} C(N^{s_k}_k,N^{s_l}_l)=C_{N^{s_k}_k\rightarrow N^{s_l}_l}+C_{N^{s_l}_l\rightarrow N^{s_k}_k}. \end{aligned}$$]]></EquationSource>
                  </Equation>Then, for a pair of audio and visual vocabularies, e.g., <InlineEquation ID="IEq154">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq154.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{f-v}$$]]></EquationSource>
                  </InlineEquation> and <InlineEquation ID="IEq155">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq155.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{f-a}$$]]></EquationSource>
                  </InlineEquation>, we have a <InlineEquation ID="IEq156">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq156.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$2D \times 2D$$]]></EquationSource>
                  </InlineEquation> symmetric causal matrix:<Equation ID="Equ6">
                    <EquationNumber>6</EquationNumber>
                    <MediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_Equ6.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \left[\begin{array}{c@{~~}c}\mathbf C ^{f-v,f-v}&\mathbf C ^{f-v,f-a} \\ \mathbf C ^{f-a,f-v}&\mathbf C ^{f-a,f-a}\end{array}\right], \end{aligned}$$]]></EquationSource>
                  </Equation>where <InlineEquation ID="IEq157">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq157.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf C ^{f-v,f-v}$$]]></EquationSource>
                  </InlineEquation>, <InlineEquation ID="IEq158">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq158.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf C ^{f-a,f-a}$$]]></EquationSource>
                  </InlineEquation>, and <InlineEquation ID="IEq159">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq159.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf C ^{f-v,f-a}$$]]></EquationSource>
                  </InlineEquation> are <InlineEquation ID="IEq160">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq160.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$D \times D$$]]></EquationSource>
                  </InlineEquation> matrices with entries <InlineEquation ID="IEq161">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq161.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$C(N^{f-v}_k,N^{f-v}_l)$$]]></EquationSource>
                  </InlineEquation>, <InlineEquation ID="IEq162">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq162.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$C(N^{f-a}_k,N^{f-a}_l)$$]]></EquationSource>
                  </InlineEquation>, and <InlineEquation ID="IEq163">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq163.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$C(N^{f-v}_k,N^{f-a}_l)$$]]></EquationSource>
                  </InlineEquation>, respectively.</Para>
                <Para>Spectral clustering can be applied directly based on this causal matrix to identify groups of codewords that have high correlations. Here we use the algorithm developed in [<CitationRef CitationID="CR28">28</CitationRef>] where the number of clusters can be determined automatically by analyzing the eigenvalues of the causal matrix. Each cluster is called an AVG, and codewords in an AVG can come from both audio and visual vocabularies. The AVGs capture temporally correlated audio and visual codewords that statistically interact over time. Each AVG can be treated as an audio-visual pattern, and all AVGs form an audio-visual dictionary.</Para>
                <Para>A total of four audio-visual dictionaries are generated in this work, by studying the temporal causal relations between different types of audio and visual codewords. They are: dictionary <InlineEquation ID="IEq164">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq164.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal D ^{f-v,f-a}$$]]></EquationSource>
                  </InlineEquation> by correlating <InlineEquation ID="IEq165">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq165.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{f-v}$$]]></EquationSource>
                  </InlineEquation> and <InlineEquation ID="IEq166">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq166.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{f-a}$$]]></EquationSource>
                  </InlineEquation>, <InlineEquation ID="IEq167">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq167.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal D ^{b-v,b-a}$$]]></EquationSource>
                  </InlineEquation> by correlating <InlineEquation ID="IEq168">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq168.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{b-v}$$]]></EquationSource>
                  </InlineEquation> and <InlineEquation ID="IEq169">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq169.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{b-a}$$]]></EquationSource>
                  </InlineEquation>, <InlineEquation ID="IEq170">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq170.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal D ^{f-v,b-a}$$]]></EquationSource>
                  </InlineEquation> by correlating <InlineEquation ID="IEq171">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq171.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{f-v}$$]]></EquationSource>
                  </InlineEquation> and <InlineEquation ID="IEq172">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq172.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{b-a}$$]]></EquationSource>
                  </InlineEquation>, and <InlineEquation ID="IEq173">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq173.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal D ^{b-v,f-a}$$]]></EquationSource>
                  </InlineEquation> by correlating <InlineEquation ID="IEq174">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq174.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{b-v}$$]]></EquationSource>
                  </InlineEquation> and <InlineEquation ID="IEq175">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq175.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{f-a}$$]]></EquationSource>
                  </InlineEquation>. As illustrated in Fig. <InternalRef RefID="Fig3">3</InternalRef>, all of these correlations reveal useful audio-visual patterns for classifying concepts.</Para>
                <Para>One intuitive way of using the AVGs for concept classification is to generate a feature value corresponding to each AVG for a given video. For instance, for the audio and/or visual codewords associated with an AVG, the values over the corresponding bins in the original visual-based and/or audio-based BoW features can be aggregated together (e.g., by taking summation or average) as the feature for the AVG. However, as illustrated in Fig. <InternalRef RefID="Fig5">5</InternalRef>, such aggregated BoW features can be problematic and cannot fully utilize the advantage of the grouplet structure. In the next Sect. <InternalRef RefID="Sec22">7</InternalRef>, we develop a distance metric learning algorithm to better use the AVGs for classifying concepts.<Figure Category="Standard" Float="Yes" ID="Fig5">
                    <Caption Language="En">
                      <CaptionNumber>Fig. 5</CaptionNumber>
                      <CaptionContent>
                        <SimplePara>An example of the aggregated BoW feature based on an AVG. In the example, assume that all codewords have equal weights, data points <InlineEquation ID="IEq133">
                            <InlineMediaObject>
                              <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq133.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                            </InlineMediaObject>
                            <EquationSource Format="TEX"><![CDATA[$$x_1$$]]></EquationSource>
                          </InlineEquation>, <InlineEquation ID="IEq134">
                            <InlineMediaObject>
                              <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq134.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                            </InlineMediaObject>
                            <EquationSource Format="TEX"><![CDATA[$$x_2$$]]></EquationSource>
                          </InlineEquation>, and <InlineEquation ID="IEq135">
                            <InlineMediaObject>
                              <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq135.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                            </InlineMediaObject>
                            <EquationSource Format="TEX"><![CDATA[$$x_3$$]]></EquationSource>
                          </InlineEquation> have the same aggregated BoW features for the given AVG (value 5 by taking summation). However, data points <InlineEquation ID="IEq136">
                            <InlineMediaObject>
                              <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq136.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                            </InlineMediaObject>
                            <EquationSource Format="TEX"><![CDATA[$$x_1$$]]></EquationSource>
                          </InlineEquation> and <InlineEquation ID="IEq137">
                            <InlineMediaObject>
                              <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq137.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                            </InlineMediaObject>
                            <EquationSource Format="TEX"><![CDATA[$$x_3$$]]></EquationSource>
                          </InlineEquation> should be more similar to each other than data points <InlineEquation ID="IEq138">
                            <InlineMediaObject>
                              <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq138.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                            </InlineMediaObject>
                            <EquationSource Format="TEX"><![CDATA[$$x_1$$]]></EquationSource>
                          </InlineEquation> and <InlineEquation ID="IEq139">
                            <InlineMediaObject>
                              <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq139.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                            </InlineMediaObject>
                            <EquationSource Format="TEX"><![CDATA[$$x_2$$]]></EquationSource>
                          </InlineEquation>. This is because <InlineEquation ID="IEq140">
                            <InlineMediaObject>
                              <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq140.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                            </InlineMediaObject>
                            <EquationSource Format="TEX"><![CDATA[$$x_1$$]]></EquationSource>
                          </InlineEquation> and <InlineEquation ID="IEq141">
                            <InlineMediaObject>
                              <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq141.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                            </InlineMediaObject>
                            <EquationSource Format="TEX"><![CDATA[$$x_3$$]]></EquationSource>
                          </InlineEquation> have the same feature values over visual codeword #1 and visual codeword #3, while <InlineEquation ID="IEq142">
                            <InlineMediaObject>
                              <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq142.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                            </InlineMediaObject>
                            <EquationSource Format="TEX"><![CDATA[$$x_1$$]]></EquationSource>
                          </InlineEquation> and <InlineEquation ID="IEq143">
                            <InlineMediaObject>
                              <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq143.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                            </InlineMediaObject>
                            <EquationSource Format="TEX"><![CDATA[$$x_2$$]]></EquationSource>
                          </InlineEquation> only have the same feature value over audio codeword #2</SimplePara>
                      </CaptionContent>
                    </Caption>
                    <MediaObject ID="MO12">
                      <ImageObject Color="Color" FileRef="MediaObjects/13735_2012_20_Fig5_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                    </MediaObject>
                  </Figure>
                </Para>
              </Section2>
            </Section1>
            <Section1 ID="Sec22">
              <Heading>Grouplet-based distance metric learning</Heading>
              <Para>Assume that we have <InlineEquation ID="IEq176">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq176.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$K$$]]></EquationSource>
                </InlineEquation> AVGs <InlineEquation ID="IEq177">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq177.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$G_k$$]]></EquationSource>
                </InlineEquation>, <InlineEquation ID="IEq178">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq178.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$k = 1,\ldots ,K$$]]></EquationSource>
                </InlineEquation> in an audio-visual dictionary <InlineEquation ID="IEq179">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq179.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\mathcal D $$]]></EquationSource>
                </InlineEquation>, where we temporarily omit upper indexes <InlineEquation ID="IEq180">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq180.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$(f-v,f-a)$$]]></EquationSource>
                </InlineEquation>, <InlineEquation ID="IEq181">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq181.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$(f-v,b-a)$$]]></EquationSource>
                </InlineEquation>, <InlineEquation ID="IEq182">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq182.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$(b-v,f-a)$$]]></EquationSource>
                </InlineEquation>, and <InlineEquation ID="IEq183">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq183.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$(b-v,b-a)$$]]></EquationSource>
                </InlineEquation> w.o.l.g., since the grouplet-based distance metric learning algorithm will be applied to each dictionary individually. Let <InlineEquation ID="IEq184">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq184.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$D_k^G(x_i,x_j)$$]]></EquationSource>
                </InlineEquation> denote the distance between data <InlineEquation ID="IEq185">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq185.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$x_i$$]]></EquationSource>
                </InlineEquation> and <InlineEquation ID="IEq186">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq186.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$x_j$$]]></EquationSource>
                </InlineEquation> computed based on the AVG <InlineEquation ID="IEq187">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq187.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$G_k$$]]></EquationSource>
                </InlineEquation>. The overall distance <InlineEquation ID="IEq188">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq188.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$D(x_i,x_j)$$]]></EquationSource>
                </InlineEquation> between data <InlineEquation ID="IEq189">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq189.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$x_i$$]]></EquationSource>
                </InlineEquation> and <InlineEquation ID="IEq190">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq190.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$x_j$$]]></EquationSource>
                </InlineEquation> is given by:<Equation ID="Equ7">
                  <EquationNumber>7</EquationNumber>
                  <MediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_Equ7.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </MediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\begin{aligned} D(x_i,x_j)=\sum \nolimits _{k=1}^K v_k D^G_k(x_i,x_j). \end{aligned}$$]]></EquationSource>
                </Equation>The SVM classifiers with RBF-like kernels (Eq. <InternalRef RefID="Equ8">8</InternalRef>) are found to provide state-of-the-art performances in several semantic concept classification tasks [<CitationRef CitationID="CR19">19</CitationRef>, <CitationRef CitationID="CR34">34</CitationRef>],<Equation ID="Equ8">
                  <EquationNumber>8</EquationNumber>
                  <MediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_Equ8.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </MediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\begin{aligned} K(x_i,x_j)=\exp \left\{ -\gamma D(x_i,x_j)\right\} . \end{aligned}$$]]></EquationSource>
                </Equation>For example, the chi-square RBF kernel usually performs well with histogram-like features [<CitationRef CitationID="CR18">18</CitationRef>, <CitationRef CitationID="CR19">19</CitationRef>], where distance <InlineEquation ID="IEq191">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq191.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$D(x_i,x_j)$$]]></EquationSource>
                </InlineEquation> in Eq. (<InternalRef RefID="Equ8">8</InternalRef>) is the chi-square distance.</Para>
              <Para>It is not trivial, however, to directly learn the optimal weights <InlineEquation ID="IEq192">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq192.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$v_k$$]]></EquationSource>
                </InlineEquation> (<InlineEquation ID="IEq193">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq193.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$k = 1,\ldots ,K$$]]></EquationSource>
                </InlineEquation>) in the SVM optimization setting, due to the exponential function in RBF-like kernels.</Para>
              <Para>In this work, we formulate an iterative QP problem to learn optimal weights <InlineEquation ID="IEq194">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq194.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$v_k$$]]></EquationSource>
                </InlineEquation> (<InlineEquation ID="IEq195">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq195.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$k = 1,\ldots ,K$$]]></EquationSource>
                </InlineEquation>). The basic idea is to incorporate the LMNN setting for distance metric learning [<CitationRef CitationID="CR40">40</CitationRef>]. The rationale is that the role of large margin in LMNN is inspired by its role in SVMs, and LMNN should inherit various strengths of SVMs [<CitationRef CitationID="CR33">33</CitationRef>]. Therefore, although we do not directly optimize <InlineEquation ID="IEq196">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq196.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$v_k$$]]></EquationSource>
                </InlineEquation> (<InlineEquation ID="IEq197">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq197.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$k = 1,\ldots ,K$$]]></EquationSource>
                </InlineEquation>) in the SVM optimization setting, the final optimal weights can still provide reasonably good performance for SVM concept classifiers.</Para>
              <Section2 ID="Sec23">
                <Heading>The LMNN formulation</Heading>
                <Para>Let <InlineEquation ID="IEq198">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq198.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$d^2_\mathbf{M }(x_i,x_j)$$]]></EquationSource>
                  </InlineEquation> denote the Mahalanobis distance metric between two data points <InlineEquation ID="IEq199">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq199.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$x_i$$]]></EquationSource>
                  </InlineEquation> and <InlineEquation ID="IEq200">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq200.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$x_j$$]]></EquationSource>
                  </InlineEquation>:<Equation ID="Equ9">
                    <EquationNumber>9</EquationNumber>
                    <MediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_Equ9.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\begin{aligned} d^2_\mathbf{M }(x_i,x_j)=(x_i-x_j)^T\mathbf M (x_i-x_j), \end{aligned}$$]]></EquationSource>
                  </Equation>where <InlineEquation ID="IEq201">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq201.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf M \ge 0$$]]></EquationSource>
                  </InlineEquation> is a positive semi-definite matrix. LMNN learns an optimal <InlineEquation ID="IEq202">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq202.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf M $$]]></EquationSource>
                  </InlineEquation> over a set of training data <InlineEquation ID="IEq203">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq203.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$(x_i,y_i)$$]]></EquationSource>
                  </InlineEquation>, <InlineEquation ID="IEq204">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq204.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$i = 1,\ldots ,N$$]]></EquationSource>
                  </InlineEquation>, where <InlineEquation ID="IEq205">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq205.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$y_i\in \{1,\ldots ,c\}$$]]></EquationSource>
                  </InlineEquation> and <InlineEquation ID="IEq206">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq206.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$c$$]]></EquationSource>
                  </InlineEquation> is the number of classes. For LMNN classification, the training process has two steps. First, <InlineEquation ID="IEq207">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq207.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$n_k$$]]></EquationSource>
                  </InlineEquation> similarly labeled target neighbors are identified for each input training datum <InlineEquation ID="IEq208">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq208.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$x_i$$]]></EquationSource>
                  </InlineEquation>. The target neighbors are selected by using prior knowledge or by simply computing <InlineEquation ID="IEq209">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq209.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$n_k$$]]></EquationSource>
                  </InlineEquation> nearest (similarly labeled) neighbors using the Euclidean distance. Let <InlineEquation ID="IEq210">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq210.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\eta _{ij} = 1$$]]></EquationSource>
                  </InlineEquation> (or <InlineEquation ID="IEq211">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq211.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$0$$]]></EquationSource>
                  </InlineEquation>) denote that <InlineEquation ID="IEq212">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq212.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$x_j$$]]></EquationSource>
                  </InlineEquation> is a target neighbor of <InlineEquation ID="IEq213">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq213.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$x_i$$]]></EquationSource>
                  </InlineEquation> (or not). In the second step, the Mahalanobis distance metric is adapted so that these target neighbors are closer to <InlineEquation ID="IEq214">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq214.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$x_i$$]]></EquationSource>
                  </InlineEquation> than all other differently labeled inputs. The Mahalanobis distance metric can be estimated by solving the following problem:<Equation ID="Equa4">
                    <MediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_Equa4.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\begin{aligned}&\min _\mathbf M \sum \nolimits _{ij}\eta _{ij}\left[d^2_\mathbf{M }(x_i,x_j)+C\sum \nolimits _l(1-y_{il})\epsilon _{ijl}\right] ,\\&\quad \text{ s.t.} d^2_\mathbf{M }(x_i,x_l) - d^2_\mathbf{M }(x_i,x_j) \ge 1 - \epsilon _{ijl}, \epsilon _{ijl} \ge 0, \mathbf M \ge 0. \end{aligned}$$]]></EquationSource>
                  </Equation>
                  <InlineEquation ID="IEq215">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq215.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$y_{il}\in \{0,1\}$$]]></EquationSource>
                  </InlineEquation> indicates whether inputs <InlineEquation ID="IEq216">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq216.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$x_i$$]]></EquationSource>
                  </InlineEquation> and <InlineEquation ID="IEq217">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq217.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$x_l$$]]></EquationSource>
                  </InlineEquation> have the same class label. <InlineEquation ID="IEq218">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq218.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\epsilon _{ijl}$$]]></EquationSource>
                  </InlineEquation> is the amount by which a differently labeled input <InlineEquation ID="IEq219">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq219.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$x_l$$]]></EquationSource>
                  </InlineEquation> invades the “perimeter” around <InlineEquation ID="IEq220">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq220.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$x_i$$]]></EquationSource>
                  </InlineEquation> defined by its target neighbor <InlineEquation ID="IEq221">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq221.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$x_j$$]]></EquationSource>
                  </InlineEquation>.</Para>
              </Section2>
              <Section2 ID="Sec24">
                <Heading>Our approach</Heading>
                <Para>By defining <InlineEquation ID="IEq222">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq222.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf{v}=[v_1,\ldots ,v_K]^T, \mathbf{D}(x_i,x_j)=[D^G_1 (x_i,x_j), \ldots ,D^G_K(x_i,x_j)]^T$$]]></EquationSource>
                  </InlineEquation>, we obtain the following problem:<Equation ID="Equa5">
                    <MediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_Equa5.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\begin{aligned}&\min _\mathbf{v } \left\{ \frac{||\mathbf v ||^2_2}{2} \!+\! C_0 \sum _{ij} \eta _{ij}\mathbf v ^T \mathbf D (x_i,x_j) \!+\! C \sum _{ijl} \eta _{ij}(1 \!-\! y_{il})\epsilon _{ijl} \right\} ,\\&\quad \text{ s.t.} \mathbf v ^T\mathbf D (x_i,x_l)-\mathbf v ^T\mathbf D (x_i,x_j)\ge 1 - \epsilon _{ijl}, \epsilon _{ijl}\ge 0, v_k\ge 0. \end{aligned}$$]]></EquationSource>
                  </Equation>
                  <InlineEquation ID="IEq223">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq223.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$||\mathbf v ||^2_2$$]]></EquationSource>
                  </InlineEquation> is the <InlineEquation ID="IEq224">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq224.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$L_2$$]]></EquationSource>
                  </InlineEquation> regularization that controls the complexity of <InlineEquation ID="IEq225">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq225.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf v $$]]></EquationSource>
                  </InlineEquation>. By introducing Lagrangian multipliers <InlineEquation ID="IEq226">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq226.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mu _{ijl} \ge 0, \gamma _{ijl} \ge 0, \text{ and} \sigma _{k} \ge 0$$]]></EquationSource>
                  </InlineEquation>, we have:<Equation ID="Equ10">
                    <EquationNumber>10</EquationNumber>
                    <MediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_Equ10.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\begin{aligned}&\min _\mathbf{v } \left\{ \frac{||\mathbf v ||^2_2}{2} + C_0 \sum \nolimits _{ij} \eta _{ij}\mathbf v ^T \mathbf D (x_i,x_j) \right.\nonumber \\&\quad -\sum \nolimits _{ijl}\mu _{ijl}\eta _{ij}\left[\mathbf v ^T\mathbf D (x_i,x_l)-\mathbf v ^T\mathbf D (x_i,x_j)-1+\epsilon _{ijl}\right]\nonumber \\&\quad \left.\!-\!\sum \nolimits _{ijl}\gamma _{ijl}\eta _{ij}\epsilon _{ijl}\!-\!\sum \nolimits _{k}\sigma _{k}v_k\!+\! C\sum \nolimits _{ijl} \eta _{ij}(1 \!-\! y_{il})\epsilon _{ijl}\right\} . \end{aligned}$$]]></EquationSource>
                  </Equation>Next, by taking derivative against <InlineEquation ID="IEq227">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq227.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\epsilon _{ijl}$$]]></EquationSource>
                  </InlineEquation> we obtain:<Equation ID="Equ11">
                    <EquationNumber>11</EquationNumber>
                    <MediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_Equ11.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\begin{aligned} C\eta _{ij}(1-y_{il})-\mu _{ijl}\eta _{ij} - \gamma _{ijl}\eta _{ij} = 0. \end{aligned}$$]]></EquationSource>
                  </Equation>That is, for any pair of <InlineEquation ID="IEq228">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq228.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$x_i$$]]></EquationSource>
                  </InlineEquation> and its target neighbor <InlineEquation ID="IEq229">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq229.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$x_j$$]]></EquationSource>
                  </InlineEquation>, since we only consider <InlineEquation ID="IEq230">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq230.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$x_l$$]]></EquationSource>
                  </InlineEquation> with <InlineEquation ID="IEq231">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq231.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$y_{il}=0$$]]></EquationSource>
                  </InlineEquation>, <InlineEquation ID="IEq232">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq232.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$0\le \mu _{ijl}\le C$$]]></EquationSource>
                  </InlineEquation>. Based on Eq. (<InternalRef RefID="Equ11">11</InternalRef>), Eq. (<InternalRef RefID="Equ10">10</InternalRef>) turns to:<Equation ID="Equ12">
                    <EquationNumber>12</EquationNumber>
                    <MediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_Equ12.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\begin{aligned}&\min _\mathbf{v }\left\{ \frac{1}{2}||\mathbf v ||^2_2 + C_0\sum \nolimits _{ij}\eta _{ij}\mathbf v ^T \mathbf D (x_i,x_j)\right.\nonumber \\&\quad \left.-\sum _{ijl} \mu _{ijl}\eta _{ij}\left[\mathbf v ^T\mathbf D (x_i,x_l) \!-\! \mathbf v ^T\mathbf D (x_i,x_j) - 1\right] \!-\! \mathbf v ^T\mathbf \sigma \right\} , \end{aligned}$$]]></EquationSource>
                  </Equation>where <InlineEquation ID="IEq233">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq233.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf \sigma = [\sigma _1,\ldots ,\sigma _K]^T$$]]></EquationSource>
                  </InlineEquation>. Then by taking derivative against <InlineEquation ID="IEq234">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq234.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf v $$]]></EquationSource>
                  </InlineEquation> we get:<Equation ID="Equ13">
                    <EquationNumber>13</EquationNumber>
                    <MediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_Equ13.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \mathbf v&= \sum \nolimits _{ijl}\mu _{ijl}\eta _{ij}\left[\mathbf D (x_i,x_l) - \mathbf D (x_i,x_j)\right]\nonumber \\&+\,\mathbf \sigma -C_0\sum \nolimits _{ij}\eta _{ij}\mathbf D (x_i,x_j). \end{aligned}$$]]></EquationSource>
                  </Equation>Define set <InlineEquation ID="IEq235">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq235.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal P $$]]></EquationSource>
                  </InlineEquation> as the set of indexes <InlineEquation ID="IEq236">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq236.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$i,j,l$$]]></EquationSource>
                  </InlineEquation> that satisfy the conditions of <InlineEquation ID="IEq237">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq237.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\eta _{ij} = 1$$]]></EquationSource>
                  </InlineEquation>, <InlineEquation ID="IEq238">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq238.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$y_{il} = 0$$]]></EquationSource>
                  </InlineEquation>, and that <InlineEquation ID="IEq239">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq239.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$x_l$$]]></EquationSource>
                  </InlineEquation> invades the “perimeter” around the input <InlineEquation ID="IEq240">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq240.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$x_i$$]]></EquationSource>
                  </InlineEquation> defined by its target neighbor <InlineEquation ID="IEq241">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq241.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$x_j$$]]></EquationSource>
                  </InlineEquation>, i.e., <InlineEquation ID="IEq242">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq242.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$0 \le D(x_i,x_l) - D(x_i,x_j) \le 1$$]]></EquationSource>
                  </InlineEquation>. Define set <InlineEquation ID="IEq243">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq243.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathcal Q $$]]></EquationSource>
                  </InlineEquation> as the set of indexes <InlineEquation ID="IEq244">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq244.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$i,j$$]]></EquationSource>
                  </InlineEquation> that satisfy <InlineEquation ID="IEq245">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq245.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\eta _{ij} = 1$$]]></EquationSource>
                  </InlineEquation>. Next, we can use <InlineEquation ID="IEq246">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq246.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mu _p$$]]></EquationSource>
                  </InlineEquation>, <InlineEquation ID="IEq247">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq247.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$p \in \mathcal P $$]]></EquationSource>
                  </InlineEquation> to replace the original notation <InlineEquation ID="IEq248">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq248.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mu _{ijl}$$]]></EquationSource>
                  </InlineEquation>, use <InlineEquation ID="IEq249">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq249.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf D _\mathcal{P }^p$$]]></EquationSource>
                  </InlineEquation>, <InlineEquation ID="IEq250">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq250.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$p \in \mathcal P $$]]></EquationSource>
                  </InlineEquation> to replace the corresponding <InlineEquation ID="IEq251">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq251.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf D (x_i,x_l)-\mathbf D (x_i,x_j)$$]]></EquationSource>
                  </InlineEquation>, and use <InlineEquation ID="IEq252">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq252.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf D _\mathcal{Q }^q$$]]></EquationSource>
                  </InlineEquation>, <InlineEquation ID="IEq253">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq253.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$q \in \mathcal Q $$]]></EquationSource>
                  </InlineEquation> to replace the corresponding <InlineEquation ID="IEq254">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq254.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf D (x_i,x_j)$$]]></EquationSource>
                  </InlineEquation>. Define <InlineEquation ID="IEq255">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq255.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf u = [\mu _{1},\ldots ,\mu _{|\mathcal P |}]^T$$]]></EquationSource>
                  </InlineEquation>, <InlineEquation ID="IEq256">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq256.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$|\mathcal P | \times K$$]]></EquationSource>
                  </InlineEquation> matrix <InlineEquation ID="IEq257">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq257.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf{D }_\mathcal{P } = \left(\mathbf D _\mathcal{P }^1,\ldots ,\mathbf D _\mathcal{P }^{|\mathcal P |}\right)^T$$]]></EquationSource>
                  </InlineEquation>, and <InlineEquation ID="IEq258">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq258.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$|\mathcal Q |\ \times \ K$$]]></EquationSource>
                  </InlineEquation> matrix <InlineEquation ID="IEq259">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq259.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf{D }_\mathcal{Q }\ =\ \left(\mathbf D _\mathcal{Q }^1,\ldots ,\mathbf D _\mathcal{Q }^{|\mathcal Q |}\right)^T$$]]></EquationSource>
                  </InlineEquation>. Through some derivation, we obtain the dual of Eq. (<InternalRef RefID="Equ12">12</InternalRef>) as follows:<Equation ID="Equ14">
                    <EquationNumber>14</EquationNumber>
                    <MediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_Equ14.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\begin{aligned}&\text{ max}_\mathbf{\sigma ,\mathbf u }\left\{ -\frac{1}{2}\mathbf u ^T\mathbf{D }_\mathcal{P }\mathbf{D }_\mathcal{P }^T\mathbf u +C_0\mathbf u ^T\mathbf{D }_\mathcal{P }\mathbf{D }_\mathcal{Q }^T\mathbf 1 _\mathcal{Q }+\mathbf u ^T\mathbf 1 _\mathcal{P }\right.\nonumber \\&\left.\quad -\frac{1}{2}\mathbf \sigma ^T\mathbf \sigma -\mathbf u ^T\mathbf{D }_\mathcal{P }\mathbf \sigma + C_0\mathbf \sigma ^T\mathbf{D }_\mathcal{Q }^T\mathbf 1 _\mathcal{Q }\right\} , \end{aligned}$$]]></EquationSource>
                  </Equation>where <InlineEquation ID="IEq260">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq260.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf 1 _\mathcal{Q }$$]]></EquationSource>
                  </InlineEquation> (<InlineEquation ID="IEq261">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq261.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf 1 _\mathcal{P }$$]]></EquationSource>
                  </InlineEquation>) is a <InlineEquation ID="IEq262">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq262.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$|\mathcal Q |$$]]></EquationSource>
                  </InlineEquation>-dim (<InlineEquation ID="IEq263">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq263.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$|\mathcal P |$$]]></EquationSource>
                  </InlineEquation>-dim) vector whose elements are all ones.</Para>
                <Para>When <InlineEquation ID="IEq264">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq264.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf \sigma $$]]></EquationSource>
                  </InlineEquation> is fixed, Eq. (<InternalRef RefID="Equ14">14</InternalRef>) can be further rewritten to the following QP problem:<Equation ID="Equ15">
                    <EquationNumber>15</EquationNumber>
                    <MediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_Equ15.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\begin{aligned}&\text{ max}_\mathbf{u }\left\{ \frac{1}{2}\mathbf u ^T\mathbf{D }_\mathcal{P }\mathbf{D }_\mathcal{P }^T\mathbf u \!+\! \mathbf u ^T\ \left(C_0\mathbf{D }_\mathcal{P }\mathbf{D }_\mathcal{Q }^T\mathbf 1 _\mathcal{Q }\!+\! \mathbf 1 _\mathcal{P }\!-\! \mathbf{D }_\mathcal{P }\mathbf \sigma \right) \right\} \!,\nonumber \\&\quad \text{ s.t.} \forall p\in \mathcal P , 0\le \mu _p\le C. \end{aligned}$$]]></EquationSource>
                  </Equation>On the other hand, when <InlineEquation ID="IEq265">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq265.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf u $$]]></EquationSource>
                  </InlineEquation> is fixed, Eq. (<InternalRef RefID="Equ14">14</InternalRef>) turns into the following QP problem:<Equation ID="Equ16">
                    <EquationNumber>16</EquationNumber>
                    <MediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_Equ16.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\begin{aligned}&\text{ max}_\mathbf{\sigma }\left\{ -\frac{1}{2}\mathbf \sigma ^T\mathbf \sigma +\mathbf \sigma ^T\left(C_0\mathbf{D }_\mathcal{Q }^T\mathbf 1 _\mathcal{Q }-\mathbf{D }_\mathcal{P }^T\mathbf u \right)\right\} ,\nonumber \\&\quad \text{ s.t.} \forall k\ =\ 1,\ldots ,K, \sigma _k\ge 0. \end{aligned}$$]]></EquationSource>
                  </Equation>Therefore, we can iteratively solve the QP problems of Eqs. (<InternalRef RefID="Equ15">15</InternalRef>) and (<InternalRef RefID="Equ16">16</InternalRef>) and obtain the desired weights <InlineEquation ID="IEq266">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq266.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\mathbf v $$]]></EquationSource>
                  </InlineEquation> through Eq. (<InternalRef RefID="Equ13">13</InternalRef>).</Para>
                <Para>For each of the QP problems, since we have positive definite Q (or positive semi-definite Q that can be made positive definite by using practical tricks), it can be solved efficiently in polynomial time.</Para>
              </Section2>
              <Section2 ID="Sec25">
                <Heading>Grouplet-based kernels</Heading>
                <Para>One of the most intuitive kernels that incorporates the AVG information is the grouplet-based chi-square RBF kernel. That is, each <InlineEquation ID="IEq267">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq267.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$D_k^G(x_i,x_j)$$]]></EquationSource>
                  </InlineEquation> is a chi-square distance:<Equation ID="Equ17">
                    <EquationNumber>17</EquationNumber>
                    <MediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_Equ17.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\begin{aligned} D_k^G(x_i,x_j)=\sum _{w_m\in G_k}\frac{\left[f_{w_m}(x_i)-f_{w_m}(x_j)\right]^2}{\frac{1}{2}\left[f_{w_m}(x_i)+f_{w_m}(x_j)\right]}, \end{aligned}$$]]></EquationSource>
                  </Equation>where <InlineEquation ID="IEq268">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq268.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$f_{w_m}(x_i)$$]]></EquationSource>
                  </InlineEquation> is the feature of <InlineEquation ID="IEq269">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq269.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$x_i$$]]></EquationSource>
                  </InlineEquation> corresponding to the codeword <InlineEquation ID="IEq270">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq270.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$w_m$$]]></EquationSource>
                  </InlineEquation> in AVG <InlineEquation ID="IEq271">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq271.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$G_k$$]]></EquationSource>
                  </InlineEquation>. When <InlineEquation ID="IEq272">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq272.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$v_k\ =\ 1$$]]></EquationSource>
                  </InlineEquation>, <InlineEquation ID="IEq273">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq273.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$k\ =\ 1,\ldots ,K$$]]></EquationSource>
                  </InlineEquation>, Eq. (<InternalRef RefID="Equ17">17</InternalRef>) will give the standard chi-square RBF kernel.</Para>
                <Para>From another perspective, we can treat each AVG as a <Emphasis Type="Italic">phrase</Emphasis>, which consists of the orderless codewords associated with that AVG. Analogous to measuring the similarity between two text segments, we should take into account the word specificity [<CitationRef CitationID="CR26">26</CitationRef>] in measuring the similarity between data points. One popular way of computing the word specificity is to use the inverse document frequency (idf). Therefore, we use the following metric to compute <InlineEquation ID="IEq274">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq274.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$D_k^G(x_i,x_j)$$]]></EquationSource>
                  </InlineEquation>:<Equation ID="Equ18">
                    <EquationNumber>18</EquationNumber>
                    <MediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_Equ18.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \frac{1}{{\sum \limits _{w_m\in G_k}\text{ idf}(w_m)}}\sum _{w_m\in G_k}\text{ idf}(w_m)\frac{\left[f_{w_m}(x_i)-f_{w_m}(x_j)\right]^2}{\frac{1}{2}\left[f_{w_m}(x_i)+f_{w_m}(x_j)\right]}. \end{aligned}$$]]></EquationSource>
                  </Equation>
                  <InlineEquation ID="IEq275">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq275.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$\text{ idf}(w_m)$$]]></EquationSource>
                  </InlineEquation> is computed as the total number of occurrences of all codewords in the training corpus divided by the total number of occurrences of <InlineEquation ID="IEq276">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq276.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$w_m$$]]></EquationSource>
                  </InlineEquation> in the training corpus. Using either the chi-square distance Eq. (<InternalRef RefID="Equ17">17</InternalRef>) or the idf-weighted chi-square distance Eq. (<InternalRef RefID="Equ18">18</InternalRef>), respectively, the distance metric learning method developed in the previous Sect. <InternalRef RefID="Sec24">7.2</InternalRef> can be applied to find the optimal metric and compute the optimal kernels for concept classification.</Para>
                <Para>Finally, as described in Sect. <InternalRef RefID="Sec16">6</InternalRef>, we have four types of audio-visual dictionaries by studying four types of audio-visual temporal correlations. The distance metric learning algorithm described in Sect. <InternalRef RefID="Sec24">7.2</InternalRef> can be applied to each type of dictionary individually, and four types of optimal kernels can be computed. After that, the Multiple Kernel Learning technique [<CitationRef CitationID="CR36">36</CitationRef>] is adopted to combine the four types of kernels for final concept detection.</Para>
              </Section2>
            </Section1>
            <Section1 ID="Sec26">
              <Heading>Experiments</Heading>
              <Para>We evaluate our algorithm over the large-scale CCV set [<CitationRef CitationID="CR19">19</CitationRef>], containing 9317 consumer videos from YouTube. The videos are captured by ordinary users under unrestricted challenging conditions, without post-editing. The original audio soundtracks are preserved, in contrast to other large-scale news or movie video sets [<CitationRef CitationID="CR22">22</CitationRef>, <CitationRef CitationID="CR34">34</CitationRef>]. This allows us to study legitimate audio-visual interactions. Each video is manually labeled to 20 semantic concepts by using Amazon Mechanical Turk. More details about the data set and category definitions can be found in [<CitationRef CitationID="CR19">19</CitationRef>]. Our experiments take similar settings as [<CitationRef CitationID="CR19">19</CitationRef>], i.e., we use the same training (4659 videos) and test (4658 videos) sets, and one-versus-all SVM classifiers. The performance is measured by Average Precision (AP, the area under uninterpolated PR curve) and Mean AP (MAP, averaged AP across concepts).</Para>
              <Para>To demonstrate the effectiveness of our method, we first evaluate the performance of the state-of-the-art BoW representations using different types of individual audio and visual features exploited in this paper, as well as the performance of their various early-fusion combinations. The AP and MAP results are shown in Fig. <InternalRef RefID="Fig6">6</InternalRef>. These BoW representations are generated using the same method as [<CitationRef CitationID="CR19">19</CitationRef>]. The results show that the individual visual SIFT, audio MFCCs, and audio transient event feature perform comparably overall, each having different advantages over different concepts. The combinations of audio and visual BoW representations through multi-modal fusion can consistently and significantly improve classification. For example, by combining the three individual features (“SIFT+MFCCs+Trans”), compared with individual features, all concepts get AP improvements, and the MAP is improved by over 33 % on a relative basis. Readers may notice that our “SIFT” performs differently than that in [<CitationRef CitationID="CR19">19</CitationRef>]. This is because we have only a single type of SIFT feature (i.e., SIFT over DoG keypoints) and generate the BoW representation using only the <InlineEquation ID="IEq277">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq277.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$1\ \times \ 1$$]]></EquationSource>
                </InlineEquation> spatial layout, while several types of keypoints and spatial layouts are used in [<CitationRef CitationID="CR19">19</CitationRef>]. Actually, our “MFCCs” performs similarly to that in [<CitationRef CitationID="CR19">19</CitationRef>], due to the similar settings for feature extraction and vocabulary construction.<Figure Category="Standard" Float="Yes" ID="Fig6">
                  <Caption Language="En">
                    <CaptionNumber>Fig. 6</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>Comparison of various BoW representations as well as their early-fusion combinations</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <MediaObject ID="MO27">
                    <ImageObject Color="Color" FileRef="MediaObjects/13735_2012_20_Fig6_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                  </MediaObject>
                </Figure>
              </Para>
              <Para>Next, we show the classification performance of using different types of individual audio-visual dictionaries, (i.e., <InlineEquation ID="IEq278">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq278.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\mathcal D ^{f-v,f-a}$$]]></EquationSource>
                </InlineEquation>, <InlineEquation ID="IEq279">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq279.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\mathcal D ^{f-v,b-a}$$]]></EquationSource>
                </InlineEquation>, <InlineEquation ID="IEq280">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq280.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\mathcal D ^{b-v,f-a}$$]]></EquationSource>
                </InlineEquation>, and <InlineEquation ID="IEq281">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq281.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\mathcal D ^{b-v,b-a}$$]]></EquationSource>
                </InlineEquation>). Each audio-visual dictionary contains about 200 <InlineEquation ID="IEq282">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq282.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\sim $$]]></EquationSource>
                </InlineEquation> 300 AVGs on average. The results are shown in Fig. <InternalRef RefID="Fig7">7</InternalRef>a–d. The goal is to demonstrate the usefulness of the proposed AVG-based distance metric learning algorithm. Here we compare 6 different approaches: the standard chi-square RBF kernel (“<InlineEquation ID="IEq283">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq283.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\chi ^2$$]]></EquationSource>
                </InlineEquation>-RBF”); the “w-direct” method that uses distance metric learning to directly combine <InlineEquation ID="IEq284">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq284.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\chi ^2$$]]></EquationSource>
                </InlineEquation> distances computed over individual audio and visual vocabularies; the chi-square RBF kernel that uses the idf information (“idf-<InlineEquation ID="IEq285">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq285.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\chi ^2$$]]></EquationSource>
                </InlineEquation>-RBF”); the “w-idf-direct” method that uses distance metric learning to directly combine idf-weighted <InlineEquation ID="IEq286">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq286.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\chi ^2$$]]></EquationSource>
                </InlineEquation> distances computed over individual audio and visual vocabularies; the weighted chi-square RBF kernel with distance metric learning that uses the AVGs (“w-<InlineEquation ID="IEq287">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq287.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\chi ^2$$]]></EquationSource>
                </InlineEquation>-RBF”); and the weighted chi-square RBF kernel with distance metric learning that uses both the idf information and the AVGs (“w-idf-<InlineEquation ID="IEq288">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq288.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\chi ^2$$]]></EquationSource>
                </InlineEquation>-RBF”). In other words, “w-direct”, “w-idf-direct”, “<InlineEquation ID="IEq289">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq289.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\chi ^2$$]]></EquationSource>
                </InlineEquation>-RBF” and “idf-<InlineEquation ID="IEq290">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq290.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\chi ^2$$]]></EquationSource>
                </InlineEquation>-RBF” do not use any AVG information. From the figures we can see that by finding appropriate weights of AVGs through our distance metric learning, we can consistently improve the detection performance. For example, for all four types of AVGs, “w-<InlineEquation ID="IEq291">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq291.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\chi ^2$$]]></EquationSource>
                </InlineEquation>-RBF” works better than “<InlineEquation ID="IEq292">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq292.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\chi ^2$$]]></EquationSource>
                </InlineEquation>-RBF” on average, and “w-idf-<InlineEquation ID="IEq293">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq293.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\chi ^2$$]]></EquationSource>
                </InlineEquation>-RBF” outperforms “idf-<InlineEquation ID="IEq294">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq294.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\chi ^2$$]]></EquationSource>
                </InlineEquation>-RBF.” Also, the advantages of “w-idf-<InlineEquation ID="IEq295">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq295.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\chi ^2$$]]></EquationSource>
                </InlineEquation>-RBF” are quite apparent, i.e., it performs the most efficiently over almost every concept across all types of AVGs. In comparison, without generating the AVGs, by directly applying distance metric learning to combine individual audio and visual vocabularies, “w-direct” and “w-idf-direct” cannot bring any overall improvements. One possible reason is due to the large amount of parameters to learn for distance metric learning in such cases, e.g., 4000 for each type of vocabulary, one corresponding to each feature dimension. This problem is effectively alleviated by incorporating the AVG representation, where the amount of parameters to learn is largely reduced.<Figure Category="Standard" Float="Yes" ID="Fig7">
                  <Caption Language="En">
                    <CaptionNumber>Fig. 7</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>Performances comparison of different approaches using individual types of audio-visual dictionaries</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <MediaObject ID="MO30">
                    <ImageObject Color="Color" FileRef="MediaObjects/13735_2012_20_Fig7_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                  </MediaObject>
                </Figure>
              </Para>
              <Para>Then, we compare the classification performance of using individual foreground and background audio and visual vocabularies (i.e., <InlineEquation ID="IEq296">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq296.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{f-v}$$]]></EquationSource>
                </InlineEquation>, <InlineEquation ID="IEq297">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq297.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{f-a}$$]]></EquationSource>
                </InlineEquation>, <InlineEquation ID="IEq298">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq298.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{b-v}$$]]></EquationSource>
                </InlineEquation>, and <InlineEquation ID="IEq299">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq299.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{b-a}$$]]></EquationSource>
                </InlineEquation>) via the BoW representation, as well as using various types of individual audio-visual dictionaries via “w-idf-<InlineEquation ID="IEq300">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq300.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\chi ^2$$]]></EquationSource>
                </InlineEquation>-RBF” kernel. The results are given in Fig. <InternalRef RefID="Fig8">8</InternalRef>. From the figure, we can see that for individual vocabularies, visual foreground performs better than visual background in general, while audio background performs better than audio foreground. Such results are within our expectation, because of the importance of the visual foreground in classifying objects and activities, as well as the effectiveness of audio background environmental sounds in classifying general concepts as shown by previous work [<CitationRef CitationID="CR5">5</CitationRef>, <CitationRef CitationID="CR19">19</CitationRef>]. Compared with the visual foreground, visual background wins over “wedding ceremony” and “non-music performance,” because of the importance of the background settings for these concepts, e.g., the flower boutique and seated crowd for “wedding ceremony,” and the stadium or stage setting for “non-music performance.” In the audio aspect, audio foreground outperforms audio background over three concepts, “dog,” “birthday,” and “music performance,” because of the usefulness of capturing consistent foreground sounds in these concepts. Through exploring temporal audio-visual interactions, audio-visual dictionaries generally outperform the corresponding individual audio or visual vocabularies. For example, the MAP of <InlineEquation ID="IEq301">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq301.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\mathcal D ^{f-v,f-a}$$]]></EquationSource>
                </InlineEquation> outperforms those of <InlineEquation ID="IEq302">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq302.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{f-v}$$]]></EquationSource>
                </InlineEquation> and <InlineEquation ID="IEq303">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq303.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{f-a}$$]]></EquationSource>
                </InlineEquation>, on a relative basis, by roughly 40 and 50 %, respectively, and the MAP of <InlineEquation ID="IEq304">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq304.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\mathcal D ^{b-v,b-a}$$]]></EquationSource>
                </InlineEquation> outperforms those of <InlineEquation ID="IEq305">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq305.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{b-v}$$]]></EquationSource>
                </InlineEquation> and <InlineEquation ID="IEq306">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq306.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\mathcal V ^{b-a}$$]]></EquationSource>
                </InlineEquation> by roughly 50 and 40 %, respectively.<Figure Category="Standard" Float="Yes" ID="Fig8">
                  <Caption Language="En">
                    <CaptionNumber>Fig. 8</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>Comparison of individual foreground/background audio/visual vocabularies and audio-visual dictionaries</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <MediaObject ID="MO31">
                    <ImageObject Color="Color" FileRef="MediaObjects/13735_2012_20_Fig8_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                  </MediaObject>
                </Figure>
              </Para>
              <Para>Finally, the four types of audio-visual dictionaries are combined together to train concept classifiers so that the advantages of all dictionaries in classifying different concepts can be exploited. Figure <InternalRef RefID="Fig9">9</InternalRef> shows the final performance, where multiple kernel learning is applied to find the optimal weights to combine kernels computed over individual audio-visual dictionaries. Here we compare our “MKL-w-idf-<InlineEquation ID="IEq307">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq307.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\chi ^2$$]]></EquationSource>
                </InlineEquation>-RBF” approach with three other alternatives: the early fusion of the BoW representations from multiple types of features (“SIFT+MFCCs+Trans”), which is considered the state-of-the-art in the literature; the “MKL-Vocabulary” method where multiple kernel learning is used to combine standard <InlineEquation ID="IEq308">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq308.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\chi ^2$$]]></EquationSource>
                </InlineEquation>-RBF kernels computed over the four types of individual audio and visual foreground and background vocabularies; and the “MKL-Dictionary” method where multiple kernel learning is used to combine <InlineEquation ID="IEq309">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq309.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\chi ^2$$]]></EquationSource>
                </InlineEquation>-RBF kernels computed over individual audio-visual dictionaries based on the AVG-based features generated by aggregating BoW bins, as described in Fig. <InternalRef RefID="Fig5">5</InternalRef>. From the figure we can see that our “MKL-w-idf-<InlineEquation ID="IEq310">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq310.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\chi ^2$$]]></EquationSource>
                </InlineEquation>-RBF” can consistently and significantly outperform other alternatives over all concepts. Compared with “SIFT+MFCCs+Trans,” “w-idf-<InlineEquation ID="IEq311">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq311.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\chi ^2$$]]></EquationSource>
                </InlineEquation>-RBF” improves the overall MAP by more than 20 %, and significant AP gains (more than 20 %) are obtained over 12 concepts, e.g., roughly 40 % gain over “basketball,” 60 % gain over “biking,” 40 % gain over “wedding reception,” 40 % gain over “wedding ceremony,” and 40 % gain over “non-music performance.” Compared with the “MKL-Dictionary” that uses AVGs in the naive way and the “MKL-Vocabulary” that does not use the AVG information, we improve the AP of every concept by more than 5 %, and over 15 concepts, the improves are more than 10 %. The results demonstrate the effectiveness of extracting useful AVGs to represent general videos and using AVG-based distance metric learning for concept classification.<Figure Category="Standard" Float="Yes" ID="Fig9">
                  <Caption Language="En">
                    <CaptionNumber>Fig. 9</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>Combining different types of audio-visual dictionaries</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <MediaObject ID="MO32">
                    <ImageObject Color="Color" FileRef="MediaObjects/13735_2012_20_Fig9_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                  </MediaObject>
                </Figure>
              </Para>
              <Para>The training process of generating AVGs is relatively expensive in computation, where the most time consuming part lies in the processes of conducting SIFT tracking and computing causal matrices. However, once the AVGs are obtained, the classification process of using such AVGs can be reasonably fast. Specifically, the complexity of generating BoW vectors as well as concept classification are similar to standard acts in the field, and we can reduce the sample frequency in conducting SIFT tracking to alleviate the computational overhead. In addition, the number of AVGs (hundreds) is usually much smaller than the original number of codewords (thousands) in the audio and visual vocabularies, and the final SVM classification is faster than traditional BoW approaches. On average, the classification of test videos can be real-time, i.e., it takes about 1 min to classify 20 concepts over a 1-min long video, using a dual-core machine with 8G ram.</Para>
            </Section1>
            <Section1 ID="Sec27">
              <Heading>Conclusion</Heading>
              <Para>An AVG representation is proposed by studying the statistical temporal causality between audio and visual codewords. Each AVG encapsulates inter-related audio and visual codewords as a whole package, which carries unique audio-visual patterns to represent the video content. We conduct coarse-level foreground/background separation in both visual and audio channels, and extract four types of AVGs based on four types of temporal audio-visual correlations, correlations between visual foreground and audio foreground codewords, between visual foreground and audio background codewords, between visual background and audio foreground codewords, and between visual background and audio background codewords. To use the AVGs for effective concept classification, a distance metric learning algorithm was further developed. Based on the LMNN setting, the algorithm optimizes an iterative QP problem to find the appropriate weights of combining individual grouplet-based distances for optimal classification. Experiments over large-scale consumer videos demonstrate that all four types of AVGs provide discriminative audio-visual cues to classify various concepts, and significant performance improvements can be obtained compared with state-of-the-art multi-modal fusion methods using BoW representations.</Para>
              <Para>It is worth mentioning that our method has some limitations. For videos that we cannot get meaningful SIFT tracks or extract meaningful audio transient events, our method will not work well. Also, the <InlineEquation ID="IEq312">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq312.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$L_2$$]]></EquationSource>
                </InlineEquation> regularization of weights <InlineEquation ID="IEq313">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq313.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$\mathbf v $$]]></EquationSource>
                </InlineEquation> is used in our distance metric learning algorithm to prevent sparse solutions, due to the relatively small number of AVGs in our experiments. For tasks with a large number of AVGs, <InlineEquation ID="IEq314">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="13735_2012_20_Article_IEq314.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="TEX"><![CDATA[$$L_1$$]]></EquationSource>
                </InlineEquation>-norm that encourages sparsity may be a better choice. In addition, the spatial relations of visual SIFT tracks can be incorporated to further help classification. The spatial-temporal audio-visual correlations can be explored in the future, e.g., by constructing spatially-correlated visual signatures first and then correlating such visual signatures with audio codewords.</Para>
            </Section1>
          </Body>
          <BodyRef FileRef="BodyRef/PDF/13735_2012_Article_20.pdf" TargetType="OnlinePDF"/>
          <BodyRef FileRef="BodyRef/PDF/13735_2012_20_TEX.zip" TargetType="TEX"/>
          <ArticleBackmatter>
            <Acknowledgments>
              <Heading>Acknowledgments</Heading>
              <SimplePara>We would like to thank the authors of [<CitationRef CitationID="CR6">6</CitationRef>] and [<CitationRef CitationID="CR27">27</CitationRef>] for sharing their code with us, and for Shih-Fu Chang for many useful discussions.</SimplePara>
            </Acknowledgments>
            <Bibliography ID="Bib1">
              <Heading>References</Heading>
              <Citation ID="CR1">
                <CitationNumber>1.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>T</Initials>
                    <FamilyName>Aach</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Kaup</FamilyName>
                  </BibAuthorName>
                  <Year>1995</Year>
                  <ArticleTitle Language="En">Bayesian algorithms for adaptive change detection in image sequences using Markov random fields</ArticleTitle>
                  <JournalTitle>Signal Process: Image Commun</JournalTitle>
                  <VolumeID>7</VolumeID>
                  <FirstPage>147</FirstPage>
                  <LastPage>160</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1016/0923-5965(95)00003-F</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Aach T, Kaup A (1995) Bayesian algorithms for adaptive change detection in image sequences using Markov random fields. Signal Process: Image Commun 7:147–160</BibUnstructured>
              </Citation>
              <Citation ID="CR2">
                <CitationNumber>2.</CitationNumber>
                <BibUnstructured>Akutsu A, Tonomura Y (1994) Video tomography: an efficient method for camerawork extraction and motion analysis. In: ACM multimedia, pp 349–356. doi: <ExternalRef><RefSource>10.1145/192593.192697</RefSource><RefTarget TargetType="DOI" Address="10.1145/192593.192697"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR3">
                <CitationNumber>3.</CitationNumber>
                <BibUnstructured>Barzelay Z, Schechner Y (2007) Harmony in motion. In: IEEE CVPR, pp 1–8. doi: <ExternalRef><RefSource>10.1109/CVPR.2007.383344</RefSource><RefTarget TargetType="DOI" Address="10.1109/CVPR.2007.383344"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR4">
                <CitationNumber>4.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>MJ</Initials>
                    <FamilyName>Beal</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>N</Initials>
                    <FamilyName>Jojic</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>H</Initials>
                    <FamilyName>Attias</FamilyName>
                  </BibAuthorName>
                  <Year>2003</Year>
                  <ArticleTitle Language="En">A graphical model for audiovisual object tracking</ArticleTitle>
                  <JournalTitle>IEEE PAMI</JournalTitle>
                  <VolumeID>25</VolumeID>
                  <IssueID>7</IssueID>
                  <FirstPage>828</FirstPage>
                  <LastPage>836</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TPAMI.2003.1206512</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Beal MJ, Jojic N, Attias H (2003) A graphical model for audiovisual object tracking. IEEE PAMI 25(7):828–836</BibUnstructured>
              </Citation>
              <Citation ID="CR5">
                <CitationNumber>5.</CitationNumber>
                <BibUnstructured>Chang S et al (2007) Large-scale multimodal semantic concept detection for consumer video. In: ACM MIR, pp 255–264. doi: <ExternalRef><RefSource>10.1145/1290082.1290118</RefSource><RefTarget TargetType="DOI" Address="10.1145/1290082.1290118"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR6">
                <CitationNumber>6.</CitationNumber>
                <BibUnstructured>Cotton C, Ellis D, Loui A (2011) Soundtrack classification by transient events. In: IEEE ICASSP, Czech Republic</BibUnstructured>
              </Citation>
              <Citation ID="CR7">
                <CitationNumber>7.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Cristani</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>B</Initials>
                    <FamilyName>Manuele</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Vittorio</FamilyName>
                  </BibAuthorName>
                  <Year>2007</Year>
                  <ArticleTitle Language="En">Audio-visual event recognition in surveillance video sequences</ArticleTitle>
                  <JournalTitle>IEEE Trans Multimedia</JournalTitle>
                  <VolumeID>9</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>257</FirstPage>
                  <LastPage>267</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TMM.2006.886263</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Cristani M, Manuele B, Vittorio M (2007) Audio-visual event recognition in surveillance video sequences. IEEE Trans Multimedia 9(2):257–267</BibUnstructured>
              </Citation>
              <Citation ID="CR8">
                <CitationNumber>8.</CitationNumber>
                <BibUnstructured>Dalal N, Triggs B (2005) Histograms of oriented gradients for human detection. In: IEEE CVPR, pp 886–893</BibUnstructured>
              </Citation>
              <Citation ID="CR9">
                <CitationNumber>9.</CitationNumber>
                <BibUnstructured>Davis J et al (2007) Information-theoretic metric learning. In: ICML, pp 209–216. doi: <ExternalRef><RefSource>10.1145/1273496.1273523</RefSource><RefTarget TargetType="DOI" Address="10.1145/1273496.1273523"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR10">
                <CitationNumber>10.</CitationNumber>
                <BibUnstructured>Ding M, Chen Y, Bressler SL (2006) Granger causality: basic theory and applications to neuroscience. In: Schelter S et al (eds) Handbook of time series analysis. Wiley, Wienheim</BibUnstructured>
              </Citation>
              <Citation ID="CR11">
                <CitationNumber>11.</CitationNumber>
                <BibUnstructured>Divvala S et al (2009) An empirical study of context in object detection. In: IEEE CVPR, Miami</BibUnstructured>
              </Citation>
              <Citation ID="CR12">
                <CitationNumber>12.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>SY</Initials>
                    <FamilyName>Elhabian</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>KM</Initials>
                    <FamilyName>El-Sayed</FamilyName>
                  </BibAuthorName>
                  <Year>2008</Year>
                  <ArticleTitle Language="En">Moving object detection in spatial domain using background removal techniques: state-of-art</ArticleTitle>
                  <JournalTitle>Recent Patents Comput Sci</JournalTitle>
                  <VolumeID>1</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>32</FirstPage>
                  <LastPage>54</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.2174/1874479610801010032</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Elhabian SY, El-Sayed KM (2008) Moving object detection in spatial domain using background removal techniques: state-of-art. Recent Patents Comput Sci 1(1):32–54</BibUnstructured>
              </Citation>
              <Citation ID="CR13">
                <CitationNumber>13.</CitationNumber>
                <BibUnstructured>Enqvist O, Josephson K, Kahl F (2009) Optimal correspondences from pairwise constraints. In: IEEE ICCV, Kyoto. doi: <ExternalRef><RefSource>10.1109/ICCV.2009.5459319</RefSource><RefTarget TargetType="DOI" Address="10.1109/ICCV.2009.5459319"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR14">
                <CitationNumber>14.</CitationNumber>
                <BibUnstructured>Globerson A, Roweis S (2006) Metric learning by collapsing classes. In: NIPS, pp 451–458</BibUnstructured>
              </Citation>
              <Citation ID="CR15">
                <CitationNumber>15.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>C</Initials>
                    <FamilyName>Granger</FamilyName>
                  </BibAuthorName>
                  <Year>1969</Year>
                  <ArticleTitle Language="En">Investigating causal relations by econometric models and cross-spectral methods</ArticleTitle>
                  <JournalTitle>Econometrica</JournalTitle>
                  <VolumeID>37</VolumeID>
                  <IssueID>3</IssueID>
                  <FirstPage>424</FirstPage>
                  <LastPage>438</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>533087</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.2307/1912791</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Granger C (1969) Investigating causal relations by econometric models and cross-spectral methods. Econometrica 37(3):424–438</BibUnstructured>
              </Citation>
              <Citation ID="CR16">
                <CitationNumber>16.</CitationNumber>
                <BibUnstructured>Iwano K et al (2007) Audio-visual speech recognition using lip information extracted from side-face images. EURASIP J ASMP 2007(1):4–12. doi: <ExternalRef><RefSource>10.1155/2007/64506</RefSource><RefTarget TargetType="DOI" Address="10.1155/2007/64506"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR17">
                <CitationNumber>17.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>W</Initials>
                    <FamilyName>Jiang</FamilyName>
                  </BibAuthorName>
                  <Etal/>
                  <Year>2010</Year>
                  <ArticleTitle Language="En">Audio-visual atoms for generic video concept classification</ArticleTitle>
                  <JournalTitle>ACM TOMCCAP</JournalTitle>
                  <VolumeID>6</VolumeID>
                  <FirstPage>1</FirstPage>
                  <LastPage>19</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1145/1823746.1823748</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Jiang W et al (2010) Audio-visual atoms for generic video concept classification. ACM TOMCCAP 6:1–19</BibUnstructured>
              </Citation>
              <Citation ID="CR18">
                <CitationNumber>18.</CitationNumber>
                <BibUnstructured>Jiang Y, Ngo CW, Yang J (2007) Towards optimal bag-of-features for object categorization and semantic video retrieval. In: ACM CIVR, pp 494–501. doi: <ExternalRef><RefSource>10.1145/1282280.1282352</RefSource><RefTarget TargetType="DOI" Address="10.1145/1282280.1282352"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR19">
                <CitationNumber>19.</CitationNumber>
                <BibUnstructured>Jiang Y et al (2011) Consumer video understanding: a benchmark database and an evaluation of human and machine performance. ACM ICMR, Trento. doi: <ExternalRef><RefSource>10.1145/1991996.1992025</RefSource><RefTarget TargetType="DOI" Address="10.1145/1991996.1992025"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR20">
                <CitationNumber>20.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>P</Initials>
                    <FamilyName>Joly</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>HK</Initials>
                    <FamilyName>Kim</FamilyName>
                  </BibAuthorName>
                  <Year>1996</Year>
                  <ArticleTitle Language="En">Efficient automatic analysis of camera work and microsegmentation of video using spatiotemporal images</ArticleTitle>
                  <JournalTitle> Signal Process: Image Commun</JournalTitle>
                  <VolumeID>8</VolumeID>
                  <IssueID>4</IssueID>
                  <FirstPage>295</FirstPage>
                  <LastPage>307 </LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1016/0923-5965(95)00054-2</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Joly P, Kim HK (1996) Efficient automatic analysis of camera work and microsegmentation of video using spatiotemporal images. Signal Process: Image Commun 8(4):295–307 </BibUnstructured>
              </Citation>
              <Citation ID="CR21">
                <CitationNumber>21.</CitationNumber>
                <BibUnstructured>Ke Y, Sukthankar R, Hebert M (2007) Event detection in crowded videos. IEEE ICCV, Brazil</BibUnstructured>
              </Citation>
              <Citation ID="CR22">
                <CitationNumber>22.</CitationNumber>
                <BibBook>
                  <BibAuthorName>
                    <Initials>I</Initials>
                    <FamilyName>Laptev</FamilyName>
                  </BibAuthorName>
                  <Etal/>
                  <Year>2008</Year>
                  <BookTitle>Learning realistic human actions from movies</BookTitle>
                  <PublisherName>IEEE CVPR</PublisherName>
                  <PublisherLocation>Alaska</PublisherLocation>
<Occurrence Type="DOI">
<Handle>10.1109/CVPR.2008.4587756</Handle>
</Occurrence>
                </BibBook>
                <BibUnstructured>Laptev I et al (2008) Learning realistic human actions from movies. IEEE CVPR, Alaska</BibUnstructured>
              </Citation>
              <Citation ID="CR23">
                <CitationNumber>23.</CitationNumber>
                <BibUnstructured>Lin WH, Hauptmann A (2002) News video classification using svm-based multimodal classifiers and combination strategies. In: Proc ACM multimedia, pp 323–326. doi: <ExternalRef><RefSource>10.1145/641007.641075</RefSource><RefTarget TargetType="DOI" Address="10.1145/641007.641075"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR24">
                <CitationNumber>24.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>DG</Initials>
                    <FamilyName>Lowe</FamilyName>
                  </BibAuthorName>
                  <Year>2004</Year>
                  <ArticleTitle Language="En">Distinctive image features from scale-invariant keypoints</ArticleTitle>
                  <JournalTitle>IJCV</JournalTitle>
                  <VolumeID>60</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>91</FirstPage>
                  <LastPage>110</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1023/B:VISI.0000029664.99615.94</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Lowe DG (2004) Distinctive image features from scale-invariant keypoints. IJCV 60(2):91–110</BibUnstructured>
              </Citation>
              <Citation ID="CR25">
                <CitationNumber>25.</CitationNumber>
                <BibUnstructured>Marszalek M, Laptev I, Schmid C (2009) Actions in context. In: IEEE CVPR, Miami</BibUnstructured>
              </Citation>
              <Citation ID="CR26">
                <CitationNumber>26.</CitationNumber>
                <BibUnstructured>Mihalcea R, Corley C, Strapparava C (2006) Corpus-based and knowledge-based measures of text semantic similarity. In: National conference on artificial intelligence, pp 775–780</BibUnstructured>
              </Citation>
              <Citation ID="CR27">
                <CitationNumber>27.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Nedungadi</FamilyName>
                  </BibAuthorName>
                  <Etal/>
                  <Year>2009</Year>
                  <ArticleTitle Language="En">Analyzing multiple spike trains with nonparametric granger causality</ArticleTitle>
                  <JournalTitle>J Comput Neurosci</JournalTitle>
                  <VolumeID>27</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>55</FirstPage>
                  <LastPage>64</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>2529985</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1007/s10827-008-0126-2</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Nedungadi A et al (2009) Analyzing multiple spike trains with nonparametric granger causality. J Comput Neurosci 27(1):55–64</BibUnstructured>
              </Citation>
              <Citation ID="CR28">
                <CitationNumber>28.</CitationNumber>
                <BibUnstructured>Ng A, Jordan M, Weiss Y (2001) On spectral clustering: analysis and an algorithm. NIPS</BibUnstructured>
              </Citation>
              <Citation ID="CR29">
                <CitationNumber>29.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>DT</Initials>
                    <FamilyName>Pham</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>JF</Initials>
                    <FamilyName>Cardoso</FamilyName>
                  </BibAuthorName>
                  <Year>2001</Year>
                  <ArticleTitle Language="En">Blind separation of instantaneous mixtures of non stationary sources</ArticleTitle>
                  <JournalTitle>IEEE Trans Signal Process</JournalTitle>
                  <VolumeID>49</VolumeID>
                  <IssueID>9</IssueID>
                  <FirstPage>1837</FirstPage>
                  <LastPage>1848</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>1852133</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/78.942614</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Pham DT, Cardoso JF (2001) Blind separation of instantaneous mixtures of non stationary sources. IEEE Trans Signal Process 49(9):1837–1848</BibUnstructured>
              </Citation>
              <Citation ID="CR30">
                <CitationNumber>30.</CitationNumber>
                <BibUnstructured>Prabhakar K et al (2010) Temporal causality for the analysis of visual events. In: IEEE CVPR, San Francisco</BibUnstructured>
              </Citation>
              <Citation ID="CR31">
                <CitationNumber>31.</CitationNumber>
                <BibUnstructured>Roweis ST (2001) One microphone source separation. NIPS</BibUnstructured>
              </Citation>
              <Citation ID="CR32">
                <CitationNumber>32.</CitationNumber>
                <BibUnstructured>Sargin M et al (2009) Audiovisual celebrity recognition in unconstrained web videos. In: IEEE ICASSP, Taipei</BibUnstructured>
              </Citation>
              <Citation ID="CR33">
                <CitationNumber>33.</CitationNumber>
                <BibUnstructured>Schölkopf B, Smola AJ (2002) Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT, Cambridge</BibUnstructured>
              </Citation>
              <Citation ID="CR34">
                <CitationNumber>34.</CitationNumber>
                <BibUnstructured>Smeaton AF, Over P, Kraaij W (2006) Evaluation campaigns and TRECVid. In: ACM MIR, pp 321–330. doi: <ExternalRef><RefSource>10.1145/1178677.1178722</RefSource><RefTarget TargetType="DOI" Address="10.1145/1178677.1178722"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR35">
                <CitationNumber>35.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>C</Initials>
                    <FamilyName>Stauffer</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>E</Initials>
                    <FamilyName>Grimson</FamilyName>
                  </BibAuthorName>
                  <Year>2000</Year>
                  <ArticleTitle Language="En">Learning patterns of activity using realtime tracking</ArticleTitle>
                  <JournalTitle>IEEE PAMI</JournalTitle>
                  <VolumeID>22</VolumeID>
                  <IssueID>8</IssueID>
                  <FirstPage>747</FirstPage>
                  <LastPage>757</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/34.868677</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Stauffer C, Grimson E (2000) Learning patterns of activity using realtime tracking. IEEE PAMI 22(8):747–757</BibUnstructured>
              </Citation>
              <Citation ID="CR36">
                <CitationNumber>36.</CitationNumber>
                <BibUnstructured>Varma M, Babu BR (2009) More generality in efficient multiple kernel learning. In: ICML, pp 1065–1072. doi: <ExternalRef><RefSource>10.1145/1553374.1553510</RefSource><RefTarget TargetType="DOI" Address="10.1145/1553374.1553510"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR37">
                <CitationNumber>37.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Walden</FamilyName>
                  </BibAuthorName>
                  <Year>2000</Year>
                  <ArticleTitle Language="En">A unified view of multitaper multivariate spectral estimation</ArticleTitle>
                  <JournalTitle>Biometrika</JournalTitle>
                  <VolumeID>87</VolumeID>
                  <IssueID>4</IssueID>
                  <FirstPage>767</FirstPage>
                  <LastPage>788</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>1813974</Handle>
                  </Occurrence>
                  <Occurrence Type="ZLBID">
                    <Handle>1028.62075</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1093/biomet/87.4.767</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Walden A (2000) A unified view of multitaper multivariate spectral estimation. Biometrika 87(4):767–788</BibUnstructured>
              </Citation>
              <Citation ID="CR38">
                <CitationNumber>38.</CitationNumber>
                <BibUnstructured>Wang B, Plumbley MD (2006) Investigating single-channel audio source separation methods based on non-negative matrix factorization. In: ICArn, pp 17–20</BibUnstructured>
              </Citation>
              <Citation ID="CR39">
                <CitationNumber>39.</CitationNumber>
                <BibUnstructured>Wang W et al (2005) Video assisted speech source separation. In: IEEE ICASSP, pp 425–428</BibUnstructured>
              </Citation>
              <Citation ID="CR40">
                <CitationNumber>40.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>K</Initials>
                    <FamilyName>Weinberger</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>L</Initials>
                    <FamilyName>Saul</FamilyName>
                  </BibAuthorName>
                  <Year>2009</Year>
                  <ArticleTitle Language="En">Distance metric learning for large margin nearest neighbor classification</ArticleTitle>
                  <JournalTitle>JMLR</JournalTitle>
                  <VolumeID>10</VolumeID>
                  <IssueID>12</IssueID>
                  <FirstPage>207</FirstPage>
                  <LastPage>244</LastPage>
                  <Occurrence Type="ZLBID">
                    <Handle>1235.68204</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Weinberger K, Saul L (2009) Distance metric learning for large margin nearest neighbor classification. JMLR 10(12):207–244</BibUnstructured>
              </Citation>
              <Citation ID="CR41">
                <CitationNumber>41.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>L</Initials>
                    <FamilyName>Wu</FamilyName>
                  </BibAuthorName>
                  <Etal/>
                  <Year>2009</Year>
                  <ArticleTitle Language="En">Scale-invariant visual language modeling for object categorization</ArticleTitle>
                  <JournalTitle>IEEE TMM</JournalTitle>
                  <VolumeID>11</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>286</FirstPage>
                  <LastPage>294</LastPage>
<Occurrence Type="DOI">
<Handle>10.1109/TMM.2008.2009692</Handle>
</Occurrence>
                </BibArticle>
                <BibUnstructured>Wu L et al (2009) Scale-invariant visual language modeling for object categorization. IEEE TMM 11(2):286–294</BibUnstructured>
              </Citation>
              <Citation ID="CR42">
                <CitationNumber>42.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>L</Initials>
                    <FamilyName>Wu</FamilyName>
                  </BibAuthorName>
                  <Etal/>
                  <Year>2010</Year>
                  <ArticleTitle Language="En">Semantics-preserving bag-of-words models and applications</ArticleTitle>
                  <JournalTitle>IEEE TIP</JournalTitle>
                  <VolumeID>19</VolumeID>
                  <IssueID>7</IssueID>
                  <FirstPage>1908</FirstPage>
                  <LastPage>1920</LastPage>
                </BibArticle>
                <BibUnstructured>Wu L et al (2010) Semantics-preserving bag-of-words models and applications. IEEE TIP 19(7):1908–1920</BibUnstructured>
              </Citation>
              <Citation ID="CR43">
                <CitationNumber>43.</CitationNumber>
                <BibUnstructured>Wu Y et al (2004) Multimodal information fusion for video concept detection. IEEE ICIP, pp 2391–2394</BibUnstructured>
              </Citation>
              <Citation ID="CR44">
                <CitationNumber>44.</CitationNumber>
                <BibUnstructured>Yang J et al (2007) Evaluating bag-of-visual-words representations in scene classification. ACM MIR, pp 197–206. doi: <ExternalRef><RefSource>10.1145/1290082.1290111</RefSource><RefTarget TargetType="DOI" Address="10.1145/1290082.1290111"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR45">
                <CitationNumber>45.</CitationNumber>
                <BibBook>
                  <BibAuthorName>
                    <Initials>B</Initials>
                    <FamilyName>Yao</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>L</Initials>
                    <FamilyName>Fei-Fei</FamilyName>
                  </BibAuthorName>
                  <Year>2010</Year>
                  <BookTitle>Grouplet: a structured image representation for recognizing human and object interactions</BookTitle>
                  <PublisherName>IEEE CVPR</PublisherName>
                  <PublisherLocation> San Francisco</PublisherLocation>
<Occurrence Type="DOI">
<Handle>10.1109/CVPR.2010.5540234</Handle>
</Occurrence>
                </BibBook>
                <BibUnstructured>Yao B, Fei-Fei L (2010) Grouplet: a structured image representation for recognizing human and object interactions. IEEE CVPR, San Francisco</BibUnstructured>
              </Citation>
            </Bibliography>
          </ArticleBackmatter>
        </Article>
      </Issue>
    </Volume>
  </Journal>
</Publisher>
