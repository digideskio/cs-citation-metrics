<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE Publisher PUBLIC "-//Springer-Verlag//DTD A++ V2.4//EN" "http://devel.springer.de/A++/V2.4/DTD/A++V2.4.dtd">
<Publisher>
  <PublisherInfo>
    <PublisherName>Springer-Verlag</PublisherName>
    <PublisherLocation>London</PublisherLocation>
  </PublisherInfo>
  <Journal OutputMedium="All">
    <JournalInfo JournalProductType="ArchiveJournal" NumberingStyle="ContentOnly">
      <JournalID>13735</JournalID>
      <JournalPrintISSN>2192-6611</JournalPrintISSN>
      <JournalElectronicISSN>2192-662X</JournalElectronicISSN>
      <JournalTitle>International Journal of Multimedia Information Retrieval</JournalTitle>
      <JournalAbbreviatedTitle>Int J Multimed Info Retr</JournalAbbreviatedTitle>
      <JournalSubjectGroup>
        <JournalSubject Type="Primary">Computer Science</JournalSubject>
        <JournalSubject Type="Secondary">Image Processing and Computer Vision</JournalSubject>
        <JournalSubject Type="Secondary">Multimedia Information Systems</JournalSubject>
        <JournalSubject Type="Secondary">Computer Science, general</JournalSubject>
        <JournalSubject Type="Secondary">Data Mining and Knowledge Discovery</JournalSubject>
        <JournalSubject Type="Secondary">Information Systems Applications (incl. Internet)</JournalSubject>
        <JournalSubject Type="Secondary">Information Storage and Retrieval</JournalSubject>
      </JournalSubjectGroup>
    </JournalInfo>
    <Volume OutputMedium="All">
      <VolumeInfo TocLevels="0" VolumeType="Regular">
        <VolumeIDStart>1</VolumeIDStart>
        <VolumeIDEnd>1</VolumeIDEnd>
        <VolumeIssueCount>4</VolumeIssueCount>
      </VolumeInfo>
      <Issue IssueType="Regular" OutputMedium="All">
        <IssueInfo IssueType="Regular" TocLevels="0">
          <IssueIDStart>1</IssueIDStart>
          <IssueIDEnd>1</IssueIDEnd>
          <IssueArticleCount>6</IssueArticleCount>
          <IssueHistory>
            <OnlineDate>
              <Year>2012</Year>
              <Month>4</Month>
              <Day>24</Day>
            </OnlineDate>
            <PrintDate>
              <Year>2012</Year>
              <Month>4</Month>
              <Day>23</Day>
            </PrintDate>
            <CoverDate>
              <Year>2012</Year>
              <Month>4</Month>
            </CoverDate>
            <PricelistYear>2012</PricelistYear>
          </IssueHistory>
          <IssueCopyright>
            <CopyrightHolderName>Springer-Verlag London Limited</CopyrightHolderName>
            <CopyrightYear>2012</CopyrightYear>
          </IssueCopyright>
        </IssueInfo>
        <Article ID="s13735-012-0001-9" OutputMedium="All">
          <ArticleInfo ArticleType="OriginalPaper" ContainsESM="No" Language="En" NumberingStyle="ContentOnly" TocLevels="0">
            <ArticleID>1</ArticleID>
            <ArticleDOI>10.1007/s13735-012-0001-9</ArticleDOI>
            <ArticleSequenceNumber>2</ArticleSequenceNumber>
            <ArticleTitle Language="En" OutputMedium="All">The heterogeneous feature selection with structural sparsity for multimedia annotation and hashing: a survey</ArticleTitle>
            <ArticleCategory>Invited Paper</ArticleCategory>
            <ArticleFirstPage>3</ArticleFirstPage>
            <ArticleLastPage>15</ArticleLastPage>
            <ArticleHistory>
              <RegistrationDate>
                <Year>2012</Year>
                <Month>1</Month>
                <Day>21</Day>
              </RegistrationDate>
              <Received>
                <Year>2011</Year>
                <Month>12</Month>
                <Day>14</Day>
              </Received>
              <Accepted>
                <Year>2012</Year>
                <Month>1</Month>
                <Day>8</Day>
              </Accepted>
              <OnlineDate>
                <Year>2012</Year>
                <Month>3</Month>
                <Day>2</Day>
              </OnlineDate>
            </ArticleHistory>
            <ArticleCopyright>
              <CopyrightHolderName>Springer-Verlag London Limited</CopyrightHolderName>
              <CopyrightYear>2012</CopyrightYear>
            </ArticleCopyright>
            <ArticleGrants Type="Regular">
              <MetadataGrant Grant="OpenAccess"/>
              <AbstractGrant Grant="OpenAccess"/>
              <BodyPDFGrant Grant="OpenAccess"/>
              <BodyHTMLGrant Grant="OpenAccess"/>
              <BibliographyGrant Grant="OpenAccess"/>
              <ESMGrant Grant="OpenAccess"/>
            </ArticleGrants>
          </ArticleInfo>
          <ArticleHeader>
            <AuthorGroup>
              <Author AffiliationIDS="Aff1">
                <AuthorName DisplayOrder="Western">
                  <GivenName>Fei</GivenName>
                  <FamilyName>Wu</FamilyName>
                </AuthorName>
                <Contact>
                  <Email>wufei@cs.zju.edu.cn</Email>
                </Contact>
              </Author>
              <Author AffiliationIDS="Aff1">
                <AuthorName DisplayOrder="Western">
                  <GivenName>Yahong</GivenName>
                  <FamilyName>Han</FamilyName>
                </AuthorName>
                <Contact>
                  <Email>yahong@zju.edu.cn</Email>
                </Contact>
              </Author>
              <Author AffiliationIDS="Aff2">
                <AuthorName DisplayOrder="Western">
                  <GivenName>Xiang</GivenName>
                  <FamilyName>Liu</FamilyName>
                </AuthorName>
                <Contact>
                  <Email>hn_lxa@126.com</Email>
                </Contact>
              </Author>
              <Author AffiliationIDS="Aff1">
                <AuthorName DisplayOrder="Western">
                  <GivenName>Jian</GivenName>
                  <FamilyName>Shao</FamilyName>
                </AuthorName>
                <Contact>
                  <Email>jshao@cs.zju.edu.cn</Email>
                </Contact>
              </Author>
              <Author AffiliationIDS="Aff1">
                <AuthorName DisplayOrder="Western">
                  <GivenName>Yueting</GivenName>
                  <FamilyName>Zhuang</FamilyName>
                </AuthorName>
                <Contact>
                  <Email>yzhuang@zju.edu.cn</Email>
                </Contact>
              </Author>
              <Author AffiliationIDS="Aff2" CorrespondingAffiliationID="Aff2">
                <AuthorName DisplayOrder="Western">
                  <GivenName>Zhongfei</GivenName>
                  <FamilyName>Zhang</FamilyName>
                </AuthorName>
                <Contact>
                  <Email>zhongfei@zju.edu.cn</Email>
                </Contact>
              </Author>
              <Affiliation ID="Aff1">
                <OrgDivision>College of Computer Science</OrgDivision>
                <OrgName>Zhejiang University</OrgName>
                <OrgAddress>
                  <City>Hangzhou</City>
                  <Country Code="CN">China</Country>
                </OrgAddress>
              </Affiliation>
              <Affiliation ID="Aff2">
                <OrgDivision>Department of Information Science and Electronic Engineering</OrgDivision>
                <OrgName>Zhejiang University</OrgName>
                <OrgAddress>
                  <City>Hangzhou</City>
                  <Country Code="CN">China</Country>
                </OrgAddress>
              </Affiliation>
            </AuthorGroup>
            <Abstract ID="Abs1" Language="En" OutputMedium="All">
              <Heading>Abstract</Heading>
              <Para>There is a rapid growth of the amount of multimedia data from real-world multimedia sharing web sites, such as Flickr and Youtube. These data are usually of high dimensionality, high order, and large scale. Moreover, different types of media data are interrelated everywhere in a complicated and extensive way by <Emphasis Type="Italic">context prior</Emphasis>. It is well known that we can obtain lots of features from multimedia such as images and videos; those high-dimensional features often describe various aspects of characteristics in multimedia. However, the obtained features are often over-complete to describe certain semantics. Therefore, the selection of limited discriminative features for certain semantics is hence crucial to make the understanding of multimedia more interpretable. Furthermore, the effective utilization of intrinsic embedding structures in various features can boost the performance of multimedia retrieval. As a result, the appropriate representation of the latent information hidden in the related features is hence crucial during multimedia understanding. This paper introduces many of the recent efforts in sparsity-based heterogenous feature selection, the representation of the intrinsic latent structure embedded in multimedia, and the related hashing index techniques.</Para>
            </Abstract>
            <KeywordGroup Language="En">
              <Heading>Keywords</Heading>
              <Keyword>Structural sparsity</Keyword>
              <Keyword>Factor decomposition</Keyword>
              <Keyword>Latent structure</Keyword>
              <Keyword>Hashing index</Keyword>
            </KeywordGroup>
            <ArticleNote Type="Misc">
              <SimplePara>This work was done when Z. Zhang was on leave from SUNY Binghamton, USA.</SimplePara>
            </ArticleNote>
          </ArticleHeader>
          <Body>
            <Section1 ID="Sec1">
              <Heading>Introduction</Heading>
              <Para>Natural images and videos can be well approximated by a small subset of elements from an <Emphasis Type="Italic">over-complete</Emphasis> dictionary. The process of choosing a good subset of dictionary elements along with the corresponding coefficients to represent a signal is known as sparse representation [<CitationRef CitationID="CR10">10</CitationRef>]. As pointed out in [<CitationRef CitationID="CR56">56</CitationRef>], the receptive files of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented, and bandpass (selective to structure at different spatial scales). Therefore, a learning algorithm is crucial to find sparse linear codes for natural scenes. The problem of finding a sparse representation for the data has become an interesting topic recently in computer vision and multimedia retrieval nowadays. The essential challenge to be resolved in sparse representation is to develop an efficient approach with which each original data element could be reconstructed from its corresponding sparse representation.</Para>
              <Para>In this paper, we focus on data mainly on images and videos. The feature selection and hashing of multimedia are the basis for image and video annotation and retrieval. The robust and appropriate techniques for feature selection and hashing can significantly improve the performance of image/video understanding, retrieval, tracking, matching, reconstruction, etc.</Para>
              <Para>
                <Figure Category="Standard" Float="Yes" ID="Fig1">
                  <Caption Language="En">
                    <CaptionNumber>Fig. 1</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>The illustration of the high-dimensional heterogeneous feature selection with structural grouping sparsity revised from [<CitationRef CitationID="CR77">77</CitationRef>]</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <MediaObject ID="MO1">
                    <ImageObject Color="Color" FileRef="MediaObjects/13735_2012_1_Fig1_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                  </MediaObject>
                </Figure>
              </Para>
              <Para>It is well known that we can extract high-dimensional features from one given image or video in the real world in different types. These different features can be roughly classified as <Emphasis Type="Italic">Local</Emphasis> (e.g., SIFT, Shape Context and GLOH) versus <Emphasis Type="Italic">Global</Emphasis> (e.g., color, shape and texture) [<CitationRef CitationID="CR49">49</CitationRef>], <Emphasis Type="Italic">Dense</Emphasis> (e.g., bag of visual words [<CitationRef CitationID="CR24">24</CitationRef>]) versus <Emphasis Type="Italic">Sparse</Emphasis> (e.g., Locality-constrained Linear Coding [<CitationRef CitationID="CR73">73</CitationRef>]), <Emphasis Type="Italic">Shadow</Emphasis> versus <Emphasis Type="Italic">Deep</Emphasis> (e.g., Hierarchical Models [<CitationRef CitationID="CR58">58</CitationRef>]), and <Emphasis Type="Italic">Multi-scale</Emphasis> (e.g., Spatial Pyramid Matching [<CitationRef CitationID="CR40">40</CitationRef>]), <Emphasis Type="Italic">Still</Emphasis> versus <Emphasis Type="Italic">Motion</Emphasis> (e.g., Optical Flow [<CitationRef CitationID="CR29">29</CitationRef>]), <Emphasis Type="Italic">Compressed</Emphasis> (e.g., Gabor wavelets [<CitationRef CitationID="CR42">42</CitationRef>]) versus and <Emphasis Type="Italic">Uncompressed</Emphasis>. We call these different types of features extracted in the same image or video the heterogeneous features, and the features of the same type the homogeneous features.</Para>
              <Para>Different subsets of heterogenous features have different intrinsic discriminative power to characterize the semantics in multimedia. That is to say, only limited groups of heterogenous features distinguish certain semantics from others. Therefore, the selected visual features for further multimedia processing are usually sparse.</Para>
              <Para>Given high-dimensional heterogeneous features in images and videos, in order to obtain the discriminative features, we often map original features into a subspace to discover their intrinsic structure by dimension reduction such as principal component analysis (PCA), Locally Linear Embedding (LLE), ISOMAP, Laplacian Eigenmap, Local Tangent Space Alignment (LTSA) and Locality Preserving Projections (LPP) [<CitationRef CitationID="CR61">61</CitationRef>]. However, it is very hard to discern what original features play an essential role during the semantic understanding in the embedded subspace after the dimension reduction is conducted. As a result, a more <Emphasis Type="Italic">interpretable</Emphasis> approach is necessary for feature selection. That is to say, given the number of extracted over-complete heterogenous features, it is essential to identify the discriminate features for certain semantics.</Para>
              <Para>Motivated by the recent advance in compressed sensing, <Emphasis Type="Italic">sparsity</Emphasis>-based feature selection approaches are developed in computer vision and multimedia retrieval [<CitationRef CitationID="CR25">25</CitationRef>, <CitationRef CitationID="CR46">46</CitationRef>, <CitationRef CitationID="CR48">48</CitationRef>, <CitationRef CitationID="CR77">77</CitationRef>, <CitationRef CitationID="CR82">82</CitationRef>]. The basic idea of sparsity-based feature selection is to impose a (structural) sparse penalty to select discriminative features. For example, Wright et al. [<CitationRef CitationID="CR76">76</CitationRef>] casts the face recognition problem as a liner regression problem with sparse constraints for regression coefficients. To solve the regression problem, Wright et al. [<CitationRef CitationID="CR76">76</CitationRef>] reformulate face recognition as an <InlineEquation ID="IEq1"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq1.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _1$$]]></EquationSource></InlineEquation>-norm problem. Cao et al. [<CitationRef CitationID="CR11">11</CitationRef>] propose learning different metric kernel functions for different heterogeneous features for image classification. After the introduction of the <InlineEquation ID="IEq2"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq2.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _1$$]]></EquationSource></InlineEquation>-norm at the group level into sparse logistic regression, a heterogeneous feature machines (HFM) is implemented in [<CitationRef CitationID="CR11">11</CitationRef>].</Para>
              <Para>For all the above approaches, the <InlineEquation ID="IEq3"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq3.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _1$$]]></EquationSource></InlineEquation>-norm (namely <Emphasis Type="Italic">lasso</Emphasis>, least absolution shrinkage and selection operator) [<CitationRef CitationID="CR71">71</CitationRef>] is effectively implemented to make the learning model both sparse and interpretable. However, for the group of features in which the pairwise correlations among them are very high, <Emphasis Type="Italic">lasso</Emphasis> tends to select only one of the pairwise correlated features and cannot induce the group effect. In the “large <InlineEquation ID="IEq4"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq4.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p$$]]></EquationSource></InlineEquation>, small <InlineEquation ID="IEq5"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq5.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$n$$]]></EquationSource></InlineEquation>” problem, the “grouped features” situation is an important concern to facilitate a model’s interpretability. In order to remedy the deficiency of <Emphasis Type="Italic">lasso</Emphasis>, group <Emphasis Type="Italic">lasso</Emphasis> [<CitationRef CitationID="CR87">87</CitationRef>] and elastic net [<CitationRef CitationID="CR93">93</CitationRef>] are proposed, respectively. If the structural <Emphasis Type="Italic">priors</Emphasis> embedded in images and videos are appropriately represented, the performance of semantic understanding for images and videos can be boosted. For example, since the extract high-dimensional heterogenous features from images and videos can be naturally divided into disjoint groups of homogeneous features, a <Emphasis Type="Italic">structural grouping sparsity</Emphasis> penalty is proposed in [<CitationRef CitationID="CR77">77</CitationRef>] to induce a (<Emphasis Type="Italic">structural</Emphasis>) sparse selection model for the identification of subgroups of homogenous features during image annotation. The motivation in [<CitationRef CitationID="CR77">77</CitationRef>] can be illustrated in Fig. <InternalRef RefID="Fig1">1</InternalRef>. After groups of heterogenous features such as color, texture, and shape are extracted from images, the structural grouping sparsity is conducted to set the coefficients (<InlineEquation ID="IEq6"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq6.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\varvec{\beta }_i$$]]></EquationSource></InlineEquation>) of the discriminative feature sets as 1 and the coefficients of other insignificant feature sets as 0. Moreover, the identified subgroup within each selected feature set is further used as the representation of each image. Due to the importance of the introduction of the structural priors into feature selection, Jenatton et al. recently propose a general definition of the <Emphasis Type="Italic">structured sparsity-inducing norm</Emphasis> in [<CitationRef CitationID="CR31">31</CitationRef>,<CitationRef CitationID="CR32">32</CitationRef>] to incorporate the prior knowledge or structural constraints to find the suitable linear features. Under the setting of the structured sparsity-inducing norm, <Emphasis Type="Italic">lasso</Emphasis>, group <Emphasis Type="Italic">lasso</Emphasis>, and even the tree-guided group lasso [<CitationRef CitationID="CR37">37</CitationRef>] are, respectively, its special cases.</Para>
              <Para>Note that the introduction of the sparsity penalty into the traditional matrix factorization can help achieve a good performance. For example, Kim and Park [<CitationRef CitationID="CR38">38</CitationRef>] propose a novel algorithm of sparse NMF to control the degree of sparseness in the nonnegative basis matrix or the nonnegative coefficient matrix. The empirical study shows that the performance can be improved if we impose the sparsity on a factor of NMF by the <InlineEquation ID="IEq7"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq7.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _1$$]]></EquationSource></InlineEquation>-norm minimization into the objective function. Sparse topical coding (STC) is proposed in [<CitationRef CitationID="CR92">92</CitationRef>] to discover latent representations of large collections of data by a nonprobabilistic formulation of the topic models. STC can directly control the sparsity of the inferred representations by the conduction of sparsity-inducing regularizers. A hierarchical Bayesian model is developed in [<CitationRef CitationID="CR43">43</CitationRef>] to integrate the dictionary learning, sparse coding, and topic modeling for the joint analysis of multiple images and (when present) the associated annotations.</Para>
              <Para>After the discriminative features are selected, we need to represent the intrinsic structures embedded in the heterogenous features. Traditionally, the high-dimensional heterogenous features in images and videos are preferred to being represented merely as concatenated vectors, whose high dimensionality always causes the problem of curse of dimensionality. Besides, as reported in [<CitationRef CitationID="CR85">85</CitationRef>], the over-compression problem occurs when the sample vector is very long and the number of training samples is small, which results in a loss of information in the dimension reduction process. At present, many of the representation approaches are proposed such as matrix, tensor, and graph. Tensor is a natural generalization of a vector or a matrix, and has been applied to computer vision, signal processing, and information retrieval [<CitationRef CitationID="CR28">28</CitationRef>, <CitationRef CitationID="CR45">45</CitationRef>, <CitationRef CitationID="CR69">69</CitationRef>]. The tensor algebra defines multilinear operators over a set of vector spaces and captures the high-order information in heterogeneous features. Usually, the traditional graph only models the homogeneous similarity and therefore ignores the high-order relations that are inherent in images and videos. In order to address this drawback of the traditional graph, hypergraph is proposed to represent more complex correlations in images and videos. Hypergraph [<CitationRef CitationID="CR5">5</CitationRef>] is a graph in which one edge can connect more than two vertices. This characteristic enables hypergraphs to represent complex and higher-order relations which are difficult to be represented in the traditional undirected or directed graphs. Recently, hypergraphs have been successfully applied to image annotation, image ranking, and music recommendation, and have received considerable attention. For example, spectral clustering is generalized from undirected graphs to hypergraphs in [<CitationRef CitationID="CR91">91</CitationRef>], where hypergraph embedding and transductive classification are further developed by spectral hypergraph clustering. Hypergraph spectral learning is utilized in [<CitationRef CitationID="CR68">68</CitationRef>] for multi-label classification, where a hypergraph is constructed to exploit the correlation information among different labels. In many real-world applications, the complex spatial–temporal or context in images and videos can be efficiently encoded by a matrix, a tensor, or a graph and then information is lost if the vector representation is used. The interesting issue is whether we can introduce a sparsity penalty into a matrix, a tensor, or a hypergraph to make the representation and learning interpretable. If there is a low-rank structure in a matrix, the penalty of the matrix rank is a good choice to enforce such sparsity. However, a matrix rank is neither continuous nor convex. As a surrogate convex of the nonconvex matrix rank function, the matrix nuclear norm (trace norm, matrix-<Emphasis Type="Italic">lasso</Emphasis>) is specifically employed to encourage the low-rank property. Nuclear norm is defined as the sum of all the singular values as a convex function. The idea of a low-rank matrix is an extension from the concept of “sparse vector” to that of “sparse matrix”. Robust principal component analysis (R-PCA) is proposed in [<CitationRef CitationID="CR75">75</CitationRef>] to recover low-rank matrices from corrupted observations by the implementation of the nuclear norm minimization for the low-rank recovery and <InlineEquation ID="IEq8"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq8.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _1$$]]></EquationSource></InlineEquation>-minimization for the error correction. An accelerated R-PCA approach is proposed in [<CitationRef CitationID="CR52">52</CitationRef>] for a large-scale image tag transduction under the setting of the nuclear norm. One <InlineEquation ID="IEq9"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq9.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell_1$$]]></EquationSource></InlineEquation>-graph is constructed by encoding the overall behavior of the data set in sparse representations in [<CitationRef CitationID="CR14">14</CitationRef>].</Para>
              <Para>How to construct an approximate index structure for images and videos with the selected features is essential to the efficient retrieval of a large scale of multimedia. A naive solution to accurately find the relevantly similar examples to a query is to search over all the samples in a database and sort them according to their similarities to the query. However, this becomes prohibitively expensive when the scale of the database is very large. To reduce the complexity of finding the relevant samples for a query, indexing techniques are necessarily required to organize images and videos. However, studies reveal that many of the index structures have an exponential dependency (in space or time or both) upon the number of the dimensions and even a simple brute-force, linear-scan approach may be more efficient than an index-based search in high-dimensional settings [<CitationRef CitationID="CR4">4</CitationRef>]. Moreover, an excellent index structure should guarantee that the similarity of two samples in the index space keeps consistent with their similarity in the original data space [<CitationRef CitationID="CR59">59</CitationRef>]. Recently, locality-sensitive hashing (LSH) and its variations have been proposed as the indexing approaches for an approximate nearest neighbor search [<CitationRef CitationID="CR17">17</CitationRef>, <CitationRef CitationID="CR47">47</CitationRef>]. The basic idea in LSH is to use a family of locality preserving hash functions to hash similar data in the high-dimensional space into the same bucket with a higher probability than these for the nonsimilar data. As shown by semantic hashing [<CitationRef CitationID="CR60">60</CitationRef>], LSH could be unstable and lead to an extremely bad result due to its randomized approximate similarity search. Unlike those approaches which randomly project the input data into an embedding space such as LSH, several machine learning approaches are recently developed to generate more compact and approximate binary codewords for data indexing, such as restricted Boltzmann machine (RBM) in semantic hashing [<CitationRef CitationID="CR60">60</CitationRef>], parameter sensitive hashing (PSH) in pose estimation [<CitationRef CitationID="CR62">62</CitationRef>] and spectral hashing [<CitationRef CitationID="CR74">74</CitationRef>]. These approaches attempt to elaborate appropriate hash functions to optimize an underlying hashing objective. Shao et al. [<CitationRef CitationID="CR63">63</CitationRef>] introduces the sparse principal component analysis (sparse PCA) and the boosting similarity sensitive hashing (Boosting SSC) into the traditional spectral hashing and calls this approach sparse spectral hashing (SSH).</Para>
            </Section1>
            <Section1 ID="Sec2">
              <Heading>Sparsity-based feature selection</Heading>
              <Section2 ID="Sec3">
                <Heading>Notation and problem formulation</Heading>
                <Para>Assume that we have a training set of <InlineEquation ID="IEq10"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq10.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$n$$]]></EquationSource></InlineEquation> labeled samples such as images and videos with <InlineEquation ID="IEq11"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq11.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$J$$]]></EquationSource></InlineEquation> labels (tags) and that the <InlineEquation ID="IEq12"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq12.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p$$]]></EquationSource></InlineEquation>-dimensional heterogenous features can be extracted from each image or video: <InlineEquation ID="IEq13"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq13.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\{ (\mathbf{ x} _i,\mathbf{ y} _i)\in \mathbb{ R} ^p\times \{0,1\}^J: i = 1,2,\ldots , n\}$$]]></EquationSource></InlineEquation>, where <InlineEquation ID="IEq14"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq14.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ x} _i =(x_{i1},\ldots ,x_{ip})^{\mathrm{ T} } \in {\mathbb{ R} }^p$$]]></EquationSource></InlineEquation> represents the <InlineEquation ID="IEq15"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq15.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p$$]]></EquationSource></InlineEquation>-dimension feature vector for the <InlineEquation ID="IEq16"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq16.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$i$$]]></EquationSource></InlineEquation>th image or video, <InlineEquation ID="IEq17"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq17.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p$$]]></EquationSource></InlineEquation> represents the dimensionality of features, <InlineEquation ID="IEq18"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq18.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ y} _i = (y_{i1},\ldots ,y_{iJ})^\mathrm{ T}\in \{0,1\}^J $$]]></EquationSource></InlineEquation> is the corresponding label vector, <InlineEquation ID="IEq19"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq19.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$y_{ij} = 1$$]]></EquationSource></InlineEquation> if the <InlineEquation ID="IEq20"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq20.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$i$$]]></EquationSource></InlineEquation>th sample has the <InlineEquation ID="IEq21"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq21.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$j$$]]></EquationSource></InlineEquation>th label and <InlineEquation ID="IEq22"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq22.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$y_{ij} = 0$$]]></EquationSource></InlineEquation> otherwise. Unlike the traditional multi-class problem where each sample only belongs to a single category: <InlineEquation ID="IEq23"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq23.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$ \sum ^J_{j=1}y_{ij}=1 $$]]></EquationSource></InlineEquation>, in multi-label setting, we relax the constraint to <InlineEquation ID="IEq24"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq24.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$ \sum ^J_{j=1}y_{ij} \ge 0 $$]]></EquationSource></InlineEquation>. Let <InlineEquation ID="IEq25"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq25.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ X} =(\mathbf{ x} _1,\ldots ,\mathbf{ x} _n)^\mathrm{ T}$$]]></EquationSource></InlineEquation> be the <InlineEquation ID="IEq26"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq26.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$n\times p$$]]></EquationSource></InlineEquation> training data matrix, and <InlineEquation ID="IEq27"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq27.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ Y} =(\mathbf{ y} _1,\ldots ,\mathbf{ y} _n)^\mathrm{ T}$$]]></EquationSource></InlineEquation> the corresponding <InlineEquation ID="IEq28"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq28.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$n \times J$$]]></EquationSource></InlineEquation> label indicator matrix.</Para>
                <Para>Suppose that the extracted <InlineEquation ID="IEq29"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq29.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p$$]]></EquationSource></InlineEquation> dimensional heterogenous features are divided into <InlineEquation ID="IEq30"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq30.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$L$$]]></EquationSource></InlineEquation> disjoint groups of homogeneous features, with <InlineEquation ID="IEq31"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq31.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p_l$$]]></EquationSource></InlineEquation> the number of features in the <InlineEquation ID="IEq32"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq32.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$l$$]]></EquationSource></InlineEquation>th group, i.e., <InlineEquation ID="IEq33"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq33.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\sum _l^L p_l = p$$]]></EquationSource></InlineEquation>. For ease of notation, we use a matrix <InlineEquation ID="IEq34"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq34.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ X} _l \in \mathbb{ R} ^{n \times {p_l}}$$]]></EquationSource></InlineEquation> to represent the features of the training data corresponding to the <InlineEquation ID="IEq35"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq35.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$l$$]]></EquationSource></InlineEquation>th group, with corresponding coefficient vector <InlineEquation ID="IEq36"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq36.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\beta _{jl} \in \mathbb{ R} ^{p_l}(l=1,2,\ldots ,L)$$]]></EquationSource></InlineEquation> for the <InlineEquation ID="IEq37"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq37.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$j$$]]></EquationSource></InlineEquation>th label. Let <InlineEquation ID="IEq38"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq38.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\varvec{\beta }_j = (\beta _{j1}^\mathrm{ T},\ldots ,\beta _{jL}^\mathrm{ T})^\mathrm{ T}$$]]></EquationSource></InlineEquation> be the entire coefficient vector for the <InlineEquation ID="IEq39"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq39.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$j$$]]></EquationSource></InlineEquation>th label; we have<Equation ID="Equ1"><EquationNumber>1</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_Equ1.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \mathbf{ X} \varvec{\beta }_j = \sum _{l=1}^L \mathbf{ X} _l\beta _{jl} \end{aligned}$$]]></EquationSource></Equation>In the following, we assume that the label indicator matrix <InlineEquation ID="IEq40"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq40.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ Y} $$]]></EquationSource></InlineEquation> is centered and that the feature matrix <InlineEquation ID="IEq41"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq41.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ X} $$]]></EquationSource></InlineEquation> is centered and standardized, namely <InlineEquation ID="IEq42"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq42.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\sum _{i=1}^n {y_{ij}}=0$$]]></EquationSource></InlineEquation>, <InlineEquation ID="IEq43"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq43.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\sum _{i=1}^n {x_{id}}=0$$]]></EquationSource></InlineEquation>, and <InlineEquation ID="IEq44"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq44.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\sum _{i=1}^n {x_{id}^2}=1$$]]></EquationSource></InlineEquation>, for <InlineEquation ID="IEq45"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq45.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$j=1,2,\ldots ,J$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq46"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq46.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$d=1,2,\ldots ,p$$]]></EquationSource></InlineEquation>. Moreover, we let <InlineEquation ID="IEq47"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq47.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$||\beta _{jl}||_2^2$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq48"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq48.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$||\beta _{jl}||_1$$]]></EquationSource></InlineEquation> denote the <InlineEquation ID="IEq49"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq49.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _2$$]]></EquationSource></InlineEquation>-norm and the <InlineEquation ID="IEq50"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq50.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _1$$]]></EquationSource></InlineEquation>-norm of vector <InlineEquation ID="IEq51"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq51.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\beta _{jl}$$]]></EquationSource></InlineEquation>, respectively.</Para>
                <Para>Denote <InlineEquation ID="IEq52"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq52.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\hat{\beta }(\delta )$$]]></EquationSource></InlineEquation> the estimated coefficients obtained by a fitting procedure <InlineEquation ID="IEq53"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq53.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\delta $$]]></EquationSource></InlineEquation>. That is to say, for the <InlineEquation ID="IEq54"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq54.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$j$$]]></EquationSource></InlineEquation>th label, we tend to train a regression model <InlineEquation ID="IEq55"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq55.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\hat{\beta _j}(\delta )$$]]></EquationSource></InlineEquation> with a penalty term as follows to select its corresponding discriminative features:<Equation ID="Equ2"><EquationNumber>2</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_Equ2.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \min _{\varvec{\hat{\beta }}_j} \;||\mathbf{ Y} _{(:,j)}-\sum _{l=1}^L \mathbf{ X} _l \hat{\beta }_{jl}||_2^2 + {\varvec{\lambda }}P(\varvec{\hat{\beta }}_j) \end{aligned}$$]]></EquationSource></Equation>where <InlineEquation ID="IEq56"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq56.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ Y} _{(:,j)}\in {(0,1)}^{(n\times 1)}$$]]></EquationSource></InlineEquation> is the <InlineEquation ID="IEq57"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq57.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$j$$]]></EquationSource></InlineEquation>th column of indicator matrix <InlineEquation ID="IEq58"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq58.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$${\mathbf{ Y} }$$]]></EquationSource></InlineEquation> and encodes the label information for the <InlineEquation ID="IEq59"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq59.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$j$$]]></EquationSource></InlineEquation>th label, <InlineEquation ID="IEq60"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq60.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$P(\varvec{\hat{\beta }}_j)$$]]></EquationSource></InlineEquation> is the regularizer which imposes structural priors to the high-dimensional features. The trained regression model combines a loss function (measuring the goodness of fit of the model to the data) with a regularized penalty (encouraging the assumed grouping structure). For example, the ridge regression uses the <InlineEquation ID="IEq61"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq61.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _2$$]]></EquationSource></InlineEquation>-norm to avoid overfitting and <Emphasis Type="Italic">lasso</Emphasis> produces sparsity on <InlineEquation ID="IEq62"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq62.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\varvec{\hat{\beta }}_j$$]]></EquationSource></InlineEquation> by the <InlineEquation ID="IEq63"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq63.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _1$$]]></EquationSource></InlineEquation>-norm. If the estimated coefficients in <InlineEquation ID="IEq64"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq64.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\hat{\beta }_{jl}$$]]></EquationSource></InlineEquation> for <InlineEquation ID="IEq65"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq65.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$j$$]]></EquationSource></InlineEquation>th label are not zero, this means that the <InlineEquation ID="IEq66"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq66.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$l$$]]></EquationSource></InlineEquation>th homogeneous features are all selected to make the <InlineEquation ID="IEq67"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq67.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$j$$]]></EquationSource></InlineEquation>th label discernible. Simultaneously, homogeneous features may be dropped out for the representation of <InlineEquation ID="IEq68"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq68.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$j$$]]></EquationSource></InlineEquation>th label due to their irrelevance. Therefore, we can set up an interpretable model for feature selection.</Para>
                <Para>The solution to <InlineEquation ID="IEq69"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq69.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\hat{\beta _j}(\delta )$$]]></EquationSource></InlineEquation> can identify all of the discriminative features for each <InlineEquation ID="IEq70"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq70.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$j$$]]></EquationSource></InlineEquation>th label; however, the individual conduction of <InlineEquation ID="IEq71"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq71.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\hat{\beta _j}(\delta )$$]]></EquationSource></InlineEquation> ignores the correlations between labels in the setting of images and videos with multiple labels. The effective utilization of the latent information hidden in the related labels somehow boosts the performance of multi-label annotation. For example, a multiple response regression model, called curds and whey (C&amp;W) is proposed in [<CitationRef CitationID="CR9">9</CitationRef>]. Curds and whey sets up the connection between multiple response regressions and canonical correlations. Therefore, the C&amp;W method can be used to boost the performance of multi-label prediction given the prediction results from the regressions of individual labels [<CitationRef CitationID="CR77">77</CitationRef>]. Multi-task feature selection (or multi-task feature learning) is an alternative to utilizing the label correlation during feature selection. Argyriou et al. [<CitationRef CitationID="CR1">1</CitationRef>] and Obozinski et al. [<CitationRef CitationID="CR55">55</CitationRef>] use the <InlineEquation ID="IEq72"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq72.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell_{1,2}$$]]></EquationSource></InlineEquation>-norm to regularize the heterogeneous features of different tasks and therefore encourage multiple features to have similar sparsity patterns across tasks (tags).</Para>
              </Section2>
              <Section2 ID="Sec4">
                <Heading>Lasso and nonnegative garotte</Heading>
                <Para>In statistical community, <Emphasis Type="Italic">lasso</Emphasis> [<CitationRef CitationID="CR71">71</CitationRef>] is a shrinkage and variable selection method for linear regression, which is a penalized least square method imposing an <InlineEquation ID="IEq73"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq73.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _1$$]]></EquationSource></InlineEquation>-norm penalty to the regression coefficients. Due to the nature of the <InlineEquation ID="IEq74"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq74.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _1$$]]></EquationSource></InlineEquation>-norm penalty, <Emphasis Type="Italic">lasso</Emphasis> continuously shrinks the coefficients toward zero, and achieves its prediction accuracy via the bias–variance trade-off. In signal processing, <Emphasis Type="Italic">lasso</Emphasis> always produces a sparse representation that selects the subset compactly expressing the input signal. In the literature, the <Emphasis Type="Italic">lasso</Emphasis>-based sparse representation methods have been successfully used to solve problems such as face recognition [<CitationRef CitationID="CR76">76</CitationRef>] and image classification [<CitationRef CitationID="CR57">57</CitationRef>].</Para>
                <Para>In order to select the most discriminative features for the annotation of images by the <InlineEquation ID="IEq75"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq75.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$j$$]]></EquationSource></InlineEquation>th tag, <Emphasis Type="Italic">lasso</Emphasis> is defined to train a regression model <InlineEquation ID="IEq76"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq76.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\hat{\beta _j}(\delta )$$]]></EquationSource></InlineEquation> on the training set of images <InlineEquation ID="IEq77"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq77.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ X} $$]]></EquationSource></InlineEquation> by a <InlineEquation ID="IEq78"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq78.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _1$$]]></EquationSource></InlineEquation>-norm:<Equation ID="Equ3"><EquationNumber>3</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_Equ3.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \min _{\varvec{\hat{\beta }}_j}\; ||\mathbf{ Y} _{(:,j)}- \mathbf{ X} \varvec{\hat{\beta }}_j||_2^2 + \lambda ||\varvec{\hat{\beta }}_j||_1 \end{aligned}$$]]></EquationSource></Equation>where <InlineEquation ID="IEq79"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq79.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\lambda > 0$$]]></EquationSource></InlineEquation> is the regularized parameter. Due to the nature of the <InlineEquation ID="IEq80"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq80.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _1$$]]></EquationSource></InlineEquation>-norm penalty, by solving (<InternalRef RefID="Equ3">3</InternalRef>), most coefficients in the estimated <InlineEquation ID="IEq81"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq81.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\varvec{\hat{\beta }}_j$$]]></EquationSource></InlineEquation> are shrinked to zero, which could be used to select the discriminative features. It is clear that (<InternalRef RefID="Equ3">3</InternalRef>) is an unconstrained convex optimization problem. Many algorithms have been proposed to solve problem (<InternalRef RefID="Equ3">3</InternalRef>), such as the quadratic programming methods [<CitationRef CitationID="CR71">71</CitationRef>], least angle regression [<CitationRef CitationID="CR19">19</CitationRef>] and Gauss-Seidel [<CitationRef CitationID="CR65">65</CitationRef>].</Para>
                <Para>It has been shown that the nonnegative matrix factorization (NMF) [<CitationRef CitationID="CR41">41</CitationRef>] can learn part-based representation. The nonnegativity constraint makes the representation easy to interpret due to purely additive combinations of nonnegative basis vectors. The model of nonnegative garrote [<CitationRef CitationID="CR7">7</CitationRef>] is proposed to solve the following optimization problem<Equation ID="Equ4"><EquationNumber>4</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_Equ4.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \min _{\varvec{\hat{\beta }}_j}\; ||\mathbf{ Y} _{(:,j)}- \mathbf{ X} \varvec{\hat{\beta }}_j||_2^2 + \lambda \sum _{l=1}^p \hat{\beta }_{jl},\nonumber \\ & \text{ s.t.} \; \hat{\beta }_{jl} \ge 0,\quad \forall l \end{aligned}$$]]></EquationSource></Equation>where <InlineEquation ID="IEq82"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq82.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\lambda > 0$$]]></EquationSource></InlineEquation> is the regularized parameter. The nonnegative garrote can be efficiently solved by the classical numerical methods such as the least angle regression (LARS) [<CitationRef CitationID="CR19">19</CitationRef>]. Breiman’s original implementation [<CitationRef CitationID="CR7">7</CitationRef>] to solve (<InternalRef RefID="Equ4">4</InternalRef>) is to shrink each ordinary least squares (OLS) estimated coefficient by a nonnegative amount whose sum is subject to an upper bound constraint (the garrote). In the extensive simulation studies, Breiman has shown that the garotte is superior to subset selection and is competitive with ridge regression. Although the motivation of <Emphasis Type="Italic">lasso</Emphasis> comes from the garotte, in overfitting or highly correlated settings, the performance of the garotte deteriorates same as the OLS. In contrast, <Emphasis Type="Italic">lasso</Emphasis> avoids the explicit use of OLS estimates [<CitationRef CitationID="CR71">71</CitationRef>].</Para>
                <Para>As mentioned before, Wright et al. [<CitationRef CitationID="CR76">76</CitationRef>] introduce <InlineEquation ID="IEq83"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq83.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _1$$]]></EquationSource></InlineEquation>-norm into face recognition and formulates the face recognition as a liner regression with sparse constraints for regression coefficients. However, <Emphasis Type="Italic">lasso</Emphasis> makes the representation unnecessarily additive. This might result in the representation not being interpretable as NMF. Moreover, the class label or discriminant information from the training set is not apparently incorporated during constructing sparse representation, which may limit the ultimate classification accuracy. Liu et al. [<CitationRef CitationID="CR46">46</CitationRef>] propose a method for supervised image recognition and refer to it as the nonnegative curds and whey (NNCW). The NNCW procedure consists of two stages. In the first stage, NNCW considers a set of sparse and nonnegative representations of a test image, each of which is a linear combination of the images within a certain class, by solving a set of regression-type NMF problems. In the second stage, NNCW incorporates these representations into a new sparse and nonnegative representation by using the group nonnegative garrote [<CitationRef CitationID="CR87">87</CitationRef>]. This procedure is particularly appropriate for discriminant analysis owing to its supervised and nonnegativity nature in sparsity pursuing.</Para>
                <Para>It is natural in group <Emphasis Type="Italic">lasso</Emphasis> to allow the size of each group to grow unbounded, that is, we replace the sum of Euclidean norms with a sum of appropriate Hilbertian norms. Under this setting, several algorithms are proposed to connect multiple kernel learning and group-lasso regularizer together [<CitationRef CitationID="CR2">2</CitationRef>]. The composite kernel learning with group structure (CKLGS) is proposed in [<CitationRef CitationID="CR86">86</CitationRef>] to select groups of discriminative features. The CKLGS method embeds the nonlinear data with discriminative features into different reproducing kernel Hilbert spaces (RKHS), and then composes these kernels to select groups of discriminative features.</Para>
              </Section2>
              <Section2 ID="Sec5">
                <Heading>Structural grouping sparsity</Heading>
                <Para>If the pairwise correlation between a group of features is very high, lasso tends to individually select only one of the pairwise correlated features and does not induce the group effect. In the “large <InlineEquation ID="IEq84"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq84.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p$$]]></EquationSource></InlineEquation>, small <InlineEquation ID="IEq85"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq85.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$n$$]]></EquationSource></InlineEquation>” problem, the “grouped features” situation is an important concern to facilitate a model’s interpretability. That is to say, <Emphasis Type="Italic">lasso</Emphasis> is limited in that it treats each input feature independent of each other and hence is incapable of capturing structural priors among heterogenous features. In order to remedy this deficiency of <Emphasis Type="Italic">lasso</Emphasis>, elastic net [<CitationRef CitationID="CR93">93</CitationRef>] and group <Emphasis Type="Italic">lasso</Emphasis> [<CitationRef CitationID="CR87">87</CitationRef>] are proposed, respectively.</Para>
                <Para>Elastic net [<CitationRef CitationID="CR93">93</CitationRef>] generalizes <Emphasis Type="Italic">lasso</Emphasis> to overcome these drawbacks. For any nonnegative <InlineEquation ID="IEq86"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq86.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\lambda _1$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq87"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq87.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\lambda _2$$]]></EquationSource></InlineEquation>, elastic net is defined as a following optimization problem:<Equation ID="Equ5"><EquationNumber>5</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_Equ5.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \min  _{\varvec{\hat{\beta }}_j}\; ||\mathbf{ Y} _{(:,j)}- \mathbf{ X} \varvec{\hat{\beta }}_j||_2^2 + \lambda ||\varvec{\hat{\beta }}_j||_2^2 + \lambda ||\varvec{\hat{\beta }}_j||_1 \end{aligned}$$]]></EquationSource></Equation>Group <Emphasis Type="Italic">lasso</Emphasis> is proposed by Yuan and Lin [<CitationRef CitationID="CR87">87</CitationRef>] by solving the following convex optimization problem:<Equation ID="Equ6"><EquationNumber>6</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_Equ6.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \min _{\varvec{\hat{\beta }}_j} \;\left\Vert\mathbf{ Y} _{(:,j)}-\sum _{l=1}^L \mathbf{ X} _l \hat{\beta }_{jl}\right\Vert_2^2 + \lambda \sum _{l=1}^L \sqrt{p_l}||\hat{\beta }_{jl} ||_2 \ \end{aligned}$$]]></EquationSource></Equation>where <InlineEquation ID="IEq88"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq88.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p$$]]></EquationSource></InlineEquation> dimension features are divided into <InlineEquation ID="IEq89"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq89.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$L$$]]></EquationSource></InlineEquation> groups, with <InlineEquation ID="IEq90"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq90.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p_l$$]]></EquationSource></InlineEquation> the number in group <InlineEquation ID="IEq91"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq91.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$l$$]]></EquationSource></InlineEquation>. Note that <InlineEquation ID="IEq92"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq92.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$||\cdot ||_2$$]]></EquationSource></InlineEquation> is the <Emphasis Type="Italic">not squared</Emphasis> Euclidean norm. This procedure acts like lasso at the group level: depending on <InlineEquation ID="IEq93"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq93.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\lambda $$]]></EquationSource></InlineEquation>, an entire group of features may be dropped out of the model. The key assumption behind the group <Emphasis Type="Italic">lasso</Emphasis> regularizer is that if a few features in one group are important, then most of the features in the same group should also be important. In fact, if the group sizes are all one, (<InternalRef RefID="Equ6">6</InternalRef>) reduces to lasso (<InternalRef RefID="Equ3">3</InternalRef>).</Para>
                <Para>Yang et al. [<CitationRef CitationID="CR83">83</CitationRef>] takes the regions within the same image as a group and proposes spatial group sparse coding (SGSC) for region tagging. In SGSC, the group structure of regions-in-image relationship is incorporated into the sparse reconstruction framework by the group <Emphasis Type="Italic">lasso</Emphasis> penalty. Experimental results show that SGSC achieves a good performance of region tagging by integrating a spatial Gaussian kernel into the group sparse reconstruction.</Para>
                <Para>If there is a linear-ordering (also known as <Emphasis Type="Italic">chain</Emphasis>) in the features, fused <Emphasis Type="Italic">lasso</Emphasis> can be used [<CitationRef CitationID="CR70">70</CitationRef>]. For example, in order to remove low-amplitude structures and globally preserve and enhance salient edges, Xu et al. [<CitationRef CitationID="CR81">81</CitationRef>] introduces an order penalty into the image smooth based on the mechanism of discretely counting spatial changes.</Para>
                <Para>The heterogenous features in images and videos are <Emphasis Type="Italic">naturally</Emphasis> grouped. For example, color and shape, respectively, discern the aspects of visual characteristics. That is to say, it is convenient to select discriminative features from high-dimensional heterogeneous features by performing feature selection at a group level. However, the group <Emphasis Type="Italic">lasso</Emphasis> does not yield sparsity within a group. That is, if the selection coefficients of a group is nonzero, the selection coefficient of each feature within that group will all be nonzero.</Para>
                <Para>In order to utilize the structure priors between heterogeneous and homogeneous features for image annotation, Wu et al. [<CitationRef CitationID="CR77">77</CitationRef>] proposes a framework of multi-label boosting by the selection of heterogeneous features with structural grouping sparsity (<Emphasis Type="Bold">MtBGS</Emphasis>). MtBGS formulates the multi-label image annotation problem as a multiple response regression model with a structural grouping penalty. A benefit of performing multi-label image annotation via regression is the ability to introduce penalties. Many of the penalties can be introduced into the regression model for a better prediction. Hastie et al. [<CitationRef CitationID="CR27">27</CitationRef>] proposes the penalized discriminant analysis (PDA) to tackle problems of overfitting in situations of large numbers of highly correlated predictors (features). PDA introduces a quadratic penalty with a symmetric and positive definite matrix <InlineEquation ID="IEq94"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq94.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\Omega $$]]></EquationSource></InlineEquation> into the objective function. Elastic net [<CitationRef CitationID="CR93">93</CitationRef>] is proposed to conduct automatic variable selection and group selection of the correlated variables simultaneously by imposing both <InlineEquation ID="IEq95"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq95.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _1$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq96"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq96.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _2$$]]></EquationSource></InlineEquation>-norm penalties. Furthermore, motivated by elastic net, Clemmensen et al. [<CitationRef CitationID="CR15">15</CitationRef>] extended PDA to sparse discriminant analysis (SDA).</Para>
                <Para>The basic motivation of imposing structural grouping penalty in MtBGS is to perform heterogeneous feature group selection and subgroup identification within homogeneous features simultaneously. As we know, some subgroups of features in high-dimensional heterogenous features have a discriminative power for predicting certain labels of a given image.</Para>
                <Para>For each label <InlineEquation ID="IEq97"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq97.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$j$$]]></EquationSource></InlineEquation> and its corresponding indicator vector, the regression model of <Emphasis Type="Bold">MtBGS</Emphasis> is defined as follows:<Equation ID="Equ7"><EquationNumber>7</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_Equ7.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \min  _{\varvec{\hat{\beta }}_j}\;\left\Vert\mathbf{ Y} _{(:,j)}-\sum _{l=1}^L \mathbf{ X} _l \hat{\beta }_{jl}\right\Vert_2^2 + \lambda _1 \sum _{l=1}^L ||\hat{\beta }_{jl} ||_2 + \lambda _2 ||\varvec{\hat{\beta }}_j||_1\nonumber \\ \end{aligned}$$]]></EquationSource></Equation>where <InlineEquation ID="IEq98"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq98.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\lambda _1 \sum _{l=1}^L ||\hat{\beta }_{jl} ||_2 + \lambda _2 ||\varvec{\hat{\beta }}_j||_1$$]]></EquationSource></InlineEquation> is the regularizer <InlineEquation ID="IEq99"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq99.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$P(\varvec{\hat{\beta }}_j)$$]]></EquationSource></InlineEquation> in (<InternalRef RefID="Equ2">2</InternalRef>) and is called the <Emphasis Type="Italic">structural grouping penalty</Emphasis> in [<CitationRef CitationID="CR77">77</CitationRef>].</Para>
                <Para>Let <InlineEquation ID="IEq100"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq100.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\hat{\varvec{\beta }_j}$$]]></EquationSource></InlineEquation> be the solution to (<InternalRef RefID="Equ7">7</InternalRef>); we predict the probability <InlineEquation ID="IEq101"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq101.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\hat{\mathbf{ y} }_u$$]]></EquationSource></InlineEquation> that unlabeled images <InlineEquation ID="IEq102"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq102.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ X} ^{u}$$]]></EquationSource></InlineEquation> belong to the <InlineEquation ID="IEq103"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq103.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$j$$]]></EquationSource></InlineEquation>th label as follows:<Equation ID="Equ8"><EquationNumber>8</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_Equ8.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \hat{\mathbf{ y} }_u = \mathbf{ X} ^{u} \hat{\varvec{\beta }}_j \end{aligned}$$]]></EquationSource></Equation>Unlike group <Emphasis Type="Italic">lasso</Emphasis>, the above structural grouping penalty in (<InternalRef RefID="Equ7">7</InternalRef>) not only selects the groups of heterogeneous features, but also identifies the subgroup of homogeneous features within each selected group.</Para>
                <Para>Note that when <InlineEquation ID="IEq104"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq104.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\lambda _1 = 0$$]]></EquationSource></InlineEquation>, (<InternalRef RefID="Equ7">7</InternalRef>) reduces to the traditional <Emphasis Type="Italic">lasso</Emphasis> under the multi-label learning setting, and <InlineEquation ID="IEq105"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq105.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\lambda _2 = 0$$]]></EquationSource></InlineEquation> for the group <Emphasis Type="Italic">lasso</Emphasis> [<CitationRef CitationID="CR87">87</CitationRef>].</Para>
                <Para>As stated before, for problems where the heterogeneous features lie in a high-dimensional space with a sparsity structure and only a few common important features are shared by labels (tasks), regularized regression methods have been proposed to recover the shared sparsity structure across tasks. According to [<CitationRef CitationID="CR9">9</CitationRef>], if the labels are correlated we may be able to obtain an accurate prediction. In order to take advantage of correlations between the labels to boost multi-label annotation, MtBGS utilizes the curds and whey (C&amp;W) [<CitationRef CitationID="CR9">9</CitationRef>] method to boost the annotation performance.</Para>
                <Para>In order to tackle problems of overfitting in situations of large numbers of highly correlated predictors, Hastie et al. [<CitationRef CitationID="CR27">27</CitationRef>] introduce a quadratic penalty with a symmetric and positive definite matrix <InlineEquation ID="IEq106"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq106.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\Omega $$]]></EquationSource></InlineEquation> into the objective function. Taking into account the ability of <Emphasis Type="Italic">elastic net</Emphasis> which simultaneously conducts automatic variable selection and group selection of correlated variables, Clemmensen et al. [<CitationRef CitationID="CR15">15</CitationRef>] formulate (single-task) MLDA as SDA by imposing both <InlineEquation ID="IEq107"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq107.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _1$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq108"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq108.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _2$$]]></EquationSource></InlineEquation> norm regularization. Han et al. [<CitationRef CitationID="CR25">25</CitationRef>] extends single-task SDA to the multi-task problem with a method called multi-task sparse discriminant analysis (MtSDA). MtSDA uses a quadratic optimization approach for prediction of the multiple labels. In SDA, the identity matrix is commonly used as the penalty matrix. MtSDA introduces a large class of equicorrelation matrices with the identity matrix as a special case and indicates that an equicorrelation matrix has a grouping effect under some conditions.</Para>
              </Section2>
              <Section2 ID="Sec6">
                <Heading>Structured sparsity-inducing norm</Heading>
                <Para>Jenatton et al. propose a general definition of structured sparsity-inducing norm in [<CitationRef CitationID="CR31">31</CitationRef>,<CitationRef CitationID="CR32">32</CitationRef>], based on which many sparsity penalties, such as lasso, group lasso, and even the tree-guided group lasso [<CitationRef CitationID="CR37">37</CitationRef>], may be instantiated.</Para>
                <Para><Emphasis Type="Bold">Definition 1 </Emphasis> (<Emphasis Type="Italic">Structured sparsity-inducing norm</Emphasis>) Given a <InlineEquation ID="IEq109"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq109.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p$$]]></EquationSource></InlineEquation>-dimensional feature vector <InlineEquation ID="IEq110"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq110.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ x} $$]]></EquationSource></InlineEquation>, let us assume that the set of groups of features <InlineEquation ID="IEq111"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq111.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ G} =\{g_1,\ldots ,g_{|\mathcal{ G} |}\}$$]]></EquationSource></InlineEquation> is defined as a subset of the power set of <InlineEquation ID="IEq112"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq112.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\{1,\ldots ,p\}$$]]></EquationSource></InlineEquation>; the structured sparsity-inducing norm <InlineEquation ID="IEq113"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq113.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\Omega (\mathbf{ x} )$$]]></EquationSource></InlineEquation> is defined as<Equation ID="Equa1"><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_Equa1.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \Omega (\mathbf{ x} ) \equiv \sum _{g \in \mathcal{ G} }w_g ||\mathbf{ x} _g||_2 \end{aligned}$$]]></EquationSource></Equation>where <InlineEquation ID="IEq114"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq114.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ x} _g\in \mathbb{ R} ^{|g|}$$]]></EquationSource></InlineEquation> is the sub-vector of <InlineEquation ID="IEq115"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq115.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ x} $$]]></EquationSource></InlineEquation> for the input feature index in group <InlineEquation ID="IEq116"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq116.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$g$$]]></EquationSource></InlineEquation>, and <InlineEquation ID="IEq117"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq117.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$w_g$$]]></EquationSource></InlineEquation> is the predefined weight for group <InlineEquation ID="IEq118"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq118.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$g$$]]></EquationSource></InlineEquation>. </Para>
                <Para>In Definition 1, if we ignore weight <InlineEquation ID="IEq119"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq119.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$w_g$$]]></EquationSource></InlineEquation> and let <InlineEquation ID="IEq120"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq120.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ G} $$]]></EquationSource></InlineEquation> be the set of singleton, i.e., <InlineEquation ID="IEq121"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq121.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ G} =\{\{1\}, \{2\}, \ldots , \{p\}\}, \Omega (\mathbf{ x} )$$]]></EquationSource></InlineEquation> is instantiated to be an <InlineEquation ID="IEq122"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq122.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _1$$]]></EquationSource></InlineEquation>-norm of vector <InlineEquation ID="IEq123"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq123.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ x} $$]]></EquationSource></InlineEquation>.</Para>
              </Section2>
              <Section2 ID="Sec7">
                <Heading>Tree and graph-guided sparsity</Heading>
                <Para>In a typical setting, the input features lie in a high-dimensional space, and one is interested in selecting a small number of features that influence the annotation output. In order to handle more general structures such as tree or graph, various models that further extend group <Emphasis Type="Italic">lasso</Emphasis> and fused <Emphasis Type="Italic">lasso</Emphasis> are proposed [<CitationRef CitationID="CR13">13</CitationRef>, <CitationRef CitationID="CR37">37</CitationRef>]. Tree-guided group <Emphasis Type="Italic">lasso</Emphasis> [<CitationRef CitationID="CR37">37</CitationRef>] is a multi-task sparse feature selection method. The penalty of tree-guided group <Emphasis Type="Italic">lasso</Emphasis> is imposed on the output direction of the coefficient matrix <InlineEquation ID="IEq124"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq124.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ B} =(\varvec{\beta }_1,\ldots ,\varvec{\beta }_J)$$]]></EquationSource></InlineEquation>, with the goal of integrating the correlations among multiple labeled tags into the process of sparse feature selection. Tree-guided group <Emphasis Type="Italic">lasso</Emphasis> is formulated as to solve the following regularized regression model:<Equation ID="Equ9"><EquationNumber>9</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_Equ9.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \min _{\hat{\mathbf{ B} }}\;||\mathbf{ Y} - \mathbf{ X} \hat{\mathbf{ B} } ||_2^2 + \gamma \sum _{d=1}^{p}\sum _{v \in \mathcal{ G} _{\mathbf{ T} }} w_v ||\hat{\mathbf{ B} }_{v}^d||_2 \end{aligned}$$]]></EquationSource></Equation>For ease of notation, in (<InternalRef RefID="Equ9">9</InternalRef>), we let <InlineEquation ID="IEq125"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq125.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ B} ^d=\mathbf{ B} _{(d,:)}$$]]></EquationSource></InlineEquation>. We call <InlineEquation ID="IEq126"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq126.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\sum _{v \in \mathcal{ G} _{\mathbf{ T} }} w_v ||\mathbf{ B} _{v}^d||_2$$]]></EquationSource></InlineEquation> the penalty of tree-guided group <Emphasis Type="Italic">lasso</Emphasis>. Specifically, <InlineEquation ID="IEq127"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq127.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\sum _{g \in \mathcal{ G} _{\mathbf{ T} }} w_v ||\mathbf{ B} _{v}^d||_2$$]]></EquationSource></InlineEquation> is a special example of <InlineEquation ID="IEq128"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq128.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\Omega (\mathbf{ B} ^d)$$]]></EquationSource></InlineEquation> in Definition 1, when a set of groups <InlineEquation ID="IEq129"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq129.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ G} _{\mathbf{ T} }$$]]></EquationSource></InlineEquation> is induced from a tree structure <InlineEquation ID="IEq130"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq130.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ T} $$]]></EquationSource></InlineEquation> that is defined on vector <InlineEquation ID="IEq131"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq131.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ B} ^d$$]]></EquationSource></InlineEquation>. For the details of definitions of <InlineEquation ID="IEq132"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq132.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$w_v$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq133"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq133.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ T} $$]]></EquationSource></InlineEquation>, refer to [<CitationRef CitationID="CR37">37</CitationRef>].</Para>
                <Para>Furthermore, let us assume that the structure of the <InlineEquation ID="IEq134"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq134.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p$$]]></EquationSource></InlineEquation>-dimensional features for each image and video <InlineEquation ID="IEq135"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq135.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ x} _i$$]]></EquationSource></InlineEquation> is available as a graph <InlineEquation ID="IEq136"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq136.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$G$$]]></EquationSource></InlineEquation> with a set of nodes <InlineEquation ID="IEq137"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq137.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$V=\{1,2,\ldots ,p\}$$]]></EquationSource></InlineEquation> and a set of edges <InlineEquation ID="IEq138"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq138.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$E$$]]></EquationSource></InlineEquation>. Let <InlineEquation ID="IEq139"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq139.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$w_{ml}\ge 0$$]]></EquationSource></InlineEquation> denote the weight of the edge <InlineEquation ID="IEq140"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq140.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$e=(m,l)\in E$$]]></EquationSource></InlineEquation>, corresponding to the correlation between two features for nodes <InlineEquation ID="IEq141"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq141.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$m$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq142"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq142.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$l$$]]></EquationSource></InlineEquation>. With <InlineEquation ID="IEq143"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq143.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$w_{ml}\ge 0$$]]></EquationSource></InlineEquation>, we only consider the positively correlated features. In order to integrate graph <InlineEquation ID="IEq144"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq144.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$G$$]]></EquationSource></InlineEquation> into the process of structural feature selection and guide the regularization process, a penalty of graph-guided fusion (G<Superscript>2</Superscript>F) [<CitationRef CitationID="CR13">13</CitationRef>] <InlineEquation ID="IEq146"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq146.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\Omega _G(\varvec{\beta })$$]]></EquationSource></InlineEquation> is imposed, and the graph-guided feature selection framework is taken as follows:<Equation ID="Equ10"><EquationNumber>10</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_Equ10.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \min _{\hat{\beta }} \frac{1}{2}\;||\mathbf{ Y} _{(:,j)}-\mathbf{ X} \varvec{\hat{\beta }}||_2^2+\gamma \Omega _G(\varvec{\hat{\beta }})+\lambda ||\varvec{\hat{\beta }}||_1 \end{aligned}$$]]></EquationSource></Equation>where the G<Superscript>2</Superscript>F penalty <InlineEquation ID="IEq148"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq148.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\Omega _G(\varvec{\hat{\beta }})$$]]></EquationSource></InlineEquation> is defined as [<CitationRef CitationID="CR13">13</CitationRef>]:<Equation ID="Equ11"><EquationNumber>11</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_Equ11.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \Omega _G(\varvec{\hat{\beta }})=\sum _{e=(m,l)\in E,m<l}w_{ml}|{\hat{\beta }_m}-{\hat{\beta }_l}| \end{aligned}$$]]></EquationSource></Equation>where <InlineEquation ID="IEq149"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq149.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\hat{\beta _m}$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq150"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq150.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\hat{\beta _l}$$]]></EquationSource></InlineEquation> are estimated coefficients in <InlineEquation ID="IEq151"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq151.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\varvec{\hat{\beta }}$$]]></EquationSource></InlineEquation> corresponding to the selection coefficients of the <InlineEquation ID="IEq152"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq152.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$m$$]]></EquationSource></InlineEquation>th and <InlineEquation ID="IEq153"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq153.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$l$$]]></EquationSource></InlineEquation>th features, respectively. The weight <InlineEquation ID="IEq154"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq154.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$w_{ml}$$]]></EquationSource></InlineEquation> measures the fusion penalty for each edge <InlineEquation ID="IEq155"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq155.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$e=(m,l)$$]]></EquationSource></InlineEquation> such that <InlineEquation ID="IEq156"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq156.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\hat{\beta }_m$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq157"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq157.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\hat{\beta }_l$$]]></EquationSource></InlineEquation> are for highly correlated features with a larger <InlineEquation ID="IEq158"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq158.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$w_{ml}$$]]></EquationSource></InlineEquation> receiving a greater fusion effect. Therefore, the graph-guided fusion penalty in (<InternalRef RefID="Equ11">11</InternalRef>) encourages highly correlated features corresponding to a densely connected sub-network in <InlineEquation ID="IEq159"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq159.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$G$$]]></EquationSource></InlineEquation> to be jointly selected as the relevant features.</Para>
                <Para>Note that, if <InlineEquation ID="IEq160"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq160.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$w_{ml}=1$$]]></EquationSource></InlineEquation> for all <InlineEquation ID="IEq161"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq161.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$e=(m,l)$$]]></EquationSource></InlineEquation>, the penalty definition in (<InternalRef RefID="Equ11">11</InternalRef>) reduces to:<Equation ID="Equ12"><EquationNumber>12</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_Equ12.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \Omega _G(\varvec{\hat{\beta }})=\sum _{e=(m,l)\in E,m<l}|\hat{\beta }_m-\hat{\beta }_l| \end{aligned}$$]]></EquationSource></Equation>The standard fused <Emphasis Type="Italic">lasso</Emphasis> [<CitationRef CitationID="CR70">70</CitationRef>] penalty <InlineEquation ID="IEq162"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq162.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\sum _{j=1}^{J-1}|\hat{\beta }_{j+1}-\hat{\beta }_j|$$]]></EquationSource></InlineEquation> is a special case of (<InternalRef RefID="Equ12">12</InternalRef>). Furthermore, if the edge set <InlineEquation ID="IEq163"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq163.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$E$$]]></EquationSource></InlineEquation> consists of edges of pairs of regions, i.e., graph <InlineEquation ID="IEq164"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq164.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$G$$]]></EquationSource></InlineEquation> is defined to be a full connected graph, the G<Superscript>2</Superscript>F penalty in (<InternalRef RefID="Equ11">11</InternalRef>) is instantiated to be the grouping pursuit penalty [<CitationRef CitationID="CR64">64</CitationRef>].</Para>
              </Section2>
              <Section2 ID="Sec8">
                <Heading>Sparsity constrained tensor factorization</Heading>
                <Para>As introduced before, nuclear norm is recently proposed to discover the low-rank structure in a matrix and is denoted as the matrix-<Emphasis Type="Italic">lasso</Emphasis>. Unlike a matrix, a tensor is a multidimensional array. More formally, an <InlineEquation ID="IEq166"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq166.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$N$$]]></EquationSource></InlineEquation>-way or <InlineEquation ID="IEq167"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq167.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$N$$]]></EquationSource></InlineEquation>th order tensor is an element of the tensor product of <InlineEquation ID="IEq168"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq168.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$N$$]]></EquationSource></InlineEquation> vector spaces, each of which has its own coordinate system. Tensors include vectors and matrices as the first-order and the second-order special cases, respectively.</Para>
                <Para>Many data in signal processing, computer vision, and multimedia retrieval can be naturally represented as a tensor (i.e., multi-way arrays). Due to the ability of release of the overfitting problem in vector-based learning, Tao et al. propose a supervised tensor learning (STL) framework in [<CitationRef CitationID="CR69">69</CitationRef>]. Based on STL and its alternating projection optimization procedure, the generalization of support vector machines (SVM) is extended to support tensor machines (STM). Wu et al. [<CitationRef CitationID="CR79">79</CitationRef>] introduces a higher-order tensor framework for video analysis, which represents image frame, audio and text in video shots as data points by the three-order tensor (<Emphasis Type="Italic">Tensor</Emphasis>Shot). A transductive support tensor machine (TSTM) is then developed to learn and classify tensorshots in videos. Since the TensorShot dimension reduction method discovers the intrinsic tensorshot manifold before classification, the dimension-reduced tensorshots of training and test data sets show that TSTM not only is a natural extension of TSVM in tensor space, but also has a more powerful classification capability.</Para>
                <Para>Tensor factorization and decomposition have several advantages over the traditional two-order matrix factorizations even when most of the data are missing. Moreover, tensor decomposition and factorization explicitly exploit the high-order structures that are lost when the regular matrix factorization approaches such as PCA, SVD, NMF, and ICA are directly implemented to tensors.</Para>
                <Para>Two of the most commonly used tensor decompositions are the Tucker decomposition [<CitationRef CitationID="CR72">72</CitationRef>] and CANDECOMP/ PARAFAC (CANonical DECOMPosition or PARAllel FACtors model, abbreviated as CP) [<CitationRef CitationID="CR12">12</CitationRef>, <CitationRef CitationID="CR26">26</CitationRef>]. Both Tucker decomposition and CP are often considered as higher-order generalizations of the matrix singular value decomposition (SVD) or PCA.</Para>
                <Para>The Tucker decomposition is a form of higher-order PCA and decomposes a tensor into a core tensor multiplied (or transformed) by a matrix along each mode. The CP decomposition factorizes a tensor into a sum of component rank-one tensors. An interesting property of tensor decompositions by CP is the uniqueness under a weak condition. However, Tucker decompositions are not unique [<CitationRef CitationID="CR39">39</CitationRef>].</Para>
                <Para>When data or signals are inherently represented by nonnegative numbers, imposing nonnegativity constraints to tensor decomposition is shown to provide a physically meaningful interpretation. The block principal pivoting method is developed to solve nonnegativity constrained least squares (NNLS) problems for computing a low-rank nonnegative CP decomposition in [<CitationRef CitationID="CR36">36</CitationRef>].</Para>
              </Section2>
            </Section1>
            <Section1 ID="Sec9">
              <Heading>Multimedia spectral hashing with sparsity</Heading>
              <Para>The summarization of multimedia (images and videos) by much more compact sets of binary bits is of strong interest to many multimedia processing applications. The summaries, or <Emphasis Type="Italic">hashes</Emphasis>, can be used as a content identification to efficiently query similar images or videos in a database. Multimedia hashing is usually implemented in two steps: first, an intermediate code is obtained by the extraction of the representative features from images and videos; second, this intermediate code is quantized by a vector quantization to generate a binary code. In general, these two steps are independent of each other.</Para>
              <Para>The hashing algorithms seek compact binary codes of data points, so that the Hamming distance between codewords correlates with a semantic similarity such as the semantic hashing [<CitationRef CitationID="CR60">60</CitationRef>]. As one of the representative hashing approaches, the spectral hashing [<CitationRef CitationID="CR74">74</CitationRef>] is formulated as the problem of the semantic hashing as a form of graph partitioning and is designed as an efficient eigenvector solution to graph partition to generate the best binary code for data indexing. Spectral hashing finds a projection from Euclidean space to Hamming space and guarantees that data points close in Euclidean space are mapped to similar binary codewords. In order to avoid NP problems, spectral relaxation is implemented in spectral hashing to obtain a number of eigenvectors with the minimal eigenvalues from a Laplacian matrix by the PCA. However, the traditional PCA suffers from the fact that each principal component is a linear combination of all the original variables; thus, it is often difficult to interpret the results [<CitationRef CitationID="CR94">94</CitationRef>].</Para>
              <Para>Assume that we have a collection of <Emphasis Type="Italic">n p</Emphasis>-dimensional data points <InlineEquation ID="IEq169"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq169.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\{ (\mathbf{ x} _i)\in \mathbb{ R} ^p: i = 1,2,\ldots , n\}$$]]></EquationSource></InlineEquation>, where <InlineEquation ID="IEq170"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq170.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ x} _i$$]]></EquationSource></InlineEquation> represents the feature vector for the <InlineEquation ID="IEq171"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq171.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$i$$]]></EquationSource></InlineEquation>th data point, and <InlineEquation ID="IEq172"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq172.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p$$]]></EquationSource></InlineEquation> represents the dimension of the features from the training data. <InlineEquation ID="IEq173"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq173.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\Theta $$]]></EquationSource></InlineEquation> is an efficient indexing function to map each <InlineEquation ID="IEq174"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq174.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x_i$$]]></EquationSource></InlineEquation> from the <InlineEquation ID="IEq175"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq175.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p$$]]></EquationSource></InlineEquation>-dimensional Euclidean space to the <InlineEquation ID="IEq176"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq176.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$k$$]]></EquationSource></InlineEquation>-dimensional Hamming space <InlineEquation ID="IEq177"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq177.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$y_i$$]]></EquationSource></InlineEquation>; we define <InlineEquation ID="IEq178"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq178.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\Theta $$]]></EquationSource></InlineEquation> as follows:<Equation ID="Equ13"><EquationNumber>13</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_Equ13.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \Theta : \mathbf{ x} _i \in \mathbb{ R} ^p \rightarrow {y}_i \in {\{-1,1\}}^k \end{aligned}$$]]></EquationSource></Equation>The indexing function <InlineEquation ID="IEq179"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq179.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\Theta $$]]></EquationSource></InlineEquation> defined above
 has the following characteristics: (1) <InlineEquation ID="IEq180"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq180.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\Theta $$]]></EquationSource></InlineEquation> is a semantic hashing function. That is to say, if the distance between the <InlineEquation ID="IEq181"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq181.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$i$$]]></EquationSource></InlineEquation>th data point <InlineEquation ID="IEq182"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq182.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x_i$$]]></EquationSource></InlineEquation> and the <InlineEquation ID="IEq183"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq183.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$j$$]]></EquationSource></InlineEquation>th data point <InlineEquation ID="IEq184"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq184.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x_j$$]]></EquationSource></InlineEquation> is small in terms of the Euclidean distance in <InlineEquation ID="IEq185"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq185.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbb{ R} ^p$$]]></EquationSource></InlineEquation> space, their distance in terms of the Hamming distance in <InlineEquation ID="IEq186"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq186.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$${\{-1,1\}}^k $$]]></EquationSource></InlineEquation> space is also small; (2) <InlineEquation ID="IEq187"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq187.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\Theta $$]]></EquationSource></InlineEquation> tends to generate a compact binary code <InlineEquation ID="IEq188"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq188.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$y_i$$]]></EquationSource></InlineEquation>. Only a small number of bits are required to code the whole data set. Additionally, the coding is expected to be <Emphasis Type="Italic">structure preserving</Emphasis>, which means that only a limited subset of features is chosen to index the original data and to preserve the intrinsic structure hidden in the images and videos. The <Emphasis Type="Italic">n p</Emphasis>-dimensional data are written as <InlineEquation ID="IEq189"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq189.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ X} \in \mathbb{ R} ^{n\times p}$$]]></EquationSource></InlineEquation> and their corresponding <Emphasis Type="Italic">n k</Emphasis>-dimensional binary codes are written as <InlineEquation ID="IEq190"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq190.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ Y} \in \{-1,1\}^{n\times k}$$]]></EquationSource></InlineEquation>.</Para>
              <Para>Spectral hashing considers such requirements as a particular problem of thresholding a subset of eigenvectors of the Laplacian graph as follows [<CitationRef CitationID="CR74">74</CitationRef>]:<Equation ID="Equ14"><EquationNumber>14</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_Equ14.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned}& \text{ minimize:}\; \sum _{ij}\;\mathbf{ W} (i,j)\parallel y_i-y_j\parallel ^2 \\ & \text{ subject} \text{ to:}\quad y_i\in {\{-1,1\}}^k\nonumber \\ & \qquad \qquad \qquad \qquad \sum _i\; y_i=0\nonumber \\ & \qquad \qquad \qquad \qquad \frac{1}{n}\sum _i y_iy_i^{\mathrm{ T}=}\mathbf{ I} \nonumber \end{aligned}$$]]></EquationSource></Equation>where <InlineEquation ID="IEq191"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq191.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ W} \in R^{N \times N}$$]]></EquationSource></InlineEquation> is the similarity matrix and <InlineEquation ID="IEq192"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq192.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ W} (i,j)=\exp (-\parallel x_i-x_j\parallel ^2/\epsilon ^2)$$]]></EquationSource></InlineEquation>. <InlineEquation ID="IEq193"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq193.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$y_i\in {\{-1,1\}}^k$$]]></EquationSource></InlineEquation> guarantees that the indexing code is binary; <InlineEquation ID="IEq194"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq194.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\sum\nolimits _i y_i=0$$]]></EquationSource></InlineEquation> guarantees that each bit has 50% to be <InlineEquation ID="IEq195"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq195.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$-1$$]]></EquationSource></InlineEquation> or 1 when we choose 0 as a threshold; and <InlineEquation ID="IEq196"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq196.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\frac{1}{n}\sum\nolimits _i\; y_iy_i^{\mathrm{ T}=\mathbf{ I} }$$]]></EquationSource></InlineEquation> guarantees that the bits are uncorrelated to each other.</Para>
              <Para>Though finding the solution to (<InternalRef RefID="Equ14">14</InternalRef>) is an NP-complete problem, by introducing the Laplacian matrix and removing the constraint <InlineEquation ID="IEq197"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq197.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$y_i\in {\{-1,1\}}^m$$]]></EquationSource></InlineEquation>, [<CitationRef CitationID="CR74">74</CitationRef>] turns it into a graph partition so that the solution is simply the <InlineEquation ID="IEq198"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq198.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$m$$]]></EquationSource></InlineEquation> eigenvectors of Laplacian matrix <Emphasis Type="Bold">L</Emphasis> with the minimal eigenvalue, where <InlineEquation ID="IEq199"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq199.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ L} =\mathbf{ D} -\mathbf{ W} $$]]></EquationSource></InlineEquation> and <Emphasis Type="Bold">D</Emphasis> is a diagonal matrix with its entries <InlineEquation ID="IEq200"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq200.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ D} (i,i)=\Sigma _j^n \mathbf{ W} (i,j)$$]]></EquationSource></InlineEquation>. As a result, the solution to (<InternalRef RefID="Equ14">14</InternalRef>) is transformed into Laplacian eigenmap dimension reduction and thus PCA is directly used in spectral hashing [<CitationRef CitationID="CR74">74</CitationRef>].</Para>
              <Para>However, PCA suffers from the fact that it is difficult to interpret the result of dimension reduction due to the lack of sparseness of the principal vectors, i.e., all the data coordinates participate in the linear combination [<CitationRef CitationID="CR94">94</CitationRef>]. Usually, the codebook is often <Emphasis Type="Italic">over-complete</Emphasis>. Intuitively, given a data point, only a limited subset of features is sufficient to represent this data point. Take an image as an example. Color-related visual words could be salient features to represent an image of rainbow and shape-related visual words are better to distinguish an image of a car from others.</Para>
              <Para>Motivated by the nature of the grouping effect in <Emphasis Type="Italic">elastic net</Emphasis>, sparse PCA in [<CitationRef CitationID="CR94">94</CitationRef>] transforms PCA as a <Emphasis Type="Italic">nonconvex</Emphasis> regression-type optimization problem via the <Emphasis Type="Italic">elastic net</Emphasis> penalty to estimate PCs with sparse loadings. Due to its nonconvex solution of sparse PCA in [<CitationRef CitationID="CR94">94</CitationRef>], a <Emphasis Type="Italic">convex relaxation</Emphasis> method is developed [<CitationRef CitationID="CR16">16</CitationRef>] to achieve a globally optimal solution to sparse PCA. In essence, the convex relaxation method is a semi-definite program. Since sparse PCA maximizes the variance explained by a particular linear combination of the input variables and constrains the number of nonzero coefficients in this combination, Shao et al. [<CitationRef CitationID="CR63">63</CitationRef>] introduces the idea of sparse PCA [<CitationRef CitationID="CR94">94</CitationRef>] into spectral hashing for data indexing and calls this approach as SSH. The proposed SSH not only achieves dimensionality reduction, but also reduces the number of the explicitly used features during indexing. In order to resolve the <Emphasis Type="Italic">out-of-sample</Emphasis> problem, same as the parameter-sensitive hashing [<CitationRef CitationID="CR62">62</CitationRef>], here SSH introduces boosting similarity sensitive coding (Boost SSC) into SSH in order to find a more practical threshold for the quantization of the binary code.</Para>
              <Para>As discussed before, The structural information is of great significance in many applications. For example, due to the importance of local features in face recognition, NMF is used to learn the facial parts of the images and shows a better performance than other holistic representation methods such as PCA and vector quantization. It is a quite interesting issue to embed structural <Emphasis Type="Italic">prior</Emphasis> into the compact binary codes by the replacement of sparse PCA with the structured sparse principal component analysis [<CitationRef CitationID="CR32">32</CitationRef>] in SSH.</Para>
            </Section1>
            <Section1 ID="Sec10">
              <Heading>Cross-media analysis and retrieval</Heading>
              <Para>As an example to showcase the applications, there are huge collections of heterogeneous media data from microblog, mobile phone, social networking Web sites, and news media Web sites. These heterogeneous media data are integrated together to reflect social behaviors. Different from the traditional structural and nonstructural data, these heterogeneous media data are referred to as <Emphasis Type="Italic">cross-media</Emphasis> with three properties: (1) <Emphasis Type="Italic">Cross-modality</Emphasis>: heterogeneous features are obtained from data in different modalities; (2) <Emphasis Type="Italic">Cross-domain</Emphasis>: heterogeneous features may be obtained from different domains (e.g., from both target domain and auxiliary domain for problems such as topic modeling, multimedia annotation); (3) <Emphasis Type="Italic">Cross space</Emphasis>: the virtual world (cyberspace) and the real-world (reality) complement each other [<CitationRef CitationID="CR23">23</CitationRef>]. Here, sparse representation also plays an important role [<CitationRef CitationID="CR78">78</CitationRef>]. For example, all of the heterogenous features from different views can be unified as a consensus representation for the cross-media semantics, and factorized into a latent spaces with a structured sparsity that can be exploited to simultaneously learn a low-dimensional latent space [<CitationRef CitationID="CR33">33</CitationRef>]; traditional canonical correlation analysis (CCA) can be extended to sparse CCA and therefore learn the multi-modal correlations of media objects [<CitationRef CitationID="CR78">78</CitationRef>]; graphical <Emphasis Type="Italic">lasso</Emphasis> can be applied to discover the network community [<CitationRef CitationID="CR22">22</CitationRef>]. Moreover, different from the traditional content-based single media retrieval systems, in content-based cross-media retrieval system, multimedia objects are retrieved uniformly. The query examples and retrieval results do not need to be of the same media type. For example, users can query images by submitting either an audio example or an image example in a cross-media retrieval system [<CitationRef CitationID="CR84">84</CitationRef>].</Para>
            </Section1>
            <Section1 ID="Sec11">
              <Heading>Computational issues</Heading>
              <Section2 ID="Sec12">
                <Heading>Complexity</Heading>
                <Para>In principle, the <InlineEquation ID="IEq201"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq201.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _1$$]]></EquationSource></InlineEquation>-norm [<CitationRef CitationID="CR71">71</CitationRef>] and the structured sparsity-inducing norm [<CitationRef CitationID="CR32">32</CitationRef>] penalized sparse feature selection problems can be solved by the generic optimization solvers. For example, the sparse penalized problems are first posed as a second-order cone programming or a quadratic programming [<CitationRef CitationID="CR71">71</CitationRef>] formulation and then solved by the interior-point methods. However, such approaches are expensive even for the problems of a moderate size.</Para>
                <Para>Recently, inspired by Nesterov’s method [<CitationRef CitationID="CR53">53</CitationRef>] and the fast iterative shrinkage-thresholding algorithm (FISTA) [<CitationRef CitationID="CR3">3</CitationRef>], the first-order gradient approach has been widely used to solve optimization problems with a convex loss function (e.g., least-square loss) and nonsmooth penalty (e.g., the <InlineEquation ID="IEq202"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq202.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _1$$]]></EquationSource></InlineEquation>-norm). It has been shown that the first-order gradient approach can achieve the optimal convergence rate <InlineEquation ID="IEq203"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq203.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$O(\frac{1}{\sqrt{\epsilon }})$$]]></EquationSource></InlineEquation> for a desired accuracy <InlineEquation ID="IEq204"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq204.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\epsilon $$]]></EquationSource></InlineEquation>. FISTA only deals with relatively simple and well-separated nonsmooth penalties, such as <InlineEquation ID="IEq205"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq205.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _1$$]]></EquationSource></InlineEquation>-norm, group <Emphasis Type="Italic">lasso</Emphasis> penalty. Though the pathwise coordinate descent [<CitationRef CitationID="CR21">21</CitationRef>] method has been widely applied to solve many complex sparsity penalties, this method may get stuck and does not converge to the exact solution for certain nonseparated penalties, such as the fused <Emphasis Type="Italic">lasso</Emphasis> [<CitationRef CitationID="CR70">70</CitationRef>] penalty.</Para>
                <Para>In order to efficiently solve the complex sparsity penalized problems, e.g., graph-guided fusion [<CitationRef CitationID="CR13">13</CitationRef>] and tree-structured groups [<CitationRef CitationID="CR30">30</CitationRef>] penalties, many proximal gradient methods [<CitationRef CitationID="CR13">13</CitationRef>, <CitationRef CitationID="CR30">30</CitationRef>] are developed. The main challenges are to find an approximation of the nonseparable and nonsmooth structured sparsity-inducing norm. The smoothing proximal gradient (SPG) [<CitationRef CitationID="CR13">13</CitationRef>] method is an efficient proximal gradient method for general structured sparse feature selection methods. According to the smoothing method in [<CitationRef CitationID="CR54">54</CitationRef>], SPG first finds a separable and smooth approximation of <InlineEquation ID="IEq206"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq206.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\Omega (\mathbf{ x} )$$]]></EquationSource></InlineEquation>, and then solves this transformed simple <InlineEquation ID="IEq207"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq207.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _1$$]]></EquationSource></InlineEquation>-norm penalized sparse learning problem by the FISTA approach. It has been proven that SPG achieves a convergence rate of <InlineEquation ID="IEq208"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq208.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$O(\frac{1}{\epsilon })$$]]></EquationSource></InlineEquation> for a desired accuracy <InlineEquation ID="IEq209"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq209.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\epsilon $$]]></EquationSource></InlineEquation> [<CitationRef CitationID="CR13">13</CitationRef>], which is faster than the subgradient method with a convergence rate of <InlineEquation ID="IEq210"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq210.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$O(\frac{1}{\epsilon ^2})$$]]></EquationSource></InlineEquation>. The gap between <InlineEquation ID="IEq211"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq211.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$O(\frac{1}{\epsilon })$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq212"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq212.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$O(\frac{1}{\sqrt{\epsilon }})$$]]></EquationSource></InlineEquation> [<CitationRef CitationID="CR3">3</CitationRef>] is due to the approximation of the nonseparable and nonsmooth structured sparsity-inducing norm penalty.</Para>
                <Para>The most common theoretical approach to understanding the behavior of the algorithms is the worst-case analysis. However, there are many algorithms that work exceedingly well in practice, but are known to perform poorly in the worst-case analysis or lack a good worst-case analysis according to the theory of the smoothed analysis [<CitationRef CitationID="CR67">67</CitationRef>]. In Tibshirani’s original paper [<CitationRef CitationID="CR71">71</CitationRef>], he has found that the model selection problem with <InlineEquation ID="IEq213"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq213.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _1$$]]></EquationSource></InlineEquation>-norm usually can be solved with the iteration number within the range of (0.5<InlineEquation ID="IEq214"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq214.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p$$]]></EquationSource></InlineEquation>, 0.75<InlineEquation ID="IEq215"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq215.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p$$]]></EquationSource></InlineEquation>) in practice. Take the algorithm of MtBGS [<CitationRef CitationID="CR77">77</CitationRef>], for example, the run-time performance of the regression model with the structural grouping sparsity is also very efficient when implemented as the <Emphasis Type="Italic">cyclic coordinate descent</Emphasis> method. From the description of the coordinate descent by the Gauss–Seidel method, we see that for a complete cycle through all the coordinates, it takes <InlineEquation ID="IEq216"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq216.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$O(k)$$]]></EquationSource></InlineEquation> operations, where <InlineEquation ID="IEq217"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq217.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$k$$]]></EquationSource></InlineEquation> is the number of the nonzero elements when the sparsity of the data is considered. Thus the complexity of the regression model with the structural grouping sparsity is roughly <InlineEquation ID="IEq218"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq218.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$O(p\times n)$$]]></EquationSource></InlineEquation>.</Para>
              </Section2>
              <Section2 ID="Sec13">
                <Heading>Consistent selection and nonconvex relaxation</Heading>
                <Para>Given high-dimensional heterogenous features and their corresponding semantics, the question is whether there is a true model and whether the true model can select all the features for the representation of their semantics; that is to say whether the selected features are <Emphasis Type="Italic">consistent</Emphasis> with the data. For example, after <InlineEquation ID="IEq219"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq219.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p$$]]></EquationSource></InlineEquation>-dimensional heterogenous features <InlineEquation ID="IEq220"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq220.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbf{ x} _i =(x_{i1},\ldots ,x_{ip})^{\mathrm{ T} } \in {\mathbb{ R} }^p$$]]></EquationSource></InlineEquation> are extracted from the <InlineEquation ID="IEq221"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq221.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$i$$]]></EquationSource></InlineEquation>th image or video, assume a <Emphasis Type="Italic">true model</Emphasis> (<Emphasis Type="Italic">an oracle</Emphasis>) is found to select all the discriminate features for the <InlineEquation ID="IEq222"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq222.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$i$$]]></EquationSource></InlineEquation>th image or video and the coefficients of the selected features are denoted as <InlineEquation ID="IEq223"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq223.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$A=\{m: \beta _m^*\ne 0 \}$$]]></EquationSource></InlineEquation> and<InlineEquation ID="IEq224"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq224.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$|A|=p_0<p$$]]></EquationSource></InlineEquation>. If <InlineEquation ID="IEq225"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq225.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\hat{\beta }(\delta )$$]]></EquationSource></InlineEquation> is the estimated coefficients produced by a fitting procedure <InlineEquation ID="IEq226"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq226.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\delta $$]]></EquationSource></InlineEquation>, the question is whether the selection result by <InlineEquation ID="IEq227"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq227.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\delta $$]]></EquationSource></InlineEquation> is the same as that of the result by the true model. According to [<CitationRef CitationID="CR20">20</CitationRef>, <CitationRef CitationID="CR95">95</CitationRef>], <InlineEquation ID="IEq228"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq228.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\delta $$]]></EquationSource></InlineEquation> is called an <Emphasis Type="Italic">oracle</Emphasis> procedure if <InlineEquation ID="IEq229"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq229.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\hat{\beta }(\delta )$$]]></EquationSource></InlineEquation> has the following oracle properties: <InlineEquation ID="IEq230"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq230.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\delta $$]]></EquationSource></InlineEquation> can identify the correct subset, <InlineEquation ID="IEq231"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq231.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\{m:\hat{\beta }_j \ne 0\}=A$$]]></EquationSource></InlineEquation>; and <InlineEquation ID="IEq232"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq232.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\delta $$]]></EquationSource></InlineEquation> has the optimal estimate rate.</Para>
                <Para>When we talk about the inconsistent selection, it means that the correct sparse subset of the relevant variables cannot be identified asymptotically with a large probability. Without loss of generality, assume that the first <InlineEquation ID="IEq233"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq233.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$q$$]]></EquationSource></InlineEquation> elements of vector <InlineEquation ID="IEq234"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq234.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\hat{\beta }(\delta )$$]]></EquationSource></InlineEquation> are nonzeroes and that others are zeros. Let <InlineEquation ID="IEq235"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq235.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\hat{\beta }(\delta )_{(1)}=(\hat{\beta }(\delta )_1,\ldots ,\hat{\beta }(\delta )_q)$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq236"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq236.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\hat{\beta }(\delta )_{(2)}=(\hat{\beta }(\delta )_{q+1},\ldots ,\hat{\beta }(\delta )_p)$$]]></EquationSource></InlineEquation>, then <InlineEquation ID="IEq237"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq237.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\hat{\beta }(\delta )_{(1)}\ne 0$$]]></EquationSource></InlineEquation> element-wise and <InlineEquation ID="IEq238"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq238.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\hat{\beta }(\delta )_{(2)}= 0$$]]></EquationSource></InlineEquation>. Recent work [<CitationRef CitationID="CR50">50</CitationRef>, <CitationRef CitationID="CR88">88</CitationRef>, <CitationRef CitationID="CR90">90</CitationRef>, <CitationRef CitationID="CR95">95</CitationRef>] has given some conditions for a consistent selection in <Emphasis Type="Italic">lasso</Emphasis>. It has been shown that in the classical case when <InlineEquation ID="IEq239"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq239.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq240"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq240.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$q$$]]></EquationSource></InlineEquation> are fixed, a simple condition, called the <Emphasis Type="Italic">Irrepresentable Condition</Emphasis> (IC) on the generated covariance matrices, is necessary and sufficient for the model selection consistency by Lasso. An <Emphasis Type="Italic">Elastic Irrepresentable Condition</Emphasis> (EIC) is given in [<CitationRef CitationID="CR34">34</CitationRef>] to show that Elastic Net can consistently select the true model if and only if EIC is satisfied. One of the consistency conditions of group <Emphasis Type="Italic">lasso</Emphasis> is given in [<CitationRef CitationID="CR44">44</CitationRef>].</Para>
                <Para>Many of penalty regularizers can be developed for feature selection; however, Fan and Li [<CitationRef CitationID="CR20">20</CitationRef>] provide a deep insight into how to choose a penalty function. In their analysis, they encourage choosing penalty functions satisfying certain mathematical conditions such that the resulting penalized likelihood estimate possesses the properties of sparsity, continuity and unbiasedness. These mathematical conditions imply that the penalty function must be singular at the origin and <Emphasis Type="Italic">nonconvex</Emphasis> over <InlineEquation ID="IEq241"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq241.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$(0,\infty )$$]]></EquationSource></InlineEquation>. Accordingly, a number of nonconvex relaxation approaches, such as the smoothly clipped absolute deviation (SCAD) penalty [<CitationRef CitationID="CR20">20</CitationRef>] and the minimax concave (MC) penalty [<CitationRef CitationID="CR89">89</CitationRef>], have been proposed. Shi et al. [<CitationRef CitationID="CR66">66</CitationRef>] treats the MC penalty as a nonconvex relaxation of the <InlineEquation ID="IEq242"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq242.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$l_0$$]]></EquationSource></InlineEquation> penalty for dictionary learning and achieves a robust and sparse representation.</Para>
              </Section2>
              <Section2 ID="Sec14">
                <Heading>Stability of selection</Heading>
                <Para>Two key issues in the design of multimedia learning algorithms are bias and variance, and one needs to find a trade-off between them. Therefore, besides a good accuracy of sparse learning algorithms, we also desire the property of a low variance, or a high <Emphasis Type="Italic">stability</Emphasis>. In a broad sense, stability means that an algorithm is well posed, so that given two very similar data sets, the algorithm’s performance varies little [<CitationRef CitationID="CR80">80</CitationRef>]. In the landmark work about stability in [<CitationRef CitationID="CR6">6</CitationRef>], stability is explored based on the sensitivity analysis, which aims at determining how much the variation of the data can influence the performance of a learning algorithm. Two sources of the instability come from the sampling mechanism used to generate the input data and the noise in the input data, respectively. The former is mainly investigated by the sampling randomness, for example using the re-sampling methods [<CitationRef CitationID="CR77">77</CitationRef>] of cross validation, jackknife [<CitationRef CitationID="CR51">51</CitationRef>], and bootstrap [<CitationRef CitationID="CR18">18</CitationRef>], whereas the latter is usually referred to as the perturbation analysis.</Para>
                <Para>In [<CitationRef CitationID="CR6">6</CitationRef>], stability is taken as an avenue for proving the generalization performance of an algorithm. More specifically, stability is a principled way of establishing bounds on the difference between the empirical and the generalization errors. In statistical learning theory, Vapnik–Chervonenkis dimension (VC-dimension) is a measure of the capacity or complexity of a statistical classification algorithm. It has been proven that an algorithm having a search space of a finite VC-dimension is stable in the sense that its stability is bounded by its VC-dimension [<CitationRef CitationID="CR35">35</CitationRef>]. Therefore, Bousquet and Elisseeff [<CitationRef CitationID="CR6">6</CitationRef>] use the stability to derive the generalization error bound based on the empirical error and the leave-one-out error.</Para>
                <Para><Emphasis Type="Italic">lasso</Emphasis> [<CitationRef CitationID="CR71">71</CitationRef>] is known to have the stability problems [<CitationRef CitationID="CR8">8</CitationRef>]. Although its predictive performance is not disastrous, the selected predictor may vary a lot. Typically, given two correlated variables, <Emphasis Type="Italic">lasso</Emphasis> only selects one of the two, at random. In [<CitationRef CitationID="CR80">80</CitationRef>], Xu et al. prove that sparsity and stability are at odds with each other. They show that sparse algorithms are not stable: if an algorithm encourages sparsity, e.g., <Emphasis Type="Italic">lasso</Emphasis>, then its sensitivity to small perturbations of the input data remains bounded away from zero. Based on the uniform stability [<CitationRef CitationID="CR6">6</CitationRef>] properties, they have proven that a sparse algorithm can have nonunique optimal solutions and is therefore ill-posed.</Para>
                <Para>Breiman [<CitationRef CitationID="CR8">8</CitationRef>] has shown that the unstable linear regression process can be stabilized by perturbing the data, getting a new predictor sequence and then averaging over many such predictor sequences. Thus, how to develop a stable feature selection model by the perturbing technique is an open problem. Furthermore, Xu et al. also prove that the stability bound of the elastic net [<CitationRef CitationID="CR93">93</CitationRef>] coincides with that of an <InlineEquation ID="IEq243"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEq243.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _2$$]]></EquationSource></InlineEquation> regularization algorithm and thus has the uniform stability. Consequently, it is interesting to explore the stability of the recent proposed sparse models, such as group lasso [<CitationRef CitationID="CR87">87</CitationRef>], SDA [<CitationRef CitationID="CR15">15</CitationRef>], and even the structured sparsity penalized regression models.</Para>
              </Section2>
            </Section1>
            <Section1 ID="Sec15">
              <Heading>Conclusion</Heading>
              <Para>This paper surveys some recent research work on heterogeneous feature selection, representation, and hashing for images and videos after the introduction of sparsity constraints. The utilization of sparsity in images and videos make multimedia understanding and retrieval interpretable. However, how to define the intrinsic spatial–temporal structure in images and videos and then to apply an appropriate sparse penalty is still an open problem for multimedia research.</Para>
            </Section1>
          </Body>
          <BodyRef FileRef="BodyRef/PDF/13735_2012_Article_1.pdf" TargetType="OnlinePDF"/>
          <BodyRef FileRef="BodyRef/PDF/13735_2012_1_TEX.zip" TargetType="TEX"/>
          <ArticleBackmatter>
            <Acknowledgments>
              <Heading>Acknowledgments</Heading>
              <SimplePara>This work was supported in part by National Basic Research Program of China (2012CB316400), NSFC (No. 90920303, 61070068) and the Fundamental Research Funds for the Central Universities. Zhongfei Zhang was also supported in part by US NSF (IIS-0812114, CCF-1017828). Yahong Han was also supported in part by Scholarship Award for Excellent Doctoral Student granted by the Ministry of Education, China.</SimplePara>
            </Acknowledgments>
            <Bibliography ID="Bib1">
              <Heading>References</Heading>
              <Citation ID="CR1">
                <CitationNumber>1.</CitationNumber>
                <BibUnstructured>Argyriou A, Evgeniou T, Pontil M (2007) Multi-task feature learning. In: Advances in neural information processing systems (NIPS)</BibUnstructured>
              </Citation>
              <Citation ID="CR2">
                <CitationNumber>2.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>FR</Initials>
                    <FamilyName>Bach</FamilyName>
                  </BibAuthorName>
                  <Year>2008</Year>
                  <ArticleTitle Language="En">Consistency of the group Lasso and multiple kernel learning</ArticleTitle>
                  <JournalTitle>J Mach Learn Res</JournalTitle>
                  <VolumeID>9</VolumeID>
                  <FirstPage>1179</FirstPage>
                  <LastPage>1225</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>2417268</Handle>
                  </Occurrence>
                  <Occurrence Type="ZLBID">
                    <Handle>1225.68147</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Bach FR (2008) Consistency of the group Lasso and multiple kernel learning. J Mach Learn Res 9:1179–1225</BibUnstructured>
              </Citation>
              <Citation ID="CR3">
                <CitationNumber>3.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Beck</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Teboulle</FamilyName>
                  </BibAuthorName>
                  <Year>2009</Year>
                  <ArticleTitle Language="En">A fast iterative shrinkage-thresholding algorithm for linear inverse problems</ArticleTitle>
                  <JournalTitle>SIAM J Imaging Sci</JournalTitle>
                  <VolumeID>2</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>193</FirstPage>
                  <LastPage>202</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>2486527</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1137/080716542</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Beck A, Teboulle M (2009) A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM J Imaging Sci 2(1):193–202</BibUnstructured>
              </Citation>
              <Citation ID="CR4">
                <CitationNumber>4.</CitationNumber>
                <BibUnstructured>Berchtold S, Bohm C, Jagadish HV, Kriegel HP, Sander J (2000) Independent quantization: an index compression technique for high-dimensional data spaces. In: Proceedings of international conference on data engineering (ICDE), pp 577–588</BibUnstructured>
              </Citation>
              <Citation ID="CR5">
                <CitationNumber>5.</CitationNumber>
                <BibBook>
                  <BibAuthorName>
                    <Initials>C</Initials>
                    <FamilyName>Berge</FamilyName>
                  </BibAuthorName>
                  <Year>1973</Year>
                  <BookTitle>Graphs and hypergraphs</BookTitle>
                  <PublisherName>North-Holland</PublisherName>
                  <PublisherLocation>Amsterdam</PublisherLocation>
                  <Occurrence Type="ZLBID">
                    <Handle>0254.05101</Handle>
                  </Occurrence>
                </BibBook>
                <BibUnstructured>Berge C (1973) Graphs and hypergraphs. North-Holland, Amsterdam</BibUnstructured>
              </Citation>
              <Citation ID="CR6">
                <CitationNumber>6.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>O</Initials>
                    <FamilyName>Bousquet</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Elisseeff</FamilyName>
                  </BibAuthorName>
                  <Year>2002</Year>
                  <ArticleTitle Language="En">Stability and generalization</ArticleTitle>
                  <JournalTitle>J Mach Learn Res</JournalTitle>
                  <VolumeID>2</VolumeID>
                  <FirstPage>499</FirstPage>
                  <LastPage>526</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>1929416</Handle>
                  </Occurrence>
                  <Occurrence Type="ZLBID">
                    <Handle>1007.68083</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Bousquet O, Elisseeff A (2002) Stability and generalization. J Mach Learn Res 2:499–526</BibUnstructured>
              </Citation>
              <Citation ID="CR7">
                <CitationNumber>7.</CitationNumber>
                <BibUnstructured>Breiman L (1995) Better subset regression using the nonnegative garrote. Technometrics 373–384</BibUnstructured>
              </Citation>
              <Citation ID="CR8">
                <CitationNumber>8.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>L</Initials>
                    <FamilyName>Breiman</FamilyName>
                  </BibAuthorName>
                  <Year>1996</Year>
                  <ArticleTitle Language="En">Heuristics of instability and stabilization in model selection</ArticleTitle>
                  <JournalTitle>Ann Stat</JournalTitle>
                  <VolumeID>24</VolumeID>
                  <IssueID>6</IssueID>
                  <FirstPage>2350</FirstPage>
                  <LastPage>2383</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>1425957</Handle>
                  </Occurrence>
                  <Occurrence Type="ZLBID">
                    <Handle>0867.62055</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1214/aos/1032181158</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Breiman L (1996) Heuristics of instability and stabilization in model selection. Ann Stat 24(6):2350–2383</BibUnstructured>
              </Citation>
              <Citation ID="CR9">
                <CitationNumber>9.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>L</Initials>
                    <FamilyName>Breiman</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Friedman</FamilyName>
                  </BibAuthorName>
                  <Year>1997</Year>
                  <ArticleTitle Language="En">Predicting multivariate responses in multiple linear regression</ArticleTitle>
                  <JournalTitle>J R Stat Soc Ser B (Methodological)</JournalTitle>
                  <VolumeID>59</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>3</FirstPage>
                  <LastPage>54</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>1436554</Handle>
                  </Occurrence>
                  <Occurrence Type="ZLBID">
                    <Handle>0897.62068</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1111/1467-9868.00054</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Breiman L, Friedman J (1997) Predicting multivariate responses in multiple linear regression. J R Stat Soc Ser B (Methodological) 59(1):3–54</BibUnstructured>
              </Citation>
              <Citation ID="CR10">
                <CitationNumber>10.</CitationNumber>
                <BibUnstructured>Candes EJ, Donoho DL (2002) New tight frames of curvelets and optimal representations of objects with piecewise C2 singularities. In: Communications on pure and applied mathematics, pp  219–266</BibUnstructured>
              </Citation>
              <Citation ID="CR11">
                <CitationNumber>11.</CitationNumber>
                <BibUnstructured>Cao L, Luo J, Liang F, Huang T (2009) Heterogeneous feature machines for visual recognition. In: Proceedings of the IEEE internation conference on computer vision (ICCV), pp 1095–1102</BibUnstructured>
              </Citation>
              <Citation ID="CR12">
                <CitationNumber>12.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>JD</Initials>
                    <FamilyName>Carroll</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>JJ</Initials>
                    <FamilyName>Chang</FamilyName>
                  </BibAuthorName>
                  <Year>1970</Year>
                  <ArticleTitle Language="En">Analysis of individual differences in multidimensional scaling via an N-way generalization of Eckart-Young decomposition</ArticleTitle>
                  <JournalTitle>Psychometrika</JournalTitle>
                  <VolumeID>35</VolumeID>
                  <IssueID>3</IssueID>
                  <FirstPage>283</FirstPage>
                  <LastPage>319</LastPage>
                  <Occurrence Type="ZLBID">
                    <Handle>0202.19101</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1007/BF02310791</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Carroll JD, Chang JJ (1970) Analysis of individual differences in multidimensional scaling via an N-way generalization of Eckart-Young decomposition. Psychometrika 35(3):283–319</BibUnstructured>
              </Citation>
              <Citation ID="CR13">
                <CitationNumber>13.</CitationNumber>
                <BibUnstructured>Chen X, Lin Q, Kim S, Carbonell J, Xing E (2010) Efficient proximal gradient method for general structured sparse learning. Preprint, arXiv:1005.4717</BibUnstructured>
              </Citation>
              <Citation ID="CR14">
                <CitationNumber>14.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>B</Initials>
                    <FamilyName>Cheng</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Yang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>S</Initials>
                    <FamilyName>Yan</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>Y</Initials>
                    <FamilyName>Fu</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>T</Initials>
                    <FamilyName>Huang</FamilyName>
                  </BibAuthorName>
                  <Year>2010</Year>
                  <ArticleTitle Language="En">Learning with <InlineEquation ID="IEqr1"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEqr1.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$l_1$$]]></EquationSource></InlineEquation>-graph for image analysis</ArticleTitle>
                  <JournalTitle>IEEE Trans Image Process (TIP)</JournalTitle>
                  <VolumeID>19</VolumeID>
                  <IssueID>4</IssueID>
                  <FirstPage>858</FirstPage>
                  <LastPage>866</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>2752089</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TIP.2009.2038764</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Cheng B, Yang J, Yan S, Fu Y, Huang T (2010) Learning with <InlineEquation ID="IEqr2"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEqr2.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$l_1$$]]></EquationSource></InlineEquation>-graph for image analysis. IEEE Trans Image Process (TIP) 19(4):858–866</BibUnstructured>
              </Citation>
              <Citation ID="CR15">
                <CitationNumber>15.</CitationNumber>
                <BibUnstructured>Clemmensen L, Hastie T, Ersbøll B (2008) Sparse discriminant analysis. <ExternalRef><RefSource>http://www-stat.stanford.edu/hastie/Papers/</RefSource><RefTarget Address="http://www-stat.stanford.edu/hastie/Papers/" TargetType="URL"/></ExternalRef>
                </BibUnstructured>
              </Citation>
              <Citation ID="CR16">
                <CitationNumber>16.</CitationNumber>
                <BibUnstructured>d’Aspremont A, El Ghaoui L, Jordan MI, Lanckriet GRG (2004) A direct formulation for sparse PCA using semidefinite programming. In: Advances in neural information processing systems (NIPS)</BibUnstructured>
              </Citation>
              <Citation ID="CR17">
                <CitationNumber>17.</CitationNumber>
                <BibUnstructured>Datar M, Immorlica N, Indyk P, Mirrokni VS (2004) Locality-sensitive hashing scheme based on p-stable distributions. In: Proceedings of annual symposium on computational geometry,  pp 253–262</BibUnstructured>
              </Citation>
              <Citation ID="CR18">
                <CitationNumber>18.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>B</Initials>
                    <FamilyName>Efron</FamilyName>
                  </BibAuthorName>
                  <Year>1979</Year>
                  <ArticleTitle Language="En">Bootstrap methods: another look at the jackknife</ArticleTitle>
                  <JournalTitle>Ann Stat</JournalTitle>
                  <VolumeID>7</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>1</FirstPage>
                  <LastPage>26</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>515681</Handle>
                  </Occurrence>
                  <Occurrence Type="ZLBID">
                    <Handle>0406.62024</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1214/aos/1176344552</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Efron B (1979) Bootstrap methods: another look at the jackknife. Ann Stat 7(1):1–26</BibUnstructured>
              </Citation>
              <Citation ID="CR19">
                <CitationNumber>19.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>B</Initials>
                    <FamilyName>Efron</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>T</Initials>
                    <FamilyName>Hastie</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>I</Initials>
                    <FamilyName>Johnstone</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>R</Initials>
                    <FamilyName>Tibshirani</FamilyName>
                  </BibAuthorName>
                  <Year>2004</Year>
                  <ArticleTitle Language="En">Least angle regression</ArticleTitle>
                  <JournalTitle>Ann Stat</JournalTitle>
                  <VolumeID>32</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>407</FirstPage>
                  <LastPage>499</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>2060166</Handle>
                  </Occurrence>
                  <Occurrence Type="ZLBID">
                    <Handle>1091.62054</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1214/009053604000000067</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Efron B, Hastie T, Johnstone I, Tibshirani R (2004) Least angle regression. Ann Stat 32(2):407–499</BibUnstructured>
              </Citation>
              <Citation ID="CR20">
                <CitationNumber>20.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Fan</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>R</Initials>
                    <FamilyName>Li</FamilyName>
                  </BibAuthorName>
                  <Year>2001</Year>
                  <ArticleTitle Language="En">Variable selection via nonconcave penalized likelihood and its oracle properties</ArticleTitle>
                  <JournalTitle>J Am Stat Assoc</JournalTitle>
                  <VolumeID>96</VolumeID>
                  <IssueID>456</IssueID>
                  <FirstPage>1348</FirstPage>
                  <LastPage>1360</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>1946581</Handle>
                  </Occurrence>
                  <Occurrence Type="ZLBID">
                    <Handle>1073.62547</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1198/016214501753382273</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Fan J, Li R (2001) Variable selection via nonconcave penalized likelihood and its oracle properties. J Am Stat Assoc 96(456):1348–1360</BibUnstructured>
              </Citation>
              <Citation ID="CR21">
                <CitationNumber>21.</CitationNumber>
                <BibUnstructured>Friedman J, Hastie T, Holger H, Tibshirani R (2007) Pathwise coordinate optimization. In: Annals of applied statistics, pp 302–332</BibUnstructured>
              </Citation>
              <Citation ID="CR22">
                <CitationNumber>22.</CitationNumber>
                <BibUnstructured>Friedman J, Hastie T, Tibshirani R (2008) Sparse inverse covariance estimation with the graphical lasso. Biostatistics 9(3)</BibUnstructured>
              </Citation>
              <Citation ID="CR23">
                <CitationNumber>23.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>V</Initials>
                    <FamilyName>Gewin</FamilyName>
                  </BibAuthorName>
                  <Year>2011</Year>
                  <ArticleTitle Language="En">Self-reflection</ArticleTitle>
                  <JournalTitle>Nature</JournalTitle>
                  <VolumeID>471</VolumeID>
                  <FirstPage>667</FirstPage>
                  <LastPage>669</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1038/nj7340-667a</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Gewin V (2011) Self-reflection. Nature 471:667–669</BibUnstructured>
              </Citation>
              <Citation ID="CR24">
                <CitationNumber>24.</CitationNumber>
                <BibUnstructured>Grauman K, Darrell T (2005) Pyramid match kernels: discriminative classification with sets of image features. In: Proceedings of international conference on computer vision (ICCV), pp 1458-1465</BibUnstructured>
              </Citation>
              <Citation ID="CR25">
                <CitationNumber>25.</CitationNumber>
                <BibUnstructured>Han Y, Wu F, Jia J, Zhuang Y , Yu. B (2010) Multi-task sparse discriminant analysis (MtSDA) with overlapping categories. In: Proceedings of the AAAI conference on artificial intelligence (AAAI), pp 469-474</BibUnstructured>
              </Citation>
              <Citation ID="CR26">
                <CitationNumber>26.</CitationNumber>
                <BibUnstructured>Harshman RA (1970) Foundations of the PARAFAC procedure: models and conditions for an “explanatory” multimodal factor analysis. University of California at Los Angeles</BibUnstructured>
              </Citation>
              <Citation ID="CR27">
                <CitationNumber>27.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>T</Initials>
                    <FamilyName>Hastie</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Buja</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>R</Initials>
                    <FamilyName>Tibshirani</FamilyName>
                  </BibAuthorName>
                  <Year>1995</Year>
                  <ArticleTitle Language="En">Penalized discriminant analysis</ArticleTitle>
                  <JournalTitle>Ann Stat</JournalTitle>
                  <VolumeID>23</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>73</FirstPage>
                  <LastPage>102</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>1331657</Handle>
                  </Occurrence>
                  <Occurrence Type="ZLBID">
                    <Handle>0821.62031</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1214/aos/1176324456</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Hastie T, Buja A, Tibshirani R (1995) Penalized discriminant analysis. Ann Stat 23(1):73–102</BibUnstructured>
              </Citation>
              <Citation ID="CR28">
                <CitationNumber>28.</CitationNumber>
                <BibUnstructured>He X, Niyogi P (2006) Tensor subspace analysis. In: Advances in neural information processing systems (NIPS)</BibUnstructured>
              </Citation>
              <Citation ID="CR29">
                <CitationNumber>29.</CitationNumber>
                <BibUnstructured>Irani M (1999) Multi-frame optical flow estimation using subspace constraints. In: Proceedings of international conference on computer vision (ICCV), pp 623–633</BibUnstructured>
              </Citation>
              <Citation ID="CR30">
                <CitationNumber>30.</CitationNumber>
                <BibUnstructured>Jenatton R , Julien M, Guillaume O, Bach F (2010) Proximal methods for sparse hierarchical dictionary learning. In: Proceedings of the 27th international conference on machine learning (ICML)</BibUnstructured>
              </Citation>
              <Citation ID="CR31">
                <CitationNumber>31.</CitationNumber>
                <BibUnstructured>Jenatton R, Audibert JY , Bach F (2009) Structured variable selection with sparsity-inducing norms. Preprint, arXiv:0904.3523</BibUnstructured>
              </Citation>
              <Citation ID="CR32">
                <CitationNumber>32.</CitationNumber>
                <BibUnstructured>Jenatton R, Obozinski G, Bach F (2010) Structured sparse principal component analysis. In: Proceedings of international conference on artificial intelligence and statistics (AISTATS)</BibUnstructured>
              </Citation>
              <Citation ID="CR33">
                <CitationNumber>33.</CitationNumber>
                <BibUnstructured>Jia Y, Salzmann M, Darrell T (2010) Factorized latent spaces with structured sparsity. In: Advances in neural information processing systems (NIPS)</BibUnstructured>
              </Citation>
              <Citation ID="CR34">
                <CitationNumber>34.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Jia</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>B</Initials>
                    <FamilyName>Yu</FamilyName>
                  </BibAuthorName>
                  <Year>2010</Year>
                  <ArticleTitle Language="En">On model selection consistency of the elastic net when p <InlineEquation ID="IEqr3"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEqr3.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\gg $$]]></EquationSource></InlineEquation> n</ArticleTitle>
                  <JournalTitle>Stat Sin</JournalTitle>
                  <VolumeID>20</VolumeID>
                  <FirstPage>595</FirstPage>
                  <LastPage>611</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>2682632</Handle>
                  </Occurrence>
                  <Occurrence Type="ZLBID">
                    <Handle>1187.62125</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Jia J, Yu B (2010) On model selection consistency of the elastic net when p <InlineEquation ID="IEqr4"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_1_Article_IEqr4.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\gg $$]]></EquationSource></InlineEquation> n. Stat Sin 20:595–611</BibUnstructured>
              </Citation>
              <Citation ID="CR35">
                <CitationNumber>35.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Kearns</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>D</Initials>
                    <FamilyName>Ron</FamilyName>
                  </BibAuthorName>
                  <Year>1999</Year>
                  <ArticleTitle Language="En">Algorithmic stability and sanity-check bounds for leave-one-out cross-validation</ArticleTitle>
                  <JournalTitle>Neural Comput</JournalTitle>
                  <VolumeID>11</VolumeID>
                  <IssueID>6</IssueID>
                  <FirstPage>1427</FirstPage>
                  <LastPage>1453</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1162/089976699300016304</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Kearns M, Ron D (1999) Algorithmic stability and sanity-check bounds for leave-one-out cross-validation. Neural Comput 11(6): 1427–1453</BibUnstructured>
              </Citation>
              <Citation ID="CR36">
                <CitationNumber>36.</CitationNumber>
                <BibUnstructured>Kim J, Park H (2011) Fast nonnegative matrix factorization: an active-set-like method and comparisons. SIAM J Sci Comput</BibUnstructured>
              </Citation>
              <Citation ID="CR37">
                <CitationNumber>37.</CitationNumber>
                <BibUnstructured>Kim S, Xing EP (2010) Tree-guided group lasso for multi-task regression with structured sparsity. In: Proceedings of the 27th international conference on machine learning (ICML)</BibUnstructured>
              </Citation>
              <Citation ID="CR38">
                <CitationNumber>38.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>H</Initials>
                    <FamilyName>Kim</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>H</Initials>
                    <FamilyName>Park</FamilyName>
                  </BibAuthorName>
                  <Year>2007</Year>
                  <ArticleTitle Language="En">Sparse non-negative matrix factorizations via alternating non-negativity-constrained least squares for microarray data analysis</ArticleTitle>
                  <JournalTitle>Bioinforma/Comput Appl Biosci</JournalTitle>
                  <VolumeID>23</VolumeID>
                  <FirstPage>1495</FirstPage>
                  <LastPage>1502</LastPage>
                </BibArticle>
                <BibUnstructured>Kim H, Park H (2007) Sparse non-negative matrix factorizations via alternating non-negativity-constrained least squares for microarray data analysis. Bioinforma/Comput Appl Biosci 23:1495–1502</BibUnstructured>
              </Citation>
              <Citation ID="CR39">
                <CitationNumber>39.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>TG</Initials>
                    <FamilyName>Kolda</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>BW</Initials>
                    <FamilyName>Bader</FamilyName>
                  </BibAuthorName>
                  <Year>2009</Year>
                  <ArticleTitle Language="En">Tensor decompositions and applications</ArticleTitle>
                  <JournalTitle>SIAM Rev</JournalTitle>
                  <VolumeID>51</VolumeID>
                  <IssueID>3</IssueID>
                  <FirstPage>455</FirstPage>
                  <LastPage>500</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>2535056</Handle>
                  </Occurrence>
                  <Occurrence Type="ZLBID">
                    <Handle>1173.65029</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1137/07070111X</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Kolda TG, Bader BW (2009) Tensor decompositions and applications. SIAM Rev 51(3):455–500</BibUnstructured>
              </Citation>
              <Citation ID="CR40">
                <CitationNumber>40.</CitationNumber>
                <BibUnstructured>Lazebnik S, Schmid C, Ponce J (2006) Beyond bags of features: spatial pyramid matching for recognizing natural scene categories. In: Proceedings of the IEEE computer society conference on computer vision and pattern recognition (CVPR)</BibUnstructured>
              </Citation>
              <Citation ID="CR41">
                <CitationNumber>41.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>D</Initials>
                    <FamilyName>Lee</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>H</Initials>
                    <FamilyName>Seung</FamilyName>
                  </BibAuthorName>
                  <Year>1999</Year>
                  <ArticleTitle Language="En">Learning the parts of objects by nonnegative matrix factorization</ArticleTitle>
                  <JournalTitle>Nature</JournalTitle>
                  <VolumeID>401</VolumeID>
                  <IssueID>6755</IssueID>
                  <FirstPage>788</FirstPage>
                  <LastPage>791</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1038/44565</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Lee D, Seung H (1999) Learning the parts of objects by nonnegative matrix factorization. Nature 401(6755):788–791</BibUnstructured>
              </Citation>
              <Citation ID="CR42">
                <CitationNumber>42.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>TS</Initials>
                    <FamilyName>Lee</FamilyName>
                  </BibAuthorName>
                  <Year>1996</Year>
                  <ArticleTitle Language="En">Image representation using 2D Gabor wavelets</ArticleTitle>
                  <JournalTitle>IEEE Trans Pattern Anal Mach Intell</JournalTitle>
                  <VolumeID>18</VolumeID>
                  <IssueID>10</IssueID>
                  <FirstPage>959</FirstPage>
                  <LastPage>971</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/34.541406</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Lee TS (1996) Image representation using 2D Gabor wavelets. IEEE Trans Pattern Anal Mach Intell 18(10):959–971</BibUnstructured>
              </Citation>
              <Citation ID="CR43">
                <CitationNumber>43.</CitationNumber>
                <BibUnstructured>Li L, Zhou M, Sapiro G, Carin L (2011) On the integration of topic modeling and dictionary learning. In: Proceedings of international conferences on machine learning (ICML)</BibUnstructured>
              </Citation>
              <Citation ID="CR44">
                <CitationNumber>44.</CitationNumber>
                <BibUnstructured>Liu H, Zhang J (2009) Estimation consistency of the group lasso and its applications. In: Proceedings of the twelfth international conference on artificial intelligence and statistics (AISTATS)</BibUnstructured>
              </Citation>
              <Citation ID="CR45">
                <CitationNumber>45.</CitationNumber>
                <BibUnstructured>Liu N, Zhang B, Yan J, Chen J (2005) Text representation: from vector to tensor. In: Proceedings of international conferences on data mining (ICDM)</BibUnstructured>
              </Citation>
              <Citation ID="CR46">
                <CitationNumber>46.</CitationNumber>
                <BibUnstructured>Liu Y, Wu F, Zhang Z, Zhuang Y , Yan S (2010) Sparse representation using nonnegative curds and whey. In: Proceedings of computer vision and pattern recognition (CVPR), pp 3578–3585</BibUnstructured>
              </Citation>
              <Citation ID="CR47">
                <CitationNumber>47.</CitationNumber>
                <BibUnstructured>Lv Q, Josephson W, Wang Z, Charikar M., Li K (2007) Multi-probe LSH: efficient indexing for high-dimensional similarity search. In: Proceedings of international conference on very large data bases (VLDB), pp 950–961</BibUnstructured>
              </Citation>
              <Citation ID="CR48">
                <CitationNumber>48.</CitationNumber>
                <BibUnstructured>Ma ZG, Yang Y, Nie FP, Uijlings J, Sebe N (2011) Exploiting the entire feature space with sparsity for automatic image annotation. In: Proceedings of the ACM multimedia (ACM MM)</BibUnstructured>
              </Citation>
              <Citation ID="CR49">
                <CitationNumber>49.</CitationNumber>
                <BibUnstructured>Maron O, Ratan A.L (1998) Multiple-instance learning for natural scene classification. In: Proceedings of the international conference on machine learning (ICML), pp 341–349</BibUnstructured>
              </Citation>
              <Citation ID="CR50">
                <CitationNumber>50.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>N</Initials>
                    <FamilyName>Meinshausen</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>B</Initials>
                    <FamilyName>Yu</FamilyName>
                  </BibAuthorName>
                  <Year>2009</Year>
                  <ArticleTitle Language="En">Lasso-type recovery of sparse representations for high-dimensional data</ArticleTitle>
                  <JournalTitle>Ann Stat</JournalTitle>
                  <VolumeID>37</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>246</FirstPage>
                  <LastPage>270</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>2488351</Handle>
                  </Occurrence>
                  <Occurrence Type="ZLBID">
                    <Handle>1155.62050</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1214/07-AOS582</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Meinshausen N, Yu B (2009) Lasso-type recovery of sparse representations for high-dimensional data. Ann Stat 37(1):246–270</BibUnstructured>
              </Citation>
              <Citation ID="CR51">
                <CitationNumber>51.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>RG</Initials>
                    <FamilyName>Miller</FamilyName>
                  </BibAuthorName>
                  <Year>1974</Year>
                  <ArticleTitle Language="En">The jackknife—a review</ArticleTitle>
                  <JournalTitle>Biometrika</JournalTitle>
                  <VolumeID>6</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>1</FirstPage>
                  <LastPage>15</LastPage>
                </BibArticle>
                <BibUnstructured>Miller RG (1974) The jackknife—a review. Biometrika 6(1):1–15</BibUnstructured>
              </Citation>
              <Citation ID="CR52">
                <CitationNumber>52.</CitationNumber>
                <BibUnstructured>Mu Y, Dong J, Yuan X, Yan S (2011) Accelerated low-rank visual recovery by random projection. In: Proceedings of IEEE conference on computer vision and pattern recognition (CVPR)</BibUnstructured>
              </Citation>
              <Citation ID="CR53">
                <CitationNumber>53.</CitationNumber>
                <BibUnstructured>Nesterov Y (2007) Gradient methods for minimizing composite objective function. Technical report, Universit catholique de Louvain. Center for Operations Research and Econometrics (CORE)</BibUnstructured>
              </Citation>
              <Citation ID="CR54">
                <CitationNumber>54.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>Y</Initials>
                    <FamilyName>Nesterov</FamilyName>
                  </BibAuthorName>
                  <Year>2005</Year>
                  <ArticleTitle Language="En">Smooth minimization of non-smooth functions</ArticleTitle>
                  <JournalTitle>Math Program</JournalTitle>
                  <VolumeID>103</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>127</FirstPage>
                  <LastPage>152</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>2166537</Handle>
                  </Occurrence>
                  <Occurrence Type="ZLBID">
                    <Handle>1079.90102</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1007/s10107-004-0552-5</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Nesterov Y (2005) Smooth minimization of non-smooth functions. Math Program 103(1):127–152</BibUnstructured>
              </Citation>
              <Citation ID="CR55">
                <CitationNumber>55.</CitationNumber>
                <BibUnstructured>Obozinski G, Taskar B, Jordan MI (2006) Multi-task feature selection. Technical report, Statistics Department, UC Berkeley</BibUnstructured>
              </Citation>
              <Citation ID="CR56">
                <CitationNumber>56.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>B</Initials>
                    <FamilyName>Olshausen</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>D</Initials>
                    <FamilyName>Field</FamilyName>
                  </BibAuthorName>
                  <Year>1996</Year>
                  <ArticleTitle Language="En">Emergence of simple-cell receptive field properties by learning a sparse code for natural images</ArticleTitle>
                  <JournalTitle>Nature</JournalTitle>
                  <VolumeID>381</VolumeID>
                  <FirstPage>607</FirstPage>
                  <LastPage>609</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1038/381607a0</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Olshausen B, Field D (1996) Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature 381:607–609</BibUnstructured>
              </Citation>
              <Citation ID="CR57">
                <CitationNumber>57.</CitationNumber>
                <BibUnstructured>Quattoni, A, Collins, M, Darrell, T (2008) Transfer learning for image classification with sparse prototype representations. In: Proceedings of computer vision and pattern recognition (CVPR), pp 1-8</BibUnstructured>
              </Citation>
              <Citation ID="CR58">
                <CitationNumber>58.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Riesenhuber</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>T</Initials>
                    <FamilyName>Poggio</FamilyName>
                  </BibAuthorName>
                  <Year>1999</Year>
                  <ArticleTitle Language="En">Hierarchical models of object recognition in cortex</ArticleTitle>
                  <JournalTitle>Nat Neurosci</JournalTitle>
                  <VolumeID>2</VolumeID>
                  <IssueID>11</IssueID>
                  <FirstPage>1019</FirstPage>
                  <LastPage>1025</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1038/14819</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Riesenhuber M, Poggio T (1999) Hierarchical models of object recognition in cortex. Nat Neurosci 2(11):1019–1025</BibUnstructured>
              </Citation>
              <Citation ID="CR59">
                <CitationNumber>59.</CitationNumber>
                <BibUnstructured>Salakhutdinov R, Hinton GE (2007) Learning a nonlinear embedding by preserving class neighbourhood structure. In: AI and statistics</BibUnstructured>
              </Citation>
              <Citation ID="CR60">
                <CitationNumber>60.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>R</Initials>
                    <FamilyName>Salakhutdinov</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>G</Initials>
                    <FamilyName>Hinton</FamilyName>
                  </BibAuthorName>
                  <Year>2009</Year>
                  <ArticleTitle Language="En">Semantic hashing</ArticleTitle>
                  <JournalTitle>Int J Approx Reason</JournalTitle>
                  <VolumeID>50</VolumeID>
                  <IssueID>7</IssueID>
                  <FirstPage>969</FirstPage>
                  <LastPage>978</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1016/j.ijar.2008.11.006</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Salakhutdinov R, Hinton G (2009) Semantic hashing. Int J Approx Reason 50(7):969–978</BibUnstructured>
              </Citation>
              <Citation ID="CR61">
                <CitationNumber>61.</CitationNumber>
                <BibUnstructured>Saul L, Weinberger K, Sha F, Ham J, Lee D (2006) Spectral methods for dimensionality reduction. In: Semisupervised learning, pp 293-308</BibUnstructured>
              </Citation>
              <Citation ID="CR62">
                <CitationNumber>62.</CitationNumber>
                <BibUnstructured>Shakhnarovich G, Viola P, Darrell T (2003) Fast pose estimation with parameter-sensitive hashing. In: Proceedings of IEEE international conference on computer vision (ICCV), pp 750-757</BibUnstructured>
              </Citation>
              <Citation ID="CR63">
                <CitationNumber>63.</CitationNumber>
                <BibUnstructured>Shao J, Wu F, Ouyang C, Zhang X (2011) Sparse spectral hashing. In: Pattern recognition letters</BibUnstructured>
              </Citation>
              <Citation ID="CR64">
                <CitationNumber>64.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>X</Initials>
                    <FamilyName>Shen</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>HC</Initials>
                    <FamilyName>Huang</FamilyName>
                  </BibAuthorName>
                  <Year>2010</Year>
                  <ArticleTitle Language="En">Grouping pursuit through a regularization solution surface</ArticleTitle>
                  <JournalTitle>J Am Stat Assoc</JournalTitle>
                  <VolumeID>105</VolumeID>
                  <IssueID>490</IssueID>
                  <FirstPage>727</FirstPage>
                  <LastPage>739</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>2724856</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1198/jasa.2010.tm09380</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Shen X, Huang HC (2010) Grouping pursuit through a regularization solution surface. J Am Stat Assoc 105(490):727–739</BibUnstructured>
              </Citation>
              <Citation ID="CR65">
                <CitationNumber>65.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>S</Initials>
                    <FamilyName>Shevade</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>S</Initials>
                    <FamilyName>Keerthi</FamilyName>
                  </BibAuthorName>
                  <Year>2008</Year>
                  <ArticleTitle Language="En">A simple and efficient algorithm for gene selection using sparse logistic regression</ArticleTitle>
                  <JournalTitle>Bioinformatics</JournalTitle>
                  <VolumeID>19</VolumeID>
                  <IssueID>17</IssueID>
                  <FirstPage>2246</FirstPage>
                  <LastPage>2253</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1093/bioinformatics/btg308</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Shevade S, Keerthi S (2008) A simple and efficient algorithm for gene selection using sparse logistic regression. Bioinformatics 19(17):2246–2253</BibUnstructured>
              </Citation>
              <Citation ID="CR66">
                <CitationNumber>66.</CitationNumber>
                <BibUnstructured>Shi J, Ren X, Dai G, Wang J, Zhang Z (2011) A non-convex relaxation approach to sparse dictionary learning. In: Proceedings of computer vision and pattern recognition (CVPR)</BibUnstructured>
              </Citation>
              <Citation ID="CR67">
                <CitationNumber>67.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>D</Initials>
                    <FamilyName>Spielman</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>S</Initials>
                    <FamilyName>Teng</FamilyName>
                  </BibAuthorName>
                  <Year>2009</Year>
                  <ArticleTitle Language="En">Smoothed analysis: an attempt to explain the behavior of algorithms in practice</ArticleTitle>
                  <JournalTitle>Commun ACM</JournalTitle>
                  <VolumeID>52</VolumeID>
                  <IssueID>10</IssueID>
                  <FirstPage>76</FirstPage>
                  <LastPage>84</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1145/1562764.1562785</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Spielman D, Teng S (2009) Smoothed analysis: an attempt to explain the behavior of algorithms in practice. Commun ACM 52(10):76–84</BibUnstructured>
              </Citation>
              <Citation ID="CR68">
                <CitationNumber>68.</CitationNumber>
                <BibUnstructured>Sun L, Ji S, Ye J (2008) Hypergraph spectral learning for multi-label classification. In: Proceeding of the 14th ACM SIGKDD international conference on knowledge discovery and data mining (SIGKDD), pp 668-676</BibUnstructured>
              </Citation>
              <Citation ID="CR69">
                <CitationNumber>69.</CitationNumber>
                <BibUnstructured>Tao D, Li X, Wu X, Hu W, Maybox J (2005) Supervised tensor learning. In: Proceedings of IEEE conference on data mining (ICDM)</BibUnstructured>
              </Citation>
              <Citation ID="CR70">
                <CitationNumber>70.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>R</Initials>
                    <FamilyName>Tibshirani</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Saunders</FamilyName>
                  </BibAuthorName>
                  <Year>2005</Year>
                  <ArticleTitle Language="En">Sparsity and smoothness via the fused lasso</ArticleTitle>
                  <JournalTitle>J R Stat Soc Ser B (Statistical Methodology)</JournalTitle>
                  <VolumeID>67</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>91</FirstPage>
                  <LastPage>108</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>2136641</Handle>
                  </Occurrence>
                  <Occurrence Type="ZLBID">
                    <Handle>1060.62049</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1111/j.1467-9868.2005.00490.x</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Tibshirani R, Saunders M (2005) Sparsity and smoothness via the fused lasso. J R Stat Soc Ser B (Statistical Methodology) 67(1):91–108</BibUnstructured>
              </Citation>
              <Citation ID="CR71">
                <CitationNumber>71.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>R</Initials>
                    <FamilyName>Tibshirani</FamilyName>
                  </BibAuthorName>
                  <Year>1996</Year>
                  <ArticleTitle Language="En">Regression shrinkage and selection via the lasso</ArticleTitle>
                  <JournalTitle>J R Stat Soc Ser B (Statistical Methodology)</JournalTitle>
                  <VolumeID>58</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>267</FirstPage>
                  <LastPage>288</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>1379242</Handle>
                  </Occurrence>
                  <Occurrence Type="ZLBID">
                    <Handle>0850.62538</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Tibshirani R (1996) Regression shrinkage and selection via the lasso. J R Stat Soc Ser B (Statistical Methodology) 58(1):267–288</BibUnstructured>
              </Citation>
              <Citation ID="CR72">
                <CitationNumber>72.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>LR</Initials>
                    <FamilyName>Tucker</FamilyName>
                  </BibAuthorName>
                  <Year>1996</Year>
                  <ArticleTitle Language="En">Some mathematical notes on three-mode factor analysis</ArticleTitle>
                  <JournalTitle>Psychometrika</JournalTitle>
                  <VolumeID>31</VolumeID>
                  <IssueID>3</IssueID>
                  <FirstPage>279</FirstPage>
                  <LastPage>311</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1007/BF02289464</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Tucker LR (1996) Some mathematical notes on three-mode factor analysis. Psychometrika 31(3):279–311</BibUnstructured>
              </Citation>
              <Citation ID="CR73">
                <CitationNumber>73.</CitationNumber>
                <BibUnstructured>Wang J, Yang J, Yu K, Lv F (2005) Locality-constrained linear coding for image classification. In: Proceedings of the IEEE computer society conference on computer vision and pattern recognition (CVPR), pp 3360-3367</BibUnstructured>
              </Citation>
              <Citation ID="CR74">
                <CitationNumber>74.</CitationNumber>
                <BibUnstructured>Weiss Y, Torralba A, Fergus R (2009) Spectral hashing. In: Advances in neural information processing systems (NIPS), pp 1753-1760</BibUnstructured>
              </Citation>
              <Citation ID="CR75">
                <CitationNumber>75.</CitationNumber>
                <BibUnstructured>Wright J, Ganesh A, Rao S , Ma Y (2009) Exact recovery of corrupted low-rank matrices. Robust principal component analysis. In: Advances in neural information processing systems (NIPS)</BibUnstructured>
              </Citation>
              <Citation ID="CR76">
                <CitationNumber>76.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Wright</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Yang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Ganesh</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>S</Initials>
                    <FamilyName>Sastry</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>Y</Initials>
                    <FamilyName>Ma</FamilyName>
                  </BibAuthorName>
                  <Year>2009</Year>
                  <ArticleTitle Language="En">Robust face recognition via sparse representation</ArticleTitle>
                  <JournalTitle>IEEE Trans Pattern Anal Mach intell</JournalTitle>
                  <VolumeID>31</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>210</FirstPage>
                  <LastPage>227</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TPAMI.2008.79</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Wright J, Yang A, Ganesh A, Sastry S, Ma Y (2009) Robust face recognition via sparse representation. IEEE Trans Pattern Anal Mach intell 31(2):210–227</BibUnstructured>
              </Citation>
              <Citation ID="CR77">
                <CitationNumber>77.</CitationNumber>
                <BibUnstructured>Wu F, Han Y, Tian Q, Zhuang Y (2010) Multi-label boosting for image annotation by structural grouping sparsity. In: Proceedings of the 2010 ACM international conference on multimedia (ACM MM), pp 15–24</BibUnstructured>
              </Citation>
              <Citation ID="CR78">
                <CitationNumber>78.</CitationNumber>
                <BibUnstructured>Wu F, Zhang H, Zhuang Y (2006) Learning semantic correlations for cross-media retrieval. In: Proceedings of IEEE international conference on image processing (ICIP), pp 1465-1468</BibUnstructured>
              </Citation>
              <Citation ID="CR79">
                <CitationNumber>79.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>F</Initials>
                    <FamilyName>Wu</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>Y</Initials>
                    <FamilyName>Liu</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>Y</Initials>
                    <FamilyName>Zhuang</FamilyName>
                  </BibAuthorName>
                  <Year>2009</Year>
                  <ArticleTitle Language="En">Tensor-based transductive learning for multimodality video semantic concept detection</ArticleTitle>
                  <JournalTitle>IEEE Trans Multimed</JournalTitle>
                  <VolumeID>11</VolumeID>
                  <IssueID>5</IssueID>
                  <FirstPage>868</FirstPage>
                  <LastPage>878 </LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TMM.2009.2021724</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Wu F, Liu Y, Zhuang Y (2009) Tensor-based transductive learning for multimodality video semantic concept detection. IEEE Trans Multimed 11(5):868–878 </BibUnstructured>
              </Citation>
              <Citation ID="CR80">
                <CitationNumber>80.</CitationNumber>
                <BibUnstructured>Xu H, Mannor S, Caramanis C (2011) Sparse algorithms are not stable: a no-free-lunch theorem. In: IEEE transactions on pattern analysis and machine intelligence</BibUnstructured>
              </Citation>
              <Citation ID="CR81">
                <CitationNumber>81.</CitationNumber>
                <BibUnstructured>Xu L, Lu C, Xu Y, Jia J (2011) Image smoothing via L0 gradient minimization. ACM Trans Graph (SIGGRAPH, Asia 2011) 30(6)</BibUnstructured>
              </Citation>
              <Citation ID="CR82">
                <CitationNumber>82.</CitationNumber>
                <BibUnstructured>Yang Y, Shen HY, Ma ZG, Huang Z, Zhou XF (2011) L21-norm regularized discriminative feature selection for unsupervised learning. In: International joint conferences on artificial intelligence (IJCAI)</BibUnstructured>
              </Citation>
              <Citation ID="CR83">
                <CitationNumber>83.</CitationNumber>
                <BibUnstructured>Yang Y, Yang Y, Huang Z, Shen HT, Nie F (2011) Tag localization with spatial correlations and joint group sparsity. In: Proceedings of computer vision and pattern recognition (CVPR), pp 881–888</BibUnstructured>
              </Citation>
              <Citation ID="CR84">
                <CitationNumber>84.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>Y</Initials>
                    <FamilyName>Yang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>F</Initials>
                    <FamilyName>Wu</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>D</Initials>
                    <FamilyName>Xu</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>Y</Initials>
                    <FamilyName>Zhuang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>LT</Initials>
                    <FamilyName>Chia</FamilyName>
                  </BibAuthorName>
                  <Year>2010</Year>
                  <ArticleTitle Language="En">Cross-media retrieval using query dependent search methods</ArticleTitle>
                  <JournalTitle>Pattern Recognit</JournalTitle>
                  <VolumeID>43</VolumeID>
                  <IssueID>8</IssueID>
                  <FirstPage>2927</FirstPage>
                  <LastPage>2936</LastPage>
                  <Occurrence Type="ZLBID">
                    <Handle>1210.68058</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1016/j.patcog.2010.02.015</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Yang Y, Wu F, Xu D, Zhuang Y, Chia LT (2010) Cross-media retrieval using query dependent search methods. Pattern Recognit 43(8):2927–2936</BibUnstructured>
              </Citation>
              <Citation ID="CR85">
                <CitationNumber>85.</CitationNumber>
                <BibUnstructured>Yu H, Bennamoun M (2006) 1D-PCA, 2D-PCA to nD-PCA. In: Proceedings of international conference on pattern recognition (ICPR), pp 181–184</BibUnstructured>
              </Citation>
              <Citation ID="CR86">
                <CitationNumber>86.</CitationNumber>
                <BibUnstructured>Yuan Y, Wu F, Zhuang Y, Shao J (2011) Image annotation by composite kernel learning with group structure. In: Proceedings of ACM conference on multimedia (ACM MM)</BibUnstructured>
              </Citation>
              <Citation ID="CR87">
                <CitationNumber>87.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Yuan</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>Y</Initials>
                    <FamilyName>Lin</FamilyName>
                  </BibAuthorName>
                  <Year>2006</Year>
                  <ArticleTitle Language="En">Model selection and estimation in regression with grouped variables</ArticleTitle>
                  <JournalTitle>J R Stat Soc Ser B (Methodological)</JournalTitle>
                  <VolumeID>68</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>49</FirstPage>
                  <LastPage>67</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>2212574</Handle>
                  </Occurrence>
                  <Occurrence Type="ZLBID">
                    <Handle>1141.62030</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1111/j.1467-9868.2005.00532.x</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Yuan M, Lin Y (2006) Model selection and estimation in regression with grouped variables. J R Stat Soc Ser B (Methodological) 68(1):49–67</BibUnstructured>
              </Citation>
              <Citation ID="CR88">
                <CitationNumber>88.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Yuan</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>Y</Initials>
                    <FamilyName>Lin</FamilyName>
                  </BibAuthorName>
                  <Year>2007</Year>
                  <ArticleTitle Language="En">On the non-negative garrotte estimator</ArticleTitle>
                  <JournalTitle>J R Stat Soc Ser B (Statistical Methodology)</JournalTitle>
                  <VolumeID>69</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>143</FirstPage>
                  <LastPage>161</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>2325269</Handle>
                  </Occurrence>
                  <Occurrence Type="ZLBID">
                    <Handle>1120.62052</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1111/j.1467-9868.2007.00581.x</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Yuan M, Lin Y (2007) On the non-negative garrotte estimator. J R Stat Soc Ser B (Statistical Methodology) 69(2):143–161</BibUnstructured>
              </Citation>
              <Citation ID="CR89">
                <CitationNumber>89.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>CH</Initials>
                    <FamilyName>Zhang</FamilyName>
                  </BibAuthorName>
                  <Year>2010</Year>
                  <ArticleTitle Language="En">Nearly unbiased variable selection under minimax concave penalty</ArticleTitle>
                  <JournalTitle>Ann Stat</JournalTitle>
                  <VolumeID>38</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>894</FirstPage>
                  <LastPage>942</LastPage>
                  <Occurrence Type="ZLBID">
                    <Handle>1183.62120</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1214/09-AOS729</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Zhang CH (2010) Nearly unbiased variable selection under minimax concave penalty. Ann Stat 38(2):894–942</BibUnstructured>
              </Citation>
              <Citation ID="CR90">
                <CitationNumber>90.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>P</Initials>
                    <FamilyName>Zhao</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>B</Initials>
                    <FamilyName>Yu</FamilyName>
                  </BibAuthorName>
                  <Year>2006</Year>
                  <ArticleTitle Language="En">On model selection consistency of Lasso</ArticleTitle>
                  <JournalTitle>J Mach Learn Res</JournalTitle>
                  <VolumeID>7</VolumeID>
                  <FirstPage>2541</FirstPage>
                  <LastPage>2563</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>2274449</Handle>
                  </Occurrence>
                  <Occurrence Type="ZLBID">
                    <Handle>1222.62008</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Zhao P, Yu B (2006) On model selection consistency of Lasso. J Mach Learn Res 7:2541–2563</BibUnstructured>
              </Citation>
              <Citation ID="CR91">
                <CitationNumber>91.</CitationNumber>
                <BibUnstructured>Zhou D, Huang J, Schölkopf B (2006) Learning with hypergraphs: clustering, classification, and embedding. In: Advances in neural information processing systems (NIPS), pp 1601–1608</BibUnstructured>
              </Citation>
              <Citation ID="CR92">
                <CitationNumber>92.</CitationNumber>
                <BibUnstructured>Zhu J, Xing EP (2011) Sparse topical coding. In: Proceedings of the 27th international conference on uncertainty in artificial intelligence (UAI)</BibUnstructured>
              </Citation>
              <Citation ID="CR93">
                <CitationNumber>93.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>H</Initials>
                    <FamilyName>Zou</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>T</Initials>
                    <FamilyName>Hastie</FamilyName>
                  </BibAuthorName>
                  <Year>2005</Year>
                  <ArticleTitle Language="En">Regularization and variable selection via the elastic net</ArticleTitle>
                  <JournalTitle>J R Stat Soc Ser B (Statistical Methodology)</JournalTitle>
                  <VolumeID>67</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>301</FirstPage>
                  <LastPage>320</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>2137327</Handle>
                  </Occurrence>
                  <Occurrence Type="ZLBID">
                    <Handle>1069.62054</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1111/j.1467-9868.2005.00503.x</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Zou H, Hastie T (2005) Regularization and variable selection via the elastic net. J R Stat Soc Ser B (Statistical Methodology) 67(2):301–320</BibUnstructured>
              </Citation>
              <Citation ID="CR94">
                <CitationNumber>94.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>H</Initials>
                    <FamilyName>Zou</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>T</Initials>
                    <FamilyName>Hastie</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>R</Initials>
                    <FamilyName>Tibshirani</FamilyName>
                  </BibAuthorName>
                  <Year>2006</Year>
                  <ArticleTitle Language="En">Sparse principal component analysis</ArticleTitle>
                  <JournalTitle>J Comput Graph Stat</JournalTitle>
                  <VolumeID>15</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>265</FirstPage>
                  <LastPage>286</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>2252527</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1198/106186006X113430</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Zou H, Hastie T, Tibshirani R (2006) Sparse principal component analysis. J Comput Graph Stat 15(2):265–286</BibUnstructured>
              </Citation>
              <Citation ID="CR95">
                <CitationNumber>95.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>H</Initials>
                    <FamilyName>Zou</FamilyName>
                  </BibAuthorName>
                  <Year>2006</Year>
                  <ArticleTitle Language="En">The adaptive lasso and its oracle properties</ArticleTitle>
                  <JournalTitle>J Am Stat Assoc</JournalTitle>
                  <VolumeID>101</VolumeID>
                  <IssueID>476</IssueID>
                  <FirstPage>1418</FirstPage>
                  <LastPage>1429</LastPage>
                  <Occurrence Type="ZLBID">
                    <Handle>1171.62326</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1198/016214506000000735</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Zou H (2006) The adaptive lasso and its oracle properties. J Am Stat Assoc 101(476):1418–1429</BibUnstructured>
              </Citation>
            </Bibliography>
          </ArticleBackmatter>
        </Article>
      </Issue>
    </Volume>
  </Journal>
</Publisher>
