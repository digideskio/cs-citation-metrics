<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE Publisher PUBLIC "-//Springer-Verlag//DTD A++ V2.4//EN" "http://devel.springer.de/A++/V2.4/DTD/A++V2.4.dtd">
<Publisher>
  <PublisherInfo>
    <PublisherName>Springer International Publishing</PublisherName>
    <PublisherLocation>Cham</PublisherLocation>
    <PublisherImprintName>Springer</PublisherImprintName>
    <PublisherURL>http://www.springer.com</PublisherURL>
  </PublisherInfo>
  <Book Language="En" OutputMedium="All">
    <BookInfo BookProductType="Professional book" ContainsESM="No" Language="En" MediaType="eBook" NumberingDepth="3" NumberingStyle="ChapterContent" OutputMedium="All" TocLevels="0">
      <BookID>978-3-319-21293-7</BookID>
      <BookTitle>Modern Stroke Rehabilitation through e-Health-based Entertainment</BookTitle>
      <BookDOI>10.1007/978-3-319-21293-7</BookDOI>
      <BookTitleID>327370</BookTitleID>
      <BookPrintISBN>978-3-319-21292-0</BookPrintISBN>
      <BookElectronicISBN>978-3-319-21293-7</BookElectronicISBN>
      <BookEdition>1st ed. 2016</BookEdition>
      <BookChapterCount>10</BookChapterCount>
      <BookCopyright>
        <CopyrightHolderName>Springer International Publishing Switzerland</CopyrightHolderName>
        <CopyrightYear>2016</CopyrightYear>
        <CopyrightStandardText Language="En">This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed.</CopyrightStandardText>
      </BookCopyright>
      <BookSubjectGroup>
        <BookSubject Code="SCT" Type="Primary">Engineering</BookSubject>
        <BookSubject Code="SCT2700X" Priority="1" Type="Secondary">Biomedical Engineering</BookSubject>
        <BookSubject Code="SCT24068" Priority="2" Type="Secondary">Circuits and Systems</BookSubject>
        <BookSubject Code="SCH55006" Priority="3" Type="Secondary">Rehabilitation</BookSubject>
        <BookSubject Code="SCH27002" Priority="4" Type="Secondary">Public Health</BookSubject>
        <BookSubject Code="SCI18059" Priority="5" Type="Secondary">Multimedia Information Systems</BookSubject>
        <SubjectCollection Code="SUCO11647">Engineering</SubjectCollection>
      </BookSubjectGroup>
      <BookstoreLocation>Electrical Engineering</BookstoreLocation>
    </BookInfo>
    <BookHeader>
      <EditorGroup>
        <Editor AffiliationIDS="Aff1">
          <EditorName DisplayOrder="Western">
            <GivenName>Emmanouela</GivenName>
            <FamilyName>Vogiatzaki</FamilyName>
          </EditorName>
          <Contact>
            <Email>emmanouela@rfsat.com</Email>
          </Contact>
        </Editor>
        <Editor AffiliationIDS="Aff2">
          <EditorName DisplayOrder="Western">
            <GivenName>Artur</GivenName>
            <FamilyName>Krukowski</FamilyName>
          </EditorName>
          <Contact>
            <Email>krukowa@intracom-telecom.com</Email>
          </Contact>
        </Editor>
        <Affiliation ID="Aff1">
          <OrgDivision>(RFSAT) Ltd</OrgDivision>
          <OrgName>Research for Science, Art and Technology</OrgName>
          <OrgAddress>
            <City>Sheffield</City>
            <Country>United Kingdom</Country>
          </OrgAddress>
        </Affiliation>
        <Affiliation ID="Aff2">
          <OrgName>Intracom S. A. Telecom Solutions</OrgName>
          <OrgAddress>
            <City>Peania</City>
            <Country>Greece</Country>
          </OrgAddress>
        </Affiliation>
      </EditorGroup>
    </BookHeader>
    <Chapter ID="b978-3-319-21293-7_4" Language="En" OutputMedium="All">
      <ChapterInfo ChapterType="OriginalPaper" ContainsESM="No" Language="En" NumberingDepth="3" NumberingStyle="ChapterContent" OutputMedium="All" TocLevels="0">
        <ChapterID>4</ChapterID>
        <ChapterNumber>Chapter 4</ChapterNumber>
        <ChapterDOI>10.1007/978-3-319-21293-7_4</ChapterDOI>
        <ChapterSequenceNumber>4</ChapterSequenceNumber>
        <ChapterTitle Language="En">Body Area Sensing Networks for Remote Health Monitoring</ChapterTitle>
        <ChapterFirstPage>85</ChapterFirstPage>
        <ChapterLastPage>136</ChapterLastPage>
        <ChapterCopyright>
          <CopyrightHolderName>Springer International Publishing Switzerland</CopyrightHolderName>
          <CopyrightYear>2016</CopyrightYear>
        </ChapterCopyright>
        <ChapterHistory>
          <RegistrationDate>
            <Year>2015</Year>
            <Month>6</Month>
            <Day>11</Day>
          </RegistrationDate>
        </ChapterHistory>
        <ChapterContext>
          <BookID>978-3-319-21293-7</BookID>
          <BookTitle>Modern Stroke Rehabilitation through e-Health-based Entertainment</BookTitle>
        </ChapterContext>
      </ChapterInfo>
      <ChapterHeader>
        <AuthorGroup>
          <Author AffiliationIDS="Aff3" CorrespondingAffiliationID="Aff3">
            <AuthorName DisplayOrder="Western">
              <GivenName>Dwaipayan</GivenName>
              <FamilyName>Biswas</FamilyName>
            </AuthorName>
            <Contact>
              <Email>db9g10@ecs.soton.ac.uk</Email>
            </Contact>
          </Author>
          <Author AffiliationIDS="Aff3">
            <AuthorName DisplayOrder="Western">
              <GivenName>Andy</GivenName>
              <FamilyName>Cranny</FamilyName>
            </AuthorName>
          </Author>
          <Author AffiliationIDS="Aff3">
            <AuthorName DisplayOrder="Western">
              <GivenName>Koushik</GivenName>
              <FamilyName>Maharatna</FamilyName>
            </AuthorName>
          </Author>
          <Affiliation ID="Aff3">
            <OrgName>University of Southampton</OrgName>
            <OrgAddress>
              <City>Southampton</City>
              <Country>United Kingdom</Country>
            </OrgAddress>
          </Affiliation>
        </AuthorGroup>
        <Abstract ID="Abs1" Language="En" OutputMedium="All">
          <Heading>Abstract</Heading>
          <Para ID="Par1">This chapter explores the field of remote sensor systems using wearable technologies that play a significant role in monitoring activities of patients in home and community settings. The focus is on body area sensing networks incorporating the primary enabling technologies: sensors for capturing the physiological and kinematic data, and data analysis techniques for extracting the clinically relevant information. With respect to the StrokeBack project, the majority of this chapter is dedicated towards physical activity monitoring—a key component in stroke rehabilitation. In particular, the domain of upper limb rehabilitation is examined since reduction of upper limb motor function is a common effect of stroke and significantly impairs the performance of patients as they engage in activities of daily life. As an example, a case study is presented where different arm movements are recognized in real time using data from inertial sensors attached to the arm. Tracking the occurrences of specific arm movements (e.g. prescribed exercises) over time can give an indication of rehabilitation progress since the frequency of these movements is expected to increase as motor functionality improves.</Para>
        </Abstract>
      </ChapterHeader>
      <Body>
        <Section1 ID="Sec1" Type="Introduction">
          <Heading>Introduction</Heading>
          <Para ID="Par2">Increased life expectancy due to better medical facilities in developed nations has increased the prevalence of health impairments among the ageing population. Among the many diseases afflicting the elderly is cerebrovascular accident (CVA), more popularly referred to as stroke, which ranks second to coronary heart diseases [<CitationRef CitationID="CR1">1</CitationRef>–<CitationRef CitationID="CR3">3</CitationRef>]. Stroke is considered as a medical emergency due to its impact, generally causing some degree of physical disability and potentially leading to death. The effect of stroke is different for each individual based on the degree and region of the brain that suffers damage and can result in partial paralysis, impaired vision, memory loss and speech problems [<CitationRef CitationID="CR4">4</CitationRef>, <CitationRef CitationID="CR5">5</CitationRef>]. The after-effects of stroke pose a serious socio-economic challenge in terms of loss of lives, disability among the survivors and the expenses incurred towards their rehabilitation and care.</Para>
          <Para ID="Par3">Patients who are left paralyzed post-stroke are often treated with physiotherapy in order to restore their motor functionality. The level of physiotherapy needs to be as intensive as possible in order to ensure fast recovery [<CitationRef CitationID="CR6">6</CitationRef>]. Apart from conventional physiotherapy, rehabilitation therapies have been provided in clinical settings using functional electrical stimulation (FES), often involving the use of a controller like an iterative learning control (ILC) [<CitationRef CitationID="CR7">7</CitationRef>, <CitationRef CitationID="CR8">8</CitationRef>]. Robotic technology has also been used in FES systems to assist patients in performing their rehabilitation exercises [<CitationRef CitationID="CR9">9</CitationRef>]. Other methods employed include virtual reality [<CitationRef CitationID="CR10">10</CitationRef>] and constrained induced movement therapy [<CitationRef CitationID="CR11">11</CitationRef>, <CitationRef CitationID="CR12">12</CitationRef>]. Although there has been some success in achieving motor recovery in patients by these rehabilitation methods, a major disadvantage is that they are restricted to use in clinical settings because they involve many appliances and hence require unnecessary patient transfer to the clinic. Moreover, the use of sophisticated robotic frameworks, the need to correctly place sensors and the use of specialized software necessitate trained personnel to use the systems, which diminish their practicality in home settings. Hence, the clinical outcome of rehabilitation is often unsatisfactory among a wide group of stroke survivors [<CitationRef CitationID="CR3">3</CitationRef>]. The high socio-economic impact of post-stroke rehabilitation demands the development of a telemedicine system for devising a long-term and effective treatment strategy in home settings, thereby also helping to reduce costly human intervention [<CitationRef CitationID="CR13">13</CitationRef>]. A specialized application of telemedicine for stroke rehabilitation called “Telestroke” has been in practice for quite a long time and has had major success in delivering “around-the-clock” specialist clinical evaluation of stroke survivors in home settings [<CitationRef CitationID="CR14">14</CitationRef>].</Para>
          <Para ID="Par4">Rehabilitation is primarily aimed at restoring a level of physical and psychological functioning within patients that allows them to re-integrate themselves back into their daily life [<CitationRef CitationID="CR15">15</CitationRef>]. Rehabilitation guidelines are specifically bespoke to the requirements of each individual with an aim towards optimizing balance, mobility and gait functionality. Monitoring of physical activity and specific physiological parameters are the key components in interventions aimed at maintaining health and well-being, preventing falls and reducing motor functionality loss and the risk of recurrent stroke [<CitationRef CitationID="CR16">16</CitationRef>]. Measurements of various physiological parameters such as heart rate, respiratory rate, blood pressure, blood oxygen saturation and muscle activity are also useful for clinical evaluation of patients during rehabilitation [<CitationRef CitationID="CR17">17</CitationRef>].</Para>
          <Para ID="Par5">Physical activity has traditionally been monitored by questionnaires, citing its usefulness in covering large subject groups and cost-effectiveness [<CitationRef CitationID="CR18">18</CitationRef>, <CitationRef CitationID="CR19">19</CitationRef>]. These questionnaires can be completed by the patients themselves on a daily basis at home; manually or electronically through computing facilities. This data helps to formulate a patient activity log which is monitored periodically by the respective clinicians. There are also a wide range of clinical tests that are quite popular and performed by the therapists to assess rehabilitation of survivors within a clinical environment. The Wolf motor function test (WMFT) [<CitationRef CitationID="CR3">3</CitationRef>, <CitationRef CitationID="CR20">20</CitationRef>, <CitationRef CitationID="CR21">21</CitationRef>], Box and Block (B&amp;B) test [<CitationRef CitationID="CR22">22</CitationRef>] and the Nine Hole Peg (NHP) test [<CitationRef CitationID="CR23">23</CitationRef>] are some of the popular means to assess the patient’s motor ability while they perform designated tasks under the observation of a therapist. These tests are generally associated with a scoring system which is used to quantify the performance of the subjects. However, subjective measures of physical activity may overestimate activity levels as compared to assessments made by objective measures [<CitationRef CitationID="CR24">24</CitationRef>]. Hence, sensor-based telemedicine modalities provide an alternative towards objective measurement for patient monitoring [<CitationRef CitationID="CR25">25</CitationRef>, <CitationRef CitationID="CR26">26</CitationRef>].</Para>
          <Para ID="Par6">Recent developments in the fields of wearable technology, sensor miniaturization, communications and advanced processing techniques have enabled a new paradigm in patient monitoring within the home environment. They have led to the development of body area network (BAN) or body sensor network (BSN) systems which comprise miniaturized wireless-enabled sensor nodes positioned directly or indirectly (e.g. clothes, belts, shoes, wristwatches, mobile devices) on the human body. They are used to capture physiological signals that are reflective of the physiological state of the subject and also used for capturing kinematic data when the subject wearing them performs any movement or activities. Apart from sensing, front-end amplification, microcontroller functions and radio transmissions have all been integrated into a single circuit, thus resulting in a system-on-chip (SoC) implementation [<CitationRef CitationID="CR27">27</CitationRef>].</Para>
          <Para ID="Par7">A holistic overview of a typical wireless BAN system for remote healthcare monitoring is shown in Fig. <InternalRef RefID="Fig1">4.1</InternalRef>. Measurements obtained from the sensor nodes are transmitted to a central hub which can be in the form of a personal digital assistant (PDA), a smartphone, a personal computer or a microcontroller-based device. The central hub acts as a gateway and can be used for displaying the vital information on a user interface or transmitting the clinically relevant information to a remote medical centre or authorized caregiver [<CitationRef CitationID="CR28">28</CitationRef>].<Figure Category="Standard" Float="Yes" ID="Fig1">
              <Caption Language="En">
                <CaptionNumber>Fig. 4.1</CaptionNumber>
                <CaptionContent>
                  <SimplePara>Overview of a wireless body area network system for remote healthcare monitoring. On-body sensors monitor physiological parameters (ECG, SpO2) and motion and postural data. The collected data is transmitted to a PDA and/or transmitted through GPRS/Bluetooth to a medical server, physician or caregiver for further intervention [<CitationRef CitationID="CR28">28</CitationRef>]</SimplePara>
                </CaptionContent>
              </Caption>
              <MediaObject ID="MO1">
                <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_4_Fig1_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
              </MediaObject>
            </Figure>
</Para>
          <Para ID="Par8">The sensor nodes may contain accelerometers, gyroscopes, magnetometers and bio-amplifiers and can be attached to the body and used for recording kinematic data. Additionally, sensors for monitoring vital physiological parameters, such as electrocardiography (ECG) and electromyography (EMG), may also be included. Recent advances in material science have led to the development of e-textile-based systems which integrate sensing capability into garments. The sensors embedded on the garments can be used for collecting ECG and EMG signals, by weaving the electrodes into the fabric, and also used for gathering kinematic data by printing elastometer-based substances on the garments and sensing changes in their electrical characteristics (resistance or capacitance) associated with stretching of the garments when movements are performed by the wearer [<CitationRef CitationID="CR24">24</CitationRef>, <CitationRef CitationID="CR28">28</CitationRef>]. Some remote health monitoring systems have also combined wearable sensors and ambient sensors (sensors and motion detectors on doors, objects of daily use such as RFID tags) with an aim of developing smart homes that provide intelligent systems for health assistance in the subject’s living environment, also referred to as ambient assisted living (AAL) [<CitationRef CitationID="CR29">29</CitationRef>, <CitationRef CitationID="CR30">30</CitationRef>]. In this context, information collected by body-worn sensors can be augmented by the data from ambient sensors, such as motion sensors, distributed throughout the home environment to determine the pattern of activities performed by the patient and provide feedback concerning exercises and living behaviours for better health management.</Para>
        </Section1>
        <Section1 ID="Sec2">
          <Heading>Physiological Monitoring</Heading>
          <Para ID="Par9">Research into remote health monitoring over recent years has led to the development of many research prototypes [<CitationRef CitationID="CR31">31</CitationRef>–<CitationRef CitationID="CR34">34</CitationRef>], commercially available systems [<CitationRef CitationID="CR35">35</CitationRef>–<CitationRef CitationID="CR41">41</CitationRef>] and smartphone-based systems [<CitationRef CitationID="CR42">42</CitationRef>–<CitationRef CitationID="CR46">46</CitationRef>]. These developments have enabled long-term physiological monitoring of various clinically important parameters including heart rate, blood pressure, oxygen saturation, respiratory rate, body temperature and galvanic skin response, thereby improving the diagnosis and treatment of life-threatening incidents involved in cardiovascular or neurodegenerative disorders [<CitationRef CitationID="CR47">47</CitationRef>].</Para>
          <Para ID="Par10">A medical sensor platform, Code-Blue, was developed at Harvard University which could monitor multiple patients and consisted of custom-designed biosensor boards for pulse oximetry, three-lead ECG, EMG and motion activity [<CitationRef CitationID="CR48">48</CitationRef>]. It also caters to the transmission of data from the sensors to multiple receivers which include PDAs with the clinical staff. Live-net is another system developed by the MIT Media Laboratory for detecting Parkinson-like symptoms and epileptic seizures by using sensors to measure three-dimensional acceleration, ECG, EMG and galvanic skin conductance [<CitationRef CitationID="CR35">35</CitationRef>].</Para>
          <Para ID="Par11">As part of the research initiatives undertaken by the European Commission over the years, some of the projects worth mentioning in the field of remote health monitoring employing a wearable sensor platform are: MyHeart [<CitationRef CitationID="CR49">49</CitationRef>], WEALTHY [<CitationRef CitationID="CR50">50</CitationRef>] and MagIC [<CitationRef CitationID="CR51">51</CitationRef>]. These projects mainly led to the development of garment-based wearable sensors for general health monitoring of people within the home and community settings. One such project was AMON that led to the development of a wrist-worn device capable of monitoring blood pressure, skin temperature, blood oxygen saturation and ECG [<CitationRef CitationID="CR52">52</CitationRef>].</Para>
          <Para ID="Par12">A custom-built u-healthcare system consisting of ECG and blood pressure sensors as well as a cell phone for signal feature extraction, communication and display was developed in [<CitationRef CitationID="CR53">53</CitationRef>]. Only abnormal or alarming ECG and blood pressure (BP) patterns are transmitted to the hospital server, rather than transmitting all collected data, to save power. A wrist-worn sensor was used to record blood pressure readings and transmitted to the hospital server if found to be out of range.</Para>
          <Para ID="Par13">A BAN including sensors to measure ECG, PPG and PCG was developed in [<CitationRef CitationID="CR54">54</CitationRef>]. Another embedded sensor system named Bi-Fi was developed for wireless bio-signal recording, incorporating ECG, EEG and SpO2 sensors which performed on-board signal processing to cut-off transmission power [<CitationRef CitationID="CR55">55</CitationRef>]. A system named BASUMA [<CitationRef CitationID="CR56">56</CitationRef>] was developed with custom non-invasive sensors for monitoring parameters such as ECG, air and blood content of a thorax, body temperature, breath rate and cough control, blood pressure, pulse rate and oxygen saturation.</Para>
          <Para ID="Par14">A research project in the Netherlands (Human++) developed a BAN consisting of three sensor nodes for acquiring, amplifying, filtering, processing and wirelessly transmitting multi-channel signals of ECG, EEG and EMG [<CitationRef CitationID="CR57">57</CitationRef>]. An autonomous pulse oximeter was developed in Belgium research centre IMEC that transfers the body heat of the wearer into electrical energy storing it in a super capacitor for short-time storage, replacing batteries [<CitationRef CitationID="CR58">58</CitationRef>].</Para>
          <Para ID="Par15">There have been some designs which have targeted novel sensor locations like the one mentioned in [<CitationRef CitationID="CR59">59</CitationRef>], where a sensor worn as a ring measures blood oxygen saturation (SpO2) and heart rate. The ring sensor has techniques integrated within for motion artefact separation to improve measurement accuracy. This system was used for diagnosing hypertension and congestive heart failure.</Para>
          <Para ID="Par16">A microphone was used for measuring respiratory rate by placing it on the neck to capture acoustic signals associated with breathing. The signals obtained were band-pass filtered to remove noise and artefacts and used for detection of obstructive sleep apnea (OSA) [<CitationRef CitationID="CR60">60</CitationRef>]. A low-power flexible, ear-worn PPG sensor was developed for heart rate monitoring which was suited for long-term monitoring owing to its unobtrusive design [<CitationRef CitationID="CR61">61</CitationRef>]. However, the system suffered from motion artefacts which adulterated the sensor signals.</Para>
          <Para ID="Par17">Chemical sensors have also been used in wearable systems to detect and monitor the presence of harmful compounds and alert people working in hazardous environments. A non-invasive wearable closed-loop quasi-continuous drug infusion system was developed by researchers for measuring blood glucose levels and infuses insulin automatically [<CitationRef CitationID="CR62">62</CitationRef>].</Para>
          <Section2 ID="Sec3">
            <Heading>Mobile Phone-Based Physiological Monitoring</Heading>
            <Para ID="Par18">The use of mobile phones has also been very popular in recent years, where the inbuilt inertial sensors have been used for collecting data. With the advent of smartphones having Internet connectivity and expandable low-cost memory, they have also been used as a base station with which other sensors communicate and also used as a display unit. We will review a few phone-based wearable health monitoring systems.</Para>
            <Para ID="Par19">A prototype system developed by Microsoft (HealthGear) consists of a non-invasive blood oximeter, a sensor assembly to measure oxygen saturation and heart rate signals, Bluetooth-enabled wireless transmission and a cell phone for user interface. This has been used for detecting mild and severe OSA. The system also had satisfactory feedback from the users in terms of wearability and functionality [<CitationRef CitationID="CR47">47</CitationRef>].</Para>
            <Para ID="Par20">A cell phone-based real-time wireless ECG monitoring system (HeartToGo) was developed for the detection of cardiovascular abnormalities [<CitationRef CitationID="CR46">46</CitationRef>]. A mobile care system developed in [<CitationRef CitationID="CR44">44</CitationRef>] utilizes a Bluetooth-enabled blood pressure monitor and ECG sensor. A mobile phone was used as the processing core with an alert mechanism generated depending on the detection of an emergency situation. Finally, an application for detecting arrhythmia events has been described in [<CitationRef CitationID="CR43">43</CitationRef>] which used a handheld device as a PDA. The PDA serves as the smart device for processing and analysing the continuously transmitted ECG signals. It also communicates remotely with a clinician, transmitting alarm signals and minute long raw ECG samples.</Para>
            <Para ID="Par21">A wearable system for stress monitoring depending on the emotional state of an individual (AUBADE) has been developed by researchers [<CitationRef CitationID="CR63">63</CitationRef>]. It consists of a 3-lead ECG sensor and a respiration rate sensor strapped to the chest, a 16-lead EMG sensor embedded in textile and a galvanic skin response sensor attached within a glove. It employs an intelligent emotion recognition module with a three-dimensional facial representation mechanism to provide feedback.</Para>
          </Section2>
          <Section2 ID="Sec4">
            <Heading>Commercially Available Systems</Heading>
            <Para ID="Par22">A range of wearable health monitoring systems are already commercially available, such as the wrist-worn device called Vivago WristCare for monitoring skin temperature, skin conductivity and movement [<CitationRef CitationID="CR42">42</CitationRef>]. A similar device known as SenseWear Armband monitors ambient temperature and heat flow [<CitationRef CitationID="CR41">41</CitationRef>]. Both communicate collected data to a base station and report any alarming situations for further evaluation by responsible clinicians. A washable lightweight vest including respiratory rate sensors, one-lead ECG for heart rate measurements and an accelerometer for activity monitoring was developed by VivaMetrics and known as LifeShirt [<CitationRef CitationID="CR38">38</CitationRef>]. Another garment-based physiological monitoring tool for monitoring heart rate, respiration rate, posture, activity, skin temperature and GPS location was developed in [<CitationRef CitationID="CR37">37</CitationRef>], known as Watchdog. Sensatex also came up with a smart T-shirt-based wearable system for measuring ECG, respiration rate and blood pressure working on conductive fibre sensors [<CitationRef CitationID="CR64">64</CitationRef>]. A mobile cardiac outpatient telemetry (MCOT) system was developed by CardioNet for measuring ambulatory ECG aimed at treating patients with arrhythmia [<CitationRef CitationID="CR36">36</CitationRef>].</Para>
            <Para ID="Par23">Manufacturers like Philips [<CitationRef CitationID="CR65">65</CitationRef>], Nellcor [<CitationRef CitationID="CR66">66</CitationRef>], Agilent [<CitationRef CitationID="CR67">67</CitationRef>] and Nonin [<CitationRef CitationID="CR68">68</CitationRef>] are providing low-cost, lightweight fingertip pulse oximeters providing real-time display of heart rate and blood oxygen saturation. Chest-worn belts and wristwatch for displaying heart rate measurements have been developed by Polar [<CitationRef CitationID="CR69">69</CitationRef>] and Omron [<CitationRef CitationID="CR70">70</CitationRef>].</Para>
          </Section2>
          <Section2 ID="Sec5">
            <Heading>Safety Monitoring Systems</Heading>
            <Para ID="Par24">A number of devices designed for safety monitoring are already commercially available. Simple systems such as Life Alert Classic [<CitationRef CitationID="CR71">71</CitationRef>] and AlertOnce [<CitationRef CitationID="CR72">72</CitationRef>] provide the basic facility of manual alarm activation through the use of a push button contained in a wristwatch. When activated by the user, an alarm message is transmitted to the nearest remote care facility, and the appropriate emergency response is initiated. By comparison, the Wellcore system is more sophisticated and uses accelerometers and advanced microprocessors to monitor the body position and is able to detect falls and generate an alarm message in times of emergency that is relayed to the nearest response centre [<CitationRef CitationID="CR73">73</CitationRef>]. Another such device in the form of a chest strap is MyHalo [<CitationRef CitationID="CR74">74</CitationRef>], which can be used for detecting falls, monitoring heart rate, skin temperature, sleep/wake patterns and activity levels. The BrickHouse system also employs an automatic fall detector and an alarm generation facility [<CitationRef CitationID="CR75">75</CitationRef>].</Para>
            <Para ID="Par25">Reliable fall detection using wearable sensors has also been a well-researched topic, and there exists various research prototypes. An automatic fall detection and alarm generation device was developed by researchers at CSEM [<CitationRef CitationID="CR76">76</CitationRef>]. They achieved high recognition precision with simulated falling situations with a wrist-worn sensor, which was easy to put on. There have been other alternative approaches where researchers have embedded a tri-axial accelerometer in a custom-designed vest to detect falls. A barometric pressure sensor has also been used to measure altitude and discriminate fall situations from normal bending down or sitting down activities [<CitationRef CitationID="CR77">77</CitationRef>]. An accelerometer embedded in an assistive device such as a cane, which is used by many elderly people for maintaining balance, was used as a fall detecting mechanism in the Smart-Fall system [<CitationRef CitationID="CR78">78</CitationRef>].</Para>
            <Para ID="Par26">Smartphones have also played a major part in modern fall detection systems, where it is augmented by the GPS facility within the phone to detect the location of the subject who has fallen [<CitationRef CitationID="CR79">79</CitationRef>]. The accelerometer within the smartphone was used effectively for robust detection of falls along with the Google map facility for locating the event, and an associated alarm was generated to the respective caregiver or to the family members through a messaging facility (sms) [<CitationRef CitationID="CR80">80</CitationRef>].</Para>
            <Para ID="Par27">More recently, there has been a paradigm shift towards prevention of fall-related injuries by pre-empting a falling incident and using airbag technology to minimize impact [<CitationRef CitationID="CR81">81</CitationRef>]. However, further development in the miniaturization of airbags is necessary to produce unobtrusive and comfortable systems that would be acceptable to the user. A system designed to detect freezing of gait (FOG) events that are commonly associated with neurological disorders such as Parkinson’s disease was developed in [<CitationRef CitationID="CR82">82</CitationRef>]. It provides subjects with a rhythmic auditory signal for stimulating them to resume walking when an FOG episode is detected. An android application, iWander, using GPS and communication facilities available on the smartphone was developed to provide assistance with tracking location for subjects suffering from dementia [<CitationRef CitationID="CR83">83</CitationRef>].</Para>
          </Section2>
        </Section1>
        <Section1 ID="Sec6">
          <Heading>Activity Monitoring</Heading>
          <Para ID="Par28">Activity monitoring is a well-researched and broad topic. Human activity monitoring has gained prominence with the use of wearable sensors and video-based sensing technologies and is a major area of remote health monitoring systems. For post-stroke rehabilitation, tracking the number of times a patient performs specific movements (e.g. exercises) with their impaired body parts (e.g. paretic arm) during training and also throughout the day can provide useful information on the progress of the patient. The frequency of specific movements and the quality of the movements performed (e.g. fluidity/smoothness) are likely to increase as the motor functionality of the patient improves. It can also provide information on the patient’s compliance to the specific guidelines set by the respective clinicians during rehabilitation training.</Para>
          <Para ID="Par29">Rehabilitation is primarily carried out by repeated exercises of the impaired limb to maximize the chances of recovery [<CitationRef CitationID="CR84">84</CitationRef>]. However, it is well perceived in the medical community that exercises alone do not suffice for achieving a speedy recovery due to various factors. This is due to the lack of motivation among patients to exercise for sustainable period of time and the fact that exercises comprise only a minor proportion of time and energy spent by a subject as compared to the wide range of activities performed throughout the day. Moreover, patients tend to compensate their paretic arm with their non-impaired arm, making rehabilitation progress slower [<CitationRef CitationID="CR85">85</CitationRef>–<CitationRef CitationID="CR87">87</CitationRef>].</Para>
          <Para ID="Par30">Home-based rehabilitation through monitoring of specific activities has gained prominence over the recent years through the development of various exercise platforms [<CitationRef CitationID="CR88">88</CitationRef>–<CitationRef CitationID="CR90">90</CitationRef>], virtual reality (VR)-based systems [<CitationRef CitationID="CR91">91</CitationRef>, <CitationRef CitationID="CR92">92</CitationRef>], gaming consoles [<CitationRef CitationID="CR93">93</CitationRef>, <CitationRef CitationID="CR94">94</CitationRef>] and the widely popular Kinect camera-based system [<CitationRef CitationID="CR95">95</CitationRef>]. These approaches mainly aim to monitor the rehabilitation progress of the patients during the exercise or the training phase in a controlled environment within a designated zone (exercise/gaming platforms and vicinity of camera systems). The main difficulty with this approach is that it offers no possibility to monitor the movement quality of the patients and their compliance with the prescribed exercises in their natural environment (i.e. while performing daily activities) which are more objective reflections of the actual rehabilitation state and of the effectiveness of the prescribed therapy.</Para>
          <Para ID="Par31">Hence, there has been a growing demand to monitor subjects as they perform their daily activities within their home and community settings. Quantifying the daily activities performed by patients would help to ascertain their degree of participation and thereby formulate a qualitative index of their lifestyle [<CitationRef CitationID="CR96">96</CitationRef>]. A taxonomy of activities known as Activities of Daily Living (ADLs) developed by [<CitationRef CitationID="CR97">97</CitationRef>] gained prominence in the research community owing to its relevance to real-world applications. Typical examples of ADLs include brushing teeth, combing, washing, cooking, bathing or walking [<CitationRef CitationID="CR98">98</CitationRef>–<CitationRef CitationID="CR102">102</CitationRef>]. Accordingly, there have been extensive research efforts to assess the accuracy of wearable sensors in classifying ADLs [<CitationRef CitationID="CR103">103</CitationRef>–<CitationRef CitationID="CR107">107</CitationRef>] which has supported medical diagnosis during rehabilitation and augmented traditional medical methods in recovery of chronic impairments [<CitationRef CitationID="CR108">108</CitationRef>].</Para>
          <Para ID="Par32">Human activity recognition (HAR) is a challenging and highly researched topic in many diverse fields which include pervasive and mobile computing [<CitationRef CitationID="CR109">109</CitationRef>, <CitationRef CitationID="CR110">110</CitationRef>], context-aware computing [<CitationRef CitationID="CR111">111</CitationRef>, <CitationRef CitationID="CR112">112</CitationRef>] and remote health monitoring systems which also include AAL [<CitationRef CitationID="CR113">113</CitationRef>–<CitationRef CitationID="CR117">117</CitationRef>]. Advances in wireless sensor technology have caused a paradigm shift from low-level data collection and transmission to high-level information integration, processing and activity recognition [<CitationRef CitationID="CR118">118</CitationRef>]. The different approaches are variants of the underlying sensor technology, the machine learning models and the environment in which the activities are performed. Before embarking on activity modelling and recognition methodologies, it is imperative to understand the different levels of granularity inherent in human behaviour.</Para>
          <Section2 ID="Sec7">
            <Heading>Movement Categories</Heading>
            <Para ID="Par33">Depending on the complexity of activities performed, they can be categorized into mainly four different levels: gestures, actions, interactions and group activities. Gestures are movements performed by the subject’s body parts which are atomic components comprising any holistic movement. Some common examples of gestures are raising the hand or stretching the leg. Actions are activities performed by individuals that are composed of multiple gestures aligned together to form a meaningful movement. For example, walking or reaching and picking a cup can be described as completed actions. Interactions are used to describe human–object or human–human interaction like making tea. Group activities, as the name suggests, are performed by multiple persons, for example, a group marching together [<CitationRef CitationID="CR118">118</CitationRef>, <CitationRef CitationID="CR119">119</CitationRef>].</Para>
            <Para ID="Par34">Activity recognition is a complex process and can be categorized into four main steps: (1) choice and deployment of appropriate sensors and the environment in which the activity will be performed; (2) collection, storage and processing of information through data analysis techniques and knowledge representation formalisms at appropriate levels of abstraction; (3) creation of computational models which allows reasoning and manipulation and (4) to develop reasoning algorithms from sensor data that helps to recognize activities performed.</Para>
          </Section2>
          <Section2 ID="Sec8">
            <Heading>Modalities of Activity Recognition</Heading>
            <Para ID="Par35">Home-based activity recognition can be classified as: vision-based and sensor-based recognition. Vision-based activity recognition uses visual sensing facilities, such as video cameras and still cameras, to monitor a subject’s movement in a designated area. The generated sensor data are video sequences or digitized visual data. Recognition of activities further takes place by the use of computer vision techniques which include feature extraction, structural modelling, movement segmentation, action extraction and movement tracking to analyse visual observations. The use of gaming consoles with camera systems and also the Microsoft Kinect has been quite popular in the field of rehabilitation. However, they suffer from occlusion problems since they are designated to mostly indoor activities with their surveillance restricted within a specific zone. Moreover, inferring the movements performed often involves the use of complex image processing algorithms [<CitationRef CitationID="CR120">120</CitationRef>].</Para>
            <Para ID="Par36">Sensor-based activity recognition was further explored owing to a paradigm shift towards monitoring of activities in unconstrained daily life settings. The sensors used in activity recognition mainly generate time series data of various parameters or state changes. The data is processed through statistical analysis methods, probabilistic models, data fusion or formal knowledge technologies for recognizing the underlying activity. Sensors for activity recognition are generally attached to the body of the subject as wearable sensors or are inherent in portable instruments such as smartphones. Sensors can also be embedded within the living environment of the subject and thereby create ambient intelligent applications such as smart environments. For example, sensors attached to objects of daily use can record human–object interaction. Recognition methodologies utilizing multimodal miniaturized sensors present in the environment is referred to as dense sensing approach. In this approach, activities are characterized by the objects that are manipulated during the performed movements in real-world settings. They are widely used in AAL through the smart home paradigm [<CitationRef CitationID="CR121">121</CitationRef>–<CitationRef CitationID="CR123">123</CitationRef>]. Sensors in smart homes are used to initiate a time-bound context-aware ADL assistance. For example, a pressure mat or force plate can indicate position and movement of a subject within a defined environment, and a pressure sensor in a bed can suggest sleeping activity of the subject [<CitationRef CitationID="CR31">31</CitationRef>]. In general, wearable sensor-based monitoring is used in pervasive and mobile computing, while the dense sensing-based approach is more suitable for intelligent environment-enabled applications. Both approaches are not mutually exclusive, however, and in some applications they can work well together such as in RFID-based activity monitoring, where objects in the environment are instrumented with tags, and users wear an RFID reader fixed to a glove or a bracelet [<CitationRef CitationID="CR117">117</CitationRef>, <CitationRef CitationID="CR124">124</CitationRef>].</Para>
          </Section2>
          <Section2 ID="Sec9">
            <Heading>Inertial Sensor-Based Activity Recognition</Heading>
            <Para ID="Par37">Activity recognition using wearable inertial sensors primarily involves the capturing of kinematic signals which are used to measure acceleration, velocity, distance, rotation, rate of rotation, angle and time. These measurements help to determine position of a limb segment or the angle of flexion of a limb joint. These classes of signals are widely used as health indicators encompassing gait, posture, spasticity, tremor and balance, some of which are subjective parameters in clinical assessment. Most of these parameters can be measured with the help of kinematic sensors like accelerometers, gyroscopes and magnetometers [<CitationRef CitationID="CR125">125</CitationRef>, <CitationRef CitationID="CR126">126</CitationRef>], thereby removing the subjective quotient from symptomatic data [<CitationRef CitationID="CR127">127</CitationRef>].</Para>
            <Para ID="Par38">Kinematic sensors are based on the transformation of physical parameters to an electrical signal which is further processed. Microelectromechanical systems (MEMS) play a key role in remote health monitoring applications, where size and power consumption are of vital importance. In MEMS-based transducers, changes in the electromechanical characteristics of the silicon are used to generate resistive or capacitive variations when the microstructure within the transducer is excited by external forces such as compression, pressure, temperature and acceleration. [<CitationRef CitationID="CR128">128</CitationRef>]. Some of the popular kinematic sensors used in the field of HAR are accelerometers, gyroscopes and magnetometers [<CitationRef CitationID="CR125">125</CitationRef>, <CitationRef CitationID="CR127">127</CitationRef>].</Para>
            <Para ID="Par39">An MEMS accelerometer is probably the most frequently used wearable sensor used for activity recognition. Gyroscopes are used primarily for computing rates of rotation (°/s) and, when combined with accelerometers, form an inertial measurement unit (IMU) [<CitationRef CitationID="CR129">129</CitationRef>], in which the gyroscopes are additionally used to compensate for errors in the accelerometers due to changes in orientation. Hence, IMUs can be used to measure acceleration, velocity, distance, rotation and orientation [<CitationRef CitationID="CR128">128</CitationRef>]. A magnetometer is another device that is sometimes used in remote monitoring applications that responds to the strength and direction of the Earth’s magnetic field. Considering that the magnetic field vector has a constant direction and magnitude within a predefined area on the Earth’s surface (given by the latitude and longitude), a magnetometer can be used to track orientation with respect to this localized constant field vector [<CitationRef CitationID="CR128">128</CitationRef>]. Magnetometers are not extensively used in health monitoring applications due to the fact that the Earth’s magnetic field can be distorted by the presence of ferromagnetic materials [<CitationRef CitationID="CR130">130</CitationRef>]. Patients requiring wheelchair support (with steel frame) or the presence of other ferromagnetic substances within the home environment (e.g. in the kitchen) are likely to distort the reference magnetic field.</Para>
          </Section2>
          <Section2 ID="Sec10">
            <Heading>Data-Driven Versus Knowledge-Driven Approach</Heading>
            <Para ID="Par40">Processing of sensor data for recognizing activities can be categorized into two approaches: data-driven and knowledge-driven. Development of activity models is important for interpreting the sensor data to infer activities. In the data-driven approach, sensor data collected as a result of the movements performed by the subjects are used to build activity models with the help of data mining techniques and relevant machine learning algorithms. Since this involves probabilistic or statistical methods of classification driven by the data, the process is generally referred to as data-driven or bottom-up approach. Although this approach has its advantages in being robust to uncertainties and temporal variation in information, it requires the availability of a large dataset for training the activity model. Further it suffers from reusability and scalability as often it has been seen that activity models developed and evaluated on a particular subject’s data does not work on the movement data of another subject owing to the large degree of variability inherent in human movement [<CitationRef CitationID="CR118">118</CitationRef>].</Para>
            <Para ID="Par41">The knowledge-driven approach, on the other hand, is used to exploit the rich prior domain knowledge to build upon an activity model. This involves knowledge acquisition, formal modelling and representation and is hence referred to as knowledge-driven or top-down approach. It is based upon the observation that most activities are performed in a relatively specific location, time and space. A suitable example would be an act of brushing teeth, which takes place in the morning, evening or at night and involves the use of a toothbrush. Similarly, cooking in the kitchen involves the use of the microwave or cutlery. This implicit relationship between activities, temporal and spatial context and the entities involved provides a rich domain knowledge and heuristics for activity modelling and pattern recognition [<CitationRef CitationID="CR118">118</CitationRef>]. This approach is semantically clear, logically simple but weak in cases of uncertainty and temporal information.</Para>
            <Para ID="Par42">Having discussed the different modalities and approaches of HAR, we will take an in-depth look into the process of activity modelling, classification and recognition using the data-driven approach. Prior to this, we shall first examine the various challenges concerning activity recognition.</Para>
          </Section2>
          <Section2 ID="Sec11">
            <Heading>Challenges in Activity Recognition</Heading>
            <Para ID="Par43">Activity recognition presents more degrees of freedom with respect to system design and implementation when compared to language processing or speech recognition [<CitationRef CitationID="CR131">131</CitationRef>]. However, owing to the diversity inherent in the data collected by different individuals performing the same action or by the same individual performing an action in different environments, it requires careful consideration of the type and placement of sensors and data analysis techniques used, depending on the application scenario and activities to be monitored [<CitationRef CitationID="CR132">132</CitationRef>].</Para>
            <Section3 ID="Sec12">
              <Heading>Class Variability</Heading>
              <Para ID="Par44">A recognition system has to be robust enough to handle intra-class variability. In this context, class refers to the activities that are to be detected by any recognition methodology. This variability is primarily due to the fact that a same activity is performed differently by different individuals. Further this might also happen with an individual who repeats the same activity over time due to factors as fatigue or environmental changes. Therefore, there can be two approaches towards training an activity recognition system.</Para>
              <Para ID="Par45">A system trained with movement data of more than one subject, or a person-independent training system would be susceptible to considerable inter-person variability [<CitationRef CitationID="CR132">132</CitationRef>]. To address this issue, the number of data points for each subject can be increased or an alternative approach can be person-dependent training, i.e. training the system on the movement data of single person. This might as well be robust enough towards capturing considerable intra-person variability. This system, however, requires the collection of a large set of data collected from one individual to train the system, thereby capturing as much variability as possible. The choice of the training sample is application dependent and hence a trade-off is required between the selection of a highly specific and discriminative dataset or a generic dataset which is potentially less discriminative but robust across multiple subjects [<CitationRef CitationID="CR131">131</CitationRef>]. In general, for remote health applications, formulating a person-centric training data would be beneficial when applied to monitoring of individual patients who demonstrate differences in levels of impairment depending on their stage of rehabilitation [<CitationRef CitationID="CR133">133</CitationRef>]. Another interesting challenge in recognizing activities is the similarity in characteristics prevalent across activities [<CitationRef CitationID="CR134">134</CitationRef>]. For example, if we would like to distinguish between drinking water from a glass and drinking coffee from a cup, it would be very difficult to differentiate from the kinematic data pertaining to both the movements. Therefore, in such cases sensors deployed in the environment like RFID tags attached to objects can prove to be helpful. Therefore, this is dictated by the requirements of the application [<CitationRef CitationID="CR124">124</CitationRef>].</Para>
              <Para ID="Par46">An intriguing problem occurs during activity recognition on continuous streaming data or real-time monitoring applications where the data needs to be segmented depending on the activities we want to monitor and those that are irrelevant to the application. This is referred to as the NULL class in the relevant literature [<CitationRef CitationID="CR135">135</CitationRef>] and is difficult to model since it represents a plethora of activities in infinite space. It can however be identified if the signal characteristics gathered from the sensor data are completely different to the ones that are being monitored and hence involves a threshold-based mechanism to filter out the unwanted data. Class imbalance is a major problem, especially during long-term monitoring where all activities being tracked do not have the similar number of occurrences. A common example would be the number of instances of a drinking action and a walking action [<CitationRef CitationID="CR136">136</CitationRef>]. There are however a couple of techniques which can be adapted to get around this problem of class imbalance. Firstly, generating artificial training data for a class that is underrepresented to balance out inequality; secondly, oversampling or interpolating smaller class sizes to match a bigger class size [<CitationRef CitationID="CR137">137</CitationRef>].</Para>
            </Section3>
            <Section3 ID="Sec13">
              <Heading>Ground Truth Annotation</Heading>
              <Para ID="Par47">Annotating the ground truth of activities being monitored in real-life scenarios is another interesting challenge, especially with data from wearable inertial sensors as opposed to data obtained from video recordings. With activities performed in the laboratory or controlled environment, annotations of the training data can be performed post hoc based on video footages. However, in nomadic settings, ground truth annotation of activities is a very difficult problem to solve. Researchers generally depend on self-recalling methods [<CitationRef CitationID="CR138">138</CitationRef>], experience sampling [<CitationRef CitationID="CR139">139</CitationRef>] and reinforcement learning, all of which involves testimonies from the subject themselves. Therefore, many researchers have based their work on a list of activities performed under a semi-naturalistic condition, where the subjects perform the movements as they would do in normal daily life, and another person annotates their activities by means of visual inspection in real time [<CitationRef CitationID="CR100">100</CitationRef>]. This therefore helps in gaining the ground truth information required for evaluating the recognition methodology.</Para>
            </Section3>
            <Section3 ID="Sec14">
              <Heading>Sensor Requirements</Heading>
              <Para ID="Par48">The experimental design gives rise to another challenge that of data collection, sensor selection, placement and the number of sensors to be used. As opposed to other computer vision problems like heart monitoring, brain activity modelling or speech recognition, HAR does not have a standard allocated dataset to start with the data analysis as it is completely dependent on the requirement, and experiments are designed in pursuit of recognizing only the selected movements. Sensor characteristics also present a significant amount of challenge for long-term monitoring of activities as hardware failures, sensor drifts and errors in the software aimed at capturing the data can lead to erroneous situations. External factors such as temperature, pressure and change in positioning due to loose straps can cause the need for frequent recalibration, thereby affecting the sensor data being recorded [<CitationRef CitationID="CR140">140</CitationRef>, <CitationRef CitationID="CR141">141</CitationRef>].</Para>
              <Para ID="Par49">One of the last challenges is power requirement of the battery-operated wireless sensors which are increasingly being used in the field of remote health monitoring. The remote monitoring system in place at present transmits the captured signals collected by the sensor nodes placed on the patient’s body to the remote server at the back-office service platform wirelessly, where the signals are analysed [<CitationRef CitationID="CR142">142</CitationRef>]. This system requires continuous transmission of data from the sensors to the server using wireless protocols taking into account the nomadic environment. The fundamental problem with continuous data transmission is the energy requirement. A result from respective investigations into continuous data transmission at 1 kHz suggests that it can be supported for 24-h monitoring using a 1200-mAh battery [<CitationRef CitationID="CR143">143</CitationRef>].</Para>
              <Para ID="Par50">The analysis presented in [<CitationRef CitationID="CR143">143</CitationRef>] regarding the power consumption and longevity of batteries pertains to transmission energy only, added to it the energy involved in pre-processing the physiological data at the sensor nodes including analogue-to-digital conversion, quantization, filtering and the microcontroller operation would bring down the effective time of monitoring to 8–10 h, thereby making the entire system power hungry and affecting the life of the batteries. An increased battery capacity like the prismatic zinc–air battery—1800 mAh operating at 1.4 V used recently in the medical community would increase the respective sizes of the sensor nodes. Furthermore, the use of Bluetooth transceivers consuming 40–55 mA with operating voltage in the range of 3–3.6 V would necessitate the use of three such zinc–air batteries, making it non-ideal in terms of volume for body-worn applications. The supply voltage is quadratically proportional to the power dissipation and therefore an optimal power supply to sustain the continuous Bluetooth transmission would have an adverse impact on the operational lifetime of the battery-powered sensor nodes. Considering Bluetooth as the primary means of communication, the energy dissipation is directly dependant on the packet format of the data being transmitted which can be optimized using standard duty cycling and might eventually lead to delays and packet loss of data which would be highly undesirable for applications involving remote health monitoring [<CitationRef CitationID="CR142">142</CitationRef>].</Para>
              <Para ID="Par51">Therefore, from the long-term system operation perspective, when implementing a wireless body area network (WBAN) that comprises heterogeneous sensors, it is imperative to select data analysis algorithms having low computational complexity. This is because energy consumption is directly proportional to the computational complexity of the processing algorithms used. Therefore, for applications such as real-time movement detection requiring online operation, it is imperative to perform the data processing (feature extraction, classification) in a low-power way on a sensor platform [<CitationRef CitationID="CR132">132</CitationRef>, <CitationRef CitationID="CR142">142</CitationRef>] itself while for applications supporting long-term behavioural or trend analysis, offline data processing may be sufficient [<CitationRef CitationID="CR144">144</CitationRef>].</Para>
            </Section3>
          </Section2>
          <Section2 ID="Sec15">
            <Heading>Activity Recognition: Process Flow</Heading>
            <Para ID="Par52">In this section, we discuss the sequence of signal processing and pattern recognition techniques that help to implement a specific activity recognition behaviour using supervised learning methodologies. The process flow is shown in Fig. <InternalRef RefID="Fig2">4.2</InternalRef>.<Figure Category="Standard" Float="Yes" ID="Fig2">
                <Caption Language="En">
                  <CaptionNumber>Fig. 4.2</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>Activity recognition process flow</SimplePara>
                  </CaptionContent>
                </Caption>
                <MediaObject ID="MO2">
                  <ImageObject Color="BlackWhite" FileRef="MediaObjects/327370_1_En_4_Fig2_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                </MediaObject>
              </Figure>
</Para>
            <Section3 ID="Sec16">
              <Heading>Data Acquisition and Pre-processing</Heading>
              <Para ID="Par53">Raw data is collected from multiple sensors attached to the body or sensors placed on objects or in the environment or from both, depending on the application requirement as discussed in Sect. <InternalRef RefID="Sec8">4.3.2</InternalRef>. Data coming from each sensor sampled at regular intervals results in a multivariate time series. However, the sampling rates of different types of sensors might differ. Therefore, there is a need to synchronize multimodal sensor data. Inertial sensor data are generally sampled at low frequencies, 20–30 Hz, depending on the movements to be monitored. Certain sensors like accelerometers, gyroscopes and magnetometers can produce data that have multiple dimensions (<Emphasis Type="Italic">X</Emphasis>-, <Emphasis Type="Italic">Y</Emphasis>- and <Emphasis Type="Italic">Z</Emphasis>-axis). Therefore, a multidimensional and a multimodal sensor output can be represented by (1), where <Emphasis Type="Italic">S</Emphasis> represents the sensor output, <Emphasis Type="Italic">m</Emphasis> represents the number of sensors and <Emphasis Type="Italic">d</Emphasis>
<Subscript>1</Subscript>
<Emphasis Type="Italic">…d</Emphasis>
<Subscript>
                  <Emphasis Type="Italic">n</Emphasis>
                </Subscript> represents the sensor-specific data sampled at regular intervals.<Equation ID="Equ1">
                  <EquationNumber>4.1</EquationNumber>
                  <MediaObject>
                    <ImageObject Color="BlackWhite" FileRef="327370_1_En_4_Chapter_Equ1.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </MediaObject>
                  <EquationSource Format="MATHML">
                    <math xmlns:xlink="http://www.w3.org/1999/xlink">
                      <mrow>
                        <msub>
                          <mi>S</mi>
                          <mi>i</mi>
                        </msub>
                        <mo>=</mo>
                        <mrow>
                          <mo>[</mo>
                          <mrow>
                            <msub>
                              <mi>d</mi>
                              <mn>1</mn>
                            </msub>
                            <mo>,</mo>
                            <msub>
                              <mi>d</mi>
                              <mn>2</mn>
                            </msub>
                            <mo>…</mo>
                            <msub>
                              <mi>d</mi>
                              <mi>n</mi>
                            </msub>
                          </mrow>
                          <mo>]</mo>
                        </mrow>
                        <mo>,</mo>
                        <mspace width="0.25em"/>
                        <mspace width="0.25em"/>
                        <mspace width="0.25em"/>
                        <mspace width="0.25em"/>
                        <mi>i</mi>
                        <mo>=</mo>
                        <mn>1</mn>
                        <mo>,</mo>
                        <mo>…</mo>
                        <mo>,</mo>
                        <mi>m</mi>
                      </mrow>
                    </math>
                  </EquationSource>
                  <EquationSource Format="TEX"><![CDATA[
$$ {S}_i=\left[{d}_1,{d}_2\dots {d}_n\right],\kern0.46em i=1,\dots, m $$
]]></EquationSource>
                </Equation>
</Para>
              <Para ID="Par54">The raw sensor data contains noise and is often corrupted by artefacts caused due to various factors. Artefacts are generally induced into the data due to sensor malfunctioning (e.g. drift) or due to unwanted movements of the body [<CitationRef CitationID="CR131">131</CitationRef>]. The pre-processing stage aims to remove the low-frequency artefacts and high-frequency noise components by using high-pass and low-pass filters [<CitationRef CitationID="CR132">132</CitationRef>]. The pre-processing algorithms also synchronize the data coming from various sensors and prepare it for the next stage of feature extraction. It preserves the signal characteristics which carries the relevant information about the activities of interest. Data from inertial sensors are in general calibrated; their units converted (as most sensor outputs are arbitrary units), normalized, resampled, synchronized, then filtered or fused in the pre-processing stage. Sensor fusion is generally performed where signals from multiple sensor axes are selected a priori, based on the activities being tracked. For example, specific accelerometer and gyroscope axes can be fused (both sensors placed on the wrist) for detecting a reach and retrieve action [<CitationRef CitationID="CR132">132</CitationRef>].</Para>
            </Section3>
            <Section3 ID="Sec17">
              <Heading>Data Segmentation</Heading>
              <Para ID="Par55">The pre-processed signal is segmented to identify only those segments that contain information about the activities that are being monitored. This process is also commonly referred to as event detection or activity spotting since it detects the signal frame representative of the activity of interest. The boundary of each segregated time series data is represented by the start and stop time. Thus, each segmented time series represents the potential activities to be monitored. Segmenting a continuous stream of data is a difficult task, especially for monitoring ADL. For example, consider a drinking activity. This may be considered as starting when reaching for a cup or glass. Alternatively, it may be considered as starting when the cup is raised to the lips. But what about when the cup is simply resting in the hand between individual sips? Should this still be classified as a drinking activity? In such circumstances, it is difficult to determine the boundaries of the activity from the signal. There are various segmentation algorithms that are used in relevant research, popular among them being the sliding window technique, energy-based segmentation, rest-position segmentation and using data from one sensor to segment another sensor reading [<CitationRef CitationID="CR131">131</CitationRef>].</Para>
              <Para ID="Par56">The sliding window technique is one of the most popular segmenting schemes followed in diverse applications. As is suggested by the name, a fixed-size window representing definite time duration is used to extract segments of a signal [<CitationRef CitationID="CR108">108</CitationRef>]. If a very small interval is chosen, there is a possibility of missing out on a relevant activity whereas a longer window size would pertain to multiple activities, thereby affecting the classification decision. Hence, a dynamic window selection technique based on a data-driven or a probabilistic approach for segmenting each individual activity would be an optimal solution although this increases the computational load [<CitationRef CitationID="CR118">118</CitationRef>].</Para>
              <Para ID="Par57">Another popular approach adopted for segmenting different activities is based on the energy content of the signal reflecting the change in intensity levels. The differences in energy levels in the signal are representative of the intensity variations of the activities that produce these kinematic signals. The energy content of a signal <Emphasis Type="Italic">s</Emphasis>(<Emphasis Type="Italic">t</Emphasis>) is given by (<InternalRef RefID="Equ2">4.2</InternalRef>).<Equation ID="Equ2">
                  <EquationNumber>4.2</EquationNumber>
                  <MediaObject>
                    <ImageObject Color="BlackWhite" FileRef="327370_1_En_4_Chapter_Equ2.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </MediaObject>
                  <EquationSource Format="MATHML">
                    <math xmlns:xlink="http://www.w3.org/1999/xlink">
                      <mrow>
                        <mi>E</mi>
                        <mo>=</mo>
                        <munderover>
                          <mstyle displaystyle="true" mathsize="10">
                            <mo>∫</mo>
                          </mstyle>
                          <mrow>
                            <mo>−</mo>
                            <mi>∞</mi>
                          </mrow>
                          <mi>∞</mi>
                        </munderover>
                        <msup>
                          <mrow>
                            <mrow>
                              <mo>|</mo>
                              <mrow>
                                <mi>s</mi>
                                <mrow>
                                  <mo>(</mo>
                                  <mi>t</mi>
                                  <mo>)</mo>
                                </mrow>
                              </mrow>
                              <mo>|</mo>
                            </mrow>
                          </mrow>
                          <mn>2</mn>
                        </msup>
                        <mi>d</mi>
                        <mi>t</mi>
                      </mrow>
                    </math>
                  </EquationSource>
                  <EquationSource Format="TEX"><![CDATA[
$$ E=\underset{-\infty }{\overset{\infty }{{\displaystyle \int }}}{\left|s(t)\right|}^2dt $$
]]></EquationSource>
                </Equation>Therefore, a threshold-based mechanism based on the value of <Emphasis Type="Italic">E</Emphasis> can help to identify segments of activities which are identical [<CitationRef CitationID="CR145">145</CitationRef>]. Researchers have explored energy-based segmentation with the assumption of a rest period between each activity which is particularly useful for gesture recognition involving discrete activities and momentary pauses [<CitationRef CitationID="CR146">146</CitationRef>].</Para>
            </Section3>
            <Section3 ID="Sec18">
              <Heading>Feature Extraction</Heading>
              <Para ID="Par58">The choice of features is a fundamental step for classification and a highly problem-dependant task. Although each of the sensors exhibits signal patterns that are distinctive for each of the movements and may be recognizable to the human eye as shown in Fig. <InternalRef RefID="Fig3">4.3</InternalRef>, in order for a machine to recognize these patterns a set of characterizing features must be extracted from the data. Features represent the transformation of the raw data into another space known as the feature space. It is a measure to define the raw data in a quantitative as well as a qualitative manner such that it characterizes the raw data. The total number of features extracted from a feature space, where ideally identical activities should be clustered together whereas features corresponding to different activities should lie far apart. The selection of features is dependent on the activities that are to be classified.<Figure Category="Standard" Float="Yes" ID="Fig3">
                  <Caption Language="En">
                    <CaptionNumber>Fig. 4.3</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>The signal patterns generated by a tri-axial gyroscope placed near the elbow for three repetitions of four different actions—reach and retrieve, lift hand, swing arm and rotate wrist</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <MediaObject ID="MO3">
                    <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_4_Fig3_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </MediaObject>
                </Figure>
</Para>
              <Para ID="Par59">The patterns shown in Fig. <InternalRef RefID="Fig4">4.4</InternalRef> clearly stress the importance of deciding on the right feature. Typical feature sets for HAR include statistical functions, time and/or frequency domain features, as well as heuristic features [<CitationRef CitationID="CR147">147</CitationRef>]. Some of the commonly used time-domain features extracted from sensor data and reported in the literature are: arithmetic mean, variance, median, skew, kurtosis, inter-quartile range, root mean square, standard deviation and correlation between axes [<CitationRef CitationID="CR148">148</CitationRef>, <CitationRef CitationID="CR149">149</CitationRef>]. Correlation between accelerometer axes can improve recognition of activities involving movements of multiple body parts. For example, the activities of walking and climbing stairs might exhibit the same degree of periodicity and magnitude of a kinematic signal, but walking involves translation in one dimension whereas climbing stairs involves translation in multiple dimensions [<CitationRef CitationID="CR150">150</CitationRef>]. The often used mathematical features of variance, inter-quartile range, root mean square and standard deviation are useful measures of the variation in the data representing an action. The statistical functions of kurtosis and skew may appear out of place when considering human activity since these are more usually associated as descriptors of the shape of a probability distribution. Nevertheless, these functions can still return values that uniquely identify different classes of activity, and therefore are rightly considered as appropriate classification features.<Figure Category="Standard" Float="Yes" ID="Fig4">
                  <Caption Language="En">
                    <CaptionNumber>Fig. 4.4</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>Examples of movement patterns that exhibit obvious differences but are essentially the same movement</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <MediaObject ID="MO4">
                    <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_4_Fig4_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </MediaObject>
                </Figure>
</Para>
              <Para ID="Par60">Commonly used frequency domain features are extracted from the coefficients of various time–frequency transforms such as the Short Time Fourier transform (STFT), Fast Fourier transform (FFT) and the Continuous or Discrete Wavelet transform (WT). The high-frequency component of an accelerometer signal also known as the AC component is primarily related to the dynamic motion of the subject like walking, running and handshaking while the low-frequency DC component of the signal is related to the gravitational acceleration and hence is informative about the orientation of the body in space and can be used to classify static postural positions [<CitationRef CitationID="CR151">151</CitationRef>].</Para>
              <Para ID="Par61">The signal energy and its distribution at different frequency bands are popular choices for discriminating activities of differing intensities. More specifically, some of the commonly used features are spectral centroid, spectral spread, estimation of frequency peak and estimation of the power of the frequency peak and signal power in different frequency bands [<CitationRef CitationID="CR148">148</CitationRef>, <CitationRef CitationID="CR151">151</CitationRef>]. Frequency domain entropy, calculated from the normalized information entropy of the discrete FFT component magnitudes of the signal, helps to discriminate activities with similar energy content. For example, cycling and running might result in similar values of energy if captured with an accelerometer placed near the hip. Cycling involves a uniform circular motion, and discrete FFT of the acceleration data in the vertical direction may show a single dominant frequency component at 1 Hz and low magnitude for all other frequencies. By comparison, the action of running may produce major FFT components in the low-frequency range 0.5–2 Hz [<CitationRef CitationID="CR100">100</CitationRef>].</Para>
            </Section3>
            <Section3 ID="Sec19">
              <Heading>Feature Selection</Heading>
              <Para ID="Par62">With a higher dimensional feature space, learning the parameters becomes a difficult task for the classifier because in such cases a large number of data samples (i.e. more training data) are required for the parameter extraction of the model and hence the computational complexity of the classification increases. The performance of the classification algorithm depends on the dimension of the feature space and hence methods to reduce the dimensionality are considered in the field of activity recognition. Particularly for real-time activity recognition, it is imperative to use the minimum number of features with an eye on computational complexity and memory utilization. If features with minimum discriminatory abilities are selected, the subsequent classification would lead to poor recognition whereas if information-rich features having a large between-class distance and small within-class distance in the feature space are selected we can expect to have a better classification. This would imply that features would take distant values in different classes and closely located values in the same class. However, before proceeding with feature selection, the feature vectors need to be pre-processed to remove the outlier points and feature normalization [<CitationRef CitationID="CR152">152</CitationRef>].</Para>
              <Para ID="Par63">An outlier is a point that appears as a result of noisy measurement and lies far away from the mean of the corresponding feature vector causing large errors during the training of the classifier. For normally distributed data, a threshold of up to three standard deviations from the mean is used to filter out the outliers. For non-normal distributions, more complex measures like cost functions are considered [<CitationRef CitationID="CR152">152</CitationRef>].</Para>
              <Para ID="Par64">Feature normalization is another key step adopted for feature values lying in different numeric ranges, such that features with large values do not dominate the cost function in the design of the classifier. A common technique is linear normalization as shown in (<InternalRef RefID="Equ3">4.3</InternalRef>), where the features are normalized by removing the mean from each sample and dividing the samples by their standard deviation. This ensures that each feature has zero mean and unit variance and can be represented as:<Equation ID="Equ3">
                  <EquationNumber>4.3</EquationNumber>
                  <MediaObject>
                    <ImageObject Color="BlackWhite" FileRef="327370_1_En_4_Chapter_Equ3.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </MediaObject>
                  <EquationSource Format="MATHML">
                    <math xmlns:xlink="http://www.w3.org/1999/xlink">
                      <mrow>
                        <msub>
                          <mover accent="true">
                            <mi>x</mi>
                            <mo>˜</mo>
                          </mover>
                          <mi>i</mi>
                        </msub>
                        <mo>=</mo>
                        <mfrac>
                          <mrow>
                            <msub>
                              <mi>x</mi>
                              <mi>i</mi>
                            </msub>
                            <mo>−</mo>
                            <mi>μ</mi>
                          </mrow>
                          <mi>σ</mi>
                        </mfrac>
                        <mo>,</mo>
                        <mspace width="0.25em"/>
                        <mspace width="0.25em"/>
                        <mspace width="0.25em"/>
                        <mspace width="0.25em"/>
                        <mi>i</mi>
                        <mo>=</mo>
                        <mn>1</mn>
                        <mo>,</mo>
                        <mn>2</mn>
                        <mo>,</mo>
                        <mn>3</mn>
                        <mo>,</mo>
                        <mo>…</mo>
                        <mo>,</mo>
                        <mi>N</mi>
                      </mrow>
                    </math>
                  </EquationSource>
                  <EquationSource Format="TEX"><![CDATA[
$$ {\tilde{x}}_i=\frac{x_i-\mu }{\sigma },\kern0.46em i=1,2,3,\dots, N $$
]]></EquationSource>
                </Equation>where <Emphasis Type="Italic">x</Emphasis>
<Subscript>
                  <Emphasis Type="Italic">i</Emphasis>
                </Subscript> represents the respective feature values, <Emphasis Type="Italic">μ</Emphasis> is the mean value, <Emphasis Type="Italic">σ</Emphasis> is the standard deviation and <InlineEquation ID="IEq1">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="327370_1_En_4_Chapter_IEq1.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="MATHML">
                    <math xmlns:xlink="http://www.w3.org/1999/xlink">
                      <mrow>
                        <msub>
                          <mover accent="true">
                            <mi>x</mi>
                            <mo>˜</mo>
                          </mover>
                          <mi>i</mi>
                        </msub>
                      </mrow>
                    </math>
                  </EquationSource>
                  <EquationSource Format="TEX"><![CDATA[
$$ {\tilde{x}}_i $$
]]></EquationSource>
                </InlineEquation> represents the normalized feature values. Alternatively, other linear techniques can be used to normalize the feature values by restricting them between a minimum and a maximum value as expressed in (<InternalRef RefID="Equ4">4.4</InternalRef>). Selecting the range depends on the nature of the data.<Equation ID="Equ4">
                  <EquationNumber>4.4</EquationNumber>
                  <MediaObject>
                    <ImageObject Color="BlackWhite" FileRef="327370_1_En_4_Chapter_Equ4.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </MediaObject>
                  <EquationSource Format="MATHML">
                    <math xmlns:xlink="http://www.w3.org/1999/xlink">
                      <mrow>
                        <msub>
                          <mover accent="true">
                            <mi>x</mi>
                            <mo>˜</mo>
                          </mover>
                          <mi>i</mi>
                        </msub>
                        <mo>=</mo>
                        <mfrac>
                          <mrow>
                            <msub>
                              <mi>x</mi>
                              <mi>i</mi>
                            </msub>
                            <mo>−</mo>
                            <mi>min</mi>
                            <mrow>
                              <mo>(</mo>
                              <mrow>
                                <msub>
                                  <mi>x</mi>
                                  <mi>i</mi>
                                </msub>
                              </mrow>
                              <mo>)</mo>
                            </mrow>
                          </mrow>
                          <mrow>
                            <mi>max</mi>
                            <mrow>
                              <mo>(</mo>
                              <mrow>
                                <msub>
                                  <mi>x</mi>
                                  <mi>i</mi>
                                </msub>
                              </mrow>
                              <mo>)</mo>
                            </mrow>
                            <mo>−</mo>
                            <mi>min</mi>
                            <mrow>
                              <mo>(</mo>
                              <mrow>
                                <msub>
                                  <mi>x</mi>
                                  <mi>i</mi>
                                </msub>
                              </mrow>
                              <mo>)</mo>
                            </mrow>
                          </mrow>
                        </mfrac>
                        <mo>,</mo>
                        <mspace width="0.25em"/>
                        <mspace width="0.25em"/>
                        <mspace width="0.25em"/>
                        <mspace width="0.25em"/>
                        <mi>i</mi>
                        <mo>=</mo>
                        <mn>1</mn>
                        <mo>,</mo>
                        <mn>2</mn>
                        <mo>,</mo>
                        <mn>3</mn>
                        <mo>,</mo>
                        <mo>…</mo>
                        <mo>,</mo>
                        <mi>N</mi>
                      </mrow>
                    </math>
                  </EquationSource>
                  <EquationSource Format="TEX"><![CDATA[
$$ {\tilde{x}}_i=\frac{x_i- \min \left({x}_i\right)}{ \max \left({x}_i\right)- \min \left({x}_i\right)},\kern0.46em i=1,2,3,\dots, N $$
]]></EquationSource>
                </Equation>
</Para>
              <Para ID="Par65">Non-linear methods of normalization are also applied for data which are not evenly distributed about their mean. In such circumstances, non-linear functions like logarithmic or sigmoid can be used to transform the feature values within specific intervals [<CitationRef CitationID="CR152">152</CitationRef>]. Another option is scaling the feature vectors by the Euclidean length of the vector.</Para>
              <Para ID="Par66">The normalization step is followed by the feature ranking step. Fisher’s Discriminant Ratio (FDR) and Bhattacharyya distance are such techniques used to quantify the discriminatory ability of each individual feature between two equi-probable classes. Another class separability technique, based on scatter matrices, can be used for a multiple-class scenario [<CitationRef CitationID="CR152">152</CitationRef>]. The rank of each individual feature is determined, where a high rank represents a small within-class variance and a large between-class distance among the data points in the respective feature space [<CitationRef CitationID="CR153">153</CitationRef>]. The class separability based on scatter matrices is further explained.</Para>
              <Para ID="Par67">The rank of each individual feature for a multiple-class scenario is determined by the <Emphasis Type="Italic">J</Emphasis> value [<CitationRef CitationID="CR152">152</CitationRef>] as calculated below:<Equation ID="Equ5">
                  <EquationNumber>4.5</EquationNumber>
                  <MediaObject>
                    <ImageObject Color="BlackWhite" FileRef="327370_1_En_4_Chapter_Equ5.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </MediaObject>
                  <EquationSource Format="MATHML">
                    <math xmlns:xlink="http://www.w3.org/1999/xlink">
                      <mrow>
                        <mi>J</mi>
                        <mo>=</mo>
                        <mfrac>
                          <mrow>
                            <msub>
                              <mi>S</mi>
                              <mi mathvariant="normal">m</mi>
                            </msub>
                          </mrow>
                          <mrow>
                            <msub>
                              <mi>S</mi>
                              <mi mathvariant="normal">b</mi>
                            </msub>
                          </mrow>
                        </mfrac>
                      </mrow>
                    </math>
                  </EquationSource>
                  <EquationSource Format="TEX"><![CDATA[
$$ J=\frac{S_{\mathrm{m}}}{S_{\mathrm{b}}} $$
]]></EquationSource>
                </Equation>
<Equation ID="Equ6">
                  <EquationNumber>4.6</EquationNumber>
                  <MediaObject>
                    <ImageObject Color="BlackWhite" FileRef="327370_1_En_4_Chapter_Equ6.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </MediaObject>
                  <EquationSource Format="MATHML">
                    <math xmlns:xlink="http://www.w3.org/1999/xlink">
                      <mrow>
                        <msub>
                          <mi>S</mi>
                          <mi mathvariant="normal">m</mi>
                        </msub>
                        <mo>=</mo>
                        <msub>
                          <mi>S</mi>
                          <mi mathvariant="normal">w</mi>
                        </msub>
                        <mo>+</mo>
                        <msub>
                          <mi>S</mi>
                          <mi mathvariant="normal">b</mi>
                        </msub>
                      </mrow>
                    </math>
                  </EquationSource>
                  <EquationSource Format="TEX"><![CDATA[
$$ {S}_{\mathrm{m}}={S}_{\mathrm{w}}+{S}_{\mathrm{b}} $$
]]></EquationSource>
                </Equation>
<Emphasis Type="Italic">S</Emphasis>
<Subscript>w</Subscript> and <Emphasis Type="Italic">S</Emphasis>
<Subscript>b</Subscript> are the within-class and between-class scatter matrices, respectively, and <Emphasis Type="Italic">S</Emphasis>
<Subscript>m</Subscript> is the mixture scatter matrix. The respective expressions are presented below:<Equation ID="Equ7">
                  <EquationNumber>4.7</EquationNumber>
                  <MediaObject>
                    <ImageObject Color="BlackWhite" FileRef="327370_1_En_4_Chapter_Equ7.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </MediaObject>
                  <EquationSource Format="MATHML">
                    <math xmlns:xlink="http://www.w3.org/1999/xlink">
                      <mrow>
                        <msub>
                          <mi>S</mi>
                          <mi mathvariant="normal">w</mi>
                        </msub>
                        <mo>=</mo>
                        <munderover>
                          <mstyle displaystyle="true" mathsize="10">
                            <mo>∑</mo>
                          </mstyle>
                          <mrow>
                            <mi>i</mi>
                            <mo>=</mo>
                            <mn>1</mn>
                          </mrow>
                          <mi>c</mi>
                        </munderover>
                        <msub>
                          <mi>P</mi>
                          <mi>i</mi>
                        </msub>
                        <mi>S</mi>
                        <mmultiscripts>
                          <mrow>
</mrow>
                          <mprescripts/>
                          <mi>i</mi>
                          <mrow>
</mrow>
                        </mmultiscripts>
                      </mrow>
                    </math>
                  </EquationSource>
                  <EquationSource Format="TEX"><![CDATA[
$$ {S}_{\mathrm{w}}={\displaystyle \sum}_{i=1}^c{P}_iS{}_i{} $$
]]></EquationSource>
                </Equation>where <Emphasis Type="Italic">P</Emphasis>
<Subscript>
                  <Emphasis Type="Italic">i</Emphasis>
                </Subscript> denotes the priori probability of a given class <Emphasis Type="Italic">i</Emphasis> = 1, 2,…<Emphasis Type="Italic">c</Emphasis> and <Emphasis Type="Italic">S</Emphasis>
<Subscript>
                  <Emphasis Type="Italic">i</Emphasis>
                </Subscript> is the respective covariance matrix of class <Emphasis Type="Italic">i</Emphasis>.<Equation ID="Equ8">
                  <EquationNumber>4.8</EquationNumber>
                  <MediaObject>
                    <ImageObject Color="BlackWhite" FileRef="327370_1_En_4_Chapter_Equ8.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </MediaObject>
                  <EquationSource Format="MATHML">
                    <math xmlns:xlink="http://www.w3.org/1999/xlink">
                      <mrow>
                        <msub>
                          <mi>S</mi>
                          <mi mathvariant="normal">b</mi>
                        </msub>
                        <mo>=</mo>
                        <munderover>
                          <mstyle displaystyle="true" mathsize="10">
                            <mo>∑</mo>
                          </mstyle>
                          <mrow>
                            <mi>i</mi>
                            <mo>=</mo>
                            <mn>1</mn>
                          </mrow>
                          <mi>c</mi>
                        </munderover>
                        <msub>
                          <mi>P</mi>
                          <mi>i</mi>
                        </msub>
                        <mrow>
                          <mo>(</mo>
                          <mrow>
                            <msub>
                              <mi>m</mi>
                              <mi>i</mi>
                            </msub>
                            <mo>−</mo>
                            <msub>
                              <mi>m</mi>
                              <mn>0</mn>
                            </msub>
                          </mrow>
                          <mo>)</mo>
                        </mrow>
                        <msup>
                          <mrow>
                            <mrow>
                              <mo>(</mo>
                              <mrow>
                                <msub>
                                  <mi>m</mi>
                                  <mi>i</mi>
                                </msub>
                                <mo>−</mo>
                                <msub>
                                  <mi>m</mi>
                                  <mn>0</mn>
                                </msub>
                              </mrow>
                              <mo>)</mo>
                            </mrow>
                          </mrow>
                          <mi>T</mi>
                        </msup>
                      </mrow>
                    </math>
                  </EquationSource>
                  <EquationSource Format="TEX"><![CDATA[
$$ {S}_{\mathrm{b}}={\displaystyle \sum}_{i=1}^c{P}_i\left({m}_i-{m}_0\right){\left({m}_i-{m}_0\right)}^T $$
]]></EquationSource>
                </Equation>
<Equation ID="Equ9">
                  <EquationNumber>4.9</EquationNumber>
                  <MediaObject>
                    <ImageObject Color="BlackWhite" FileRef="327370_1_En_4_Chapter_Equ9.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </MediaObject>
                  <EquationSource Format="MATHML">
                    <math xmlns:xlink="http://www.w3.org/1999/xlink">
                      <mrow>
                        <msub>
                          <mi>m</mi>
                          <mn>0</mn>
                        </msub>
                        <mo>=</mo>
                        <munderover>
                          <mstyle displaystyle="true" mathsize="10">
                            <mo>∑</mo>
                          </mstyle>
                          <mrow>
                            <mi>i</mi>
                            <mo>=</mo>
                            <mn>1</mn>
                          </mrow>
                          <mi>c</mi>
                        </munderover>
                        <msub>
                          <mi>P</mi>
                          <mi>i</mi>
                        </msub>
                        <mi>m</mi>
                        <mmultiscripts>
                          <mrow>
</mrow>
                          <mprescripts/>
                          <mi>i</mi>
                          <mrow>
</mrow>
                        </mmultiscripts>
                      </mrow>
                    </math>
                  </EquationSource>
                  <EquationSource Format="TEX"><![CDATA[
$$ {m}_0={\displaystyle \sum}_{i=1}^c{P}_im{}_i{} $$
]]></EquationSource>
                </Equation>
<Emphasis Type="Italic">m</Emphasis>
<Subscript>0</Subscript> is the global mean vector. A high value of <Emphasis Type="Italic">J</Emphasis> represents a small within-class variance and a large between-class distance among the data points in the respective feature space [<CitationRef CitationID="CR152">152</CitationRef>].</Para>
              <Para ID="Par68">The ranked features are sorted in a descending order to determine the one having the highest rank. As opposed to other popular multi-class feature ranking algorithms used in activity recognition like the ReliefF algorithm [<CitationRef CitationID="CR3">3</CitationRef>] and Clamping technique [<CitationRef CitationID="CR108">108</CitationRef>], the use of scatter matrices is computationally less complex and at the same time it quantifies the scatter of feature vectors in the respective feature space [<CitationRef CitationID="CR152">152</CitationRef>].</Para>
              <Para ID="Par69">We come to the core problem area wherein we have to now select a subset of <Emphasis Type="Italic">l</Emphasis> features from the best ranked <Emphasis Type="Italic">m</Emphasis> features (where <Emphasis Type="Italic">l</Emphasis> ≤ <Emphasis Type="Italic">m</Emphasis>). The two major approaches are—scalar feature selection and feature vector selection. In the scalar feature selection technique, each feature is treated individually and their class separability measure is ascertained using any of the above-mentioned criterion <Emphasis Type="Italic">c</Emphasis>(<Emphasis Type="Italic">k</Emphasis>) (FDR, Scatter Matrices) for each feature. Features are then ranked in a descending order according to criterion <Emphasis Type="Italic">c</Emphasis>(<Emphasis Type="Italic">k</Emphasis>), and <Emphasis Type="Italic">l</Emphasis> best features are considered for classification. Considering features individually involves low computational complexity but is not effective for complex classification problems and for cases where features are mutually correlated. Feature vector selection can be approached in two ways:<OrderedList>
                  <ListItem>
                    <ItemNumber>1.</ItemNumber>
                    <ItemContent>
                      <Para ID="Par70">
<Emphasis Type="Italic">Filter approach</Emphasis>: The features are selected independent of any classification technique. For each combination of features chosen, we apply the class separability criterion as mentioned above and select the best feature combination. Considering <Emphasis Type="Italic">m</Emphasis> = 10 and <Emphasis Type="Italic">l</Emphasis> = 5, we can have 252 feature vector combinations which are very large, and the number <Emphasis Type="Italic">l</Emphasis> is also not known a priori.</Para>
                    </ItemContent>
                  </ListItem>
                  <ListItem>
                    <ItemNumber>2.</ItemNumber>
                    <ItemContent>
                      <Para ID="Par71">
<Emphasis Type="Italic">Wrapper approach</Emphasis>: The selection of the feature is based in association with the classifier to be employed. For each chosen feature vector combination, the classification error probability of the classifier is estimated, and the feature combination with the minimum error is chosen. This approach can be further computationally complex depending on the choice of the classifier.</Para>
                    </ItemContent>
                  </ListItem>
                </OrderedList>
</Para>
              <Para ID="Par72">However, to reduce the complexity there are some effective searching techniques, which have been proposed to select the best feature vector combination: sequential backward selection, sequential forward selection and floating search method [<CitationRef CitationID="CR152">152</CitationRef>]. We will discuss the sequential forward selection technique (<Emphasis Type="Italic">sfs</Emphasis>) with a working example of a feature vector comprising four different features [<Emphasis Type="Italic">X</Emphasis>
<Subscript>1</Subscript>, <Emphasis Type="Italic">X</Emphasis>
<Subscript>2</Subscript>, <Emphasis Type="Italic">X</Emphasis>
<Subscript>3</Subscript>, <Emphasis Type="Italic">X</Emphasis>
<Subscript>4</Subscript>]. First, we compute the best ranked feature, say <Emphasis Type="Italic">X</Emphasis>
<Subscript>
                  <Emphasis Type="Italic">2</Emphasis>
                </Subscript>, and evaluate the classification performance with <Emphasis Type="Italic">X</Emphasis>
<Subscript>2</Subscript>. Secondly, we compute all two-dimensional feature vector combinations with <Emphasis Type="Italic">X</Emphasis>
<Subscript>2</Subscript>: [<Emphasis Type="Italic">X</Emphasis>
<Subscript>1</Subscript>, <Emphasis Type="Italic">X</Emphasis>
<Subscript>2</Subscript>], [<Emphasis Type="Italic">X</Emphasis>
<Subscript>2</Subscript>, <Emphasis Type="Italic">X</Emphasis>
<Subscript>3</Subscript>], [<Emphasis Type="Italic">X</Emphasis>
<Subscript>2</Subscript>, <Emphasis Type="Italic">X</Emphasis>
<Subscript>4</Subscript>] and evaluate the classification performance for each of the combinations. Thirdly, we compute all three-dimensional feature vector combination with <Emphasis Type="Italic">X</Emphasis>
<Subscript>2</Subscript>: [<Emphasis Type="Italic">X</Emphasis>
<Subscript>1</Subscript>, <Emphasis Type="Italic">X</Emphasis>
<Subscript>2</Subscript>, <Emphasis Type="Italic">X</Emphasis>
<Subscript>3</Subscript>], [<Emphasis Type="Italic">X</Emphasis>
<Subscript>1</Subscript>, <Emphasis Type="Italic">X</Emphasis>
<Subscript>2</Subscript>, <Emphasis Type="Italic">X</Emphasis>
<Subscript>4</Subscript>] and evaluate the classifier performance with both the combinations. Finally, we select the best feature vector combination as the desired features [<CitationRef CitationID="CR152">152</CitationRef>]. Similarly for the sequential backward selection technique, we start with a combination of three features, eliminating one feature from each of the combinations and then evaluate its performance with respect to the classification algorithm employed. From each respective feature combination, we again eliminate one feature and evaluate the two-dimensional feature combinations.</Para>
              <Para ID="Par73">Both these methods suffer from the fact that once a feature is selected or discarded in the forward or backward selection technique, respectively, there is no possibility for it to be discarded or reconsidered again. This problem is referred to as the nesting effect. Therefore, a flexible technique to reconsider previously discarded features and vice versa is known as the floating search method [<CitationRef CitationID="CR152">152</CitationRef>].</Para>
              <Para ID="Par74">A popular analytical technique often reported in the literature is Principal Component Analysis (PCA), or the Karhunen–Loeve transform, which transforms feature vectors into a smaller number of uncorrelated variables referred to as the principle components [<CitationRef CitationID="CR150">150</CitationRef>, <CitationRef CitationID="CR151">151</CitationRef>]. Another popular approach is Independent Component Analysis (ICA), often applied in problems of blind source separation, which attempts to decompose a multivariate signal into statistically independent non-Gaussian signals [<CitationRef CitationID="CR154">154</CitationRef>]. The choice of relevant features and choice of ranking or selection technique are completely dependent on the activities, type of sensors and the application scenario.</Para>
            </Section3>
            <Section3 ID="Sec20">
              <Heading>Classification</Heading>
              <Para ID="Par75">A wide range of classifiers have been used for activity recognition in recent years [<CitationRef CitationID="CR155">155</CitationRef>]. The determining factors for the selection of the classifier are accuracy, ease of development and speed of real-time execution [<CitationRef CitationID="CR131">131</CitationRef>]. Two distinct approaches can be used in classifying human activities—supervised and unsupervised learning. In supervised learning, the association of the training dataset comprises selected feature vectors with each class label is known beforehand [<CitationRef CitationID="CR151">151</CitationRef>]. In unsupervised learning, only the number of classes is known and the system assigns a class label to each instance in the training dataset. Clustering-based unsupervised learning has been used in the field of activity recognition [<CitationRef CitationID="CR153">153</CitationRef>, <CitationRef CitationID="CR156">156</CitationRef>].</Para>
              <Para ID="Par76">In HAR, the classification schemes used can be broadly categorized into three themes: probabilistic models, discriminative approach and template-based similarity metrics, as described below.<OrderedList>
                  <ListItem>
                    <ItemNumber>1.</ItemNumber>
                    <ItemContent>
                      <Para ID="Par77">
<Emphasis Type="Italic">Probabilistic models</Emphasis>: Probabilistic models are quite commonly used for behaviour modelling since they are an efficient means of representing random variables, dependence and temporal variation. In this approach, the activity samples are modelled using Gaussian mixture, yielding promising results for offline learning when a large amount of data is available for training. Generative probabilistic models such as HMMs have been used to model activity sequences and have been extended to hierarchical models like Conditional Random Fields (CRFs) and Dynamic Bayesian Networks (DBNs) [<CitationRef CitationID="CR156">156</CitationRef>]. Hidden Markov Models (HMMs) have been very popular in speech recognition and have also been used in applications for hand gesture recognition [<CitationRef CitationID="CR157">157</CitationRef>, <CitationRef CitationID="CR158">158</CitationRef>]. In general, the HMM is trained on pre-defined class labels using the Baum–Welch algorithm and is tested on new instances. The Baum–Welch algorithm is a generalized Expectation Maximization (EM) algorithm that computes the maximum likelihood estimates of the parameters of an HMM given the observations as training data [<CitationRef CitationID="CR159">159</CitationRef>, <CitationRef CitationID="CR160">160</CitationRef>]. The problem with HMMs is the first-order Markov assumption where the current state depends only on the previous one. Further, the probability of a change in the hidden state does not depend upon the time that has elapsed since entering into the current state. Therefore, a time dependence has been added to HMMs and they have been augmented to semi-HMMs where the hidden process is semi-Markovian rather than Markovian. Coupled HMMs have also gained prominence which is considered as a collection of HMMs, where the state at time <Emphasis Type="Italic">t</Emphasis> for each HMM is conditioned by the states at time <Emphasis Type="Italic">t</Emphasis>-1 of all HMMs in the collection. They are used to model the dynamic relationships between several signals [<CitationRef CitationID="CR161">161</CitationRef>].</Para>
                    </ItemContent>
                  </ListItem>
                  <ListItem>
                    <ItemNumber>2.</ItemNumber>
                    <ItemContent>
                      <Para ID="Par78">
<Emphasis Type="Italic">Discriminative approach</Emphasis>: The classification is based on the construction of the decision boundaries in the feature space, specifying regions for each class. The decision boundaries are constructed on the feature vectors of the training set, through an iterative or a geometric consideration. The Artificial Neural Network (ANN) commonly used for detecting ADL consists of inputs and outputs with a processing or a hidden layer in between. The inputs are the independent variable, and the outputs represent the dependent variable. The internal (hidden) layers can be adjusted through optimization algorithms such as the resilient back-propagation or scale-conjugate algorithms [<CitationRef CitationID="CR106">106</CitationRef>].</Para>
                      <Para ID="Par79">The <Emphasis Type="Italic">k</Emphasis>-Nearest Neighbour (<Emphasis Type="Italic">k</Emphasis>-NN) [<CitationRef CitationID="CR162">162</CitationRef>] and the Nearest Mean (NM) classifiers work directly on the geometrical distances between feature vectors from different classes [<CitationRef CitationID="CR163">163</CitationRef>]. Support Vector Machines (SVMs) work by constructing boundaries that maximize the margins between the nearest features relative to two distinct classes. SVM is a very popular technique in machine learning community and generally produces high accuracy rates with moderate computational complexity (depending on the number of support vectors used) [<CitationRef CitationID="CR108">108</CitationRef>, <CitationRef CitationID="CR164">164</CitationRef>]. In principle, it is a binary classifier but has been extended to handle multiple classes using the “one-versus-all” or the “one-versus-one” scheme [<CitationRef CitationID="CR165">165</CitationRef>]. However, both of these methods can be computationally intensive depending on the number of target classes. The Naive Bayes classifier has also been successfully used over the years [<CitationRef CitationID="CR116">116</CitationRef>, <CitationRef CitationID="CR149">149</CitationRef>]. It assumes conditional independence among all feature vectors given a class label and learns about the conditional probability of each feature. They require large amounts of data and do not explicitly model any temporal information which is very important in activity recognition [<CitationRef CitationID="CR118">118</CitationRef>]. Finally, binary tree classifiers have been widely popular in the field of HAR, where the classification process is articulated in several different steps. At each step, a binary decision is made based on different strategies like the threshold-based or template-matching. With each stage, the classification is progressively refined as the tree descends along the branches [<CitationRef CitationID="CR166">166</CitationRef>]. The <Emphasis Type="Italic">C4.5</Emphasis> Decision Tree (DT) algorithm is by far the most popular algorithm and has been used to achieve successful recognition of daily living activities [<CitationRef CitationID="CR100">100</CitationRef>, <CitationRef CitationID="CR148">148</CitationRef>].</Para>
                    </ItemContent>
                  </ListItem>
                  <ListItem>
                    <ItemNumber>3.</ItemNumber>
                    <ItemContent>
                      <Para ID="Par80">
<Emphasis Type="Italic">Template-based similarity metrics</Emphasis>: The template-matching technique exploits the similarity between the observed data (testing dataset) and the pre-stored activity templates which are user defined or obtained from the training dataset. They can employ, for example, a <Emphasis Type="Italic">k</Emphasis>-NN classifier using the Euclidean distance computed between the testing and training dataset having a fixed window size or dynamic time warping [<CitationRef CitationID="CR167">167</CitationRef>, <CitationRef CitationID="CR168">168</CitationRef>] in the case of varying window size. Another popular template-matching technique used is string matching [<CitationRef CitationID="CR169">169</CitationRef>]. The choice of a classifier depends on the trade-off between the computational complexity, memory requirements and the recognition accuracy.</Para>
                    </ItemContent>
                  </ListItem>
                </OrderedList>
</Para>
            </Section3>
            <Section3 ID="Sec21">
              <Heading>Cross-Validation</Heading>
              <Para ID="Par81">One technique quite common in classification problems is the use of cross-validation. During estimation of the model parameters, validation of the employed classification algorithm is essential to judge its robustness. This is particularly applicable in supervised learning techniques, i.e. where the class labels of the dataset are known. The available cohort of data is divided into training and testing datasets. The training set is used to train the classifier whereas the test set is used to estimate the error rate of the trained classifier. The cross-validation technique is also known as the re-sampling method and can be divided into three approaches: random sampling, <Emphasis Type="Italic">k</Emphasis>-fold cross-validation and leave-one-out cross-validation [<CitationRef CitationID="CR131">131</CitationRef>].<OrderedList>
                  <ListItem>
                    <ItemNumber>1.</ItemNumber>
                    <ItemContent>
                      <Para ID="Par82">
<Emphasis Type="Italic">Random sampling</Emphasis>: The available dataset is split into <Emphasis Type="Italic">K</Emphasis> parts, where each split randomly selects a fixed number of elements. Each training set is used to retrain the classifier, and the error <Emphasis Type="Italic">E</Emphasis>
<Subscript>
                          <Emphasis Type="Italic">i</Emphasis>
                        </Subscript> is estimated after classifying the test set. The true error estimate could be obtained by averaging the separate error estimates <Emphasis Type="Italic">E</Emphasis>
<Subscript>
                          <Emphasis Type="Italic">i</Emphasis>
                        </Subscript>.</Para>
                    </ItemContent>
                  </ListItem>
                  <ListItem>
                    <ItemNumber>2.</ItemNumber>
                    <ItemContent>
                      <Para ID="Par83">
<Emphasis Type="Italic">K-fold cross-validation</Emphasis>: It is used to independently partition the whole dataset into <Emphasis Type="Italic">K</Emphasis> parts. For each <Emphasis Type="Italic">K</Emphasis> experiment, <Emphasis Type="Italic">K-1</Emphasis>, folds are used as the training dataset to train the classifier and the remaining one dataset is used as the test dataset. The advantage of <Emphasis Type="Italic">K</Emphasis>-fold cross-validation over random sampling is that in this technique all data samples in the dataset are eventually used for training and testing. The error computed over each run could be averaged to estimate a true error.</Para>
                    </ItemContent>
                  </ListItem>
                  <ListItem>
                    <ItemNumber>3.</ItemNumber>
                    <ItemContent>
                      <Para ID="Par84">
<Emphasis Type="Italic">Leave-one-out cross-validation</Emphasis>: In this method, an exhaustive validation is performed, where out of the total <Emphasis Type="Italic">N</Emphasis> available data samples, <Emphasis Type="Italic">N</Emphasis>-1 data samples are used to train the classifier while the one remaining sample is used to test the classifier. This is repeated such that each individual data sample takes the role of the classifier test data. The true error could then be calculated as the mean of the error over all <Emphasis Type="Italic">N</Emphasis> runs performed.</Para>
                    </ItemContent>
                  </ListItem>
                </OrderedList>
</Para>
              <Para ID="Par85">The number of folds is determined by the size of the available dataset. With a large number of folds, we can achieve a near accurate estimation of the classifier’s performance though this requires a longer computation time. With a smaller number of folds, we can gain on the computation time but sacrifice on the performance of the classifier. Hence, a large number of folds on a sparse dataset lead to a more robust classifier. In the literature, the value of <Emphasis Type="Italic">K</Emphasis> is generally chosen as 10 and therefore 10 runs of tenfold cross-validation are performed. The accuracy is calculated by averaging the performance of the classifier over the 10 runs [<CitationRef CitationID="CR170">170</CitationRef>].</Para>
            </Section3>
            <Section3 ID="Sec22">
              <Heading>Performance Evaluation</Heading>
              <Para ID="Par86">The recognition performance of an activity recognition system can be evaluated in terms of correct classification through true positives (TPs) or false negatives (FNs). Classification can also lead to false detection of activities that did not occur and can be estimated through false negatives (FNs) and false positives (FPs). There are a few well-known performance metrics that are widely used, such as confusion matrices, accuracy, precision, recall or receiver operating characteristics (ROC) curves.</Para>
              <Para ID="Par87">A confusion matrix is a popular means of evaluating the classifier performance for multi-class problems. It summarizes the misclassifications of the different activity classes. The rows of the matrix represent the actual number of instances in each activity classes while the columns represent the predicted instances in each activity classes. Precision [TP/(TP + FP)], recall [TP/(TP + FN)] and the overall accuracy [(TP + TN)/All] can be easily computed for each activity class from the matrix. Normalized confusion matrices are commonly used for unbalanced datasets where there is a significant amount of difference in the number of ground truth annotations of the activity classes [<CitationRef CitationID="CR131">131</CitationRef>, <CitationRef CitationID="CR171">171</CitationRef>, <CitationRef CitationID="CR172">172</CitationRef>].</Para>
              <Para ID="Par88">For binary classification problems (i.e. involving two classes), the overall (average) correct classification or accuracy is used as a measure of the classifier’s performance which might not always be applicable for multi-class classification because of possible dissimilar classification rates of different classes affecting the overall performance measure. Hence, we can measure the sensitivity of a given class from the confusion matrix <Emphasis Type="Italic">N</Emphasis> following the scheme proposed in [<CitationRef CitationID="CR173">173</CitationRef>]. The sensitivity <Emphasis Type="Italic">S</Emphasis> of class <Emphasis Type="Italic">i</Emphasis> estimates the number of patterns correctly predicted to be in class <Emphasis Type="Italic">i</Emphasis> with respect to the total number of patterns in class <Emphasis Type="Italic">i</Emphasis> [<CitationRef CitationID="CR173">173</CitationRef>]:<Equation ID="Equ10">
                  <EquationNumber>4.10</EquationNumber>
                  <MediaObject>
                    <ImageObject Color="BlackWhite" FileRef="327370_1_En_4_Chapter_Equ10.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </MediaObject>
                  <EquationSource Format="MATHML">
                    <math xmlns:xlink="http://www.w3.org/1999/xlink">
                      <mrow>
                        <msub>
                          <mi>S</mi>
                          <mi>i</mi>
                        </msub>
                        <mo>=</mo>
                        <mfrac>
                          <mrow>
                            <msub>
                              <mi>N</mi>
                              <mrow>
                                <mi>i</mi>
                                <mi>i</mi>
                              </mrow>
                            </msub>
                          </mrow>
                          <mrow>
                            <msub>
                              <mi>f</mi>
                              <mi>i</mi>
                            </msub>
                          </mrow>
                        </mfrac>
                        <mo>×</mo>
                        <mn>100</mn>
                      </mrow>
                    </math>
                  </EquationSource>
                  <EquationSource Format="TEX"><![CDATA[
$$ {S}_i=\frac{N_{ii}}{f_i}\times 100 $$
]]></EquationSource>
                </Equation>
<Equation ID="Equ11">
                  <EquationNumber>4.11</EquationNumber>
                  <MediaObject>
                    <ImageObject Color="BlackWhite" FileRef="327370_1_En_4_Chapter_Equ11.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </MediaObject>
                  <EquationSource Format="MATHML">
                    <math xmlns:xlink="http://www.w3.org/1999/xlink">
                      <mrow>
                        <msub>
                          <mi>f</mi>
                          <mi>i</mi>
                        </msub>
                        <mo>=</mo>
                        <munderover>
                          <mstyle displaystyle="true" mathsize="10">
                            <mo>∑</mo>
                          </mstyle>
                          <mrow>
                            <mi>j</mi>
                            <mo>=</mo>
                            <mn>1</mn>
                          </mrow>
                          <mi>c</mi>
                        </munderover>
                        <msub>
                          <mi>C</mi>
                          <mrow>
                            <mi>i</mi>
                            <mi>j</mi>
                          </mrow>
                        </msub>
                      </mrow>
                    </math>
                  </EquationSource>
                  <EquationSource Format="TEX"><![CDATA[
$$ {f}_i={\displaystyle \sum}_{j=1}^c{C}_{ij} $$
]]></EquationSource>
                </Equation>where <Emphasis Type="Italic">i</Emphasis> = 1…<Emphasis Type="Italic">c</Emphasis>, and <Emphasis Type="Italic">c</Emphasis> is the total number of classes. The diagonal and the off-diagonal elements of the confusion matrix correspond to correctly classified and misclassified patterns, respectively. <Emphasis Type="Italic">C</Emphasis>
<Subscript>
                  <Emphasis Type="Italic">ij</Emphasis>
                </Subscript> represents the number of times that the patterns are predicted to be in class <Emphasis Type="Italic">j</Emphasis> when they really belong to class <Emphasis Type="Italic">i</Emphasis>. A sample confusion matrix <Emphasis Type="Italic">N</Emphasis> is shown in Fig. <InternalRef RefID="Fig5">4.5</InternalRef>.<Figure Category="Standard" Float="Yes" ID="Fig5">
                  <Caption Language="En">
                    <CaptionNumber>Fig. 4.5</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>An example of a confusion matrix for four classes</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <MediaObject ID="MO5">
                    <ImageObject Color="BlackWhite" FileRef="MediaObjects/327370_1_En_4_Fig5_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </MediaObject>
                </Figure>
</Para>
              <Para ID="Par89">This example shows near perfect classification since all left to right diagonal elements approach unity and all off-diagonal elements approach zero. Therefore, the sensitivity of class <Emphasis Type="Italic">A</Emphasis> (expressed as a percentage), can be computed as <InlineEquation ID="IEq2">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="327370_1_En_4_Chapter_IEq2.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="MATHML">
                    <math xmlns:xlink="http://www.w3.org/1999/xlink">
                      <mrow>
                        <msub>
                          <mi>S</mi>
                          <mi>A</mi>
                        </msub>
                        <mo>=</mo>
                        <mfrac>
                          <mrow>
                            <mn>0.9</mn>
                          </mrow>
                          <mrow>
                            <mrow>
                              <mo>(</mo>
                              <mrow>
                                <mn>0.9</mn>
                                <mo>+</mo>
                                <mn>0.1</mn>
                              </mrow>
                              <mo>)</mo>
                            </mrow>
                          </mrow>
                        </mfrac>
                        <mo>=</mo>
                        <mspace width="0.25em"/>
                        <mn>90</mn>
                        <mspace width="0.25em"/>
                        <mi>%</mi>
                      </mrow>
                    </math>
                  </EquationSource>
                  <EquationSource Format="TEX"><![CDATA[
$$ {S}_A=\frac{0.9}{\left(0.9+0.1\right)}=90\;\% $$
]]></EquationSource>
                </InlineEquation> and the overall accuracy (expressed as a percentage) can be computed as <InlineEquation ID="IEq3">
                  <InlineMediaObject>
                    <ImageObject Color="BlackWhite" FileRef="327370_1_En_4_Chapter_IEq3.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </InlineMediaObject>
                  <EquationSource Format="MATHML">
                    <math xmlns:xlink="http://www.w3.org/1999/xlink">
                      <mrow>
                        <mi mathvariant="normal">Accuracy</mi>
                        <mo>=</mo>
                        <mfrac>
                          <mrow>
                            <mrow>
                              <mo>(</mo>
                              <mrow>
                                <mn>0.9</mn>
                                <mo>+</mo>
                                <mn>0.9</mn>
                                <mo>+</mo>
                                <mn>0.98</mn>
                                <mo>+</mo>
                                <mn>0.9</mn>
                              </mrow>
                              <mo>)</mo>
                            </mrow>
                          </mrow>
                          <mn>4</mn>
                        </mfrac>
                        <mo>=</mo>
                        <mspace width="0.25em"/>
                        <mn>92</mn>
                        <mspace width="0.25em"/>
                        <mi>%</mi>
                        <mo>.</mo>
                      </mrow>
                    </math>
                  </EquationSource>
                  <EquationSource Format="TEX"><![CDATA[
$$ \mathrm{Accuracy}=\frac{\left(0.9+0.9+0.98+0.9\right)}{4}=92\;\%. $$
]]></EquationSource>
                </InlineEquation>
</Para>
            </Section3>
          </Section2>
          <Section2 ID="Sec23">
            <Heading>Movement Quality Estimation</Heading>
            <Para ID="Par90">As mentioned in Sect. <InternalRef RefID="Sec6">4.3</InternalRef>, one of the prime objectives of activity monitoring is to have a qualitative feedback on the state of the patient or the subject undergoing rehabilitation. The recognition of activities, as mentioned in Sect. <InternalRef RefID="Sec9">4.3.3</InternalRef>, includes the detection and classification of particular limb movements, which is beneficial in many neurodegenerative pathologies, where regular exercise of the impaired limb is necessary to achieve rehabilitation. A system capable of tracking the use of the impaired limb (during exercises and ADL) and classifying the type of movements performed over time would indicate improvement in motor functionality.</Para>
            <Para ID="Par91">Movement smoothness has been widely used to determine the motor performance of both healthy subjects and stroke survivors [<CitationRef CitationID="CR174">174</CitationRef>]. One visible feature in the earliest movements performed by early stroke survivors is the lack of smoothness. Their movement profile appears to be comprising a series of discrete and episodic sub-movements. However, the movements become suppler as the recovery proceeds. For upper arm impairment post-stroke, the rate of recovery is usually rapid in the first few weeks. The B&amp;B test [<CitationRef CitationID="CR22">22</CitationRef>] and the NHP test [<CitationRef CitationID="CR23">23</CitationRef>] have been traditionally performed by the clinicians to assess rehabilitation in clinical settings, which are reliable measures of gross manual dexterity and arm functionality. However, with the advent of lightweight, low-cost, wireless body-worn sensors capable of recording kinematic movement for long durations, there is an increasing demand for assessing the functional ability of patients within the home settings.</Para>
            <Para ID="Par92">Data from inertial sensors, especially those from accelerometer have been used to quantify the degree of smoothness in the kinematic movements recorded. The jerk metric and the variations in the speed profile are the most common features used to represent smoothness in movements. The jerk metric characterizes the average rate of change of acceleration in an underlying movement. It is calculated by dividing the negative mean jerk magnitude by the peak speed. Normalizing the jerk to the maximum speed helps to make it a measure of movement smoothness [<CitationRef CitationID="CR175">175</CitationRef>].</Para>
            <Para ID="Par93">The normalized mean speed, average speed normalized by the maximum speed, is another important metric and essentially captures the discontinuities in the underlying episodic sub-movements of the subjects [<CitationRef CitationID="CR174">174</CitationRef>]. The peak metric is a common metric used to quantify movement smoothness usually obtained by counting the number of peaks from a gradient analysis of the raw acceleration data or on the speed signal (first integral of acceleration). A smaller number of peaks indicates fewer occurrences of changes in acceleration or deceleration and hence reflects the smoothness of the movement [<CitationRef CitationID="CR175">175</CitationRef>].</Para>
            <Para ID="Par94">Relevant studies performed have shown that features extracted from the sensor data conform to the clinical findings [<CitationRef CitationID="CR175">175</CitationRef>]. The B&amp;B and the NHP scores showed an improvement in arm functionality over the experimental duration of 3 weeks. The trends observed with the extracted features (jerk metric and number of peaks) from the acceleration data support the clinical observations. Hence low-cost body-worn sensors can be used in a pervasive manner to determine the rehabilitation of patients in the home environment.</Para>
          </Section2>
          <Section2 ID="Sec24">
            <Heading>State-of-the-Art Activity Recognition Systems</Heading>
            <Para ID="Par95">Table <InternalRef RefID="Tab1">4.1</InternalRef> lists a few state-of-the-art activity recognition systems that have been developed in recent years, mainly aimed towards ADL monitoring. Here, we have highlighted the activities being monitored, the sensors and their positioning, the classification schemes used and the accuracies obtained.<Table Float="Yes" ID="Tab1">
                <Caption Language="En">
                  <CaptionNumber>Table 4.1</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>State-of-the-art systems developed in recent years for ADL monitoring</SimplePara>
                  </CaptionContent>
                </Caption>
                <tgroup align="left" cols="4">
                  <colspec align="left" colname="c1" colnum="1"/>
                  <colspec align="left" colname="c2" colnum="2"/>
                  <colspec align="left" colname="c3" colnum="3"/>
                  <colspec align="left" colname="c4" colnum="4"/>
                  <thead>
                    <row>
                      <entry colname="c1">
                        <SimplePara>Sensors</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Activities</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>Evaluation</SimplePara>
                      </entry>
                      <entry colname="c4">
                        <SimplePara>Ref.</SimplePara>
                      </entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry colname="c1">
                        <SimplePara>Inertial sensor on the thigh for activity recognition; marker-based system using 12 cameras to track the three-dimensional location</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Lying, sitting, standing, walking, and transitions from sit-to-stand, lie-to-sit, stand-to-sit and sit-to-lie</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>HMM; overall accuracy about 90 %; accuracy increases with location data; high setup cost and a fixed coverage area for location tracking using the camera system</SimplePara>
                      </entry>
                      <entry colname="c4">
                        <SimplePara>[<CitationRef CitationID="CR120">120</CitationRef>]</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>Wristwatch-based sensor containing accelerometer, altimeter and temperature sensors</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>BADLs—brushing, dressing, walking upstairs/downstairs and sleeping; IADLs—washing dishes, ironing, watching television</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>Neural networks and SVM; average accuracy of 90 %; dressing, ironing, brushing and washing are sometimes confused; system is cost-effective and non-intrusive</SimplePara>
                      </entry>
                      <entry colname="c4">
                        <SimplePara>[<CitationRef CitationID="CR108">108</CitationRef>]</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>Two tri-axial accelerometers in a smartphone</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>ADLs such as sitting down, standing up, walking and stopping</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>SVM; Accuracy of 92–100 %; limited by the number of activities and the charge on the smartphone</SimplePara>
                      </entry>
                      <entry colname="c4">
                        <SimplePara>[<CitationRef CitationID="CR176">176</CitationRef>]</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>Five bi-axial accelerometers</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>20 different activities including walking, sitting, standing, lying down, climbing stairs, folding laundry, etc.</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>Decision tree; accuracy of 44–94 %; riding elevator and stretching confused with other activities; large number of sensors; wired connectivity and need for real-time synchronization are drawbacks</SimplePara>
                      </entry>
                      <entry colname="c4">
                        <SimplePara>[<CitationRef CitationID="CR100">100</CitationRef>]</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>Accelerometer on thigh and waist; RFID reader on hand glove; RFID tag on objects of daily use</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>18 activities—handshaking, rope jumping, brushing, making phone calls, reading, using umbrella and other ADLs</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>Decision trees; overall accuracy of 95 %; walking was detected by 84 %—confused with standing/running. The glove “iGrabber” is too big to wear for elderly people and tagging of different objects with RFID is exhaustive</SimplePara>
                      </entry>
                      <entry colname="c4">
                        <SimplePara>[<CitationRef CitationID="CR177">177</CitationRef>]</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>Infrared sensors to detect location and movement; door contacts; microphones; digital temperature and hygrometry sensor; tri-axial accelerometer; and magnetometer</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Bathing, dressing, resting, use of toilet, moving in/out of bed/chair, feeding</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>SVM; accuracy of 90 % except for dressing (75 %) and hygiene (64 %) which had fewer occurrences in the training database; use of multiple ambient sensors is expensive; needs to be tested on the elderly population</SimplePara>
                      </entry>
                      <entry colname="c4">
                        <SimplePara>[<CitationRef CitationID="CR178">178</CitationRef>]</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>Four accelerometers on the thigh, sternum and lower arm</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Posture of human body, lying, sitting, standing, walking, climbing stairs, cycling, driving, use of wheel chair and running</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>Average accuracy of 90 %, uses wired sensors and hence is intrusive</SimplePara>
                      </entry>
                      <entry colname="c4">
                        <SimplePara>[<CitationRef CitationID="CR179">179</CitationRef>]</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>Tri-axial accelerometer placed on the wrist</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>8 tasks from the WMFT—reach and retrieve, card flipping, grasping, etc.</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>Dynamic Time Warping; average accuracy of 96.5 %, the system is small, non-intrusive and uses only one sensor; which is advantageous</SimplePara>
                      </entry>
                      <entry colname="c4">
                        <SimplePara>[<CitationRef CitationID="CR180">180</CitationRef>]</SimplePara>
                      </entry>
                    </row>
                  </tbody>
                </tgroup>
              </Table>
</Para>
          </Section2>
        </Section1>
        <Section1 ID="Sec25">
          <Heading>Case Study</Heading>
          <Para ID="Par96">Our review of the existing modalities reveals that the majority of the published work on activity recognition has been devoted towards monitoring of gross dynamic human movements such as sleeping, sitting, standing, cycling and running. Apart from these, efforts have also been made to ascertain the activity levels of subjects in daily activities like bathing, personal grooming, toileting and drinking. Compared to those, very little work has been done in terms of activity recognition for elementary arm movements in nomadic environment, which can help to track the involvement of the impaired arm in daily living activities and thereby help in upper limb rehabilitation for stroke patients.</Para>
          <Para ID="Par97">Let us consider a case study where we report on a systematic exploration to recognize three fundamental movements of the upper limb that are generally associated with ADL, using data collected only from a wrist-worn, wireless tri-axial accelerometer. The motivation was to detect the occurrence of these specific arm movements in a real-world scenario (i.e. unconstrained environment), using the minimal number of body-worn inertial sensors. Enumerating occurrences of particular arm movements (e.g. prescribed exercises) during daily activities over time can provide a measure of rehabilitation progress in a wide range of remote health monitoring applications. It will be particularly useful in the monitoring of arm rehabilitation progress in pathologies associated with neurodegenerative diseases such as stroke or cerebral palsy.</Para>
          <Para ID="Par98">Continuous monitoring of activities in an unconstrained scenario involves data segmentation and activity recognition. Here, we concentrate only on the activity recognition part as a proof-of-concept methodology owing to the qualitative non-uniqueness of an activity pattern exhibited by a subject and due to inter-person variability. The accuracy of any movement recognition technique is dependent on the system components and requirements, covering areas such as type of activities, number of activities, type of sensors, number of sensors, placement of sensors [<CitationRef CitationID="CR108">108</CitationRef>], level of data fusion and most importantly the classification methodology adopted. Further, there is a requirement for subject-specific training, especially when tracking activities that are susceptible to individual and temporal variation [<CitationRef CitationID="CR100">100</CitationRef>]. Recognition strategies generally follow one of three themes. Firstly, data collected under controlled conditions (e.g. in the laboratory) is used for training as well as testing, resulting in high accuracies [<CitationRef CitationID="CR148">148</CitationRef>]. Secondly, controlled and uncontrolled data (e.g. out-of-laboratory) are used for both training and testing, which results in reasonably higher movement recognition accuracy [<CitationRef CitationID="CR100">100</CitationRef>, <CitationRef CitationID="CR181">181</CitationRef>]. Finally, using controlled data for training and only uncontrolled data for testing, results in lower accuracies but is more realistic of real-world applications [<CitationRef CitationID="CR100">100</CitationRef>, <CitationRef CitationID="CR181">181</CitationRef>]. To explore the levels of recognition accuracy for a robust classification mechanism applicable in the field of home-based rehabilitation, in this study we use controlled data for training and uncontrolled data for testing. Here, a subject with arm dexterity is requested to follow a particular exercise regime involving the impaired arm in a controlled environment (clinic or home) and is later monitored to track occurrences of these specific movements while they perform daily activities, involving the impaired arm.</Para>
          <Para ID="Par99">This facilitates a measure of rehabilitation progress. In this exploration, we consider three specific arm movements (i.e. actions) all of which involve rotations of the forearm about various axes. In principle, the three chosen elementary movements (<Emphasis Type="Italic">Action A</Emphasis>, <Emphasis Type="Italic">B</Emphasis> and <Emphasis Type="Italic">C</Emphasis>) constitute a significant proportion of the complex movements performed with the upper limb in daily life and also resemble three of the tasks (8, 9 and 15, respectively) in the streamlined WMFT [<CitationRef CitationID="CR182">182</CitationRef>, <CitationRef CitationID="CR183">183</CitationRef>]. The selected movements are:<UnorderedList Mark="None">
              <ItemContent>
                <Para ID="Par100">
<Emphasis Type="Italic">Action A</Emphasis>—Reach and retrieve an object (extension and flexion of the forearm).</Para>
              </ItemContent>
              <ItemContent>
                <Para ID="Par101">
<Emphasis Type="Italic">Action B</Emphasis>—Lift cup to mouth (rotation of the forearm about the elbow).</Para>
              </ItemContent>
              <ItemContent>
                <Para ID="Par102">
<Emphasis Type="Italic">Action C</Emphasis>—Perform pouring or (un)locking action (rotation of the wrist about long axis of forearm).</Para>
              </ItemContent>
            </UnorderedList>
</Para>
          <Para ID="Par103">The fundamental concept of this exploration is to first form a set of three clusters in multidimensional feature space, using selective features from a ranked set of features generated from person-centric data collected in a constrained <Emphasis Type="Italic">training</Emphasis> phase (e.g. in the laboratory). Each cluster formed represents a particular type of movement. Data collected subsequently during an unconstrained <Emphasis Type="Italic">testing</Emphasis> phase (e.g. out-of-laboratory) is tested for its proximity to each of the clusters by using a minimum distance classifier. The basic philosophy of our methodology is illustrated in Fig. <InternalRef RefID="Fig6">4.6</InternalRef>, where three clusters A, B and C are formed on the <Emphasis Type="Italic">training</Emphasis> dataset corresponding to the three movements (<Emphasis Type="Italic">Actions A</Emphasis>, <Emphasis Type="Italic">B</Emphasis> and <Emphasis Type="Italic">C</Emphasis>), respectively, in a two-dimensional feature space (Feature 1 (<Emphasis Type="Italic">f</Emphasis>
<Subscript>1</Subscript>) and Feature 2 (<Emphasis Type="Italic">f</Emphasis>
<Subscript>2</Subscript>)).<Figure Category="Standard" Float="Yes" ID="Fig6">
              <Caption Language="En">
                <CaptionNumber>Fig. 4.6</CaptionNumber>
                <CaptionContent>
                  <SimplePara>Illustration of the clustering and minimum distance classifier-based methodology</SimplePara>
                </CaptionContent>
              </Caption>
              <MediaObject ID="MO6">
                <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_4_Fig6_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
              </MediaObject>
            </Figure>
</Para>
          <Para ID="Par104">The distance of the <Emphasis Type="Italic">test</Emphasis> vector <Emphasis Type="Italic">T</Emphasis> from each of the three cluster centroids is represented by the distances <Emphasis Type="Italic">d</Emphasis>
<Subscript>
              <Emphasis Type="Italic">A</Emphasis>
            </Subscript>, <Emphasis Type="Italic">d</Emphasis>
<Subscript>
              <Emphasis Type="Italic">B</Emphasis>
            </Subscript> and <Emphasis Type="Italic">d</Emphasis>
<Subscript>
              <Emphasis Type="Italic">C</Emphasis>
            </Subscript>. These three distance measures are compared to estimate the proximity of the <Emphasis Type="Italic">test</Emphasis> dataset <Emphasis Type="Italic">T</Emphasis> to each cluster and assigned to the nearest one. This methodology can be further scaled up by forming more clusters corresponding to new categories of movements and associating a new dataset (corresponding to the movement to be detected) to the proximal cluster. The formation of unique clusters corresponding to each performed movement can be achieved by selecting the optimum number of features which help to discriminate movement patterns in the respective feature space.</Para>
          <Para ID="Par105">We used the regularized Mahalanobis distance-based <Emphasis Type="Italic">k</Emphasis>-means clustering technique to form the clusters on the person-centric <Emphasis Type="Italic">training</Emphasis> data (collected in the <Emphasis Type="Italic">laboratory</Emphasis> setup) and use 10 runs of a tenfold cross-validation technique to determine the best combination of cluster forming features. A minimum distance classifier based on Euclidean and Mahalanobis distance was used for associating the <Emphasis Type="Italic">test</Emphasis> data (collected during “<Emphasis Type="Italic">making-a-cup-of-tea”</Emphasis>) to the formed clusters in the same feature space [<CitationRef CitationID="CR170">170</CitationRef>]. We adopted a person-dependent, i.e. personalized approach, thereby formulating the clusters on a set of person-centric <Emphasis Type="Italic">training data</Emphasis> in view of the large degree of inter-person variability reflecting on the different levels of impairment and rehabilitation stage applicable in the field of remote health monitoring. This would be beneficial in monitoring the stage of rehabilitation of each individual.</Para>
          <Para ID="Par106">Although it is generally accepted that cluster analysis is primarily used for unsupervised learning (where the class labels for the training data are not available), the <Emphasis Type="Italic">k</Emphasis>-means algorithm can also be used for supervised learning where the class labels of the training data are known a priori [<CitationRef CitationID="CR184">184</CitationRef>, <CitationRef CitationID="CR185">185</CitationRef>]. In our exploration, the class labels for the training data relating to the three movements performed in a constrained <Emphasis Type="Italic">training</Emphasis> phase are already known. This helps to have a definitive estimate of the underlying cluster structure to be formed on the data (three clusters), thereby facilitating a faster convergence during cluster formation for reduced time complexity [<CitationRef CitationID="CR152">152</CitationRef>]. The primary steps involved in our overall approach are illustrated in Fig. <InternalRef RefID="Fig7">4.7</InternalRef> and described in detail in the following sections.<Figure Category="Standard" Float="Yes" ID="Fig7">
              <Caption Language="En">
                <CaptionNumber>Fig. 4.7</CaptionNumber>
                <CaptionContent>
                  <SimplePara>Training and testing phases of the data processing [<CitationRef CitationID="CR153">153</CitationRef>]</SimplePara>
                </CaptionContent>
              </Caption>
              <MediaObject ID="MO7">
                <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_4_Fig7_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
              </MediaObject>
            </Figure>
</Para>
          <Section2 ID="Sec26">
            <Heading>Data Acquisition and Pre-processing</Heading>
            <Para ID="Par107">A Shimmer 9DoF wireless kinematic sensor module containing mutually orthogonal tri-axial accelerometers, rate gyroscopes and magnetometers was used as the sensing platform.</Para>
            <Para ID="Par108">For our experiments, we only use the tri-axial accelerometer with ±1.5 g range selected and exclude the magnetometer since it can be affected by the presence of ferromagnetic materials which are expected to be present in the natural environment [<CitationRef CitationID="CR130">130</CitationRef>]. We also leave out the gyroscope in view of using a minimal number of sensors aimed at reducing the amount of data processing involved. The dorsal side of the forearm proximal to the wrist on the dominant arm was chosen as the sensing position since it was likely to produce significant sensor responses to the arm movements being investigated. The dorsal side was in contact with the <Emphasis Type="Italic">XY</Emphasis> plane of the sensor with the <Emphasis Type="Italic">X</Emphasis>-axis pointing towards the fingers and the <Emphasis Type="Italic">Z</Emphasis>-axis pointing away from the dorsal aspect (cf. Fig. <InternalRef RefID="Fig8">4.8</InternalRef>). Sensor data was collected at a rate of 50 Hz, deemed sufficient for assessing habitual limb movement [<CitationRef CitationID="CR164">164</CitationRef>, <CitationRef CitationID="CR108">108</CitationRef>]. In this exploration, experimental data was collected from four healthy subjects (age range 24–40, male, all right arm dominant) within an open laboratory, with an attached kitchen at the University of Southampton. To generate the <Emphasis Type="Italic">training</Emphasis> phase data for the target cluster formation, all four participants performed 240 trials of <Emphasis Type="Italic">Action A</Emphasis>, 120 trials of <Emphasis Type="Italic">Action B</Emphasis> and 120 trials of <Emphasis Type="Italic">Action C</Emphasis>, separated into groups of five repetitions, with each group of trials being separated by approximately 3 min. This was done to avoid unrepresentative data due to fatigue and to minimize the effects of unconscious self-learning of the activities. The data collection produces a large <Emphasis Type="Italic">training</Emphasis> set offering greater convenience for accurate recognition [<CitationRef CitationID="CR23">23</CitationRef>] since the cluster formation on the <Emphasis Type="Italic">training data</Emphasis> inherently captures the person-centric nature of movement patterns.<Figure Category="Standard" Float="Yes" ID="Fig8">
                <Caption Language="En">
                  <CaptionNumber>Fig. 4.8</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>Sensor module showing the coordinate system for the positive direction of the accelerometer axes (<Emphasis Type="Italic">left</Emphasis>) and when worn on the wrist showing the <Emphasis Type="Italic">X</Emphasis>- and <Emphasis Type="Italic">Y</Emphasis>-axis, the <Emphasis Type="Italic">Z</Emphasis>-axis points away from the plane (<Emphasis Type="Italic">right</Emphasis>)</SimplePara>
                  </CaptionContent>
                </Caption>
                <MediaObject ID="MO8">
                  <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_4_Fig8_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                </MediaObject>
              </Figure>
</Para>
            <Para ID="Par109">On a separate day, participants were recalled and requested to perform four repetitions of the activity list in Table <InternalRef RefID="Tab2">4.2</InternalRef> at a comfortable speed in a kitchen, with a 10-min rest period between repetitions. The activity list was designed to emulate the process of “<Emphasis Type="Italic">making-a-cup-of-tea</Emphasis>”, a common activity performed in daily life, having repeated occurrences of the three elementary types of arm movement (<Emphasis Type="Italic">actions</Emphasis>). The activity list in our experiment protocol comprises 20 individual activities including 10 occurrences of <Emphasis Type="Italic">Action A</Emphasis>, and 5 each of <Emphasis Type="Italic">Action B</Emphasis> and <Emphasis Type="Italic">Action C</Emphasis>. There were no restrictions on the various physical factors of the experiment such as the seating position or standing position with respect to the kitchen surface or the time required to complete the actions. The experiment was unconstrained to ensure a wider range of variability in the data paving the way for a robust arm movement recognition system which will produce acceptable levels of accuracy in a real-world application. The activity list was prepared to facilitate the evaluation of the recognition methodology under semi-naturalistic conditions. The start and stop time of the activities were noted down by the researcher observing them as they performed their designated tasks. The corresponding data collected was segmented using the annotations from the researcher and used for the testing phase.<Table Float="Yes" ID="Tab2">
                <Caption Language="En">
                  <CaptionNumber>Table 4.2</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>Use case activity list—“making-a-cup-of-tea”</SimplePara>
                  </CaptionContent>
                </Caption>
                <tgroup align="left" cols="3">
                  <colspec align="left" colname="c1" colnum="1"/>
                  <colspec align="left" colname="c2" colnum="2"/>
                  <colspec align="left" colname="c3" colnum="3"/>
                  <thead>
                    <row>
                      <entry nameend="c2" namest="c1">
                        <SimplePara>Activity</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>Action</SimplePara>
                      </entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry colname="c1">
                        <SimplePara>1.</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Fetch cup from desk</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>A</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>2.</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Place cup on kitchen surface</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>A</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>3.</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Fetch kettle</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>A</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>4.</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Pour out extra water from kettle</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>C</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>5.</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Put kettle onto charging point</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>A</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>6.</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Reach out for the power switch on the wall</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>A</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>7.</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Drink a glass of water while waiting for kettle to boil</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>B</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>8.</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Reach out to switch off the kettle</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>A</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>9.</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Pour hot water from the kettle into cup</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>C</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>10.</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Fetch milk from the shelf</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>A</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>11.</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Pour milk into cup</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>C</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>12.</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Put the bottle of milk back on shelf</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>A</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>13.</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Fetch cup from kitchen surface</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>A</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>14.</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Have a sip and taste the drink</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>B</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>15.</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Have another sip while walking back to desk</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>B</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>16.</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Unlock drawer</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>C</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>17.</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Retrieve biscuits from drawer</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>A</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>18.</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Eat a biscuit</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>B</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>19.</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Lock drawer</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>C</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>20.</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Have a drink</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>B</SimplePara>
                      </entry>
                    </row>
                  </tbody>
                </tgroup>
              </Table>
</Para>
            <Para ID="Par110">The data structure for the <Emphasis Type="Italic">training</Emphasis> and <Emphasis Type="Italic">testing</Emphasis> phase used in this analysis is summarized in Table <InternalRef RefID="Tab3">4.3</InternalRef>.<Table Float="Yes" ID="Tab3">
                <Caption Language="En">
                  <CaptionNumber>Table 4.3</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>Data structure used in the training and testing phases</SimplePara>
                  </CaptionContent>
                </Caption>
                <tgroup align="left" cols="2">
                  <colspec align="left" colname="c1" colnum="1"/>
                  <colspec align="left" colname="c2" colnum="2"/>
                  <thead>
                    <row>
                      <entry colname="c1">
                        <SimplePara>Attributes</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Quantification</SimplePara>
                      </entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry colname="c1">
                        <SimplePara>Number of subjects</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>4</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>Training dataset</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>[Action A—240; Action B—120; Action C—120] = 480</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>Testing dataset</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>[Action A—10; Action B—5; Action C—5] × 4 trials = 80</SimplePara>
                      </entry>
                    </row>
                  </tbody>
                </tgroup>
              </Table>
</Para>
            <Para ID="Par111">The tri-axial accelerometer located on the wrist transmits data along with a time stamp to a host computer using the Bluetooth wireless transmission protocol, and each activity performed by a subject is marked to record the start and end of the movement for all trials performed during the <Emphasis Type="Italic">training</Emphasis> and <Emphasis Type="Italic">testing</Emphasis> phases. The raw sensor data is band-pass filtered with a third-order Butterworth filter having cut-off frequencies of 0.1 Hz and 12 Hz to respectively attenuate the low-frequency artefacts and high-frequency noise components introduced in the data due to physical effects such as drift [<CitationRef CitationID="CR13">13</CitationRef>].</Para>
          </Section2>
          <Section2 ID="Sec27">
            <Heading>Feature Extraction</Heading>
            <Para ID="Par112">Each sensor data stream exhibits signal patterns that are distinctive for each of the arm movements, which is characterized by a set of features extracted from the signals [<CitationRef CitationID="CR164">164</CitationRef>]. In this investigation, we consider ten time-domain features which are listed in Table <InternalRef RefID="Tab4">4.4</InternalRef>.<Table Float="Yes" ID="Tab4">
                <Caption Language="En">
                  <CaptionNumber>Table 4.4</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>List of features considered in this investigation</SimplePara>
                  </CaptionContent>
                </Caption>
                <tgroup align="left" cols="3">
                  <colspec align="left" colname="c1" colnum="1"/>
                  <colspec align="left" colname="c2" colnum="2"/>
                  <colspec align="left" colname="c3" colnum="3"/>
                  <thead>
                    <row>
                      <entry colname="c1">
                        <SimplePara>No.</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Features</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>Description</SimplePara>
                      </entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry colname="c1">
                        <SimplePara>1</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Standard deviation</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>Measure of the variability from the mean of the signal</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>2</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Root mean square (rms)</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>Measure of the signal energy normalized by the number of samples</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>3</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Information entropy</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>Measure of the randomness of a signal [<CitationRef CitationID="CR186">186</CitationRef>]</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>4</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Jerk metric</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>rms value of the second derivative of the data normalized with respect to the maximum value of the first derivative [<CitationRef CitationID="CR3">3</CitationRef>]</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>5</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Peak number</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>Obtained from gradient analysis of the signal</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>6</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Maximum peak amplitude</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>Measure of the amplitude of the peaks obtained after gradient analysis</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>7</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Absolute difference</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>Absolute difference between the maximum and the minimum value of a signal</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>8</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Index of dispersion</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>Ratio of variance to the mean</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>9</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Kurtosis</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>Measure of the “peakedness” of a signal</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>10</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Skewness</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>Measure of the asymmetry of a signal [<CitationRef CitationID="CR187">187</CitationRef>]</SimplePara>
                      </entry>
                    </row>
                  </tbody>
                </tgroup>
              </Table>
</Para>
            <Para ID="Par113">We compute 10 one-dimensional features from the data produced by each accelerometer axis (<Emphasis Type="Italic">acc_x</Emphasis>, <Emphasis Type="Italic">acc_y</Emphasis>, <Emphasis Type="Italic">acc_z</Emphasis>) for each movement trial of each subject. The subsequent process of feature selection and cluster formation is performed on the sensor-specific feature space (comprising 30 features), represented as:<Equation ID="Equ12">
                <EquationNumber>4.12</EquationNumber>
                <MediaObject>
                  <ImageObject Color="BlackWhite" FileRef="327370_1_En_4_Chapter_Equ12.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                </MediaObject>
                <EquationSource Format="MATHML">
                  <math xmlns:xlink="http://www.w3.org/1999/xlink">
                    <mrow>
                      <mi mathvariant="normal">FS</mi>
                      <mo>=</mo>
                      <mrow>
                        <mo>[</mo>
                        <mrow>
                          <msub>
                            <mi>f</mi>
                            <mn>1</mn>
                          </msub>
                          <mo>_</mo>
                          <mi>x</mi>
                          <mo>…</mo>
                          <msub>
                            <mi>f</mi>
                            <mrow>
                              <mn>10</mn>
                            </mrow>
                          </msub>
                          <mo>_</mo>
                          <mi>x</mi>
                          <mo>,</mo>
                          <msub>
                            <mi>f</mi>
                            <mn>1</mn>
                          </msub>
                          <mo>_</mo>
                          <mi>y</mi>
                          <mo>…</mo>
                          <msub>
                            <mi>f</mi>
                            <mrow>
                              <mn>10</mn>
                            </mrow>
                          </msub>
                          <mo>_</mo>
                          <mi>y</mi>
                          <mo>,</mo>
                          <msub>
                            <mi>f</mi>
                            <mn>1</mn>
                          </msub>
                          <mo>_</mo>
                          <mi>z</mi>
                          <mo>…</mo>
                          <msub>
                            <mi>f</mi>
                            <mrow>
                              <mn>10</mn>
                            </mrow>
                          </msub>
                          <mo>_</mo>
                          <mi>z</mi>
                        </mrow>
                        <mo>]</mo>
                      </mrow>
                    </mrow>
                  </math>
                </EquationSource>
                <EquationSource Format="TEX"><![CDATA[
$$ \mathrm{F}\mathrm{S}=\left[{f}_1\_x\dots {f}_{10}\_x,{f}_1\_y\dots {f}_{10}\_y,{f}_1\_z\dots {f}_{10}\_z\right] $$
]]></EquationSource>
              </Equation>where FS represents the respective feature space formed by considering the ten individual features (<Emphasis Type="Italic">f</Emphasis>
<Subscript>1</Subscript>…<Emphasis Type="Italic">f</Emphasis>
<Subscript>10</Subscript>) computed on each tri-axial data segment together. The suffix (<Emphasis Type="Italic">x</Emphasis>, <Emphasis Type="Italic">y</Emphasis> or <Emphasis Type="Italic">z</Emphasis>) represents the sensor axis on which the respective feature was computed. The movements performed by the subjects were segmented using the annotations from the researcher, therefore each feature was computed on data segments of varying lengths representative of the time taken to complete each movement trial.</Para>
          </Section2>
          <Section2 ID="Sec28">
            <Heading>Feature Selection</Heading>
            <Para ID="Par114">The extracted features are linearly normalized, and the best features for each subject are selected by using the low-complexity class-separability measure based on scatter matrices (cf. Sect. <InternalRef RefID="Sec19">4.3.6.4</InternalRef>) which ranks the 30 features. The ranked features are sorted in descending order with respect to their <Emphasis Type="Italic">R</Emphasis> values. We employ a sequential forward selection (<Emphasis Type="Italic">sfs</Emphasis>) technique (cf. Sect. <InternalRef RefID="Sec19">4.3.6.4</InternalRef>), selecting the first <Emphasis Type="Italic">i</Emphasis> features of the ranked feature set in each iteration (<Emphasis Type="Italic">i</Emphasis> = 2…30) and check if the data from the <Emphasis Type="Italic">training</Emphasis> phase can be correctly clustered in a multidimensional feature space as described in the next section.</Para>
          </Section2>
          <Section2 ID="Sec29">
            <Heading>Cluster Formation on the Training Dataset</Heading>
            <Para ID="Par115">The fundamental concept of cluster analysis is to form groups of similar objects as a means of distinguishing them from each other and can be applied in any discipline involving multivariate data [<CitationRef CitationID="CR188">188</CitationRef>]. With a given dataset <Emphasis Type="Italic">X</Emphasis> = {<Emphasis Type="Italic">x</Emphasis>
<Subscript>
                <Emphasis Type="Italic">i</Emphasis>
              </Subscript>}, <Emphasis Type="Italic">i</Emphasis> = 1…<Emphasis Type="Italic">n</Emphasis> to be clustered into a set of <Emphasis Type="Italic">k</Emphasis> clusters, the <Emphasis Type="Italic">k</Emphasis>-means algorithm iterates to minimize the squared error between the empirical mean of a cluster and the individual data points, defined as the cost function, <Emphasis Type="Italic">J</Emphasis>:<Equation ID="Equ13">
                <EquationNumber>4.13</EquationNumber>
                <MediaObject>
                  <ImageObject Color="BlackWhite" FileRef="327370_1_En_4_Chapter_Equ13.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                </MediaObject>
                <EquationSource Format="MATHML">
                  <math xmlns:xlink="http://www.w3.org/1999/xlink">
                    <mrow>
                      <mi>J</mi>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <mi>θ</mi>
                          <mi mathvariant="normal">,</mi>
                          <mi>u</mi>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                      <mo>=</mo>
                      <munderover>
                        <mstyle displaystyle="true" mathsize="10">
                          <mo>∑</mo>
                        </mstyle>
                        <mrow>
                          <mi>i</mi>
                          <mo>=</mo>
                          <mn>1</mn>
                        </mrow>
                        <mi>n</mi>
                      </munderover>
                      <munderover>
                        <mstyle displaystyle="true" mathsize="10">
                          <mo>∑</mo>
                        </mstyle>
                        <mrow>
                          <mi>j</mi>
                          <mo>=</mo>
                          <mn>1</mn>
                        </mrow>
                        <mi>k</mi>
                      </munderover>
                      <msub>
                        <mi>u</mi>
                        <mrow>
                          <mi>i</mi>
                          <mi>j</mi>
                        </mrow>
                      </msub>
                      <mo>∥</mo>
                      <msub>
                        <mi>x</mi>
                        <mi>i</mi>
                      </msub>
                      <mo>−</mo>
                      <msub>
                        <mi>θ</mi>
                        <mi>j</mi>
                      </msub>
                      <msup>
                        <mo>∥</mo>
                        <mn>2</mn>
                      </msup>
                    </mrow>
                  </math>
                </EquationSource>
                <EquationSource Format="TEX"><![CDATA[
$$ J\left(\theta, u\right)={\displaystyle \sum}_{i=1}^n{\displaystyle \sum}_{j=1}^k{u}_{ij}\parallel {x}_i-{\theta}_j{\parallel}^2 $$
]]></EquationSource>
              </Equation>where <Emphasis Type="Italic">θ</Emphasis>
<Subscript>
                <Emphasis Type="Italic">j</Emphasis>
              </Subscript> is the cluster centre, and <Emphasis Type="Italic">u</Emphasis>
<Subscript>
                <Emphasis Type="Italic">ij</Emphasis>
              </Subscript> = 1 if <Emphasis Type="Italic">x</Emphasis>
<Subscript>
                <Emphasis Type="Italic">i</Emphasis>
              </Subscript> lies close to <Emphasis Type="Italic">θ</Emphasis>
<Subscript>
                <Emphasis Type="Italic">j</Emphasis>
              </Subscript> or 0 if otherwise [<CitationRef CitationID="CR189">189</CitationRef>]. Initially, <Emphasis Type="Italic">k</Emphasis> centroids are defined and the data vectors are assigned to a cluster label depending on how close they are to each centroid. The <Emphasis Type="Italic">k</Emphasis> centroids are recalculated from the newly defined clusters, and the process of reassignment of each data vector to each new centroid is repeated. The algorithm iterates over this loop until the data vectors from the dataset <Emphasis Type="Italic">X</Emphasis> form clusters and the cost function <Emphasis Type="Italic">J</Emphasis> is minimized [<CitationRef CitationID="CR152">152</CitationRef>].</Para>
            <Para ID="Par116">The Euclidean distance used to compute the squared distance between the vectors <Emphasis Type="Italic">x</Emphasis>
<Subscript>
                <Emphasis Type="Italic">i</Emphasis>
              </Subscript> and the mean of each cluster <Emphasis Type="Italic">θ</Emphasis>
<Subscript>
                <Emphasis Type="Italic">j</Emphasis>
              </Subscript> has an undesirable effect of splitting large and elongated clusters, since most real datasets do not have a well-defined, isolated and spherical underlying cluster structure. By comparison, the use of the Mahalanobis distance, which involves computing the covariance matrix of the data vector, causes a large cluster to absorb nearby smaller clusters, leading to the creation of unusually large or small clusters. Hence, we use the regularized Mahalanobis distance as mentioned in [<CitationRef CitationID="CR189">189</CitationRef>] which prevents the clustering algorithm from producing unusually large or small clusters. The distance measure <Emphasis Type="Italic">J</Emphasis> is given by:<Equation ID="Equ14">
                <EquationNumber>4.14</EquationNumber>
                <MediaObject>
                  <ImageObject Color="BlackWhite" FileRef="327370_1_En_4_Chapter_Equ14.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                </MediaObject>
                <EquationSource Format="MATHML">
                  <math xmlns:xlink="http://www.w3.org/1999/xlink">
                    <mrow>
                      <mi>J</mi>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <msub>
                            <mi>x</mi>
                            <mi>i</mi>
                          </msub>
                          <mi mathvariant="normal">,</mi>
                          <msub>
                            <mi>θ</mi>
                            <mi>j</mi>
                          </msub>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                      <mo>=</mo>
                      <msup>
                        <mrow>
                          <mrow>
                            <mo>(</mo>
                            <mrow>
                              <msub>
                                <mi>x</mi>
                                <mi>i</mi>
                              </msub>
                              <mo>−</mo>
                              <msub>
                                <mi>θ</mi>
                                <mi>j</mi>
                              </msub>
                            </mrow>
                            <mo>)</mo>
                          </mrow>
                        </mrow>
                        <mi>T</mi>
                      </msup>
                      <mrow>
                        <mo>[</mo>
                        <mrow>
                          <mrow>
                            <mo>(</mo>
                            <mrow>
                              <mn>1</mn>
                              <mo>−</mo>
                              <mi>λ</mi>
                            </mrow>
                            <mo>)</mo>
                          </mrow>
                          <msup>
                            <mrow>
                              <mrow>
                                <mo>(</mo>
                                <mrow>
                                  <msub>
                                    <mi>Σ</mi>
                                    <mi>j</mi>
                                  </msub>
                                  <mo>+</mo>
                                  <mi>ε</mi>
                                  <mi>Ι</mi>
                                </mrow>
                                <mo>)</mo>
                              </mrow>
                            </mrow>
                            <mrow>
                              <mo>−</mo>
                              <mn>1</mn>
                            </mrow>
                          </msup>
                          <mo>+</mo>
                          <mi>λ</mi>
                          <mi>Ι</mi>
                        </mrow>
                        <mo>]</mo>
                      </mrow>
                      <mrow>
                        <mo>(</mo>
                        <mrow>
                          <msub>
                            <mi>x</mi>
                            <mi>i</mi>
                          </msub>
                          <mo>−</mo>
                          <msub>
                            <mi>θ</mi>
                            <mi>j</mi>
                          </msub>
                        </mrow>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                  </math>
                </EquationSource>
                <EquationSource Format="TEX"><![CDATA[
$$ J\left({x}_i,{\theta}_j\right)={\left({x}_i-{\theta}_j\right)}^T\left[\left(1-\lambda \right){\left({\varSigma}_j+\varepsilon I\right)}^{-1}+\lambda I\right]\left({x}_i-{\theta}_j\right) $$
]]></EquationSource>
              </Equation>where <Emphasis Type="Italic">Σ</Emphasis>
<Subscript>
                <Emphasis Type="Italic">j</Emphasis>
              </Subscript> is the covariance matrix of the <Emphasis Type="Italic">k</Emphasis>-th cluster and <Emphasis Type="Italic">I</Emphasis> is the <Emphasis Type="Italic">d × d</Emphasis> identity matrix, <Emphasis Type="Italic">d</Emphasis> is the input dimensionality (no. of feature vectors representing the data vector) and <Emphasis Type="Italic">ε</Emphasis>(10<Superscript>−6</Superscript>) is the regularization parameter. The value of <Emphasis Type="Italic">λ</Emphasis> can be used as a parameter to control the choice of distance measure to be used, with <Emphasis Type="Italic">λ</Emphasis> = 0, <Emphasis Type="Italic">J</Emphasis> is the squared Mahalanobis distance and when <Emphasis Type="Italic">λ</Emphasis> = 1, <Emphasis Type="Italic">J</Emphasis> is the squared Euclidean distance [<CitationRef CitationID="CR189">189</CitationRef>]. In our exploration, we start with an initial value of <Emphasis Type="Italic">λ</Emphasis> = 1 (Euclidean) and after three iterations change it to <Emphasis Type="Italic">λ</Emphasis> = 0 (Mahalanobis). The cluster formation on the <Emphasis Type="Italic">training</Emphasis> dataset is associated with a cross-validation step to determine the best combination of the features, discussed in detail in the following section.</Para>
          </Section2>
          <Section2 ID="Sec30">
            <Heading>Cross-Validation on the Training Dataset</Heading>
            <Para ID="Par117">We perform 10 runs of tenfold cross-validation on the feature vectors computed from the accelerometer data which characterizes the movement trials of the <Emphasis Type="Italic">training</Emphasis> phase (480 trials for each healthy subject) to form three clusters representing the three arm movements [<CitationRef CitationID="CR170">170</CitationRef>]. The key steps involved in the cross-validation process have been highlighted below:<UnorderedList Mark="Bullet">
                <ItemContent>
                  <Para ID="Par118">The cluster formation using the regularized Mahalanobis distance (as discussed in Sect. <InternalRef RefID="Sec10">4.3.4</InternalRef>) runs on the <Emphasis Type="Italic">training</Emphasis> dataset for each subject that comprises feature vectors (30 features) extracted from each sensor data segment.</Para>
                </ItemContent>
                <ItemContent>
                  <Para ID="Par119">The algorithm runs in conjunction with the <Emphasis Type="Italic">sfs</Emphasis> algorithm sequentially selecting a combination of 2–30 ranked features in each step (<Emphasis Type="Italic">i</Emphasis>).</Para>
                </ItemContent>
                <ItemContent>
                  <Para ID="Par120">For a particular set of feature vectors selected (<Emphasis Type="Italic">i</Emphasis>), 10 runs (<Emphasis Type="Italic">n</Emphasis>) of tenfold cross-validation are carried out whereby ten segments of the <Emphasis Type="Italic">training</Emphasis> data are created.</Para>
                </ItemContent>
                <ItemContent>
                  <Para ID="Par121">In each run (<Emphasis Type="Italic">n</Emphasis>) of the stipulated 10 runs, one segment is used as the <Emphasis Type="Italic">test</Emphasis> dataset while the rest of the nine segments are used as the <Emphasis Type="Italic">training</Emphasis> dataset.</Para>
                </ItemContent>
                <ItemContent>
                  <Para ID="Par122">We set a threshold of 25 % of the expected number of data points for each of the three clusters formed (i.e. 240 ± 60 for <Emphasis Type="Italic">Action A</Emphasis> and 120 ± 30 for <Emphasis Type="Italic">Action B</Emphasis> and <Emphasis Type="Italic">Action C</Emphasis>). This threshold value was experimentally selected since it produced the best results [<CitationRef CitationID="CR153">153</CitationRef>]. If the number of data points in each cluster is within the threshold, we consider it as correctly clustered for that particular combination (<Emphasis Type="Italic">i</Emphasis>) of features selected (where <Emphasis Type="Italic">i =</Emphasis> 2,…,30).</Para>
                </ItemContent>
                <ItemContent>
                  <Para ID="Par123">We compute the distance of the mean of the <Emphasis Type="Italic">training</Emphasis> dataset for each class label from the cluster centroids and thereby assign each cluster with the class label that has its closest proximity to that particular class of a <Emphasis Type="Italic">training</Emphasis> dataset.</Para>
                </ItemContent>
                <ItemContent>
                  <Para ID="Par124">We then use a minimum distance classifier to compute the distance of the <Emphasis Type="Italic">test</Emphasis> dataset (one segment of the stipulated tenfolds) from the centroid of each cluster in a multidimensional feature space (considering the feature combination of the current step, <Emphasis Type="Italic">i</Emphasis>) based upon: Euclidean distance and Mahalanobis distance. The Mahalanobis distance is used to measure the distance of a point from a data distribution. The data distribution is characterized by the mean and the covariance matrix which defines the shape of how the data is distributed in the feature space and is generally hypothesized as a multivariate Gaussian distribution. Here, the Mahalanobis distance takes into consideration the covariance of the clusters along with their mean for the maximum likelihood estimation of the covariance matrix and hence is effective for clusters with larger variance along one or many directions and in general having an ellipsoidal shape [<CitationRef CitationID="CR152">152</CitationRef>].</Para>
                </ItemContent>
                <ItemContent>
                  <Para ID="Par125">The <Emphasis Type="Italic">test</Emphasis> dataset is assigned to a particular cluster depending on the minimum distance computed for each of the two measures.</Para>
                </ItemContent>
                <ItemContent>
                  <Para ID="Par126">The predicted label is verified with respect to the known annotations, thereby ascertaining the accuracy of the prediction for a single run (<Emphasis Type="Italic">n</Emphasis>).</Para>
                </ItemContent>
                <ItemContent>
                  <Para ID="Par127">The accuracy of prediction for a particular feature combination is determined by averaging the results produced over the runs (<Emphasis Type="Italic">n</Emphasis> ≤ 10) forming successful clusters. This process is repeated for each of the sequentially selected best ranked feature combinations (<Emphasis Type="Italic">i =</Emphasis> 2…30).</Para>
                </ItemContent>
              </UnorderedList>
</Para>
            <Para ID="Par128">Therefore at the end of all iterations (i.e. <Emphasis Type="Italic">i</Emphasis> = 30), we have a detailed list of the feature combinations that resulted in a successful cluster formation and the corresponding accuracies achieved both with Euclidean and Mahalanobis distance measures, for each subject. This information (minimal number of feature combination, resulting in the best accuracy) is used for the target classification of the <Emphasis Type="Italic">testing</Emphasis> phase data collected in the semi-naturalistic setup by associating them to the formed clusters in the chosen feature space. For all the subjects, the number of features chosen was in the range of 2–27 (selected out of total of 30) and the individual sensitivities were in the range of 90–100 %.</Para>
          </Section2>
          <Section2 ID="Sec31">
            <Heading>Prospective Evaluation with the Testing Dataset</Heading>
            <Para ID="Par129">The data from the <Emphasis Type="Italic">testing</Emphasis> phase (cf. Table <InternalRef RefID="Tab2">4.2</InternalRef>) collected in the semi-naturalistic setup during the “<Emphasis Type="Italic">making-a-cup-of-tea</Emphasis>” session (80 test vectors for each healthy subject [(10<Emphasis Type="Italic">A</Emphasis> + 5<Emphasis Type="Italic">B</Emphasis> + 5<Emphasis Type="Italic">C</Emphasis>) × 4 trials] and 40 test vectors for each stroke patient [(10<Emphasis Type="Italic">A</Emphasis> + 5<Emphasis Type="Italic">B</Emphasis> + 5<Emphasis Type="Italic">C</Emphasis>) × 2 trials]) is pre-processed for each type of sensor, and only those features are extracted from each test vector which resulted in the best accuracy in the cross-validation of the <Emphasis Type="Italic">training</Emphasis> data. We use a Euclidean and Mahalanobis distance based classifier to compute the distance of each test vector (represented by the extracted features) from the centroid of each cluster in a multidimensional feature space. The test vector is assigned to a particular cluster depending on the minimum distance computed for each of the two measures. Predicted label is verified with respect to annotations in the activity list of Table <InternalRef RefID="Tab2">4.2</InternalRef>.</Para>
          </Section2>
          <Section2 ID="Sec32">
            <Heading>Results and Analysis</Heading>
            <Para ID="Par130">Typical variations in accelerometer data recorded during a single example of each action are illustrated in Fig. <InternalRef RefID="Fig9">4.9</InternalRef>. We had in total 80 movement trials (<Emphasis Type="Italic">actions</Emphasis>) to be recognized (40 of <Emphasis Type="Italic">A</Emphasis>, 20 of <Emphasis Type="Italic">B</Emphasis>, 20 of <Emphasis Type="Italic">C</Emphasis>) for each subject. The results of the prospective evaluation in terms of the sensitivity of recognizing the movements performed in the <Emphasis Type="Italic">testing</Emphasis> phase are presented in Table <InternalRef RefID="Tab5">4.5</InternalRef>. The table also shows the minimum number of features that were required to successfully form the three clusters for each subject. The number of features has been determined by 10 runs of tenfold cross-validation on the <Emphasis Type="Italic">training</Emphasis> phase data as discussed in Sect. <InternalRef RefID="Sec30">4.4.5</InternalRef>. The results in general show that each subject required a different minimum number of features to successfully form three separate clusters from the <Emphasis Type="Italic">training</Emphasis> data, reflecting the variability in arm movement patterns between individuals. The final two columns in Table <InternalRef RefID="Tab5">4.5</InternalRef> show the overall detection accuracy (total number of recognized actions expressed as a percentage of the total number of actions performed) for each subject, which covers the range 61–100 % (average of 88 %) and 61–93 % (average of 80 %) for the two different distance measures (Euclidean and Mahalanobis).<Figure Category="Standard" Float="Yes" ID="Fig9">
                <Caption Language="En">
                  <CaptionNumber>Fig. 4.9</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>Data from a tri-axial accelerometer located on the wrist collected while performing arm actions A, B and C with a healthy subject (<Emphasis Type="Italic">left</Emphasis>) and a stroke survivor (<Emphasis Type="Italic">right</Emphasis>) [<CitationRef CitationID="CR170">170</CitationRef>]</SimplePara>
                  </CaptionContent>
                </Caption>
                <MediaObject ID="MO9">
                  <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_4_Fig9_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                </MediaObject>
              </Figure>
<Table Float="Yes" ID="Tab5">
                <Caption Language="En">
                  <CaptionNumber>Table 4.5</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>Calculated recognition accuracies and number of features required for each subject performing the activity “making-a-cup-of-tea” when using Euclidean (Euc) and Mahalanobis (Mah) metrics to assign clusters</SimplePara>
                  </CaptionContent>
                </Caption>
                <tgroup align="left" cols="10">
                  <colspec align="left" colname="c1" colnum="1"/>
                  <colspec align="left" colname="c2" colnum="2"/>
                  <colspec align="left" colname="c3" colnum="3"/>
                  <colspec align="left" colname="c4" colnum="4"/>
                  <colspec align="left" colname="c5" colnum="5"/>
                  <colspec align="left" colname="c6" colnum="6"/>
                  <colspec align="left" colname="c7" colnum="7"/>
                  <colspec align="left" colname="c8" colnum="8"/>
                  <colspec align="left" colname="c9" colnum="9"/>
                  <colspec align="left" colname="c10" colnum="10"/>
                  <thead>
                    <row>
                      <entry colname="c1" morerows="2">
                        <SimplePara>Subject</SimplePara>
                      </entry>
                      <entry colname="c2" morerows="2">
                        <SimplePara>Features</SimplePara>
                      </entry>
                      <entry nameend="c10" namest="c3">
                        <SimplePara>Recognition accuracies (%)</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c3">
                        <SimplePara>A</SimplePara>
                      </entry>
                      <entry colname="c4">
                        <SimplePara>A</SimplePara>
                      </entry>
                      <entry colname="c5">
                        <SimplePara>B</SimplePara>
                      </entry>
                      <entry colname="c6">
                        <SimplePara>B</SimplePara>
                      </entry>
                      <entry colname="c7">
                        <SimplePara>C</SimplePara>
                      </entry>
                      <entry colname="c8">
                        <SimplePara>C</SimplePara>
                      </entry>
                      <entry colname="c9">
                        <SimplePara>Overall</SimplePara>
                      </entry>
                      <entry colname="c10">
                        <SimplePara>Overall</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c3">
                        <SimplePara>Euc</SimplePara>
                      </entry>
                      <entry colname="c4">
                        <SimplePara>Mah</SimplePara>
                      </entry>
                      <entry colname="c5">
                        <SimplePara>Euc</SimplePara>
                      </entry>
                      <entry colname="c6">
                        <SimplePara>Mah</SimplePara>
                      </entry>
                      <entry colname="c7">
                        <SimplePara>Euc</SimplePara>
                      </entry>
                      <entry colname="c8">
                        <SimplePara>Mah</SimplePara>
                      </entry>
                      <entry colname="c9">
                        <SimplePara>Euc</SimplePara>
                      </entry>
                      <entry colname="c10">
                        <SimplePara>Mah</SimplePara>
                      </entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry colname="c1">
                        <SimplePara>Subject 1</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>11</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>100</SimplePara>
                      </entry>
                      <entry colname="c4">
                        <SimplePara>98</SimplePara>
                      </entry>
                      <entry colname="c5">
                        <SimplePara>100</SimplePara>
                      </entry>
                      <entry colname="c6">
                        <SimplePara>100</SimplePara>
                      </entry>
                      <entry colname="c7">
                        <SimplePara>100</SimplePara>
                      </entry>
                      <entry colname="c8">
                        <SimplePara>75</SimplePara>
                      </entry>
                      <entry colname="c9">
                        <SimplePara>100</SimplePara>
                      </entry>
                      <entry colname="c10">
                        <SimplePara>93</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>Subject 2</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>2</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>80</SimplePara>
                      </entry>
                      <entry colname="c4">
                        <SimplePara>75</SimplePara>
                      </entry>
                      <entry colname="c5">
                        <SimplePara>5</SimplePara>
                      </entry>
                      <entry colname="c6">
                        <SimplePara>5</SimplePara>
                      </entry>
                      <entry colname="c7">
                        <SimplePara>80</SimplePara>
                      </entry>
                      <entry colname="c8">
                        <SimplePara>95</SimplePara>
                      </entry>
                      <entry colname="c9">
                        <SimplePara>61</SimplePara>
                      </entry>
                      <entry colname="c10">
                        <SimplePara>61</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>Subject 3</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>7</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>95</SimplePara>
                      </entry>
                      <entry colname="c4">
                        <SimplePara>85</SimplePara>
                      </entry>
                      <entry colname="c5">
                        <SimplePara>100</SimplePara>
                      </entry>
                      <entry colname="c6">
                        <SimplePara>100</SimplePara>
                      </entry>
                      <entry colname="c7">
                        <SimplePara>90</SimplePara>
                      </entry>
                      <entry colname="c8">
                        <SimplePara>60</SimplePara>
                      </entry>
                      <entry colname="c9">
                        <SimplePara>95</SimplePara>
                      </entry>
                      <entry colname="c10">
                        <SimplePara>83</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>Subject 4</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>23</SimplePara>
                      </entry>
                      <entry colname="c3">
                        <SimplePara>95</SimplePara>
                      </entry>
                      <entry colname="c4">
                        <SimplePara>63</SimplePara>
                      </entry>
                      <entry colname="c5">
                        <SimplePara>100</SimplePara>
                      </entry>
                      <entry colname="c6">
                        <SimplePara>100</SimplePara>
                      </entry>
                      <entry colname="c7">
                        <SimplePara>85</SimplePara>
                      </entry>
                      <entry colname="c8">
                        <SimplePara>100</SimplePara>
                      </entry>
                      <entry colname="c9">
                        <SimplePara>94</SimplePara>
                      </entry>
                      <entry colname="c10">
                        <SimplePara>81</SimplePara>
                      </entry>
                    </row>
                  </tbody>
                </tgroup>
              </Table>
</Para>
            <Para ID="Par131">A list of ranked features is presented in Table <InternalRef RefID="Tab6">4.6</InternalRef> (sorted in descending order), highlighting the features selected for each subject. This shows that for each subject, the ranked order of features is different (reflecting on the different ways in which they perform a movement) and also shows the difference in the number of features required to form the clusters. In general, we can consider the recognition accuracies achieved as being favourable, with the obvious exception of detecting <Emphasis Type="Italic">Action B</Emphasis> with Subject 2 with both the Euclidean and Mahalanobis minimum distance classifiers. It is worth noting that this subject required the smallest number of features to form clusters from the <Emphasis Type="Italic">training</Emphasis> data. This is somewhat counterintuitive—fewer features imply sufficient differences in arm movement patterns to make unique cluster formation easier. While this may be the case, however, the low detection accuracy for <Emphasis Type="Italic">Action B</Emphasis> could be accounted for by poor repeatability by the subject in this particular arm movement.<Table Float="Yes" ID="Tab6">
                <Caption Language="En">
                  <CaptionNumber>Table 4.6</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>Number of features and their ranked order for each subject required to accurately classify the three separate arm movements in the activity “making-a-cup-of-tea”</SimplePara>
                  </CaptionContent>
                </Caption>
                <tgroup align="left" cols="2">
                  <colspec align="left" colname="c1" colnum="1"/>
                  <colspec align="left" colname="c2" colnum="2"/>
                  <thead>
                    <row>
                      <entry colname="c1">
                        <SimplePara>Subject</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>Ranked features</SimplePara>
                      </entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry colname="c1">
                        <SimplePara>Subject 1</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>stddev_y, rms_y, rms_z, stddev_z, rms_x, diff_y, stddev_x, diff_z, max_mag_y, diff_x, max_mag_z</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>Subject 2</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>stddev_y, rms_y</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>Subject 3</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>rms_z, rms_x, stddev_y, stddev_x, rms_y, entropy_z, stddev_z</SimplePara>
                      </entry>
                    </row>
                    <row>
                      <entry colname="c1">
                        <SimplePara>Subject 4</SimplePara>
                      </entry>
                      <entry colname="c2">
                        <SimplePara>stddev_y, rms_y, stddev_x, rms_x, diff_y, max_mag_y, diff_x, max_mag_x, kurtosis_x, kurtosis_z, skewness_z, entropy_y, diff_z, max_mag_z, kurtosis_y, stddev_z, entropy_x, skewness_x, peaks_y, skewness_y, entropy_z, rms_z, peaks_x</SimplePara>
                      </entry>
                    </row>
                  </tbody>
                </tgroup>
              </Table>
</Para>
            <Para ID="Par132">Changes in the detection accuracies for Subject 2 using additional features are shown in Fig. <InternalRef RefID="Fig10">4.10</InternalRef>, which reveals that increasing the number of features beyond 2 does not necessarily improve accuracy. In comparison with the results achieved, the minimum distance classifier based on Euclidean distance appears as the favourable choice of distance metric. This is favourable since computation of the Euclidean distance is far less complex than that of the Mahalanobis distance which involves the maximum likelihood estimation of the covariance matrix.<Figure Category="Standard" Float="Yes" ID="Fig10">
                <Caption Language="En">
                  <CaptionNumber>Fig. 4.10</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>Change in accuracy with number of features selected (for Subject2) using Euclidean distance [<CitationRef CitationID="CR170">170</CitationRef>]</SimplePara>
                  </CaptionContent>
                </Caption>
                <MediaObject ID="MO10">
                  <ImageObject Color="BlackWhite" FileRef="MediaObjects/327370_1_En_4_Fig10_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                </MediaObject>
              </Figure>
</Para>
          </Section2>
          <Section2 ID="Sec33">
            <Heading>Application Framework</Heading>
            <Para ID="Par133">In view of this exploration and the achieved results, we believe that the proposed clustering and minimum distance classifier-based methodology can be implemented in real life for monitoring arm rehabilitation progress in remote health monitoring applications. This methodology could be used to track the number of times a patient performs specific arm movements with their paretic arm throughout the day. Therefore, we highlight the application framework for a patient monitoring system.</Para>
            <Para ID="Par134">It is evident from the results that there is a need for a completely personalized approach of detecting elementary arm movements. Various factors like stage of rehabilitation and whether the affected arm was originally dominant play a significant role in detection since they affect the level of repeatability of individual movements as well as introduce a high degree of temporal variation. For any patient suffering from arm dexterity, the specific movements (or exercises) that need to be tracked as defined by clinicians need to be performed multiple times, following an exercise regime or a gaming session, in a controlled environment (clinic or home). The sensor data collected during this phase can be analysed through cross-validation to determine the best cluster forming features and obtain the centroids of each cluster corresponding to each movement. This helps to perform a clinical profiling of the individual patient with respect to their movement quality. Movements performed in the uncontrolled nomadic environment (which can involve daily activities) can be associated to the proximal cluster centroid using the minimum distance classifier.</Para>
            <Para ID="Par135">It is also important to mention that this methodology can be adaptable to the changing movement patterns of the patients over time reflective of an improvement in their motor functionality depending on the rehabilitation. The patient’s training data can be collected periodically as and when requested by the clinician and the cluster centroids and the associated features can be recomputed to reflect the new movement patterns. This information can be further used by the minimum distance classifier to recognize movements performed in daily life. In StrokeBack, this methodology has been implemented in an offline–online resource sharing mechanism aimed at detection of arm movements in real time. The processing overview is illustrated in Fig. <InternalRef RefID="Fig11">4.11</InternalRef>.<Figure Category="Standard" Float="Yes" ID="Fig11">
                <Caption Language="En">
                  <CaptionNumber>Fig. 4.11</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>Overview of the offline–online processing—the <Emphasis Type="Italic">training</Emphasis> dataset is processed offline and the <Emphasis Type="Italic">testing</Emphasis> dataset is processed online. The computation of the selected features on the <Emphasis Type="Italic">testing</Emphasis> data and computation of the minimum distance from the pre-computed cluster centroids were done in ASIC for real-time detection of arm movements</SimplePara>
                  </CaptionContent>
                </Caption>
                <MediaObject ID="MO11">
                  <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_4_Fig11_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                </MediaObject>
              </Figure>
</Para>
            <Para ID="Par136">The time and memory intensive process of feature computation, selection and cluster formation, on the <Emphasis Type="Italic">training</Emphasis> data were done in an offline mode. The computation of the selected features on the <Emphasis Type="Italic">testing</Emphasis> data and computation of the minimum distance (Euclidean) from the pre-computed cluster centroids in near real time were implemented as a low-power hardware (i.e. application-specific integrated circuit—ASIC). The fabricated ASIC, implementing the minimum distance classifier, is envisaged to be embedded on a sensor node, illustrated in Fig. <InternalRef RefID="Fig12">4.12</InternalRef>, along with a microcontroller and other processing components like A/D converter, memory and de-noising circuit (digital filters), which can be used to recognize arm movements performed in daily life for real-time remote monitoring. This provides an energy-efficient solution for WSN operation over long durations [<CitationRef CitationID="CR142">142</CitationRef>].<Figure Category="Standard" Float="Yes" ID="Fig12">
                <Caption Language="En">
                  <CaptionNumber>Fig. 4.12</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>Overview of an envisaged sensor node with ASIC, microcontroller and memory</SimplePara>
                  </CaptionContent>
                </Caption>
                <MediaObject ID="MO12">
                  <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_4_Fig12_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                </MediaObject>
              </Figure>
</Para>
            <Para ID="Par137">The cluster centroids and <Emphasis Type="Italic">feature-code</Emphasis> (selected features) are uploaded to the on-board memory when the sensor module is plugged into the docking station for charging at the end of a monitoring session. The on-board microcontroller is aimed at controlling the address and the data signals to and from the ASIC, placed within the sensor node. The classification results produced in real time can also be stored within the memory along with the raw sensor data, which can be uploaded to a local host computer and also to the patient health record system (PHR). The ASIC chip takes (9<Emphasis Type="Italic">n</Emphasis> + 30) clock cycles (where <Emphasis Type="Italic">n</Emphasis> is the number input of data samples) in the worst case, considering it has to compute all the 30 features from the <Emphasis Type="Italic">testing</Emphasis> dataset and compute the Euclidean distance to the three cluster centroids (cf. Fig. <InternalRef RefID="Fig6">4.6</InternalRef>). In our implementation, we consider a signal of the <Emphasis Type="Italic">testing</Emphasis> dataset to have 256 data samples, which can be represented on a dyadic scale and therefore we can implement any multiplication or division through a shift operation, which is important for obtaining low-power circuit operation. The design when synthesized on a 20-MHz clock frequency consumes a dynamic power of 25.9 mW and takes 0.12 ms [(9 × 256 + 30) @50 ns] to recognize a performed arm movement. This offline–online processing approach satisfies the application requirements of remote monitoring for arm rehabilitation. The processing (feature extraction, feature selection and clustering) of data collected during exercise (<Emphasis Type="Italic">training</Emphasis> data) needs only be done in an offline mode when requested by the clinician, depending on the rehabilitation progress of the patient. The online detection module can be used to associate the activities performed in nomadic settings (<Emphasis Type="Italic">testing</Emphasis> data) to the pre-computed cluster centroids in real time.</Para>
          </Section2>
          <Section2 ID="Sec34">
            <Heading>Discussion</Heading>
            <Para ID="Par138">In this proof-of-concept methodology, we demonstrate that we can detect all three arm movements performed in a real-world scenario with an accuracy of 61–100 % using between 2 and 23 time-domain features and both the Euclidean distance- and the Mahalanobis distance-based classifiers. Furthermore, our methodology exhibits an overall average accuracy of 88 % across all subjects and arm movement types using the Euclidean distance metric where the number of features used is subject specific, reflecting the variability inherent in human movement. The methodology will be scalable when applied to a larger group of subjects as it looks for the best combination of features to achieve the highest overall accuracy for each individual subject which when applied in the field of remote health monitoring will provide a gross measure of subject-specific rehabilitation. This methodology can help to augment clinical findings and provide a quantitative measure on the rehabilitation progress of patients over time outside of clinical environments.</Para>
            <Para ID="Par139">Since this methodology has been implemented as offline–online processing system, it can be trained to detect any category of movements in an offline mode and the ASIC can be used to detect those movements in real time. This methodology could be extended for use with patients suffering from other neurodegenerative disorders exhibiting less fluidic movement profiles. It can also be extended towards monitoring of lower limb movements. Real-time detection of arm movements can be very useful in a wide array of applications in the field of sports, human–computer interaction, or other treatments of arm dexterity. Therefore, the developed system can be used to track movements of required body segments in these respective fields outside a controlled environment. Hence, through this exploration, we have touched upon majority of the activity recognition steps as illustrated in Fig. <InternalRef RefID="Fig2">4.2</InternalRef> and discussed in Sect. <InternalRef RefID="Sec15">4.3.6</InternalRef>.</Para>
          </Section2>
        </Section1>
      </Body>
      <BodyRef FileRef="978-3-319-21293-7_Chapter_4.pdf" OutputMedium="Online" PDFType="Typeset" TargetType="OnlinePDF"/>
      <ChapterBackmatter>
        <Bibliography ID="Bib1">
          <Heading>References</Heading>
          <Citation ID="CR1">
            <CitationNumber>1.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>S</Initials>
                <FamilyName>Meairs</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>N</Initials>
                <FamilyName>Wahlgren</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>U</Initials>
                <FamilyName>Dirnagl</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2006</Year>
              <ArticleTitle Language="En">Stroke research priorities for the next decade—a representative view of the European scientific community</ArticleTitle>
              <JournalTitle>Cerebrovasc. Dis.</JournalTitle>
              <VolumeID>22</VolumeID>
              <IssueID>2–3</IssueID>
              <FirstPage>75</FirstPage>
              <LastPage>82</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1159/000093098</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>S. Meairs, N. Wahlgren, U. Dirnagl et al., Stroke research priorities for the next decade—a representative view of the European scientific community. Cerebrovasc. Dis. <Emphasis Type="Bold">22</Emphasis>(2–3), 75–82 (2006)</BibUnstructured>
          </Citation>
          <Citation ID="CR2">
            <CitationNumber>2.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>CJ</Initials>
                <FamilyName>Murray</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>AD</Initials>
                <FamilyName>Lopez</FamilyName>
              </BibAuthorName>
              <Year>1997</Year>
              <ArticleTitle Language="En">Alternative projections of mortality and disability by cause 1990–2020: Global burden of disease study</ArticleTitle>
              <JournalTitle>Lancet</JournalTitle>
              <VolumeID>349</VolumeID>
              <IssueID>9064</IssueID>
              <FirstPage>1498</FirstPage>
              <LastPage>1504</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1016/S0140-6736(96)07492-2</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>C.J. Murray, A.D. Lopez, Alternative projections of mortality and disability by cause 1990–2020: Global burden of disease study. Lancet <Emphasis Type="Bold">349</Emphasis>(9064), 1498–1504 (1997)</BibUnstructured>
          </Citation>
          <Citation ID="CR3">
            <CitationNumber>3.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>S</Initials>
                <FamilyName>Patel</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>R</Initials>
                <FamilyName>Hughes</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>T</Initials>
                <FamilyName>Hester</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2010</Year>
              <ArticleTitle Language="En">A novel approach to monitor rehabilitation outcomes in stroke survivors using wearable technology</ArticleTitle>
              <JournalTitle>Proc. IEEE</JournalTitle>
              <VolumeID>98</VolumeID>
              <IssueID>3</IssueID>
              <FirstPage>450</FirstPage>
              <LastPage>461</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/JPROC.2009.2038727</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>S. Patel, R. Hughes, T. Hester et al., A novel approach to monitor rehabilitation outcomes in stroke survivors using wearable technology. Proc. IEEE <Emphasis Type="Bold">98</Emphasis>(3), 450–461 (2010)</BibUnstructured>
          </Citation>
          <Citation ID="CR4">
            <CitationNumber>4.</CitationNumber>
            <BibUnstructured>Effects of stroke, <ExternalRef>
                <RefSource>http://www.strokeassociation.org/STROKEORG/AboutStroke/EffectsofStroke/Effects-of-Stroke_UCM_308534_SubHomePage.jsp</RefSource>
                <RefTarget Address="http://www.strokeassociation.org/STROKEORG/AboutStroke/EffectsofStroke/Effects-of-Stroke_UCM_308534_SubHomePage.jsp" TargetType="URL"/>
              </ExternalRef>. Accessed 1 Jan 2015</BibUnstructured>
          </Citation>
          <Citation ID="CR5">
            <CitationNumber>5.</CitationNumber>
            <BibUnstructured>Brain—effects of a stroke, <ExternalRef>
                <RefSource>http://www.ama-assn.org/ama/pub/physician-resources/patient-education-materials/atlas-of-human-body/brain-effects-stroke.page</RefSource>
                <RefTarget Address="http://www.ama-assn.org/ama/pub/physician-resources/patient-education-materials/atlas-of-human-body/brain-effects-stroke.page" TargetType="URL"/>
              </ExternalRef>. Accessed 1 Jan 2015</BibUnstructured>
          </Citation>
          <Citation ID="CR6">
            <CitationNumber>6.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>M</Initials>
                <FamilyName>Zampolini</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>E</Initials>
                <FamilyName>Todeschini</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>GM</Initials>
                <FamilyName>Bernabeu</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2007</Year>
              <ArticleTitle Language="En">Tele-rehabilitation: present and future</ArticleTitle>
              <JournalTitle>Ann. Ist. Super. Sanita</JournalTitle>
              <VolumeID>44</VolumeID>
              <IssueID>2</IssueID>
              <FirstPage>125</FirstPage>
              <LastPage>134</LastPage>
            </BibArticle>
            <BibUnstructured>M. Zampolini, E. Todeschini, G.M. Bernabeu et al., Tele-rehabilitation: present and future. Ann. Ist. Super. Sanita <Emphasis Type="Bold">44</Emphasis>(2), 125–134 (2007)</BibUnstructured>
          </Citation>
          <Citation ID="CR7">
            <CitationNumber>7.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>F</Initials>
                <FamilyName>Le</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>I</Initials>
                <FamilyName>Markovsky</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>CT</Initials>
                <FamilyName>Freeman</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>E</Initials>
                <FamilyName>Rogers</FamilyName>
              </BibAuthorName>
              <Year>2010</Year>
              <ArticleTitle Language="En">Identification of electrically stimulated muscle models of stroke patients</ArticleTitle>
              <JournalTitle>Control Eng. Pract.</JournalTitle>
              <VolumeID>18</VolumeID>
              <IssueID>4</IssueID>
              <FirstPage>396</FirstPage>
              <LastPage>407</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1016/j.conengprac.2009.12.007</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>F. Le, I. Markovsky, C.T. Freeman, E. Rogers, Identification of electrically stimulated muscle models of stroke patients. Control Eng. Pract. <Emphasis Type="Bold">18</Emphasis>(4), 396–407 (2010)</BibUnstructured>
          </Citation>
          <Citation ID="CR8">
            <CitationNumber>8.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>CT</Initials>
                <FamilyName>Freeman</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>A-M</Initials>
                <FamilyName>Hughes</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>JH</Initials>
                <FamilyName>Burridge</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2009</Year>
              <ArticleTitle Language="En">A model of the upper extremity using FES for stroke rehabilitation</ArticleTitle>
              <JournalTitle>J. Biomech. Eng.</JournalTitle>
              <VolumeID>131</VolumeID>
              <IssueID>3</IssueID>
              <FirstPage>031011</FirstPage>
              <Occurrence Type="DOI">
                <Handle>10.1115/1.3005332</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>C.T. Freeman, A.-M. Hughes, J.H. Burridge et al., A model of the upper extremity using FES for stroke rehabilitation. J. Biomech. Eng. <Emphasis Type="Bold">131</Emphasis>(3), 031011 (2009)</BibUnstructured>
          </Citation>
          <Citation ID="CR9">
            <CitationNumber>9.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>CT</Initials>
                <FamilyName>Freeman</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>E</Initials>
                <FamilyName>Rogers</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>A</Initials>
                <FamilyName>Hughes</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2012</Year>
              <ArticleTitle Language="En">Iterative learning control in health care: Electrical stimulation and robotic-assisted upper-limb stroke rehabilitation</ArticleTitle>
              <JournalTitle>IEEE Control Syst.</JournalTitle>
              <VolumeID>32</VolumeID>
              <IssueID>1</IssueID>
              <FirstPage>18</FirstPage>
              <LastPage>43</LastPage>
              <Occurrence Type="AMSID">
                <Handle>2918610</Handle>
              </Occurrence>
              <Occurrence Type="DOI">
                <Handle>10.1109/MCS.2011.2173261</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>C.T. Freeman, E. Rogers, A. Hughes et al., Iterative learning control in health care: Electrical stimulation and robotic-assisted upper-limb stroke rehabilitation. IEEE Control Syst. <Emphasis Type="Bold">32</Emphasis>(1), 18–43 (2012)</BibUnstructured>
          </Citation>
          <Citation ID="CR10">
            <CitationNumber>10.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>SV</Initials>
                <FamilyName>Adamovich</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>GG</Initials>
                <FamilyName>Fluet</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>A</Initials>
                <FamilyName>Mathai</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2009</Year>
              <ArticleTitle Language="En">Design of a complex virtual reality simulation to train finger motion for persons with hemiparesis: a proof of concept study</ArticleTitle>
              <JournalTitle>J. Neuroeng. Rehabil.</JournalTitle>
              <VolumeID>6</VolumeID>
              <FirstPage>28</FirstPage>
              <Occurrence Type="DOI">
                <Handle>10.1186/1743-0003-6-28</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>S.V. Adamovich, G.G. Fluet, A. Mathai et al., Design of a complex virtual reality simulation to train finger motion for persons with hemiparesis: a proof of concept study. J. Neuroeng. Rehabil. <Emphasis Type="Bold">6</Emphasis>, 28 (2009)</BibUnstructured>
          </Citation>
          <Citation ID="CR11">
            <CitationNumber>11.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>SL</Initials>
                <FamilyName>Wolf</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>CJ</Initials>
                <FamilyName>Winstein</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>JP</Initials>
                <FamilyName>Miller</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2008</Year>
              <ArticleTitle Language="En">Retention of upper limb function in stroke survivors who have received constraint-induced movement therapy: the EXCITE randomised trial</ArticleTitle>
              <JournalTitle>Lancet Neurol.</JournalTitle>
              <VolumeID>7</VolumeID>
              <IssueID>1</IssueID>
              <FirstPage>33</FirstPage>
              <LastPage>40</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1016/S1474-4422(07)70294-6</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>S.L. Wolf, C.J. Winstein, J.P. Miller et al., Retention of upper limb function in stroke survivors who have received constraint-induced movement therapy: the EXCITE randomised trial. Lancet Neurol. <Emphasis Type="Bold">7</Emphasis>(1), 33–40 (2008)</BibUnstructured>
          </Citation>
          <Citation ID="CR12">
            <CitationNumber>12.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>E</Initials>
                <FamilyName>Taub</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>DM</Initials>
                <FamilyName>Morris</FamilyName>
              </BibAuthorName>
              <Year>2001</Year>
              <ArticleTitle Language="En">Constraint-induced movement therapy to enhance recovery after stroke</ArticleTitle>
              <JournalTitle>Curr. Atheroscler. Rep.</JournalTitle>
              <VolumeID>3</VolumeID>
              <IssueID>4</IssueID>
              <FirstPage>279</FirstPage>
              <LastPage>286</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1007/s11883-001-0020-0</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>E. Taub, D.M. Morris, Constraint-induced movement therapy to enhance recovery after stroke. Curr. Atheroscler. Rep. <Emphasis Type="Bold">3</Emphasis>(4), 279–286 (2001)</BibUnstructured>
          </Citation>
          <Citation ID="CR13">
            <CitationNumber>13.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>J</Initials>
                <FamilyName>Birns</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>A</Initials>
                <FamilyName>Bhalla</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>A</Initials>
                <FamilyName>Rudd</FamilyName>
              </BibAuthorName>
              <Year>2010</Year>
              <ArticleTitle Language="En">Telestroke: a concept in practice</ArticleTitle>
              <JournalTitle>Age Ageing</JournalTitle>
              <VolumeID>39</VolumeID>
              <IssueID>6</IssueID>
              <FirstPage>666</FirstPage>
              <LastPage>667</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1093/ageing/afq125</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>J. Birns, A. Bhalla, A. Rudd, Telestroke: a concept in practice. Age Ageing <Emphasis Type="Bold">39</Emphasis>(6), 666–667 (2010)</BibUnstructured>
          </Citation>
          <Citation ID="CR14">
            <CitationNumber>14.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>SR</Initials>
                <FamilyName>Levine</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>M</Initials>
                <FamilyName>Gorman</FamilyName>
              </BibAuthorName>
              <Year>1999</Year>
              <ArticleTitle Language="En">Telestroke: the application of telemedicine for stroke</ArticleTitle>
              <JournalTitle>Stroke</JournalTitle>
              <VolumeID>30</VolumeID>
              <IssueID>2</IssueID>
              <FirstPage>464</FirstPage>
              <LastPage>469</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1161/01.STR.30.2.464</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>S.R. Levine, M. Gorman, Telestroke: the application of telemedicine for stroke. Stroke <Emphasis Type="Bold">30</Emphasis>(2), 464–469 (1999)</BibUnstructured>
          </Citation>
          <Citation ID="CR15">
            <CitationNumber>15.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>KD</Initials>
                <FamilyName>Nguyen</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>I-M</Initials>
                <FamilyName>Chen</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>Z</Initials>
                <FamilyName>Luo</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2011</Year>
              <ArticleTitle Language="En">A wearable sensing system for tracking and monitoring of functional arm movement</ArticleTitle>
              <JournalTitle>IEEE/ASME Trans Mechatron.</JournalTitle>
              <VolumeID>16</VolumeID>
              <IssueID>2</IssueID>
              <FirstPage>213</FirstPage>
              <LastPage>220</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/TMECH.2009.2039222</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>K.D. Nguyen, I.-M. Chen, Z. Luo et al., A wearable sensing system for tracking and monitoring of functional arm movement. IEEE/ASME Trans Mechatron. <Emphasis Type="Bold">16</Emphasis>(2), 213–220 (2011)</BibUnstructured>
          </Citation>
          <Citation ID="CR16">
            <CitationNumber>16.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>N</Initials>
                <FamilyName>Salbach</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>D</Initials>
                <FamilyName>Brooks</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>J</Initials>
                <FamilyName>Romano</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>L</Initials>
                <FamilyName>Woon</FamilyName>
              </BibAuthorName>
              <Year>2013</Year>
              <ArticleTitle Language="En">The relationship between clinical measures and daily physical activity and participation in ambulatory, community-dwelling people with stroke</ArticleTitle>
              <JournalTitle>J. Nov. Physiother.</JournalTitle>
              <VolumeID>3</VolumeID>
              <IssueID>182</IssueID>
              <FirstPage>2</FirstPage>
            </BibArticle>
            <BibUnstructured>N. Salbach, D. Brooks, J. Romano, L. Woon, The relationship between clinical measures and daily physical activity and participation in ambulatory, community-dwelling people with stroke. J. Nov. Physiother. <Emphasis Type="Bold">3</Emphasis>(182), 2 (2013)</BibUnstructured>
          </Citation>
          <Citation ID="CR17">
            <CitationNumber>17.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>S</Initials>
                <FamilyName>Patel</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>H</Initials>
                <FamilyName>Park</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>P</Initials>
                <FamilyName>Bonato</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2012</Year>
              <ArticleTitle Language="En">A review of wearable sensors and systems with application in rehabilitation</ArticleTitle>
              <JournalTitle>J. Neuroeng. Rehabil.</JournalTitle>
              <VolumeID>9</VolumeID>
              <IssueID>1</IssueID>
              <FirstPage>21</FirstPage>
              <LastPage>37</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1186/1743-0003-9-21</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>S. Patel, H. Park, P. Bonato et al., A review of wearable sensors and systems with application in rehabilitation. J. Neuroeng. Rehabil. <Emphasis Type="Bold">9</Emphasis>(1), 21–37 (2012)</BibUnstructured>
          </Citation>
          <Citation ID="CR18">
            <CitationNumber>18.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>SJ</Initials>
                <FamilyName>Strath</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>LA</Initials>
                <FamilyName>Kaminsky</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>BE</Initials>
                <FamilyName>Ainsworth</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2013</Year>
              <ArticleTitle Language="En">Guide to the assessment of physical activity: clinical and research applications—a scientific statement from the American heart association</ArticleTitle>
              <JournalTitle>Circulation</JournalTitle>
              <VolumeID>128</VolumeID>
              <IssueID>20</IssueID>
              <FirstPage>2259</FirstPage>
              <LastPage>2279</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1161/01.cir.0000435708.67487.da</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>S.J. Strath, L.A. Kaminsky, B.E. Ainsworth et al., Guide to the assessment of physical activity: clinical and research applications—a scientific statement from the American heart association. Circulation <Emphasis Type="Bold">128</Emphasis>(20), 2259–2279 (2013)</BibUnstructured>
          </Citation>
          <Citation ID="CR19">
            <CitationNumber>19.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>C</Initials>
                <FamilyName>Do Lee</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>AR</Initials>
                <FamilyName>Folsom</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>SN</Initials>
                <FamilyName>Blair</FamilyName>
              </BibAuthorName>
              <Year>2003</Year>
              <ArticleTitle Language="En">Physical activity and stroke risk a meta-analysis</ArticleTitle>
              <JournalTitle>Stroke</JournalTitle>
              <VolumeID>34</VolumeID>
              <IssueID>10</IssueID>
              <FirstPage>2475</FirstPage>
              <LastPage>2481</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1161/01.STR.0000091843.02517.9D</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>C. Do Lee, A.R. Folsom, S.N. Blair, Physical activity and stroke risk a meta-analysis. Stroke <Emphasis Type="Bold">34</Emphasis>(10), 2475–2481 (2003)</BibUnstructured>
          </Citation>
          <Citation ID="CR20">
            <CitationNumber>20.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>SL</Initials>
                <FamilyName>Wolf</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>PA</Initials>
                <FamilyName>Catlin</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>M</Initials>
                <FamilyName>Ellis</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2001</Year>
              <ArticleTitle Language="En">Assessing Wolf motor function test as outcome measure for research in patients after stroke</ArticleTitle>
              <JournalTitle>Stroke</JournalTitle>
              <VolumeID>32</VolumeID>
              <IssueID>7</IssueID>
              <FirstPage>1635</FirstPage>
              <LastPage>1639</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1161/01.STR.32.7.1635</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>S.L. Wolf, P.A. Catlin, M. Ellis et al., Assessing Wolf motor function test as outcome measure for research in patients after stroke. Stroke <Emphasis Type="Bold">32</Emphasis>(7), 1635–1639 (2001)</BibUnstructured>
          </Citation>
          <Citation ID="CR21">
            <CitationNumber>21.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>DM</Initials>
                <FamilyName>Morris</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>G</Initials>
                <FamilyName>Uswatte</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>JE</Initials>
                <FamilyName>Crago</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2001</Year>
              <ArticleTitle Language="En">The reliability of the wolf motor function test for assessing upper extremity function after stroke</ArticleTitle>
              <JournalTitle>Arch. Phys. Med. Rehabil.</JournalTitle>
              <VolumeID>82</VolumeID>
              <IssueID>6</IssueID>
              <FirstPage>750</FirstPage>
              <LastPage>755</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1053/apmr.2001.23183</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>D.M. Morris, G. Uswatte, J.E. Crago et al., The reliability of the wolf motor function test for assessing upper extremity function after stroke. Arch. Phys. Med. Rehabil. <Emphasis Type="Bold">82</Emphasis>(6), 750–755 (2001)</BibUnstructured>
          </Citation>
          <Citation ID="CR22">
            <CitationNumber>22.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>T</Initials>
                <FamilyName>Platz</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>C</Initials>
                <FamilyName>Pinkowski</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>F</Initials>
                <FamilyName>Wijck</FamilyName>
                <Particle>van</Particle>
              </BibAuthorName>
              <Etal/>
              <Year>2005</Year>
              <ArticleTitle Language="En">Reliability and validity of arm function assessment with standardized guidelines for the Fugl-Meyer test, action research arm test and box and block test: a multicentre study</ArticleTitle>
              <JournalTitle>Clin. Rehabil.</JournalTitle>
              <VolumeID>19</VolumeID>
              <IssueID>4</IssueID>
              <FirstPage>404</FirstPage>
              <LastPage>411</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1191/0269215505cr832oa</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>T. Platz, C. Pinkowski, F. van Wijck et al., Reliability and validity of arm function assessment with standardized guidelines for the Fugl-Meyer test, action research arm test and box and block test: a multicentre study. Clin. Rehabil. <Emphasis Type="Bold">19</Emphasis>(4), 404–411 (2005)</BibUnstructured>
          </Citation>
          <Citation ID="CR23">
            <CitationNumber>23.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>A</Initials>
                <FamilyName>Heller</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>D</Initials>
                <FamilyName>Wade</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>VA</Initials>
                <FamilyName>Wood</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>1987</Year>
              <ArticleTitle Language="En">Arm function after stroke: measurement and recovery over the first three months</ArticleTitle>
              <JournalTitle>J. Neurol. Neurosurg. Psychiatry</JournalTitle>
              <VolumeID>50</VolumeID>
              <IssueID>6</IssueID>
              <FirstPage>714</FirstPage>
              <LastPage>719</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1136/jnnp.50.6.714</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>A. Heller, D. Wade, V.A. Wood et al., Arm function after stroke: measurement and recovery over the first three months. J. Neurol. Neurosurg. Psychiatry <Emphasis Type="Bold">50</Emphasis>(6), 714–719 (1987)</BibUnstructured>
          </Citation>
          <Citation ID="CR24">
            <CitationNumber>24.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>B</Initials>
                <FamilyName>Resnick</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>K</Initials>
                <FamilyName>Michael</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>M</Initials>
                <FamilyName>Shaughnessy</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2008</Year>
              <ArticleTitle Language="En">Inflated perceptions of physical activity after stroke: pairing self-report with physiologic measures</ArticleTitle>
              <JournalTitle>J. Phys. Act. Health</JournalTitle>
              <VolumeID>5</VolumeID>
              <IssueID>2</IssueID>
              <FirstPage>308</FirstPage>
            </BibArticle>
            <BibUnstructured>B. Resnick, K. Michael, M. Shaughnessy et al., Inflated perceptions of physical activity after stroke: pairing self-report with physiologic measures. J. Phys. Act. Health <Emphasis Type="Bold">5</Emphasis>(2), 308 (2008)</BibUnstructured>
          </Citation>
          <Citation ID="CR25">
            <CitationNumber>25.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>D</Initials>
                <FamilyName>Rand</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>JJ</Initials>
                <FamilyName>Eng</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>P-F</Initials>
                <FamilyName>Tang</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2009</Year>
              <ArticleTitle Language="En">How active are people with stroke? Use of accelerometers to assess physical activity</ArticleTitle>
              <JournalTitle>Stroke</JournalTitle>
              <VolumeID>40</VolumeID>
              <IssueID>1</IssueID>
              <FirstPage>163</FirstPage>
              <LastPage>168</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1161/STROKEAHA.108.523621</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>D. Rand, J.J. Eng, P.-F. Tang et al., How active are people with stroke? Use of accelerometers to assess physical activity. Stroke <Emphasis Type="Bold">40</Emphasis>(1), 163–168 (2009)</BibUnstructured>
          </Citation>
          <Citation ID="CR26">
            <CitationNumber>26.</CitationNumber>
            <BibUnstructured>F. Naya, R. Ohmura, F. Takayanagi et al., Workers’ routine activity recognition using body movements and location information, in <Emphasis Type="Italic">Proceedings of the IEEE 10th International Symposium on Wearable Computers</Emphasis>, Montreux, 11–14 Oct 2006</BibUnstructured>
          </Citation>
          <Citation ID="CR27">
            <CitationNumber>27.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>O</Initials>
                <FamilyName>Brand</FamilyName>
              </BibAuthorName>
              <Year>2006</Year>
              <ArticleTitle Language="En">Microsensor integration into systems-on-chip</ArticleTitle>
              <JournalTitle>Proc. IEEE</JournalTitle>
              <VolumeID>94</VolumeID>
              <IssueID>6</IssueID>
              <FirstPage>1160</FirstPage>
              <LastPage>1176</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/JPROC.2006.873618</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>O. Brand, Microsensor integration into systems-on-chip. Proc. IEEE <Emphasis Type="Bold">94</Emphasis>(6), 1160–1176 (2006)</BibUnstructured>
          </Citation>
          <Citation ID="CR28">
            <CitationNumber>28.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>E</Initials>
                <FamilyName>Jovanov</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>A</Initials>
                <FamilyName>Milenkovic</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>C</Initials>
                <FamilyName>Otto</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>PC</Initials>
                <FamilyName>Groen</FamilyName>
                <Particle>de</Particle>
              </BibAuthorName>
              <Year>2005</Year>
              <ArticleTitle Language="En">A wireless body area network of intelligent motion sensors for computer assisted physical rehabilitation</ArticleTitle>
              <JournalTitle>J. Neuroeng. Rehabil.</JournalTitle>
              <VolumeID>2</VolumeID>
              <FirstPage>6</FirstPage>
              <LastPage>15</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1186/1743-0003-2-6</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>E. Jovanov, A. Milenkovic, C. Otto, P.C. de Groen, A wireless body area network of intelligent motion sensors for computer assisted physical rehabilitation. J. Neuroeng. Rehabil. <Emphasis Type="Bold">2</Emphasis>, 6–15 (2005)</BibUnstructured>
          </Citation>
          <Citation ID="CR29">
            <CitationNumber>29.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>L</Initials>
                <FamilyName>Caldani</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>M</Initials>
                <FamilyName>Pacelli</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>D</Initials>
                <FamilyName>Farina</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>R</Initials>
                <FamilyName>Paradiso</FamilyName>
              </BibAuthorName>
              <Year>2010</Year>
              <ArticleTitle Language="En">E-textile platforms for rehabilitation</ArticleTitle>
              <JournalTitle>Conf. Proc. IEEE Eng. Med. Biol. Soc.</JournalTitle>
              <VolumeID>2010</VolumeID>
              <FirstPage>5181</FirstPage>
              <LastPage>5184</LastPage>
              <BibArticleDOI>10.1109/IEMBS.2010.5626148</BibArticleDOI>
            </BibArticle>
            <BibUnstructured>L. Caldani, M. Pacelli, D. Farina, R. Paradiso, E-textile platforms for rehabilitation. Conf. Proc. IEEE Eng. Med. Biol. Soc. <Emphasis Type="Bold">2010</Emphasis>, 5181–5184 (2010). doi:<ExternalRef>
                <RefSource>10.1109/IEMBS.2010.5626148</RefSource>
                <RefTarget Address="10.1109/IEMBS.2010.5626148" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR30">
            <CitationNumber>30.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>HG</Initials>
                <FamilyName>Kang</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>DF</Initials>
                <FamilyName>Mahoney</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>H</Initials>
                <FamilyName>Hoenig</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2010</Year>
              <ArticleTitle Language="En">In situ monitoring of health in older adults: technologies and issues</ArticleTitle>
              <JournalTitle>J. Am. Geriatr. Soc.</JournalTitle>
              <VolumeID>58</VolumeID>
              <IssueID>8</IssueID>
              <FirstPage>1579</FirstPage>
              <LastPage>1586</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1111/j.1532-5415.2010.02959.x</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>H.G. Kang, D.F. Mahoney, H. Hoenig et al., In situ monitoring of health in older adults: technologies and issues. J. Am. Geriatr. Soc. <Emphasis Type="Bold">58</Emphasis>(8), 1579–1586 (2010)</BibUnstructured>
          </Citation>
          <Citation ID="CR31">
            <CitationNumber>31.</CitationNumber>
            <BibUnstructured>L. Fernández, J.M. Blasco, J.F. Hernández, E. Monton, Wireless sensor networks in ambient intelligence, in I Workshop on Technologies for Healthcare &amp; Healthy Lifestyle—WTHS’ 06, Technical University of Valencia, Apr 2006</BibUnstructured>
          </Citation>
          <Citation ID="CR32">
            <CitationNumber>32.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>JRC</Initials>
                <FamilyName>Chien</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>CC</Initials>
                <FamilyName>Tai</FamilyName>
              </BibAuthorName>
              <Year>2005</Year>
              <ArticleTitle Language="En">A new wireless-type physiological signal measuring system using a PDA and Bluetooth technology</ArticleTitle>
              <JournalTitle>Biomed. Eng. Appl. Basis Comm.</JournalTitle>
              <VolumeID>17</VolumeID>
              <IssueID>5</IssueID>
              <FirstPage>229</FirstPage>
              <LastPage>235</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.4015/S1016237205000342</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>J.R.C. Chien, C.C. Tai, A new wireless-type physiological signal measuring system using a PDA and Bluetooth technology. Biomed. Eng. Appl. Basis Comm. <Emphasis Type="Bold">17</Emphasis>(5), 229–235 (2005)</BibUnstructured>
          </Citation>
          <Citation ID="CR33">
            <CitationNumber>33.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>BS</Initials>
                <FamilyName>Lin</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>BS</Initials>
                <FamilyName>Lin</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>NK</Initials>
                <FamilyName>Chou</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2006</Year>
              <ArticleTitle Language="En">RTWPMS: a real-time wireless physiological monitoring system</ArticleTitle>
              <JournalTitle>IEEE Trans. Inf. Technol. Biomed.</JournalTitle>
              <VolumeID>10</VolumeID>
              <IssueID>4</IssueID>
              <FirstPage>647</FirstPage>
              <LastPage>656</LastPage>
              <BibArticleDOI>10.1109/TITB.2006.874194</BibArticleDOI>
              <Occurrence Type="DOI">
                <Handle>10.1109/TITB.2006.874194</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>B.S. Lin, B.S. Lin, N.K. Chou et al., RTWPMS: a real-time wireless physiological monitoring system. IEEE Trans. Inf. Technol. Biomed. <Emphasis Type="Bold">10</Emphasis>(4), 647–656 (2006). doi:<ExternalRef>
                <RefSource>10.1109/TITB.2006.874194</RefSource>
                <RefTarget Address="10.1109/TITB.2006.874194" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR34">
            <CitationNumber>34.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>U</Initials>
                <FamilyName>Anliker</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>JA</Initials>
                <FamilyName>Ward</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>P</Initials>
                <FamilyName>Lukowicz</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2004</Year>
              <ArticleTitle Language="En">AMON: a wearable multiparameter medical monitoring and alert system</ArticleTitle>
              <JournalTitle>IEEE Trans. Inf. Technol. Biomed.</JournalTitle>
              <VolumeID>8</VolumeID>
              <IssueID>4</IssueID>
              <FirstPage>415</FirstPage>
              <LastPage>427</LastPage>
              <BibArticleDOI>10.1109/TITB.2004.837888</BibArticleDOI>
              <Occurrence Type="DOI">
                <Handle>10.1109/TITB.2004.837888</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>U. Anliker, J.A. Ward, P. Lukowicz et al., AMON: a wearable multiparameter medical monitoring and alert system. IEEE Trans. Inf. Technol. Biomed. <Emphasis Type="Bold">8</Emphasis>(4), 415–427 (2004). doi:<ExternalRef>
                <RefSource>10.1109/TITB.2004.837888</RefSource>
                <RefTarget Address="10.1109/TITB.2004.837888" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR35">
            <CitationNumber>35.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>M</Initials>
                <FamilyName>Sung</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>C</Initials>
                <FamilyName>Marci</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>A</Initials>
                <FamilyName>Pentland</FamilyName>
              </BibAuthorName>
              <Year>2005</Year>
              <ArticleTitle Language="En">Wearable feedback system for rehabilitation</ArticleTitle>
              <JournalTitle>J. Neuroeng. Rehabil.</JournalTitle>
              <VolumeID>2</VolumeID>
              <FirstPage>17</FirstPage>
              <BibArticleDOI>10.1186/1743-0003-2-17</BibArticleDOI>
              <Occurrence Type="DOI">
                <Handle>10.1186/1743-0003-2-17</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>M. Sung, C. Marci, A. Pentland, Wearable feedback system for rehabilitation. J. Neuroeng. Rehabil. <Emphasis Type="Bold">2</Emphasis>, 17 (2005). doi:<ExternalRef>
                <RefSource>10.1186/1743-0003-2-17</RefSource>
                <RefTarget Address="10.1186/1743-0003-2-17" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR36">
            <CitationNumber>36.</CitationNumber>
            <BibUnstructured>Ambulatory ECG, <ExternalRef>
                <RefSource>http://www.cardionet.com/</RefSource>
                <RefTarget Address="http://www.cardionet.com/" TargetType="URL"/>
              </ExternalRef>. Accessed 1 Jan 2015</BibUnstructured>
          </Citation>
          <Citation ID="CR37">
            <CitationNumber>37.</CitationNumber>
            <BibUnstructured>WatchDog, <ExternalRef>
                <RefSource>http://www.foster-miller.com/</RefSource>
                <RefTarget Address="http://www.foster-miller.com/" TargetType="URL"/>
              </ExternalRef>. Accessed 1 Feb 2015</BibUnstructured>
          </Citation>
          <Citation ID="CR38">
            <CitationNumber>38.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>KJ</Initials>
                <FamilyName>Heilman</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>SW</Initials>
                <FamilyName>Porges</FamilyName>
              </BibAuthorName>
              <Year>2007</Year>
              <ArticleTitle Language="En">Accuracy of the LifeShirt (Vivometrics) in the detection of cardiac rhythms</ArticleTitle>
              <JournalTitle>Biol. Psychol.</JournalTitle>
              <VolumeID>75</VolumeID>
              <IssueID>3</IssueID>
              <FirstPage>300</FirstPage>
              <LastPage>305</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1016/j.biopsycho.2007.04.001</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>K.J. Heilman, S.W. Porges, Accuracy of the LifeShirt (Vivometrics) in the detection of cardiac rhythms. Biol. Psychol. <Emphasis Type="Bold">75</Emphasis>(3), 300–305 (2007)</BibUnstructured>
          </Citation>
          <Citation ID="CR39">
            <CitationNumber>39.</CitationNumber>
            <BibUnstructured>CleveMed, <ExternalRef>
                <RefSource>http://www.clevemed.com/</RefSource>
                <RefTarget Address="http://www.clevemed.com/" TargetType="URL"/>
              </ExternalRef>. Accessed 1 Jan 2015</BibUnstructured>
          </Citation>
          <Citation ID="CR40">
            <CitationNumber>40.</CitationNumber>
            <BibUnstructured>Micropaq Monitor, <ExternalRef>
                <RefSource>http://www.welchallyn.com/</RefSource>
                <RefTarget Address="http://www.welchallyn.com/" TargetType="URL"/>
              </ExternalRef>. Accessed 1 Feb 2015</BibUnstructured>
          </Citation>
          <Citation ID="CR41">
            <CitationNumber>41.</CitationNumber>
            <BibUnstructured>C.B. Liden, M. Wolowicz, J. Stivoric et al., Characterization and implications of the sensors incorporated into the SenseWear armband for energy expenditure and activity detection, <ExternalRef>
                <RefSource>http://www.bodymedia.com/Professionals/Whitepapers/Characterization-and-Implications-of-the-Sensors-Incorporated-into-the-SenseWear</RefSource>
                <RefTarget Address="http://www.bodymedia.com/Professionals/Whitepapers/Characterization-and-Implications-of-the-Sensors-Incorporated-into-the-SenseWear" TargetType="URL"/>
              </ExternalRef>. Accessed 25 Mar 2015</BibUnstructured>
          </Citation>
          <Citation ID="CR42">
            <CitationNumber>42.</CitationNumber>
            <BibUnstructured>Wristcare, <ExternalRef>
                <RefSource>http://www.istsec.fi/vivago-pam/</RefSource>
                <RefTarget Address="http://www.istsec.fi/vivago-pam/" TargetType="URL"/>
              </ExternalRef>. Accessed 1 Feb 2015</BibUnstructured>
          </Citation>
          <Citation ID="CR43">
            <CitationNumber>43.</CitationNumber>
            <BibUnstructured>R. Fensli, E. Gunnarson, T. Gundersen, A wearable ECG-recording system for continuous arrhythmia monitoring in a wireless tele-home-care situation, in <Emphasis Type="Italic">Proceedings 18th IEEE Symposium On Computer-Based Medical Systems</Emphasis>, Dublin, 2005</BibUnstructured>
          </Citation>
          <Citation ID="CR44">
            <CitationNumber>44.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>RG</Initials>
                <FamilyName>Lee</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>YC</Initials>
                <FamilyName>Chen</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>CC</Initials>
                <FamilyName>Hsiao</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>CL</Initials>
                <FamilyName>Tseng</FamilyName>
              </BibAuthorName>
              <Year>2007</Year>
              <ArticleTitle Language="En">A mobile care system with alert mechanism</ArticleTitle>
              <JournalTitle>IEEE Trans. Inf. Technol. Biomed.</JournalTitle>
              <VolumeID>11</VolumeID>
              <IssueID>5</IssueID>
              <FirstPage>507</FirstPage>
              <LastPage>517</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/TITB.2006.888701</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>R.G. Lee, Y.C. Chen, C.C. Hsiao, C.L. Tseng, A mobile care system with alert mechanism. IEEE Trans. Inf. Technol. Biomed. <Emphasis Type="Bold">11</Emphasis>(5), 507–517 (2007)</BibUnstructured>
          </Citation>
          <Citation ID="CR45">
            <CitationNumber>45.</CitationNumber>
            <BibUnstructured>P. Leijdekkers, V. Gay, A self-test to detect a heart attack using a mobile phone and wearable sensors, in <Emphasis Type="Italic">Proceedings of the 21st IEEE International Symposium on Computer-Based Medical Systems</Emphasis>, Jyväskylä, 17–19 June 2008. doi:<ExternalRef>
                <RefSource>10.1109/CBMS.2008.59</RefSource>
                <RefTarget Address="10.1109/CBMS.2008.59" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR46">
            <CitationNumber>46.</CitationNumber>
            <BibUnstructured>Z. Jin, J. Oresko, S. Huang, A.C. Cheng, HeartToGo: a personalized medicine technology for cardiovascular disease prevention and detection, in <Emphasis Type="Italic">Proceedings of Life Science Systems and Applications Workshop</Emphasis>, Bethesda, 9–10 Apr 2009. doi:<ExternalRef>
                <RefSource>10.1109/LISSA.2009.4906714</RefSource>
                <RefTarget Address="10.1109/LISSA.2009.4906714" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR47">
            <CitationNumber>47.</CitationNumber>
            <BibUnstructured>N. Oliver, F. Flores-Mangas, HealthGear: a real-time wearable system for monitoring and analysing physiological signals, in <Emphasis Type="Italic">Proceedings of International Workshop on Wearable and Implantable Body Sensor Networks</Emphasis>, MIT, Massachusetts, 3–5 Apr 2006. doi:<ExternalRef>
                <RefSource>10.1109/BSN.2006.27</RefSource>
                <RefTarget Address="10.1109/BSN.2006.27" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR48">
            <CitationNumber>48.</CitationNumber>
            <BibUnstructured>V. Shnayder, B. Chen, K. Lorincz et al., Sensor networks for medical care, in <Emphasis Type="Italic">Proceedings of the 3rd International Conference on Embedded Networked Sensor Systems</Emphasis>, San Diego, 2–4 Nov 2005. doi:<ExternalRef>
                <RefSource>10.1145/1098918.1098979</RefSource>
                <RefTarget Address="10.1145/1098918.1098979" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR49">
            <CitationNumber>49.</CitationNumber>
            <BibUnstructured>J. Habetha, The MyHeart project-fighting cardiovascular diseases by prevention and early diagnosis, in <Emphasis Type="Italic">Proceedings of the 28th IEEE EMBS Annual International Conference</Emphasis>, New York, 30 Aug–3 Sep 2006. doi:<ExternalRef>
                <RefSource>10.1109/EMBS.2006.260937</RefSource>
                <RefTarget Address="10.1109/EMBS.2006.260937" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR50">
            <CitationNumber>50.</CitationNumber>
            <BibUnstructured>M. Pacelli, G. Loriga, N. Taccini, R. Paradiso, Sensing fabrics for monitoring physiological and biomechanical variables: e-textile solutions, in <Emphasis Type="Italic">Proceedings of the 3rd IEEE-EMBS International Summer School and Symposium on Medical Devices and Biosensors</Emphasis>, MIT, Massachusetts, 4–6 Sep 2006. doi:<ExternalRef>
                <RefSource>10.1109/ISSMDBS.2006.360082</RefSource>
                <RefTarget Address="10.1109/ISSMDBS.2006.360082" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR51">
            <CitationNumber>51.</CitationNumber>
            <BibUnstructured>M. Di Rienzo, F. Rizzo, G. Brambilla et al., MagIC system: a new textile-based wearable device for biological signal monitoring. Applicability in daily life and clinical setting, in <Emphasis Type="Italic">Proceedings of the IEEE-EMBS 27th Annual International Conference</Emphasis>, PRC, Shanghai, 1–4 Sep 2005. doi:<ExternalRef>
                <RefSource>10.1109/EMBS.2005.1616161</RefSource>
                <RefTarget Address="10.1109/EMBS.2005.1616161" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR52">
            <CitationNumber>52.</CitationNumber>
            <BibUnstructured>P. Lukowicz, U. Anliker, J. Ward et al., Amon: a wearable medical computer for high risk patients, in <Emphasis Type="Italic">Proceedings of the 6th International Symposium on Wearable Computers</Emphasis>, Seattle, 7–10 Oct 2002. doi:<ExternalRef>
                <RefSource>10.1109/ISWC.2002.1167230</RefSource>
                <RefTarget Address="10.1109/ISWC.2002.1167230" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR53">
            <CitationNumber>53.</CitationNumber>
            <BibUnstructured>W.Y. Chung, S.C. Lee, S.H. Toh, WSN based mobile u-healthcare system with ECG, blood pressure measurement function, in <Emphasis Type="Italic">Proceedings of the IEEE-EMBS 30th Annual International Conference</Emphasis>, Vancouver, 20–25 Aug 2008. doi:<ExternalRef>
                <RefSource>10.1109/IEMBS.2008.4649461</RefSource>
                <RefTarget Address="10.1109/IEMBS.2008.4649461" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR54">
            <CitationNumber>54.</CitationNumber>
            <BibUnstructured>A. Volmer, R. Orglmeister, Wireless body sensor network for low-power motion-tolerant synchronized vital sign measurement, in <Emphasis Type="Italic">Proceedings of the IEEE-EMBS 30th Annual International Conference</Emphasis>, Vancouver, 20–25 Aug 2008. doi:<ExternalRef>
                <RefSource>10.1109/IEMBS.2008.4649941</RefSource>
                <RefTarget Address="10.1109/IEMBS.2008.4649941" TargetType="DOI"/>
              </ExternalRef>.</BibUnstructured>
          </Citation>
          <Citation ID="CR55">
            <CitationNumber>55.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>S</Initials>
                <FamilyName>Farshchi</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>A</Initials>
                <FamilyName>Pesterev</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>PH</Initials>
                <FamilyName>Nuyujukian</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2007</Year>
              <ArticleTitle Language="En">Bi-Fi: an embedded sensor/system architecture for remote biological monitoring</ArticleTitle>
              <JournalTitle>IEEE Trans. Inf. Technol. Biomed.</JournalTitle>
              <VolumeID>11</VolumeID>
              <IssueID>6</IssueID>
              <FirstPage>611</FirstPage>
              <LastPage>618</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/TITB.2007.897600</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>S. Farshchi, A. Pesterev, P.H. Nuyujukian et al., Bi-Fi: an embedded sensor/system architecture for remote biological monitoring. IEEE Trans. Inf. Technol. Biomed. <Emphasis Type="Bold">11</Emphasis>(6), 611–618 (2007)</BibUnstructured>
          </Citation>
          <Citation ID="CR56">
            <CitationNumber>56.</CitationNumber>
            <BibUnstructured>N. Loew, K.J. Winzer, G. Becher et al., Medical sensors of the BASUMA body sensor network, in <Emphasis Type="Italic">Proceedings of the 4th International Workshop on Wearable and Implantable Body Sensor Networks</Emphasis>, Aachen University, 26–28 March 2007. doi:<ExternalRef>
                <RefSource>10.1007/978-3-540-70994-7_30</RefSource>
                <RefTarget Address="10.1007/978-3-540-70994-7_30" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR57">
            <CitationNumber>57.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>B</Initials>
                <FamilyName>Gyselinckx</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>J</Initials>
                <FamilyName>Penders</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>R</Initials>
                <FamilyName>Vullers</FamilyName>
              </BibAuthorName>
              <Year>2007</Year>
              <ArticleTitle Language="En">Potential and challenges of body area networks for cardiac monitoring</ArticleTitle>
              <JournalTitle>J Electrocard</JournalTitle>
              <VolumeID>40</VolumeID>
              <IssueID>6</IssueID>
              <FirstPage>S165</FirstPage>
              <LastPage>S168</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1016/j.jelectrocard.2007.06.016</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>B. Gyselinckx, J. Penders, R. Vullers, Potential and challenges of body area networks for cardiac monitoring. J Electrocard <Emphasis Type="Bold">40</Emphasis>(6), S165–S168 (2007)</BibUnstructured>
          </Citation>
          <Citation ID="CR58">
            <CitationNumber>58.</CitationNumber>
            <BibUnstructured>T. Torfs, V. Leonov, C. Van Hoof, B Gyselinckx, Body-heat powered autonomous pulse oximeter, in <Emphasis Type="Italic">Proceedings of the 5th IEEE Conference on Sensors</Emphasis>, Daegu, 22–25 Oct 2006. doi:<ExternalRef>
                <RefSource>10.1109/ICSENS.2007.355497</RefSource>
                <RefTarget Address="10.1109/ICSENS.2007.355497" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR59">
            <CitationNumber>59.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>HH</Initials>
                <FamilyName>Asada</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>P</Initials>
                <FamilyName>Shaltis</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>A</Initials>
                <FamilyName>Reisner</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2003</Year>
              <ArticleTitle Language="En">Mobile monitoring with wearable photoplethysmographic biosensors</ArticleTitle>
              <JournalTitle>IEEE Eng. Med. Biol. Mag.</JournalTitle>
              <VolumeID>22</VolumeID>
              <IssueID>3</IssueID>
              <FirstPage>28</FirstPage>
              <LastPage>40</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/MEMB.2003.1213624</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>H.H. Asada, P. Shaltis, A. Reisner et al., Mobile monitoring with wearable photoplethysmographic biosensors. IEEE Eng. Med. Biol. Mag. <Emphasis Type="Bold">22</Emphasis>(3), 28–40 (2003)</BibUnstructured>
          </Citation>
          <Citation ID="CR60">
            <CitationNumber>60.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>P</Initials>
                <FamilyName>Corbishley</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>E</Initials>
                <FamilyName>Rodriguez-Villegas</FamilyName>
              </BibAuthorName>
              <Year>2008</Year>
              <ArticleTitle Language="En">Breathing detection: towards a miniaturized, wearable, battery-operated monitoring system</ArticleTitle>
              <JournalTitle>IEEE Trans. Inf. Technol. Biomed.</JournalTitle>
              <VolumeID>55</VolumeID>
              <IssueID>1</IssueID>
              <FirstPage>196</FirstPage>
              <LastPage>204</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/TBME.2007.910679</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>P. Corbishley, E. Rodriguez-Villegas, Breathing detection: towards a miniaturized, wearable, battery-operated monitoring system. IEEE Trans. Inf. Technol. Biomed. <Emphasis Type="Bold">55</Emphasis>(1), 196–204 (2008)</BibUnstructured>
          </Citation>
          <Citation ID="CR61">
            <CitationNumber>61.</CitationNumber>
            <BibUnstructured>J.A.C. Patterson, D.G. McIlwraith, G.Z. Yang, A flexible, low noise reflective PPG sensor platform for ear-worn heart rate monitoring, in <Emphasis Type="Italic">Proceedings of the 6th International Workshop on Wearable and Implantable Body Sensor Networks</Emphasis>, Berkeley, 3–5 June 2009. doi:<ExternalRef>
                <RefSource>10.1109/BSN.2009.16</RefSource>
                <RefTarget Address="10.1109/BSN.2009.16" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR62">
            <CitationNumber>62.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>R</Initials>
                <FamilyName>Dudde</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>T</Initials>
                <FamilyName>Vering</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>G</Initials>
                <FamilyName>Piechotta</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>R</Initials>
                <FamilyName>Hintsche</FamilyName>
              </BibAuthorName>
              <Year>2006</Year>
              <ArticleTitle Language="En">Computer-aided continuous drug infusion: setup and test of a mobile closed-loop system for the continuous automated infusion of insulin</ArticleTitle>
              <JournalTitle>IEEE Trans. Inf. Technol. Biomed.</JournalTitle>
              <VolumeID>10</VolumeID>
              <IssueID>2</IssueID>
              <FirstPage>395</FirstPage>
              <LastPage>402</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/TITB.2006.864477</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>R. Dudde, T. Vering, G. Piechotta, R. Hintsche, Computer-aided continuous drug infusion: setup and test of a mobile closed-loop system for the continuous automated infusion of insulin. IEEE Trans. Inf. Technol. Biomed. <Emphasis Type="Bold">10</Emphasis>(2), 395–402 (2006)</BibUnstructured>
          </Citation>
          <Citation ID="CR63">
            <CitationNumber>63.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>CD</Initials>
                <FamilyName>Katsis</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>G</Initials>
                <FamilyName>Ganiatsas</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>DI</Initials>
                <FamilyName>Fotiadis</FamilyName>
              </BibAuthorName>
              <Year>2006</Year>
              <ArticleTitle Language="En">An integrated telemedicine platform for the assessment of affective physiological states</ArticleTitle>
              <JournalTitle>Diagn. Pathol.</JournalTitle>
              <VolumeID>1</VolumeID>
              <IssueID>1</IssueID>
              <FirstPage>16</FirstPage>
              <LastPage>24</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1186/1746-1596-1-16</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>C.D. Katsis, G. Ganiatsas, D.I. Fotiadis, An integrated telemedicine platform for the assessment of affective physiological states. Diagn. Pathol. <Emphasis Type="Bold">1</Emphasis>(1), 16–24 (2006)</BibUnstructured>
          </Citation>
          <Citation ID="CR64">
            <CitationNumber>64.</CitationNumber>
            <BibUnstructured>Sensatex smart T-shirt, <ExternalRef>
                <RefSource>http://www.sensatex.com/</RefSource>
                <RefTarget Address="http://www.sensatex.com/" TargetType="URL"/>
              </ExternalRef>. Accessed 1 Jan 2015</BibUnstructured>
          </Citation>
          <Citation ID="CR65">
            <CitationNumber>65.</CitationNumber>
            <BibUnstructured>Philips, <ExternalRef>
                <RefSource>http://www.healthcare.philips.com</RefSource>
                <RefTarget Address="http://www.healthcare.philips.com" TargetType="URL"/>
              </ExternalRef>. Accessed 1 Jan 2015</BibUnstructured>
          </Citation>
          <Citation ID="CR66">
            <CitationNumber>66.</CitationNumber>
            <BibUnstructured>Nellcor, <ExternalRef>
                <RefSource>http://www.nellcor.com/</RefSource>
                <RefTarget Address="http://www.nellcor.com/" TargetType="URL"/>
              </ExternalRef>. Accessed 1 Jan 2015</BibUnstructured>
          </Citation>
          <Citation ID="CR67">
            <CitationNumber>67.</CitationNumber>
            <BibUnstructured>Agilent Technologies, <ExternalRef>
                <RefSource>http://www.agilent.com/</RefSource>
                <RefTarget Address="http://www.agilent.com/" TargetType="URL"/>
              </ExternalRef>. Accessed 1 Jan 2015</BibUnstructured>
          </Citation>
          <Citation ID="CR68">
            <CitationNumber>68.</CitationNumber>
            <BibUnstructured>Nonin, <ExternalRef>
                <RefSource>http://www.nonin.com/</RefSource>
                <RefTarget Address="http://www.nonin.com/" TargetType="URL"/>
              </ExternalRef>. Accessed 1 Jan 2015</BibUnstructured>
          </Citation>
          <Citation ID="CR69">
            <CitationNumber>69.</CitationNumber>
            <BibUnstructured>Polar, <ExternalRef>
                <RefSource>http://www.polarusa.com/</RefSource>
                <RefTarget Address="http://www.polarusa.com/" TargetType="URL"/>
              </ExternalRef>. Accessed 1 Jan 2015</BibUnstructured>
          </Citation>
          <Citation ID="CR70">
            <CitationNumber>70.</CitationNumber>
            <BibUnstructured>Omron, <ExternalRef>
                <RefSource>http://www.omron.com/</RefSource>
                <RefTarget Address="http://www.omron.com/" TargetType="URL"/>
              </ExternalRef>. Accessed 1 Jan 2015</BibUnstructured>
          </Citation>
          <Citation ID="CR71">
            <CitationNumber>71.</CitationNumber>
            <BibUnstructured>Life Alert Classic, <ExternalRef>
                <RefSource>http://www.lifealertmedical.com/classic.html</RefSource>
                <RefTarget Address="http://www.lifealertmedical.com/classic.html" TargetType="URL"/>
              </ExternalRef>. Accessed 1 Jan 2015</BibUnstructured>
          </Citation>
          <Citation ID="CR72">
            <CitationNumber>72.</CitationNumber>
            <BibUnstructured>AlertOne medical alert system, <ExternalRef>
                <RefSource>http://www.alert-1.com/</RefSource>
                <RefTarget Address="http://www.alert-1.com/" TargetType="URL"/>
              </ExternalRef>. Accessed 1 Jan 2015</BibUnstructured>
          </Citation>
          <Citation ID="CR73">
            <CitationNumber>73.</CitationNumber>
            <BibUnstructured>Automatic fall detection, <ExternalRef>
                <RefSource>http://www.wellcore.com/learn/automatic-falldetection</RefSource>
                <RefTarget Address="http://www.wellcore.com/learn/automatic-falldetection" TargetType="URL"/>
              </ExternalRef>. Accessed 1 Jan 2015</BibUnstructured>
          </Citation>
          <Citation ID="CR74">
            <CitationNumber>74.</CitationNumber>
            <BibUnstructured>MyHalo, <ExternalRef>
                <RefSource>http://www.halomonitoring.com</RefSource>
                <RefTarget Address="http://www.halomonitoring.com" TargetType="URL"/>
              </ExternalRef>. Accessed 1 Jan 2015</BibUnstructured>
          </Citation>
          <Citation ID="CR75">
            <CitationNumber>75.</CitationNumber>
            <BibUnstructured>BrickHouse, <ExternalRef>
                <RefSource>http://www.brickhousealert.com/personal-emergency-medicalalarm.html</RefSource>
                <RefTarget Address="http://www.brickhousealert.com/personal-emergency-medicalalarm.html" TargetType="URL"/>
              </ExternalRef>. Accessed 1 Jan 2015</BibUnstructured>
          </Citation>
          <Citation ID="CR76">
            <CitationNumber>76.</CitationNumber>
            <BibUnstructured>Centre Suisse d’Electronique et de Microtechnique, <ExternalRef>
                <RefSource>http://www.csem.ch/docs/Show.aspx?id = 6026</RefSource>
                <RefTarget Address="http://www.csem.ch/docs/Show.aspx?id%20=%206026" TargetType="URL"/>
              </ExternalRef>. Accessed 1 Jan 2015</BibUnstructured>
          </Citation>
          <Citation ID="CR77">
            <CitationNumber>77.</CitationNumber>
            <BibUnstructured>F. Bianchi, S.J. Redmond, M.R. Narayanan et al., IEEE Trans. Neural Syst. Rehabil. Eng. <Emphasis Type="Bold">18</Emphasis>(6), 619–627 (2010)</BibUnstructured>
          </Citation>
          <Citation ID="CR78">
            <CitationNumber>78.</CitationNumber>
            <BibUnstructured>M. Lan, A. Nahapetian, A. Vahdatpour et al., SmartFall: an automatic fall detection system based on subsequence matching for the Smart Cane, in <Emphasis Type="Italic">Proceedings of the 4th International Conference on Body Area Networks</Emphasis>, UCLA, Los Angeles, 1–3 Apr 2009. doi:<ExternalRef>
                <RefSource>10.4108/ICST.BODYNETS2009.5873</RefSource>
                <RefTarget Address="10.4108/ICST.BODYNETS2009.5873" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR79">
            <CitationNumber>79.</CitationNumber>
            <BibUnstructured>F. Sposaro, G. Tyson, iFall: an Android application for fall monitoring and response, in <Emphasis Type="Italic">Proceedings of the IEEE-EMBS Annual International Conference</Emphasis>, Minneapolis, 2–6 Sep 2009. doi:<ExternalRef>
                <RefSource>10.1109/IEMBS.2009.5334912</RefSource>
                <RefTarget Address="10.1109/IEMBS.2009.5334912" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR80">
            <CitationNumber>80.</CitationNumber>
            <BibUnstructured>G. Yavuz, M. Kocak, G. Ergun et al., A smartphone based fall detector with online location support, in <Emphasis Type="Italic">Proceedings of the International Workshop on Sensing for App Phones</Emphasis>, Zurich, 2010</BibUnstructured>
          </Citation>
          <Citation ID="CR81">
            <CitationNumber>81.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>T</Initials>
                <FamilyName>Tamura</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>T</Initials>
                <FamilyName>Yoshimura</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>M</Initials>
                <FamilyName>Sekine</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2009</Year>
              <ArticleTitle Language="En">A wearable airbag to prevent fall injuries</ArticleTitle>
              <JournalTitle>IEEE Trans. Inf. Technol. Biomed.</JournalTitle>
              <VolumeID>13</VolumeID>
              <IssueID>6</IssueID>
              <FirstPage>910</FirstPage>
              <LastPage>914</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/TITB.2009.2033673</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>T. Tamura, T. Yoshimura, M. Sekine et al., A wearable airbag to prevent fall injuries. IEEE Trans. Inf. Technol. Biomed. <Emphasis Type="Bold">13</Emphasis>(6), 910–914 (2009)</BibUnstructured>
          </Citation>
          <Citation ID="CR82">
            <CitationNumber>82.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>M</Initials>
                <FamilyName>Bachlin</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>M</Initials>
                <FamilyName>Plotnik</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>D</Initials>
                <FamilyName>Roggen</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2010</Year>
              <ArticleTitle Language="En">Wearable assistant for Parkinson’s disease patients with the freezing of gait symptom</ArticleTitle>
              <JournalTitle>IEEE Trans. Inf. Technol. Biomed.</JournalTitle>
              <VolumeID>14</VolumeID>
              <IssueID>2</IssueID>
              <FirstPage>436</FirstPage>
              <LastPage>446</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/TITB.2009.2036165</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>M. Bachlin, M. Plotnik, D. Roggen et al., Wearable assistant for Parkinson’s disease patients with the freezing of gait symptom. IEEE Trans. Inf. Technol. Biomed. <Emphasis Type="Bold">14</Emphasis>(2), 436–446 (2010)</BibUnstructured>
          </Citation>
          <Citation ID="CR83">
            <CitationNumber>83.</CitationNumber>
            <BibUnstructured>F. Sposaro, J. Danielson, G. Tyson, iWander: an Android application for dementia patients, in <Emphasis Type="Italic">Proceedings of the IEEE-EMBS Annual International Conference</Emphasis>, Buenos Aires, 31 Aug–4 Sep 2010. doi:<ExternalRef>
                <RefSource>10.1109/IEMBS.2010.5627669</RefSource>
                <RefTarget Address="10.1109/IEMBS.2010.5627669" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR84">
            <CitationNumber>84.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>H</Initials>
                <FamilyName>Feys</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>W</Initials>
                <FamilyName>Weerdt</FamilyName>
                <Particle>De</Particle>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>G</Initials>
                <FamilyName>Verbeke</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2004</Year>
              <ArticleTitle Language="En">Early and repetitive stimulation of the arm can substantially improve the long-term outcome after stroke: a 5-year follow-up study of a randomized trial</ArticleTitle>
              <JournalTitle>Stroke</JournalTitle>
              <VolumeID>35</VolumeID>
              <IssueID>4</IssueID>
              <FirstPage>924</FirstPage>
              <LastPage>929</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1161/01.STR.0000121645.44752.f7</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>H. Feys, W. De Weerdt, G. Verbeke et al., Early and repetitive stimulation of the arm can substantially improve the long-term outcome after stroke: a 5-year follow-up study of a randomized trial. Stroke <Emphasis Type="Bold">35</Emphasis>(4), 924–929 (2004)</BibUnstructured>
          </Citation>
          <Citation ID="CR85">
            <CitationNumber>85.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>L</Initials>
                <FamilyName>Legg</FamilyName>
              </BibAuthorName>
              <Year>2004</Year>
              <ArticleTitle Language="En">Rehabilitation therapy services for stroke patients living at home: systematic review of randomised trials</ArticleTitle>
              <JournalTitle>Lancet</JournalTitle>
              <VolumeID>363</VolumeID>
              <IssueID>9406</IssueID>
              <FirstPage>352</FirstPage>
              <LastPage>356</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1016/S0140-6736(04)15434-2</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>L. Legg, Rehabilitation therapy services for stroke patients living at home: systematic review of randomised trials. Lancet <Emphasis Type="Bold">363</Emphasis>(9406), 352–356 (2004)</BibUnstructured>
          </Citation>
          <Citation ID="CR86">
            <CitationNumber>86.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>B</Initials>
                <FamilyName>Langhammer</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>JK</Initials>
                <FamilyName>Stanghelle</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>B</Initials>
                <FamilyName>Lindmark</FamilyName>
              </BibAuthorName>
              <Year>2008</Year>
              <ArticleTitle Language="En">Exercise and health-related quality of life during the first year following acute stroke. A randomized controlled trial</ArticleTitle>
              <JournalTitle>Brain Inj.</JournalTitle>
              <VolumeID>22</VolumeID>
              <IssueID>2</IssueID>
              <FirstPage>135</FirstPage>
              <LastPage>145</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1080/02699050801895423</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>B. Langhammer, J.K. Stanghelle, B. Lindmark, Exercise and health-related quality of life during the first year following acute stroke. A randomized controlled trial. Brain Inj. <Emphasis Type="Bold">22</Emphasis>(2), 135–145 (2008)</BibUnstructured>
          </Citation>
          <Citation ID="CR87">
            <CitationNumber>87.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>MC</Initials>
                <FamilyName>Cramp</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>RJ</Initials>
                <FamilyName>Greenwood</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>M</Initials>
                <FamilyName>Gill</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2010</Year>
              <ArticleTitle Language="En">Effectiveness of a community-based low intensity exercise programme for ambulatory stroke survivors</ArticleTitle>
              <JournalTitle>Disabil. Rehabil.</JournalTitle>
              <VolumeID>32</VolumeID>
              <IssueID>3</IssueID>
              <FirstPage>239</FirstPage>
              <LastPage>247</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.3109/09638280903095916</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>M.C. Cramp, R.J. Greenwood, M. Gill et al., Effectiveness of a community-based low intensity exercise programme for ambulatory stroke survivors. Disabil. Rehabil. <Emphasis Type="Bold">32</Emphasis>(3), 239–247 (2010)</BibUnstructured>
          </Citation>
          <Citation ID="CR88">
            <CitationNumber>88.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>H</Initials>
                <FamilyName>Hermens</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>B</Initials>
                <FamilyName>Huijgen</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>C</Initials>
                <FamilyName>Giacomozzi</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2007</Year>
              <ArticleTitle Language="En">Clinical assessment of the HELLODOC tele-rehabilitation service</ArticleTitle>
              <JournalTitle>Ann. Ist. Super. Sanita</JournalTitle>
              <VolumeID>44</VolumeID>
              <IssueID>2</IssueID>
              <FirstPage>154</FirstPage>
              <LastPage>163</LastPage>
            </BibArticle>
            <BibUnstructured>H. Hermens, B. Huijgen, C. Giacomozzi et al., Clinical assessment of the HELLODOC tele-rehabilitation service. Ann. Ist. Super. Sanita <Emphasis Type="Bold">44</Emphasis>(2), 154–163 (2007)</BibUnstructured>
          </Citation>
          <Citation ID="CR89">
            <CitationNumber>89.</CitationNumber>
            <BibUnstructured>A. Timmermans, P. Saini, R. Willmann et al., Home stroke rehabilitation for the upper limbs, in <Emphasis Type="Italic">Proceedings of the IEEE-EMBS 29th Annual International Conference</Emphasis>, Lyon, 22–26 Aug 2007. doi:<ExternalRef>
                <RefSource>10.1109/IEMBS.2007.4353214</RefSource>
                <RefTarget Address="10.1109/IEMBS.2007.4353214" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR90">
            <CitationNumber>90.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>C</Initials>
                <FamilyName>Mavroidis</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>J</Initials>
                <FamilyName>Nikitczuk</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>B</Initials>
                <FamilyName>Weinberg</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2005</Year>
              <ArticleTitle Language="En">Smart portable rehabilitation devices</ArticleTitle>
              <JournalTitle>J. Neuroeng. Rehabil.</JournalTitle>
              <VolumeID>2</VolumeID>
              <FirstPage>18</FirstPage>
              <BibArticleDOI>10.1186/1743-0003-2-18</BibArticleDOI>
              <Occurrence Type="DOI">
                <Handle>10.1186/1743-0003-2-18</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>C. Mavroidis, J. Nikitczuk, B. Weinberg et al., Smart portable rehabilitation devices. J. Neuroeng. Rehabil. <Emphasis Type="Bold">2</Emphasis>, 18 (2005). doi:<ExternalRef>
                <RefSource>10.1186/1743-0003-2-18</RefSource>
                <RefTarget Address="10.1186/1743-0003-2-18" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR91">
            <CitationNumber>91.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>MK</Initials>
                <FamilyName>Holden</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>TA</Initials>
                <FamilyName>Dyar</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>L</Initials>
                <FamilyName>Dayan-Cimadoro</FamilyName>
              </BibAuthorName>
              <Year>2007</Year>
              <ArticleTitle Language="En">Telerehabilitation using a virtual environment improves upper extremity function in patients with stroke</ArticleTitle>
              <JournalTitle>IEEE Trans. Neural Syst. Rehabil. Eng.</JournalTitle>
              <VolumeID>15</VolumeID>
              <IssueID>1</IssueID>
              <FirstPage>36</FirstPage>
              <LastPage>42</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/TNSRE.2007.891388</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>M.K. Holden, T.A. Dyar, L. Dayan-Cimadoro, Telerehabilitation using a virtual environment improves upper extremity function in patients with stroke. IEEE Trans. Neural Syst. Rehabil. Eng. <Emphasis Type="Bold">15</Emphasis>(1), 36–42 (2007)</BibUnstructured>
          </Citation>
          <Citation ID="CR92">
            <CitationNumber>92.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>D</Initials>
                <FamilyName>Jack</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>R</Initials>
                <FamilyName>Boian</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>AS</Initials>
                <FamilyName>Merians</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2001</Year>
              <ArticleTitle Language="En">Virtual reality-enhanced stroke rehabilitation</ArticleTitle>
              <JournalTitle>IEEE Trans. Neural Syst. Rehabil. Eng.</JournalTitle>
              <VolumeID>9</VolumeID>
              <IssueID>3</IssueID>
              <FirstPage>308</FirstPage>
              <LastPage>318</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/7333.948460</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>D. Jack, R. Boian, A.S. Merians et al., Virtual reality-enhanced stroke rehabilitation. IEEE Trans. Neural Syst. Rehabil. Eng. <Emphasis Type="Bold">9</Emphasis>(3), 308–318 (2001)</BibUnstructured>
          </Citation>
          <Citation ID="CR93">
            <CitationNumber>93.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>D</Initials>
                <FamilyName>Rand</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>R</Initials>
                <FamilyName>Kizony</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>PTL</Initials>
                <FamilyName>Weiss</FamilyName>
              </BibAuthorName>
              <Year>2008</Year>
              <ArticleTitle Language="En">The Sony PlayStation II EyeToy: low-cost virtual reality for use in rehabilitation</ArticleTitle>
              <JournalTitle>J. Neurol. Phys. Ther.</JournalTitle>
              <VolumeID>32</VolumeID>
              <IssueID>4</IssueID>
              <FirstPage>155</FirstPage>
              <LastPage>163</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1097/NPT.0b013e31818ee779</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>D. Rand, R. Kizony, P.T.L. Weiss, The Sony PlayStation II EyeToy: low-cost virtual reality for use in rehabilitation. J. Neurol. Phys. Ther. <Emphasis Type="Bold">32</Emphasis>(4), 155–163 (2008)</BibUnstructured>
          </Citation>
          <Citation ID="CR94">
            <CitationNumber>94.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>G</Initials>
                <FamilyName>Saposnik</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>R</Initials>
                <FamilyName>Teasell</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>M</Initials>
                <FamilyName>Mamdani</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2010</Year>
              <ArticleTitle Language="En">Effectiveness of virtual reality using Wii gaming technology in stroke rehabilitation a pilot randomized clinical trial and proof of principle</ArticleTitle>
              <JournalTitle>Stroke</JournalTitle>
              <VolumeID>41</VolumeID>
              <IssueID>7</IssueID>
              <FirstPage>1477</FirstPage>
              <LastPage>1484</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1161/STROKEAHA.110.584979</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>G. Saposnik, R. Teasell, M. Mamdani et al., Effectiveness of virtual reality using Wii gaming technology in stroke rehabilitation a pilot randomized clinical trial and proof of principle. Stroke <Emphasis Type="Bold">41</Emphasis>(7), 1477–1484 (2010)</BibUnstructured>
          </Citation>
          <Citation ID="CR95">
            <CitationNumber>95.</CitationNumber>
            <BibUnstructured>I. Oikonomidis, N. Kyriazis, A.A. Argyros, Efficient model-based 3D tracking of hand articulations using Kinect, in <Emphasis Type="Italic">Proceedings of the 22nd British Machine Vision Conference</Emphasis>, University of Dundee, 29 Aug–2 Sep 2011</BibUnstructured>
          </Citation>
          <Citation ID="CR96">
            <CitationNumber>96.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>N</Initials>
                <FamilyName>Gebruers</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>C</Initials>
                <FamilyName>Vanroy</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>S</Initials>
                <FamilyName>Truijen</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2010</Year>
              <ArticleTitle Language="En">Monitoring of physical activity after stroke: a systematic review of accelerometry-based measures</ArticleTitle>
              <JournalTitle>Arch. Phys. Med. Rehabil.</JournalTitle>
              <VolumeID>91</VolumeID>
              <IssueID>2</IssueID>
              <FirstPage>288</FirstPage>
              <LastPage>297</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1016/j.apmr.2009.10.025</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>N. Gebruers, C. Vanroy, S. Truijen et al., Monitoring of physical activity after stroke: a systematic review of accelerometry-based measures. Arch. Phys. Med. Rehabil. <Emphasis Type="Bold">91</Emphasis>(2), 288–297 (2010)</BibUnstructured>
          </Citation>
          <Citation ID="CR97">
            <CitationNumber>97.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>S</Initials>
                <FamilyName>Katz</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>TD</Initials>
                <FamilyName>Downs</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>HR</Initials>
                <FamilyName>Cash</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>RC</Initials>
                <FamilyName>Grotz</FamilyName>
              </BibAuthorName>
              <Year>1970</Year>
              <ArticleTitle Language="En">Progress in development of the index of ADL</ArticleTitle>
              <JournalTitle>Gerontologist</JournalTitle>
              <VolumeID>10</VolumeID>
              <IssueID>1</IssueID>
              <FirstPage>20</FirstPage>
              <LastPage>30</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1093/geront/10.1_Part_1.20</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>S. Katz, T.D. Downs, H.R. Cash, R.C. Grotz, Progress in development of the index of ADL. Gerontologist <Emphasis Type="Bold">10</Emphasis>(1), 20–30 (1970)</BibUnstructured>
          </Citation>
          <Citation ID="CR98">
            <CitationNumber>98.</CitationNumber>
            <BibUnstructured>G. Pirkl, K. Stockinger, K. Kunze, P. Lukowicz, Adapting magnetic resonant coupling based relative positioning technology for wearable activity recognition, in <Emphasis Type="Italic">Proceedings of the 12th IEEE International Symposium on Wearable Computers</Emphasis>, Pittsburgh, 28 Sep–1 Oct 2008. doi:<ExternalRef>
                <RefSource>10.1109/ISWC.2008.4911584</RefSource>
                <RefTarget Address="10.1109/ISWC.2008.4911584" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR99">
            <CitationNumber>99.</CitationNumber>
            <BibUnstructured>C. Mattmann, O. Amft, H. Harms et al., Recognizing upper body postures using textile strain sensors, in <Emphasis Type="Italic">Proceedings of the 11th IEEE International Symposium on Wearable Computers</Emphasis>, Boston, 11–13 Oct 2007. doi:<ExternalRef>
                <RefSource>10.1109/ISWC.2007.4373773</RefSource>
                <RefTarget Address="10.1109/ISWC.2007.4373773" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR100">
            <CitationNumber>100.</CitationNumber>
            <BibUnstructured>L. Bao, S.S. Intille, Activity recognition from user-annotated acceleration data, in <Emphasis Type="Italic">Proceedings of the 2nd International Conference on Pervasive Computing</Emphasis>, Linz, 21–23 Apr 2004. doi: <ExternalRef>
                <RefSource>10.1007/978-3-540-24646-6_1</RefSource>
                <RefTarget Address="10.1007/978-3-540-24646-6_1" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR101">
            <CitationNumber>101.</CitationNumber>
            <BibUnstructured>J. Lester, T. Choudhury, G. Borriello, A practical approach to recognizing physical activities, in <Emphasis Type="Italic">Proceedings of the 4th International Conference on Pervasive Computing</Emphasis>, Dublin, 7–10 May 2006. doi: <ExternalRef>
                <RefSource>10.1007/11748625_1</RefSource>
                <RefTarget Address="10.1007/11748625_1" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR102">
            <CitationNumber>102.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>FI</Initials>
                <FamilyName>Mahoney</FamilyName>
              </BibAuthorName>
              <Year>1965</Year>
              <ArticleTitle Language="En">Functional evaluation: the Barthel index</ArticleTitle>
              <JournalTitle>Md. State Med. J.</JournalTitle>
              <VolumeID>14</VolumeID>
              <FirstPage>61</FirstPage>
              <LastPage>65</LastPage>
            </BibArticle>
            <BibUnstructured>F.I. Mahoney, Functional evaluation: the Barthel index. Md. State Med. J. <Emphasis Type="Bold">14</Emphasis>, 61–65 (1965)</BibUnstructured>
          </Citation>
          <Citation ID="CR103">
            <CitationNumber>103.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>SW</Initials>
                <FamilyName>Lee</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>K</Initials>
                <FamilyName>Mase</FamilyName>
              </BibAuthorName>
              <Year>2002</Year>
              <ArticleTitle Language="En">Activity and location recognition using wearable sensors</ArticleTitle>
              <JournalTitle>IEEE Pervasive Comput.</JournalTitle>
              <VolumeID>1</VolumeID>
              <IssueID>3</IssueID>
              <FirstPage>24</FirstPage>
              <LastPage>32</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/MPRV.2002.1037719</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>S.W. Lee, K. Mase, Activity and location recognition using wearable sensors. IEEE Pervasive Comput. <Emphasis Type="Bold">1</Emphasis>(3), 24–32 (2002)</BibUnstructured>
          </Citation>
          <Citation ID="CR104">
            <CitationNumber>104.</CitationNumber>
            <BibUnstructured>K. Van Laerhoven, H.W. Gellersen, Spine versus porcupine: a study in distributed wearable activity recognition, in <Emphasis Type="Italic">Proceedings of the 8th International Symposium on Wearable Computers</Emphasis>, Arlington, 31 Oct–3 Nov 2004. doi:<ExternalRef>
                <RefSource>10.1109/ISWC.2004.40</RefSource>
                <RefTarget Address="10.1109/ISWC.2004.40" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR105">
            <CitationNumber>105.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>F</Initials>
                <FamilyName>Foerster</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>J</Initials>
                <FamilyName>Fahrenberg</FamilyName>
              </BibAuthorName>
              <Year>2000</Year>
              <ArticleTitle Language="En">Motion pattern and posture: correctly assessed by calibrated accelerometers</ArticleTitle>
              <JournalTitle>Behav. Res. Methods Instrum. Comput.</JournalTitle>
              <VolumeID>32</VolumeID>
              <IssueID>3</IssueID>
              <FirstPage>450</FirstPage>
              <LastPage>457</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.3758/BF03200815</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>F. Foerster, J. Fahrenberg, Motion pattern and posture: correctly assessed by calibrated accelerometers. Behav. Res. Methods Instrum. Comput. <Emphasis Type="Bold">32</Emphasis>(3), 450–457 (2000)</BibUnstructured>
          </Citation>
          <Citation ID="CR106">
            <CitationNumber>106.</CitationNumber>
            <BibUnstructured>K. Van Laerhoven, O. Cakmakci, What shall we teach our pants? in <Emphasis Type="Italic">Proceedings of the 4th International Symposium on Wearable Computers</Emphasis>, Atlanta, 16–17 Oct 2000. doi:<ExternalRef>
                <RefSource>10.1109/ISWC.2000.888468</RefSource>
                <RefTarget Address="10.1109/ISWC.2000.888468" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR107">
            <CitationNumber>107.</CitationNumber>
            <BibUnstructured>K. Van Laerhoven, K.A. Aidoo, S. Lowette, Real-time analysis of data from many sensors with neural networks, in <Emphasis Type="Italic">Proceedings of the 5th International Symposium on Wearable Computers</Emphasis>, Zurich, 8–9 Oct 2001. doi:<ExternalRef>
                <RefSource>10.1109/ISWC.2001.962112</RefSource>
                <RefTarget Address="10.1109/ISWC.2001.962112" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR108">
            <CitationNumber>108.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>S</Initials>
                <FamilyName>Chernbumroong</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>S</Initials>
                <FamilyName>Cang</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>A</Initials>
                <FamilyName>Atkins</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>H</Initials>
                <FamilyName>Yu</FamilyName>
              </BibAuthorName>
              <Year>2013</Year>
              <ArticleTitle Language="En">Elderly activities recognition and classification for applications in assisted living</ArticleTitle>
              <JournalTitle>Expert Syst. Appl.</JournalTitle>
              <VolumeID>40</VolumeID>
              <IssueID>5</IssueID>
              <FirstPage>1662</FirstPage>
              <LastPage>1674</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1016/j.eswa.2012.09.004</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>S. Chernbumroong, S. Cang, A. Atkins, H. Yu, Elderly activities recognition and classification for applications in assisted living. Expert Syst. Appl. <Emphasis Type="Bold">40</Emphasis>(5), 1662–1674 (2013)</BibUnstructured>
          </Citation>
          <Citation ID="CR109">
            <CitationNumber>109.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>M</Initials>
                <FamilyName>Weiser</FamilyName>
              </BibAuthorName>
              <Year>1991</Year>
              <ArticleTitle Language="En">The computer for the 21st century</ArticleTitle>
              <JournalTitle>Sci. Am.</JournalTitle>
              <VolumeID>265</VolumeID>
              <IssueID>3</IssueID>
              <FirstPage>94</FirstPage>
              <LastPage>104</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1038/scientificamerican0991-94</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>M. Weiser, The computer for the 21st century. Sci. Am. <Emphasis Type="Bold">265</Emphasis>(3), 94–104 (1991)</BibUnstructured>
          </Citation>
          <Citation ID="CR110">
            <CitationNumber>110.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>T</Initials>
                <FamilyName>Choudhury</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>S</Initials>
                <FamilyName>Consolvo</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>B</Initials>
                <FamilyName>Harrison</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2008</Year>
              <ArticleTitle Language="En">The mobile sensing platform: an embedded activity recognition system</ArticleTitle>
              <JournalTitle>IEEE Pervasive Comput.</JournalTitle>
              <VolumeID>7</VolumeID>
              <IssueID>2</IssueID>
              <FirstPage>32</FirstPage>
              <LastPage>41</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/MPRV.2008.39</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>T. Choudhury, S. Consolvo, B. Harrison et al., The mobile sensing platform: an embedded activity recognition system. IEEE Pervasive Comput. <Emphasis Type="Bold">7</Emphasis>(2), 32–41 (2008)</BibUnstructured>
          </Citation>
          <Citation ID="CR111">
            <CitationNumber>111.</CitationNumber>
            <BibUnstructured>M. Stikic, B. Schiele, Activity recognition from sparsely labeled data using multi-instance learning, in <Emphasis Type="Italic">Proceedings of 4th International Symposium on Location and Context Awareness</Emphasis>, Tokyo, 7–8 May 2009. doi:<ExternalRef>
                <RefSource>10.1007/978-3-642-01721-6_10</RefSource>
                <RefTarget Address="10.1007/978-3-642-01721-6_10" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR112">
            <CitationNumber>112.</CitationNumber>
            <BibUnstructured>C.R. Wren, E.M. Tapia, Toward scalable activity recognition for sensor networks, in <Emphasis Type="Italic">Proceedings of 2nd International Symposium on Location and Context Awareness</Emphasis>, Dublin, 10–11 May 2006. doi: <ExternalRef>
                <RefSource>10.1007/11752967_12</RefSource>
                <RefTarget Address="10.1007/11752967_12" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR113">
            <CitationNumber>113.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>G</Initials>
                <FamilyName>Singla</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>DJ</Initials>
                <FamilyName>Cook</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>M</Initials>
                <FamilyName>Schmitter-Edgecombe</FamilyName>
              </BibAuthorName>
              <Year>2010</Year>
              <ArticleTitle Language="En">Recognizing independent and joint activities among multiple residents in smart environments</ArticleTitle>
              <JournalTitle>J. Ambient. Intell. Humani. Comput.</JournalTitle>
              <VolumeID>1</VolumeID>
              <IssueID>1</IssueID>
              <FirstPage>57</FirstPage>
              <LastPage>63</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1007/s12652-009-0007-1</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>G. Singla, D.J. Cook, M. Schmitter-Edgecombe, Recognizing independent and joint activities among multiple residents in smart environments. J. Ambient. Intell. Humani. Comput. <Emphasis Type="Bold">1</Emphasis>(1), 57–63 (2010)</BibUnstructured>
          </Citation>
          <Citation ID="CR114">
            <CitationNumber>114.</CitationNumber>
            <BibUnstructured>T. Kleinberger, M. Becker, E. Ras et al., Ambient intelligence in assisted living: enable elderly people to handle future interfaces, In: <Emphasis Type="Italic">Proceedings of the 4th International Conference on Universal Access in Human-Computer Interaction</Emphasis>, Beijing, 22–27 July 2007. doi: <ExternalRef>
                <RefSource>10.1007/978-3-540-73281-5_11</RefSource>
                <RefTarget Address="10.1007/978-3-540-73281-5_11" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR115">
            <CitationNumber>115.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>L</Initials>
                <FamilyName>Chen</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>C</Initials>
                <FamilyName>Nugent</FamilyName>
              </BibAuthorName>
              <Year>2009</Year>
              <ArticleTitle Language="En">Ontology-based activity recognition in intelligent pervasive environments</ArticleTitle>
              <JournalTitle>Int. J. Web Inf. Sys.</JournalTitle>
              <VolumeID>5</VolumeID>
              <IssueID>4</IssueID>
              <FirstPage>410</FirstPage>
              <LastPage>430</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1108/17440080911006199</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>L. Chen, C. Nugent, Ontology-based activity recognition in intelligent pervasive environments. Int. J. Web Inf. Sys. <Emphasis Type="Bold">5</Emphasis>(4), 410–430 (2009)</BibUnstructured>
          </Citation>
          <Citation ID="CR116">
            <CitationNumber>116.</CitationNumber>
            <BibUnstructured>T. Van Kasteren, B. Krose, Bayesian activity recognition in residence for elders, in <Emphasis Type="Italic">Proceedings of the 3rd IET Conference on Intelligent Environments</Emphasis>, University of Ulm, 24–25 Sep 2007. ISBN: 978-0-86341-853-2</BibUnstructured>
          </Citation>
          <Citation ID="CR117">
            <CitationNumber>117.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>M</Initials>
                <FamilyName>Philipose</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>KP</Initials>
                <FamilyName>Fishkin</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>M</Initials>
                <FamilyName>Perkowitz</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2004</Year>
              <ArticleTitle Language="En">Inferring activities from interactions with objects</ArticleTitle>
              <JournalTitle>IEEE Pervasive Comput.</JournalTitle>
              <VolumeID>3</VolumeID>
              <IssueID>4</IssueID>
              <FirstPage>50</FirstPage>
              <LastPage>57</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/MPRV.2004.7</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>M. Philipose, K.P. Fishkin, M. Perkowitz et al., Inferring activities from interactions with objects. IEEE Pervasive Comput. <Emphasis Type="Bold">3</Emphasis>(4), 50–57 (2004)</BibUnstructured>
          </Citation>
          <Citation ID="CR118">
            <CitationNumber>118.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>L</Initials>
                <FamilyName>Chen</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>J</Initials>
                <FamilyName>Hoey</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>CD</Initials>
                <FamilyName>Nugent</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2012</Year>
              <ArticleTitle Language="En">Sensor-based activity recognition</ArticleTitle>
              <JournalTitle>IEEE Trans. Syst. Man Cybern. Part C Appl. Rev.</JournalTitle>
              <VolumeID>42</VolumeID>
              <IssueID>6</IssueID>
              <FirstPage>790</FirstPage>
              <LastPage>808</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/TSMCC.2012.2198883</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>L. Chen, J. Hoey, C.D. Nugent et al., Sensor-based activity recognition. IEEE Trans. Syst. Man Cybern. Part C Appl. Rev. <Emphasis Type="Bold">42</Emphasis>(6), 790–808 (2012)</BibUnstructured>
          </Citation>
          <Citation ID="CR119">
            <CitationNumber>119.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>NC</Initials>
                <FamilyName>Krishnan</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>DJ</Initials>
                <FamilyName>Cook</FamilyName>
              </BibAuthorName>
              <Year>2014</Year>
              <ArticleTitle Language="En">Activity recognition on streaming sensor data</ArticleTitle>
              <JournalTitle>Pervasive Mob. Comput.</JournalTitle>
              <VolumeID>10</VolumeID>
              <FirstPage>138</FirstPage>
              <LastPage>154</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1016/j.pmcj.2012.07.003</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>N.C. Krishnan, D.J. Cook, Activity recognition on streaming sensor data. Pervasive Mob. Comput. <Emphasis Type="Bold">10</Emphasis>, 138–154 (2014)</BibUnstructured>
          </Citation>
          <Citation ID="CR120">
            <CitationNumber>120.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>C</Initials>
                <FamilyName>Zhu</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>W</Initials>
                <FamilyName>Sheng</FamilyName>
              </BibAuthorName>
              <Year>2011</Year>
              <ArticleTitle Language="En">Motion-and location-based online human daily activity recognition</ArticleTitle>
              <JournalTitle>Pervasive Mobile Computing</JournalTitle>
              <VolumeID>7</VolumeID>
              <IssueID>2</IssueID>
              <FirstPage>256</FirstPage>
              <LastPage>269</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1016/j.pmcj.2010.11.004</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>C. Zhu, W. Sheng, Motion-and location-based online human daily activity recognition. Pervasive Mobile Computing <Emphasis Type="Bold">7</Emphasis>(2), 256–269 (2011)</BibUnstructured>
          </Citation>
          <Citation ID="CR121">
            <CitationNumber>121.</CitationNumber>
            <BibUnstructured>N. Vodjdani, The ambient assisted living joint programme, in <Emphasis Type="Italic">Proceedings of the 2nd Electronics System-Integration Technology Conference</Emphasis>, Greenwich, 1–4 Sep 2008. doi:<ExternalRef>
                <RefSource>10.1109/ESTC.2008.4684311</RefSource>
                <RefTarget Address="10.1109/ESTC.2008.4684311" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR122">
            <CitationNumber>122.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>M</Initials>
                <FamilyName>Chan</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>D</Initials>
                <FamilyName>Estève</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>C</Initials>
                <FamilyName>Escriba</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>E</Initials>
                <FamilyName>Campo</FamilyName>
              </BibAuthorName>
              <Year>2008</Year>
              <ArticleTitle Language="En">A review of smart homes - present state and future challenges</ArticleTitle>
              <JournalTitle>Comput. Methods Programs Biomed.</JournalTitle>
              <VolumeID>91</VolumeID>
              <IssueID>1</IssueID>
              <FirstPage>55</FirstPage>
              <LastPage>81</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1016/j.cmpb.2008.02.001</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>M. Chan, D. Estève, C. Escriba, E. Campo, A review of smart homes - present state and future challenges. Comput. Methods Programs Biomed. <Emphasis Type="Bold">91</Emphasis>(1), 55–81 (2008)</BibUnstructured>
          </Citation>
          <Citation ID="CR123">
            <CitationNumber>123.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>S</Initials>
                <FamilyName>Helal</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>W</Initials>
                <FamilyName>Mann</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>H</Initials>
                <FamilyName>El-Zabadani</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2005</Year>
              <ArticleTitle Language="En">The gator tech smart house: a programmable pervasive space</ArticleTitle>
              <JournalTitle>Computer</JournalTitle>
              <VolumeID>38</VolumeID>
              <IssueID>3</IssueID>
              <FirstPage>50</FirstPage>
              <LastPage>60</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/MC.2005.107</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>S. Helal, W. Mann, H. El-Zabadani et al., The gator tech smart house: a programmable pervasive space. Computer <Emphasis Type="Bold">38</Emphasis>(3), 50–60 (2005)</BibUnstructured>
          </Citation>
          <Citation ID="CR124">
            <CitationNumber>124.</CitationNumber>
            <BibUnstructured>M. Stikic, T. Huynh, K. Van Laerhoven, B. Schiele, ADL recognition based on the combination of RFID and accelerometer sensing, in <Emphasis Type="Italic">Proceedings of 2nd International Conference on Pervasive Computing Technologies for Healthcare</Emphasis>, Tampere, 30 Jan–1 Feb 2008. doi:<ExternalRef>
                <RefSource>10.1109/PCTHEALTH.2008.4571084</RefSource>
                <RefTarget Address="10.1109/PCTHEALTH.2008.4571084" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR125">
            <CitationNumber>125.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>B</Initials>
                <FamilyName>Najafi</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>K</Initials>
                <FamilyName>Aminian</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>A</Initials>
                <FamilyName>Paraschiv-Ionescu</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2003</Year>
              <ArticleTitle Language="En">Ambulatory system for human motion analysis using a kinematic sensor: monitoring of daily physical activity in the elderly</ArticleTitle>
              <JournalTitle>IEEE Trans. Biomed. Eng.</JournalTitle>
              <VolumeID>50</VolumeID>
              <IssueID>6</IssueID>
              <FirstPage>711</FirstPage>
              <LastPage>723</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/TBME.2003.812189</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>B. Najafi, K. Aminian, A. Paraschiv-Ionescu et al., Ambulatory system for human motion analysis using a kinematic sensor: monitoring of daily physical activity in the elderly. IEEE Trans. Biomed. Eng. <Emphasis Type="Bold">50</Emphasis>(6), 711–723 (2003)</BibUnstructured>
          </Citation>
          <Citation ID="CR126">
            <CitationNumber>126.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>JR</Initials>
                <FamilyName>Kwapisz</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>GM</Initials>
                <FamilyName>Weiss</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>SA</Initials>
                <FamilyName>Moore</FamilyName>
              </BibAuthorName>
              <Year>2010</Year>
              <ArticleTitle Language="En">Activity recognition using cell phone accelerometers</ArticleTitle>
              <JournalTitle>ACM SIGKDD Explor. Newsl.</JournalTitle>
              <VolumeID>12</VolumeID>
              <IssueID>2</IssueID>
              <FirstPage>74</FirstPage>
              <LastPage>82</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1145/1964897.1964918</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>J.R. Kwapisz, G.M. Weiss, S.A. Moore, Activity recognition using cell phone accelerometers. ACM SIGKDD Explor. Newsl. <Emphasis Type="Bold">12</Emphasis>(2), 74–82 (2010)</BibUnstructured>
          </Citation>
          <Citation ID="CR127">
            <CitationNumber>127.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>R</Initials>
                <FamilyName>Bogue</FamilyName>
              </BibAuthorName>
              <Year>2007</Year>
              <ArticleTitle Language="En">MEMS sensors: past, present and future</ArticleTitle>
              <JournalTitle>Sens. Rev.</JournalTitle>
              <VolumeID>27</VolumeID>
              <IssueID>1</IssueID>
              <FirstPage>7</FirstPage>
              <LastPage>13</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1108/02602280710729068</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>R. Bogue, MEMS sensors: past, present and future. Sens. Rev. <Emphasis Type="Bold">27</Emphasis>(1), 7–13 (2007)</BibUnstructured>
          </Citation>
          <Citation ID="CR128">
            <CitationNumber>128.</CitationNumber>
            <BibChapter>
              <BibAuthorName>
                <Initials>A</Initials>
                <FamilyName>Cranny</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>A</Initials>
                <FamilyName>Beriain</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>H</Initials>
                <FamilyName>Solar</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2014</Year>
              <ChapterTitle Language="En">Vital sign sensing technology</ChapterTitle>
              <BibEditorName>
                <Initials>K</Initials>
                <FamilyName>Maharatna</FamilyName>
              </BibEditorName>
              <BibEditorName>
                <Initials>S</Initials>
                <FamilyName>Bonfiglio</FamilyName>
              </BibEditorName>
              <Eds/>
              <BookTitle>Systems Design for Remote Healthcare</BookTitle>
              <PublisherName>Springer</PublisherName>
              <PublisherLocation>Heidelberg</PublisherLocation>
              <FirstPage>55</FirstPage>
              <LastPage>92</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1007/978-1-4614-8842-2_3</Handle>
              </Occurrence>
              <ISBN>978-1-4614-8842-2</ISBN>
            </BibChapter>
            <BibUnstructured>A. Cranny, A. Beriain, H. Solar et al., Vital sign sensing technology, in <Emphasis Type="Italic">Systems Design for Remote Healthcare</Emphasis>, ed. by K. Maharatna, S. Bonfiglio (Springer, Heidelberg, 2014), pp. 55–92. ISBN 978-1-4614-8842-2</BibUnstructured>
          </Citation>
          <Citation ID="CR129">
            <CitationNumber>129.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>RE</Initials>
                <FamilyName>Mayagoitia</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>AV</Initials>
                <FamilyName>Nene</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>PH</Initials>
                <FamilyName>Veltink</FamilyName>
              </BibAuthorName>
              <Year>2002</Year>
              <ArticleTitle Language="En">Accelerometer and rate gyroscope measurement of kinematics: an inexpensive alternative to optical motion analysis systems</ArticleTitle>
              <JournalTitle>J. Biomech.</JournalTitle>
              <VolumeID>35</VolumeID>
              <IssueID>4</IssueID>
              <FirstPage>537</FirstPage>
              <LastPage>542</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1016/S0021-9290(01)00231-7</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>R.E. Mayagoitia, A.V. Nene, P.H. Veltink, Accelerometer and rate gyroscope measurement of kinematics: an inexpensive alternative to optical motion analysis systems. J. Biomech. <Emphasis Type="Bold">35</Emphasis>(4), 537–542 (2002)</BibUnstructured>
          </Citation>
          <Citation ID="CR130">
            <CitationNumber>130.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>C</Initials>
                <FamilyName>Kendell</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>ED</Initials>
                <FamilyName>Lemaire</FamilyName>
              </BibAuthorName>
              <Year>2009</Year>
              <ArticleTitle Language="En">Effect of mobility devices on orientation sensors that contain magnetometers</ArticleTitle>
              <JournalTitle>J. Rehabil. Res. Dev.</JournalTitle>
              <VolumeID>46</VolumeID>
              <IssueID>7</IssueID>
              <FirstPage>957</FirstPage>
              <LastPage>962</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1682/JRRD.2008.09.0132</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>C. Kendell, E.D. Lemaire, Effect of mobility devices on orientation sensors that contain magnetometers. J. Rehabil. Res. Dev. <Emphasis Type="Bold">46</Emphasis>(7), 957–962 (2009)</BibUnstructured>
          </Citation>
          <Citation ID="CR131">
            <CitationNumber>131.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>A</Initials>
                <FamilyName>Bulling</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>U</Initials>
                <FamilyName>Blanke</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>B</Initials>
                <FamilyName>Schiele</FamilyName>
              </BibAuthorName>
              <Year>2014</Year>
              <ArticleTitle Language="En">A tutorial on human activity recognition using body-worn inertial sensors</ArticleTitle>
              <JournalTitle>ACM Comput. Surv. (CSUR)</JournalTitle>
              <VolumeID>46</VolumeID>
              <IssueID>3</IssueID>
              <FirstPage>33</FirstPage>
              <BibArticleDOI>10.1145/2499621</BibArticleDOI>
              <Occurrence Type="DOI">
                <Handle>10.1145/2499621</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>A. Bulling, U. Blanke, B. Schiele, A tutorial on human activity recognition using body-worn inertial sensors. ACM Comput. Surv. (CSUR) <Emphasis Type="Bold">46</Emphasis>(3), 33 (2014). doi:<ExternalRef>
                <RefSource>10.1145/2499621</RefSource>
                <RefTarget Address="10.1145/2499621" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR132">
            <CitationNumber>132.</CitationNumber>
            <BibUnstructured>D. Biswas, A. Cranny, A. Rahim et al., On the sensor choice and data analysis for classification of elementary upper limb movements, in <Emphasis Type="Italic">Proceedings of IEEE-EMBS International Conference on Biomedical and Health Informatics</Emphasis>, Valencia, 1–4 June 2014. doi:<ExternalRef>
                <RefSource>10.1109/BHI.2014.6864471</RefSource>
                <RefTarget Address="10.1109/BHI.2014.6864471" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR133">
            <CitationNumber>133.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>X</Initials>
                <FamilyName>Sun</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>H</Initials>
                <FamilyName>Kashima</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>N</Initials>
                <FamilyName>Ueda</FamilyName>
              </BibAuthorName>
              <Year>2013</Year>
              <ArticleTitle Language="En">Large-scale personalized human activity recognition using online multitask learning</ArticleTitle>
              <JournalTitle>IEEE Trans. Knowl. Data Eng.</JournalTitle>
              <VolumeID>25</VolumeID>
              <IssueID>11</IssueID>
              <FirstPage>2551</FirstPage>
              <LastPage>2563</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/TKDE.2012.246</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>X. Sun, H. Kashima, N. Ueda, Large-scale personalized human activity recognition using online multitask learning. IEEE Trans. Knowl. Data Eng. <Emphasis Type="Bold">25</Emphasis>(11), 2551–2563 (2013)</BibUnstructured>
          </Citation>
          <Citation ID="CR134">
            <CitationNumber>134.</CitationNumber>
            <BibUnstructured>O. Amft, M. Kusserow, G. Tröster, Probabilistic parsing of dietary activity events, in <Emphasis Type="Italic">Proceedings of 4th International Workshop on Wearable and Implantable Body Sensor Networks</Emphasis>, Aachen, 26–28 Mar 2007. doi: <ExternalRef>
                <RefSource>10.1007/978-3-540-70994-7_41</RefSource>
                <RefTarget Address="10.1007/978-3-540-70994-7_41" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR135">
            <CitationNumber>135.</CitationNumber>
            <BibUnstructured>O. Amft, Self-taught learning for activity spotting in on-body motion sensor data, in <Emphasis Type="Italic">Proceedings of 15th Annual International Symposium on Wearable Computers</Emphasis>, San Francisco, 12–15 June 2011. doi:<ExternalRef>
                <RefSource>10.1109/ISWC.2011.37</RefSource>
                <RefTarget Address="10.1109/ISWC.2011.37" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR136">
            <CitationNumber>136.</CitationNumber>
            <BibUnstructured>U. Blanke, B. Schiele, M. Kreil et al., All for one or one for all? Combining heterogeneous features for activity spotting, in <Emphasis Type="Italic">Proceedings of 8th IEEE International Conference on Pervasive Computing and Communications Workshops</Emphasis>, Mannheim, 29 Mar–2 Apr 2010. doi:<ExternalRef>
                <RefSource>10.1109/PERCOMW.2010.5470597</RefSource>
                <RefTarget Address="10.1109/PERCOMW.2010.5470597" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR137">
            <CitationNumber>137.</CitationNumber>
            <BibUnstructured>A. Bulling, C. Weichel, H. Gellersen, EyeContext: recognition of high-level contextual cues from human visual behaviour, in <Emphasis Type="Italic">Proceedings of SIGCHI 31st International Conference on Human Factors in Computing Systems</Emphasis>, Paris, 27 Mar–3 Apr 2013. doi:<ExternalRef>
                <RefSource>10.1145/2470654.2470697</RefSource>
                <RefTarget Address="10.1145/2470654.2470697" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR138">
            <CitationNumber>138.</CitationNumber>
            <BibUnstructured>K. Van Laerhoven, D. Kilian, B. Schiele, Using rhythm awareness in long-term activity recognition, in <Emphasis Type="Italic">Proceedings of 12th IEEE International Symposium on Wearable Computers</Emphasis>, Pittsburgh, 2008. doi:<ExternalRef>
                <RefSource>10.1109/ISWC.2008.4911586</RefSource>
                <RefTarget Address="10.1109/ISWC.2008.4911586" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR139">
            <CitationNumber>139.</CitationNumber>
            <BibUnstructured>A. Kapoor, E. Horvitz, Experience sampling for building predictive user models: a comparative study, in <Emphasis Type="Italic">Proceedings of 26th SIGCHI Annual Conference on Human Factors in Computing Systems</Emphasis>, Florence, 5–10 Apr 2008. doi:<ExternalRef>
                <RefSource>10.1145/1357054.1357159</RefSource>
                <RefTarget Address="10.1145/1357054.1357159" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR140">
            <CitationNumber>140.</CitationNumber>
            <BibUnstructured>H. Bayati, J. del R Millán, R. Chavarriaga, Unsupervised adaptation to on-body sensor displacement in acceleration-based activity recognition, in <Emphasis Type="Italic">Proceedings of the 15th Annual International Symposium on Wearable Computers</Emphasis>, San Francisco, 12–15 June 2011. doi:<ExternalRef>
                <RefSource>10.1109/ISWC.2011.11</RefSource>
                <RefTarget Address="10.1109/ISWC.2011.11" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR141">
            <CitationNumber>141.</CitationNumber>
            <BibUnstructured>K. Kunze, P. Lukowicz, Dealing with sensor displacement in motion-based on body activity recognition systems, in <Emphasis Type="Italic">Proceedings of the 10th International Conference on Ubiquitous Computing</Emphasis>, Seoul, 2008. doi:<ExternalRef>
                <RefSource>10.1145/1409635.1409639</RefSource>
                <RefTarget Address="10.1145/1409635.1409639" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR142">
            <CitationNumber>142.</CitationNumber>
            <BibUnstructured>K. Maharatna, E.B. Mazomenos, J. Morgan, S. Bonfiglio, Towards the development of next-generation remote healthcare system: some practical considerations, in <Emphasis Type="Italic">Proceedings of IEEE International Symposium on Circuits and Systems</Emphasis>, Seoul, 20–23 May 2012. doi:<ExternalRef>
                <RefSource>10.1109/ISCAS.2012.6270390</RefSource>
                <RefTarget Address="10.1109/ISCAS.2012.6270390" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR143">
            <CitationNumber>143.</CitationNumber>
            <BibUnstructured>R. Balani, Energy consumption analysis for Bluetooth, wifi and cellular networks, <ExternalRef>
                <RefSource>http://nesl.ee.ucla.edu/fw/documents/reports/2007/PowerAnalysis.pdf</RefSource>
                <RefTarget Address="http://nesl.ee.ucla.edu/fw/documents/reports/2007/PowerAnalysis.pdf" TargetType="URL"/>
              </ExternalRef>. Accessed 24 Mar 2014</BibUnstructured>
          </Citation>
          <Citation ID="CR144">
            <CitationNumber>144.</CitationNumber>
            <BibUnstructured>K. Van Laerhoven, E. Berlin, When else did this happen? Efficient subsequence representation and matching for wearable activity data, in <Emphasis Type="Italic">Proceedings of the International Symposium on Wearable Computers</Emphasis>, Linz, 4–7 Sep 2009. doi:<ExternalRef>
                <RefSource>10.1109/ISWC.2009.23</RefSource>
                <RefTarget Address="10.1109/ISWC.2009.23" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR145">
            <CitationNumber>145.</CitationNumber>
            <BibUnstructured>E. Guenterberg, S. Ostadabbas, H. Ghasemzadeh, R. Jafari, An automatic segmentation technique in body sensor networks based on signal energy, in <Emphasis Type="Italic">Proceedings of the 4th International Conference on Body Area Networks</Emphasis>, Los Angeles, 1–3 Apr 2009. doi:<ExternalRef>
                <RefSource>10.4108/ICST.BODYNETS2009.6036</RefSource>
                <RefTarget Address="10.4108/ICST.BODYNETS2009.6036" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR146">
            <CitationNumber>146.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>AF</Initials>
                <FamilyName>Bobick</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>SS</Initials>
                <FamilyName>Intille</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>JW</Initials>
                <FamilyName>Davis</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2000</Year>
              <ArticleTitle Language="En">Perceptual user interfaces: the Kids Room</ArticleTitle>
              <JournalTitle>Commun. ACM</JournalTitle>
              <VolumeID>43</VolumeID>
              <IssueID>3</IssueID>
              <FirstPage>60</FirstPage>
              <LastPage>61</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1145/330534.330541</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>A.F. Bobick, S.S. Intille, J.W. Davis et al., Perceptual user interfaces: the Kids Room. Commun. ACM <Emphasis Type="Bold">43</Emphasis>(3), 60–61 (2000)</BibUnstructured>
          </Citation>
          <Citation ID="CR147">
            <CitationNumber>147.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>K</Initials>
                <FamilyName>Altun</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>B</Initials>
                <FamilyName>Barshan</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>O</Initials>
                <FamilyName>Tunçel</FamilyName>
              </BibAuthorName>
              <Year>2010</Year>
              <ArticleTitle Language="En">Comparative study on classifying human activities with miniature inertial and magnetic sensors</ArticleTitle>
              <JournalTitle>Pattern Recogn.</JournalTitle>
              <VolumeID>43</VolumeID>
              <IssueID>10</IssueID>
              <FirstPage>3605</FirstPage>
              <LastPage>3620</LastPage>
              <Occurrence Type="ZLBID">
                <Handle>1213.68513</Handle>
              </Occurrence>
              <Occurrence Type="DOI">
                <Handle>10.1016/j.patcog.2010.04.019</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>K. Altun, B. Barshan, O. Tunçel, Comparative study on classifying human activities with miniature inertial and magnetic sensors. Pattern Recogn. <Emphasis Type="Bold">43</Emphasis>(10), 3605–3620 (2010)</BibUnstructured>
          </Citation>
          <Citation ID="CR148">
            <CitationNumber>148.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>J</Initials>
                <FamilyName>Parkka</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>M</Initials>
                <FamilyName>Ermes</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>P</Initials>
                <FamilyName>Korpipaa</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2006</Year>
              <ArticleTitle Language="En">Activity classification using realistic data from wearable sensors</ArticleTitle>
              <JournalTitle>IEEE Trans. Inf. Technol. Biomed.</JournalTitle>
              <VolumeID>10</VolumeID>
              <IssueID>1</IssueID>
              <FirstPage>119</FirstPage>
              <LastPage>128</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/TITB.2005.856863</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>J. Parkka, M. Ermes, P. Korpipaa et al., Activity classification using realistic data from wearable sensors. IEEE Trans. Inf. Technol. Biomed. <Emphasis Type="Bold">10</Emphasis>(1), 119–128 (2006)</BibUnstructured>
          </Citation>
          <Citation ID="CR149">
            <CitationNumber>149.</CitationNumber>
            <BibUnstructured>U. Maurer, A. Smailagic, D.P. Siewiorek, M. Deisher, Activity recognition and monitoring using multiple sensors on different body positions, in <Emphasis Type="Italic">Proceedings of International Workshop on Wearable and Implantable Body Sensor Networks</Emphasis>, Cambridge, 3–5 Apr 2006. doi:<ExternalRef>
                <RefSource>10.1109/BSN.2006.6</RefSource>
                <RefTarget Address="10.1109/BSN.2006.6" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR150">
            <CitationNumber>150.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>JY</Initials>
                <FamilyName>Yang</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>JS</Initials>
                <FamilyName>Wang</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>YP</Initials>
                <FamilyName>Chen</FamilyName>
              </BibAuthorName>
              <Year>2008</Year>
              <ArticleTitle Language="En">Using acceleration measurements for activity recognition: an effective learning algorithm for constructing neural classifiers</ArticleTitle>
              <JournalTitle>Pattern Recogn. Lett.</JournalTitle>
              <VolumeID>29</VolumeID>
              <IssueID>16</IssueID>
              <FirstPage>2213</FirstPage>
              <LastPage>2220</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1016/j.patrec.2008.08.002</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>J.Y. Yang, J.S. Wang, Y.P. Chen, Using acceleration measurements for activity recognition: an effective learning algorithm for constructing neural classifiers. Pattern Recogn. Lett. <Emphasis Type="Bold">29</Emphasis>(16), 2213–2220 (2008)</BibUnstructured>
          </Citation>
          <Citation ID="CR151">
            <CitationNumber>151.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>A</Initials>
                <FamilyName>Mannini</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>AM</Initials>
                <FamilyName>Sabatini</FamilyName>
              </BibAuthorName>
              <Year>2010</Year>
              <ArticleTitle Language="En">Machine learning methods for classifying human physical activity from on-body accelerometers</ArticleTitle>
              <JournalTitle>Sensors</JournalTitle>
              <VolumeID>10</VolumeID>
              <IssueID>2</IssueID>
              <FirstPage>1154</FirstPage>
              <LastPage>1175</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.3390/s100201154</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>A. Mannini, A.M. Sabatini, Machine learning methods for classifying human physical activity from on-body accelerometers. Sensors <Emphasis Type="Bold">10</Emphasis>(2), 1154–1175 (2010)</BibUnstructured>
          </Citation>
          <Citation ID="CR152">
            <CitationNumber>152.</CitationNumber>
            <BibBook>
              <BibEditorName>
                <Initials>S</Initials>
                <FamilyName>Theodoridis</FamilyName>
              </BibEditorName>
              <BibEditorName>
                <Initials>K</Initials>
                <FamilyName>Koutroumbas</FamilyName>
              </BibEditorName>
              <Eds/>
              <Year>2009</Year>
              <BookTitle>Pattern Recognition</BookTitle>
              <EditionNumber>4</EditionNumber>
              <PublisherName>Academic</PublisherName>
              <PublisherLocation>London</PublisherLocation>
              <ISBN>9780080949123</ISBN>
            </BibBook>
            <BibUnstructured>S. Theodoridis, K. Koutroumbas (eds.), <Emphasis Type="Italic">Pattern Recognition</Emphasis>, 4th edn. (Academic, London, 2009). ISBN 9780080949123</BibUnstructured>
          </Citation>
          <Citation ID="CR153">
            <CitationNumber>153.</CitationNumber>
            <BibUnstructured>D. Biswas, A. Cranny, N. Gupta et al., Recognition of elementary upper limb movements in an activity of daily living using data from wrist mounted accelerometers, in <Emphasis Type="Italic">Proceedings of IEEE Computer Society International Conference on Health Informatics</Emphasis>, Verona, 15–17 Sep 2014. doi:<ExternalRef>
                <RefSource>10.1109/ICHI.2014.40</RefSource>
                <RefTarget Address="10.1109/ICHI.2014.40" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR154">
            <CitationNumber>154.</CitationNumber>
            <BibUnstructured>J. Mantyjarvi, J. Himberg, T. Seppanen, Recognizing human motion with multiple acceleration sensors, in <Emphasis Type="Italic">Proceedings of IEEE International Conference on Systems, Man, and Cybernetics</Emphasis>, Tucson, 7–10 Oct 2001. doi:<ExternalRef>
                <RefSource>10.1109/ICSMC.2001.973004</RefSource>
                <RefTarget Address="10.1109/ICSMC.2001.973004" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR155">
            <CitationNumber>155.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>AK</Initials>
                <FamilyName>Jain</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>RPW</Initials>
                <FamilyName>Duin</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>J</Initials>
                <FamilyName>Mao</FamilyName>
              </BibAuthorName>
              <Year>2000</Year>
              <ArticleTitle Language="En">Statistical pattern recognition: a review</ArticleTitle>
              <JournalTitle>IEEE Trans. Pattern Anal. Mach. Intell.</JournalTitle>
              <VolumeID>22</VolumeID>
              <IssueID>1</IssueID>
              <FirstPage>4</FirstPage>
              <LastPage>37</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/34.824819</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>A.K. Jain, R.P.W. Duin, J. Mao, Statistical pattern recognition: a review. IEEE Trans. Pattern Anal. Mach. Intell. <Emphasis Type="Bold">22</Emphasis>(1), 4–37 (2000)</BibUnstructured>
          </Citation>
          <Citation ID="CR156">
            <CitationNumber>156.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>L</Initials>
                <FamilyName>Atallah</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>GZ</Initials>
                <FamilyName>Yang</FamilyName>
              </BibAuthorName>
              <Year>2009</Year>
              <ArticleTitle Language="En">The use of pervasive sensing for behaviour profiling—a survey</ArticleTitle>
              <JournalTitle>Pervasive Mob. Comput.</JournalTitle>
              <VolumeID>5</VolumeID>
              <IssueID>5</IssueID>
              <FirstPage>447</FirstPage>
              <LastPage>464</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1016/j.pmcj.2009.06.009</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>L. Atallah, G.Z. Yang, The use of pervasive sensing for behaviour profiling—a survey. Pervasive Mob. Comput. <Emphasis Type="Bold">5</Emphasis>(5), 447–464 (2009)</BibUnstructured>
          </Citation>
          <Citation ID="CR157">
            <CitationNumber>157.</CitationNumber>
            <BibUnstructured>C. Zhu, W. Sheng, Recognizing human daily activity using a single inertial sensor, in <Emphasis Type="Italic">Proceedings of 8th World Congress on Intelligent Control and Automation</Emphasis>, PRC, Jinan, 7–9 July 2010. doi:10.1109/WCICA.2010.5555072</BibUnstructured>
          </Citation>
          <Citation ID="CR158">
            <CitationNumber>158.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>CC</Initials>
                <FamilyName>Yang</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>YL</Initials>
                <FamilyName>Hsu</FamilyName>
              </BibAuthorName>
              <Year>2010</Year>
              <ArticleTitle Language="En">A review of accelerometry-based wearable motion detectors for physical activity monitoring</ArticleTitle>
              <JournalTitle>Sensors</JournalTitle>
              <VolumeID>10</VolumeID>
              <IssueID>8</IssueID>
              <FirstPage>7772</FirstPage>
              <LastPage>7788</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.3390/s100807772</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>C.C. Yang, Y.L. Hsu, A review of accelerometry-based wearable motion detectors for physical activity monitoring. Sensors <Emphasis Type="Bold">10</Emphasis>(8), 7772–7788 (2010)</BibUnstructured>
          </Citation>
          <Citation ID="CR159">
            <CitationNumber>159.</CitationNumber>
            <BibUnstructured>N. Oliver, A.P. Pentland, F. Berard, Lafter: lips and face real time tracker, in <Emphasis Type="Italic">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</Emphasis>, San Juan, 17–19 June 1997. doi:<ExternalRef>
                <RefSource>10.1109/CVPR.1997.609309</RefSource>
                <RefTarget Address="10.1109/CVPR.1997.609309" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR160">
            <CitationNumber>160.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>S</Initials>
                <FamilyName>Mitra</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>T</Initials>
                <FamilyName>Acharya</FamilyName>
              </BibAuthorName>
              <Year>2007</Year>
              <ArticleTitle Language="En">Gesture recognition: a survey</ArticleTitle>
              <JournalTitle>IEEE Trans. Syst. Man Cybern. Part C Appl. Rev.</JournalTitle>
              <VolumeID>37</VolumeID>
              <IssueID>3</IssueID>
              <FirstPage>311</FirstPage>
              <LastPage>324</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/TSMCC.2007.893280</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>S. Mitra, T. Acharya, Gesture recognition: a survey. IEEE Trans. Syst. Man Cybern. Part C Appl. Rev. <Emphasis Type="Bold">37</Emphasis>(3), 311–324 (2007)</BibUnstructured>
          </Citation>
          <Citation ID="CR161">
            <CitationNumber>161.</CitationNumber>
            <BibUnstructured>P. Natarajan, R. Nevatia, Coupled hidden semi Markov models for activity recognition, in <Emphasis Type="Italic">Proceedings of IEEE Workshop on Motion and Video Computing</Emphasis>, Austin, 23–24 Feb 2007. doi: <ExternalRef>
                <RefSource>10.1109/WMVC.2007.12</RefSource>
                <RefTarget Address="10.1109/WMVC.2007.12" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR162">
            <CitationNumber>162.</CitationNumber>
            <BibUnstructured>K. Kunze, M. Barry, E.A. Heinz et al., Towards recognizing tai chi: an initial experiment using wearable sensors, in <Emphasis Type="Italic">Proceedings of the 3rd International Forum on Applied Wearable Computing</Emphasis>, Bremen, 15–16 Mar 2006</BibUnstructured>
          </Citation>
          <Citation ID="CR163">
            <CitationNumber>163.</CitationNumber>
            <BibUnstructured>K.T. Song, Y.Q. Wang, Remote activity monitoring of the elderly using a two-axis accelerometer, in <Emphasis Type="Italic">Proceedings of the CACS Automatic Control Conference</Emphasis>, Tainan, 18–19 Nov 2005</BibUnstructured>
          </Citation>
          <Citation ID="CR164">
            <CitationNumber>164.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>O</Initials>
                <FamilyName>Banos</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>M</Initials>
                <FamilyName>Damas</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>H</Initials>
                <FamilyName>Pomares</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2012</Year>
              <ArticleTitle Language="En">Daily living activity recognition based on statistical feature quality group selection</ArticleTitle>
              <JournalTitle>Expert Syst. Appl.</JournalTitle>
              <VolumeID>39</VolumeID>
              <IssueID>9</IssueID>
              <FirstPage>8013</FirstPage>
              <LastPage>8021</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1016/j.eswa.2012.01.164</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>O. Banos, M. Damas, H. Pomares et al., Daily living activity recognition based on statistical feature quality group selection. Expert Syst. Appl. <Emphasis Type="Bold">39</Emphasis>(9), 8013–8021 (2012)</BibUnstructured>
          </Citation>
          <Citation ID="CR165">
            <CitationNumber>165.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>CW</Initials>
                <FamilyName>Hsu</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>CJ</Initials>
                <FamilyName>Lin</FamilyName>
              </BibAuthorName>
              <Year>2002</Year>
              <ArticleTitle Language="En">A comparison of methods for multiclass support vector machines</ArticleTitle>
              <JournalTitle>IEEE Trans. Neural Netw.</JournalTitle>
              <VolumeID>13</VolumeID>
              <IssueID>2</IssueID>
              <FirstPage>415</FirstPage>
              <LastPage>425</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/72.991427</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>C.W. Hsu, C.J. Lin, A comparison of methods for multiclass support vector machines. IEEE Trans. Neural Netw. <Emphasis Type="Bold">13</Emphasis>(2), 415–425 (2002)</BibUnstructured>
          </Citation>
          <Citation ID="CR166">
            <CitationNumber>166.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>M</Initials>
                <FamilyName>Mathie</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>BG</Initials>
                <FamilyName>Celler</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>NH</Initials>
                <FamilyName>Lovell</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>A</Initials>
                <FamilyName>Coster</FamilyName>
              </BibAuthorName>
              <Year>2004</Year>
              <ArticleTitle Language="En">Classification of basic daily movements using a triaxial accelerometer</ArticleTitle>
              <JournalTitle>Med. Biol. Eng. Comput.</JournalTitle>
              <VolumeID>42</VolumeID>
              <IssueID>5</IssueID>
              <FirstPage>679</FirstPage>
              <LastPage>687</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1007/BF02347551</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>M. Mathie, B.G. Celler, N.H. Lovell, A. Coster, Classification of basic daily movements using a triaxial accelerometer. Med. Biol. Eng. Comput. <Emphasis Type="Bold">42</Emphasis>(5), 679–687 (2004)</BibUnstructured>
          </Citation>
          <Citation ID="CR167">
            <CitationNumber>167.</CitationNumber>
            <BibUnstructured>C. Pham, T. Plötz, P. Olivier, A dynamic time warping approach to real-time activity recognition for food preparation, in <Emphasis Type="Italic">Proceedings of the 1st International Conference on Ambient Intelligence</Emphasis>, Malaga, 10–12 Nov 2010. doi:<ExternalRef>
                <RefSource>10.1007/978-3-642-16917-5_3</RefSource>
                <RefTarget Address="10.1007/978-3-642-16917-5_3" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR168">
            <CitationNumber>168.</CitationNumber>
            <BibUnstructured>U. Blanke, R. Rehner, B. Schiele, South by south-east or sitting at the desk: can orientation be a place? in <Emphasis Type="Italic">Proceedings of the 15th Annual International Symposium on Wearable Computers</Emphasis>, San Francisco, 12–15 June 2011. doi:<ExternalRef>
                <RefSource>10.1109/ISWC.2011.18</RefSource>
                <RefTarget Address="10.1109/ISWC.2011.18" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR169">
            <CitationNumber>169.</CitationNumber>
            <BibUnstructured>T. Stiefmeier, D. Roggen, G. Tröster, Gestures are strings: efficient online gesture spotting and classification using string matching, in <Emphasis Type="Italic">Proceedings of the ICST 2nd International Conference on Body Area Networks</Emphasis>, Florence, 11–13 June 2007. doi:<ExternalRef>
                <RefSource>10.4108/bodynets.2007.143</RefSource>
                <RefTarget Address="10.4108/bodynets.2007.143" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR170">
            <CitationNumber>170.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>D</Initials>
                <FamilyName>Biswas</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>A</Initials>
                <FamilyName>Cranny</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>N</Initials>
                <FamilyName>Gupta</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2015</Year>
              <ArticleTitle Language="En">Recognizing upper limb movements with wrist worn inertial sensors using k-means clustering classification</ArticleTitle>
              <JournalTitle>Hum. Mov. Sci.</JournalTitle>
              <VolumeID>40</VolumeID>
              <FirstPage>59</FirstPage>
              <LastPage>76</LastPage>
              <BibArticleDOI>10.1016/j.humov.2014.11.013</BibArticleDOI>
              <Occurrence Type="DOI">
                <Handle>10.1016/j.humov.2014.11.013</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>D. Biswas, A. Cranny, N. Gupta et al., Recognizing upper limb movements with wrist worn inertial sensors using k-means clustering classification. Hum. Mov. Sci. <Emphasis Type="Bold">40</Emphasis>, 59–76 (2015). doi:<ExternalRef>
                <RefSource>10.1016/j.humov.2014.11.013</RefSource>
                <RefTarget Address="10.1016/j.humov.2014.11.013" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR171">
            <CitationNumber>171.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>JA</Initials>
                <FamilyName>Ward</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>P</Initials>
                <FamilyName>Lukowicz</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>HW</Initials>
                <FamilyName>Gellersen</FamilyName>
              </BibAuthorName>
              <Year>2011</Year>
              <ArticleTitle Language="En">Performance metrics for activity recognition</ArticleTitle>
              <JournalTitle>ACM Trans. Intell. Syst. Technol.</JournalTitle>
              <VolumeID>2</VolumeID>
              <IssueID>1</IssueID>
              <FirstPage>6</FirstPage>
              <BibArticleDOI>10.1145/1889681.1889687</BibArticleDOI>
              <Occurrence Type="DOI">
                <Handle>10.1145/1889681.1889687</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>J.A. Ward, P. Lukowicz, H.W. Gellersen, Performance metrics for activity recognition. ACM Trans. Intell. Syst. Technol. <Emphasis Type="Bold">2</Emphasis>(1), 6 (2011). doi:<ExternalRef>
                <RefSource>10.1145/1889681.1889687</RefSource>
                <RefTarget Address="10.1145/1889681.1889687" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR172">
            <CitationNumber>172.</CitationNumber>
            <BibUnstructured>D. Minnen, T. Westeyn, T. Starner et al., Performance metrics and evaluation issues for continuous activity recognition. ACM Trans. Intell. Syst. Technol. <Emphasis Type="Bold">2</Emphasis>(1), (2011). doi:<ExternalRef>
                <RefSource>10.1145/1889681.1889687</RefSource>
                <RefTarget Address="10.1145/1889681.1889687" TargetType="DOI"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR173">
            <CitationNumber>173.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>JC</Initials>
                <FamilyName>Fernandez Caballero</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>FJ</Initials>
                <FamilyName>Martinez</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>C</Initials>
                <FamilyName>Hervás</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>PA</Initials>
                <FamilyName>Gutiérrez</FamilyName>
              </BibAuthorName>
              <Year>2010</Year>
              <ArticleTitle Language="En">Sensitivity versus accuracy in multiclass problems using memetic pareto evolutionary neural networks</ArticleTitle>
              <JournalTitle>IEEE Trans. Neural Netw.</JournalTitle>
              <VolumeID>21</VolumeID>
              <IssueID>5</IssueID>
              <FirstPage>750</FirstPage>
              <LastPage>770</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/TNN.2010.2041468</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>J.C. Fernandez Caballero, F.J. Martinez, C. Hervás, P.A. Gutiérrez, Sensitivity versus accuracy in multiclass problems using memetic pareto evolutionary neural networks. IEEE Trans. Neural Netw. <Emphasis Type="Bold">21</Emphasis>(5), 750–770 (2010)</BibUnstructured>
          </Citation>
          <Citation ID="CR174">
            <CitationNumber>174.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>B</Initials>
                <FamilyName>Rohrer</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>S</Initials>
                <FamilyName>Fasoli</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>HI</Initials>
                <FamilyName>Krebs</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2002</Year>
              <ArticleTitle Language="En">Movement smoothness changes during stroke recovery</ArticleTitle>
              <JournalTitle>J. Neurosci.</JournalTitle>
              <VolumeID>22</VolumeID>
              <IssueID>18</IssueID>
              <FirstPage>8297</FirstPage>
              <LastPage>8304</LastPage>
            </BibArticle>
            <BibUnstructured>B. Rohrer, S. Fasoli, H.I. Krebs et al., Movement smoothness changes during stroke recovery. J. Neurosci. <Emphasis Type="Bold">22</Emphasis>(18), 8297–8304 (2002)</BibUnstructured>
          </Citation>
          <Citation ID="CR175">
            <CitationNumber>175.</CitationNumber>
            <BibUnstructured>D. Biswas, A. Cranny, K. Maharatna et al., Movement fluidity of the impaired arm during stroke rehabilitation, in <Emphasis Type="Italic">Proceedings of IEEE-EMBS International Conference on Biomedical and Health Informatics</Emphasis>, Valencia, 1–4 June 2014, <ExternalRef>
                <RefSource>http://emb.citengine.com/event/bhi-2014/paper-details?pdID = 12</RefSource>
                <RefTarget Address="http://emb.citengine.com/event/bhi-2014/paper-details?pdID%20=%2012" TargetType="URL"/>
              </ExternalRef>. Accessed 24 Mar 2015</BibUnstructured>
          </Citation>
          <Citation ID="CR176">
            <CitationNumber>176.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>D</Initials>
                <FamilyName>Fuentes</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>L</Initials>
                <FamilyName>Gonzalez-Abril</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>C</Initials>
                <FamilyName>Angulo</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2012</Year>
              <ArticleTitle Language="En">Online motion recognition using an accelerometer in a mobile device</ArticleTitle>
              <JournalTitle>Expert Syst. Appl.</JournalTitle>
              <VolumeID>39</VolumeID>
              <IssueID>3</IssueID>
              <FirstPage>2461</FirstPage>
              <LastPage>2465</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1016/j.eswa.2011.08.098</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>D. Fuentes, L. Gonzalez-Abril, C. Angulo et al., Online motion recognition using an accelerometer in a mobile device. Expert Syst. Appl. <Emphasis Type="Bold">39</Emphasis>(3), 2461–2465 (2012)</BibUnstructured>
          </Citation>
          <Citation ID="CR177">
            <CitationNumber>177.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>YJ</Initials>
                <FamilyName>Hong</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>IJ</Initials>
                <FamilyName>Kim</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>SC</Initials>
                <FamilyName>Ahn</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>HG</Initials>
                <FamilyName>Kim</FamilyName>
              </BibAuthorName>
              <Year>2010</Year>
              <ArticleTitle Language="En">Mobile health monitoring system based on activity recognition using accelerometer</ArticleTitle>
              <JournalTitle>Simul. Model. Pract. Theory</JournalTitle>
              <VolumeID>18</VolumeID>
              <IssueID>4</IssueID>
              <FirstPage>446</FirstPage>
              <LastPage>455</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1016/j.simpat.2009.09.002</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>Y.J. Hong, I.J. Kim, S.C. Ahn, H.G. Kim, Mobile health monitoring system based on activity recognition using accelerometer. Simul. Model. Pract. Theory <Emphasis Type="Bold">18</Emphasis>(4), 446–455 (2010)</BibUnstructured>
          </Citation>
          <Citation ID="CR178">
            <CitationNumber>178.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>A</Initials>
                <FamilyName>Fleury</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>M</Initials>
                <FamilyName>Vacher</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>N</Initials>
                <FamilyName>Noury</FamilyName>
              </BibAuthorName>
              <Year>2010</Year>
              <ArticleTitle Language="En">SVM-based multimodal classification of activities of daily living in health smart homes: sensors, algorithms, and first experimental results</ArticleTitle>
              <JournalTitle>IEEE Trans. Inf. Technol. Biomed.</JournalTitle>
              <VolumeID>14</VolumeID>
              <IssueID>2</IssueID>
              <FirstPage>274</FirstPage>
              <LastPage>283</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/TITB.2009.2037317</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>A. Fleury, M. Vacher, N. Noury, SVM-based multimodal classification of activities of daily living in health smart homes: sensors, algorithms, and first experimental results. IEEE Trans. Inf. Technol. Biomed. <Emphasis Type="Bold">14</Emphasis>(2), 274–283 (2010)</BibUnstructured>
          </Citation>
          <Citation ID="CR179">
            <CitationNumber>179.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>J</Initials>
                <FamilyName>Bussmann</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>W</Initials>
                <FamilyName>Martens</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>J</Initials>
                <FamilyName>Tulen</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2001</Year>
              <ArticleTitle Language="En">Measuring daily behaviour using ambulatory accelerometry: the activity monitor</ArticleTitle>
              <JournalTitle>Behav. Res. Methods Instrum. Comput.</JournalTitle>
              <VolumeID>33</VolumeID>
              <IssueID>3</IssueID>
              <FirstPage>349</FirstPage>
              <LastPage>356</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.3758/BF03195388</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>J. Bussmann, W. Martens, J. Tulen et al., Measuring daily behaviour using ambulatory accelerometry: the activity monitor. Behav. Res. Methods Instrum. Comput. <Emphasis Type="Bold">33</Emphasis>(3), 349–356 (2001)</BibUnstructured>
          </Citation>
          <Citation ID="CR180">
            <CitationNumber>180.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>R</Initials>
                <FamilyName>Muscillo</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>M</Initials>
                <FamilyName>Schmid</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>S</Initials>
                <FamilyName>Conforto</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>T</Initials>
                <FamilyName>D’Alessio</FamilyName>
              </BibAuthorName>
              <Year>2011</Year>
              <ArticleTitle Language="En">Early recognition of upper limb motor tasks through accelerometers: real-time implementation of a DTW-based algorithm</ArticleTitle>
              <JournalTitle>Comput. Biol. Med.</JournalTitle>
              <VolumeID>41</VolumeID>
              <IssueID>3</IssueID>
              <FirstPage>164</FirstPage>
              <LastPage>172</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1016/j.compbiomed.2011.01.007</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>R. Muscillo, M. Schmid, S. Conforto, T. D’Alessio, Early recognition of upper limb motor tasks through accelerometers: real-time implementation of a DTW-based algorithm. Comput. Biol. Med. <Emphasis Type="Bold">41</Emphasis>(3), 164–172 (2011)</BibUnstructured>
          </Citation>
          <Citation ID="CR181">
            <CitationNumber>181.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>M</Initials>
                <FamilyName>Ermes</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>J</Initials>
                <FamilyName>Parkka</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>J</Initials>
                <FamilyName>Mantyjarvi</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>I</Initials>
                <FamilyName>Korhonen</FamilyName>
              </BibAuthorName>
              <Year>2008</Year>
              <ArticleTitle Language="En">Detection of daily activities and sports with wearable sensors in controlled and uncontrolled conditions</ArticleTitle>
              <JournalTitle>IEEE Trans. Inf. Technol. Biomed.</JournalTitle>
              <VolumeID>12</VolumeID>
              <IssueID>1</IssueID>
              <FirstPage>20</FirstPage>
              <LastPage>26</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/TITB.2007.899496</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>M. Ermes, J. Parkka, J. Mantyjarvi, I. Korhonen, Detection of daily activities and sports with wearable sensors in controlled and uncontrolled conditions. IEEE Trans. Inf. Technol. Biomed. <Emphasis Type="Bold">12</Emphasis>(1), 20–26 (2008)</BibUnstructured>
          </Citation>
          <Citation ID="CR182">
            <CitationNumber>182.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>CY</Initials>
                <FamilyName>Wu</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>T</Initials>
                <FamilyName>Fu</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>KC</Initials>
                <FamilyName>Lin</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2011</Year>
              <ArticleTitle Language="En">Assessing the streamlined Wolf motor function test as an outcome measure for stroke rehabilitation</ArticleTitle>
              <JournalTitle>Neurorehabil. Neural Repair</JournalTitle>
              <VolumeID>25</VolumeID>
              <IssueID>2</IssueID>
              <FirstPage>194</FirstPage>
              <LastPage>199</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1177/1545968310381249</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>C.Y. Wu, T. Fu, K.C. Lin et al., Assessing the streamlined Wolf motor function test as an outcome measure for stroke rehabilitation. Neurorehabil. Neural Repair <Emphasis Type="Bold">25</Emphasis>(2), 194–199 (2011)</BibUnstructured>
          </Citation>
          <Citation ID="CR183">
            <CitationNumber>183.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>HF</Initials>
                <FamilyName>Chen</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>CY</Initials>
                <FamilyName>Wu</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>KC</Initials>
                <FamilyName>Lin</FamilyName>
              </BibAuthorName>
              <Etal/>
              <Year>2012</Year>
              <ArticleTitle Language="En">Rasch validation of the streamlined Wolf motor function test in people with chronic stroke and subacute stroke</ArticleTitle>
              <JournalTitle>Phys. Ther.</JournalTitle>
              <VolumeID>92</VolumeID>
              <IssueID>8</IssueID>
              <FirstPage>1017</FirstPage>
              <LastPage>1126</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.2522/ptj.20110175</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>H.F. Chen, C.Y. Wu, K.C. Lin et al., Rasch validation of the streamlined Wolf motor function test in people with chronic stroke and subacute stroke. Phys. Ther. <Emphasis Type="Bold">92</Emphasis>(8), 1017–1126 (2012)</BibUnstructured>
          </Citation>
          <Citation ID="CR184">
            <CitationNumber>184.</CitationNumber>
            <BibBook>
              <BibAuthorName>
                <Initials>JL</Initials>
                <FamilyName>Semmlow</FamilyName>
              </BibAuthorName>
              <Year>2008</Year>
              <BookTitle>Bio Signal and Medical Image Processing</BookTitle>
              <EditionNumber>2</EditionNumber>
              <PublisherName>CRC Press</PublisherName>
              <PublisherLocation>Boca Raton</PublisherLocation>
              <ISBN>9780203024058</ISBN>
            </BibBook>
            <BibUnstructured>J.L. Semmlow, <Emphasis Type="Italic">Bio Signal and Medical Image Processing</Emphasis>, 2nd edn. (CRC Press, Boca Raton, 2008). ISBN 9780203024058</BibUnstructured>
          </Citation>
          <Citation ID="CR185">
            <CitationNumber>185.</CitationNumber>
            <BibBook>
              <BibEditorName>
                <Initials>T</Initials>
                <FamilyName>Hastie</FamilyName>
              </BibEditorName>
              <BibEditorName>
                <Initials>R</Initials>
                <FamilyName>Tibshirani</FamilyName>
              </BibEditorName>
              <BibEditorName>
                <Initials>J</Initials>
                <FamilyName>Friedman</FamilyName>
              </BibEditorName>
              <Eds/>
              <Year>2009</Year>
              <BookTitle>The Elements of Statistical Learning: Data Mining, Inference and Prediction</BookTitle>
              <PublisherName>Springer</PublisherName>
              <PublisherLocation>Heidelberg</PublisherLocation>
              <ISBN>978-0-387-84858-7</ISBN>
            </BibBook>
            <BibUnstructured>T. Hastie, R. Tibshirani, J. Friedman (eds.), <Emphasis Type="Italic">The Elements of Statistical Learning: Data Mining, Inference and Prediction</Emphasis> (Springer, Heidelberg, 2009). ISBN 978-0-387-84858-7</BibUnstructured>
          </Citation>
          <Citation ID="CR186">
            <CitationNumber>186.</CitationNumber>
            <BibUnstructured>I.J. Kim, S. Im, E. Hong et al., ADL classification using triaxial accelerometers and rfid, in <Emphasis Type="Italic">Proceedings of the International Workshop on Ubiquitous Convergence Technolog</Emphasis>y, PRC, Beijing, 20–21 Nov 2007</BibUnstructured>
          </Citation>
          <Citation ID="CR187">
            <CitationNumber>187.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>SA</Initials>
                <FamilyName>Lowe</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>G</Initials>
                <FamilyName>ÓLaighin</FamilyName>
              </BibAuthorName>
              <Year>2014</Year>
              <ArticleTitle Language="En">Monitoring human health behaviour in one’s living environment: a technological review</ArticleTitle>
              <JournalTitle>Med. Eng. Phys.</JournalTitle>
              <VolumeID>36</VolumeID>
              <IssueID>2</IssueID>
              <FirstPage>147</FirstPage>
              <LastPage>168</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1016/j.medengphy.2013.11.010</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>S.A. Lowe, G. ÓLaighin, Monitoring human health behaviour in one’s living environment: a technological review. Med. Eng. Phys. <Emphasis Type="Bold">36</Emphasis>(2), 147–168 (2014)</BibUnstructured>
          </Citation>
          <Citation ID="CR188">
            <CitationNumber>188.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>T</Initials>
                <FamilyName>Warren Liao</FamilyName>
              </BibAuthorName>
              <Year>2005</Year>
              <ArticleTitle Language="En">Clustering of time series data - a survey</ArticleTitle>
              <JournalTitle>Pattern Recognition</JournalTitle>
              <VolumeID>38</VolumeID>
              <IssueID>11</IssueID>
              <FirstPage>1857</FirstPage>
              <LastPage>1874</LastPage>
              <Occurrence Type="ZLBID">
                <Handle>1077.68803</Handle>
              </Occurrence>
              <Occurrence Type="DOI">
                <Handle>10.1016/j.patcog.2005.01.025</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>T. Warren Liao, Clustering of time series data - a survey. Pattern Recognition <Emphasis Type="Bold">38</Emphasis>(11), 1857–1874 (2005)</BibUnstructured>
          </Citation>
          <Citation ID="CR189">
            <CitationNumber>189.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>J</Initials>
                <FamilyName>Mao</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>AK</Initials>
                <FamilyName>Jain</FamilyName>
              </BibAuthorName>
              <Year>1996</Year>
              <ArticleTitle Language="En">A self-organizing network for hyper ellipsoidal clustering (HEC)</ArticleTitle>
              <JournalTitle>IEEE Trans. Neural Netw.</JournalTitle>
              <VolumeID>7</VolumeID>
              <IssueID>1</IssueID>
              <FirstPage>16</FirstPage>
              <LastPage>29</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/72.478389</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>J. Mao, A.K. Jain, A self-organizing network for hyper ellipsoidal clustering (HEC). IEEE Trans. Neural Netw. <Emphasis Type="Bold">7</Emphasis>(1), 16–29 (1996)</BibUnstructured>
          </Citation>
        </Bibliography>
      </ChapterBackmatter>
    </Chapter>
  </Book>
</Publisher>
