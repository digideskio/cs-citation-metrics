<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE Publisher PUBLIC "-//Springer-Verlag//DTD A++ V2.4//EN" "http://devel.springer.de/A++/V2.4/DTD/A++V2.4.dtd">
<Publisher>
  <PublisherInfo>
    <PublisherName>Springer US</PublisherName>
    <PublisherLocation>Boston, MA</PublisherLocation>
    <PublisherImprintName>Springer</PublisherImprintName>
  </PublisherInfo>
  <Series>
    <SeriesInfo ID="Series_1" SeriesType="Series" TocLevels="0">
      <SeriesID>8766</SeriesID>
      <SeriesTitle Language="En">Handbooks in Health, Work, and Disability</SeriesTitle>
      <SeriesAbbreviatedTitle>Handbooks in Health, Work, and Disability</SeriesAbbreviatedTitle>
    </SeriesInfo>
    <Book Language="En" OutputMedium="All">
      <BookInfo BookProductType="Handbook" ContainsESM="No" Language="En" MediaType="eBook" NumberingStyle="ChapterOnly" OutputMedium="All" TocLevels="0">
        <BookID>978-1-4614-4839-6</BookID>
        <BookTitle>Handbook of Occupational Health and Wellness</BookTitle>
        <BookSequenceNumber>1</BookSequenceNumber>
        <BookDOI>10.1007/978-1-4614-4839-6</BookDOI>
        <BookTitleID>215798</BookTitleID>
        <BookPrintISBN>978-1-4614-4838-9</BookPrintISBN>
        <BookElectronicISBN>978-1-4614-4839-6</BookElectronicISBN>
        <BookEdition>2012</BookEdition>
        <BookChapterCount>26</BookChapterCount>
        <BookCopyright>
          <CopyrightHolderName>Springer Science+Business Media New York</CopyrightHolderName>
          <CopyrightYear>2012</CopyrightYear>
        </BookCopyright>
        <BookSubjectGroup>
          <BookSubject Code="SCY" Type="Primary">Psychology</BookSubject>
          <BookSubject Code="SCY12020" Priority="1" Type="Secondary">Health Psychology</BookSubject>
          <BookSubject Code="SCH42001" Priority="2" Type="Secondary">Occupational Medicine/Industrial Medicine</BookSubject>
          <BookSubject Code="SCH27002" Priority="3" Type="Secondary">Public Health</BookSubject>
          <BookSubject Code="SCH55006" Priority="4" Type="Secondary">Rehabilitation</BookSubject>
          <SubjectCollection Code="SUCO41168">Behavioral Science and Psychology</SubjectCollection>
        </BookSubjectGroup>
        <BookContext>
          <SeriesID>8766</SeriesID>
        </BookContext>
      </BookInfo>
      <BookHeader>
        <EditorGroup>
          <Editor AffiliationIDS="AffID1">
            <EditorName DisplayOrder="Western">
              <GivenName>Robert J.</GivenName>
              <FamilyName>Gatchel</FamilyName>
            </EditorName>
            <Contact>
              <Phone>817-272-2541</Phone>
              <Email>gatchel@uta.edu</Email>
            </Contact>
          </Editor>
          <Editor AffiliationIDS="AffID2">
            <EditorName DisplayOrder="Western">
              <GivenName>Izabela Z.</GivenName>
              <FamilyName>Schultz</FamilyName>
            </EditorName>
            <Contact>
              <Phone>604-822-5251</Phone>
              <Email>ischultz@telus.net</Email>
            </Contact>
          </Editor>
          <Affiliation ID="AffID1">
            <OrgDivision>, Department of Psychology</OrgDivision>
            <OrgName>University of Texas at Arlington</OrgName>
            <OrgAddress>
              <Street>S. Nedderman Drive 501</Street>
              <City>Arlington</City>
              <Postcode>76019-0528</Postcode>
              <State>Texas</State>
              <Country>USA</Country>
            </OrgAddress>
          </Affiliation>
          <Affiliation ID="AffID2">
            <OrgDivision>, Dept of Educational and Counseling Psych</OrgDivision>
            <OrgName>University of British Columbia</OrgName>
            <OrgAddress>
              <Street>Main Hall 2125</Street>
              <City>Vancouver</City>
              <Postcode>V6T 1Z4</Postcode>
              <State>British Columbia</State>
              <Country>Canada</Country>
            </OrgAddress>
          </Affiliation>
        </EditorGroup>
      </BookHeader>
      <Part ID="Part5" OutputMedium="All">
        <PartInfo OutputMedium="All" TocLevels="0">
          <PartID>5</PartID>
          <PartSequenceNumber>5</PartSequenceNumber>
          <PartTitle>RESEARCH, EVALUATION, DIVERSITY &amp; PRACTICE</PartTitle>
          <PartChapterCount>5</PartChapterCount>
          <PartContext>
            <SeriesID>8766</SeriesID>
            <BookID>978-1-4614-4839-6</BookID>
            <BookTitle>Handbook of Occupational Health and Wellness</BookTitle>
          </PartContext>
        </PartInfo>
        <Chapter ID="b978-1-4614-4839-6_23" Language="En" OutputMedium="All">
          <ChapterInfo ChapterType="OriginalPaper" ContainsESM="No" Language="En" NumberingStyle="ChapterOnly" OutputMedium="All" TocLevels="0">
            <ChapterID>23</ChapterID>
            <ChapterNumber>23</ChapterNumber>
            <ChapterDOI>10.1007/978-1-4614-4839-6_23</ChapterDOI>
            <ChapterSequenceNumber>23</ChapterSequenceNumber>
            <ChapterTitle Language="En">Program Evaluation of Prevention and Intervention Methods</ChapterTitle>
            <ChapterFirstPage>495</ChapterFirstPage>
            <ChapterLastPage>511</ChapterLastPage>
            <ChapterCopyright>
              <CopyrightHolderName>Springer Science+Business Media New York</CopyrightHolderName>
              <CopyrightYear>2012</CopyrightYear>
            </ChapterCopyright>
            <ChapterHistory>
              <RegistrationDate>
                <Year>2012</Year>
                <Month>7</Month>
                <Day>4</Day>
              </RegistrationDate>
              <OnlineDate>
                <Year>2012</Year>
                <Month>11</Month>
                <Day>19</Day>
              </OnlineDate>
            </ChapterHistory>
            <ChapterContext>
              <SeriesID>8766</SeriesID>
              <PartID>5</PartID>
              <BookID>978-1-4614-4839-6</BookID>
              <BookTitle>Handbook of Occupational Health and Wellness</BookTitle>
            </ChapterContext>
          </ChapterInfo>
          <ChapterHeader>
            <AuthorGroup>
              <Author AffiliationIDS="Aff00231" CorrespondingAffiliationID="Aff00231">
                <AuthorName DisplayOrder="Western">
                  <GivenName>Richard</GivenName>
                  <GivenName>C.</GivenName>
                  <FamilyName>Robinson</FamilyName>
                  <Degrees>Ph.D.</Degrees>
                </AuthorName>
                <Contact>
                  <Email>richard.robinson11@gmail.com</Email>
                </Contact>
              </Author>
              <Author AffiliationIDS="Aff00232">
                <AuthorName DisplayOrder="Western">
                  <GivenName>John</GivenName>
                  <GivenName>P.</GivenName>
                  <FamilyName>Garofalo</FamilyName>
                  <Degrees>PhD</Degrees>
                </AuthorName>
                <Role>Associate Chair and Associate Professor</Role>
              </Author>
              <Author AffiliationIDS="Aff00233">
                <AuthorName DisplayOrder="Western">
                  <GivenName>Pamela</GivenName>
                  <FamilyName>Behnk</FamilyName>
                  <Degrees>MPH</Degrees>
                </AuthorName>
              </Author>
              <Affiliation ID="Aff00231">
                <OrgName>The University of Texas Southwestern Medical Center at Dallas</OrgName>
                <OrgAddress>
                  <Street>5323 Harry Hines Blvd</Street>
                  <City>Dallas</City>
                  <State>Texas</State>
                  <Postcode>75390</Postcode>
                  <Country>USA</Country>
                </OrgAddress>
              </Affiliation>
              <Affiliation ID="Aff00232">
                <OrgName>Washington State University</OrgName>
                <OrgAddress>
                  <Street>14204 NE Salmon Creek Ave VCLS208</Street>
                  <City>Vancouver</City>
                  <State>WA</State>
                  <Postcode>98686</Postcode>
                  <Country>USA</Country>
                </OrgAddress>
              </Affiliation>
              <Affiliation ID="Aff00233">
                <OrgName>University of Kansas School of Medicine</OrgName>
                <OrgAddress>
                  <Street>1010 North Kansas</Street>
                  <City>Witchita</City>
                  <State>KS</State>
                  <Postcode>67214</Postcode>
                  <Country>USA</Country>
                </OrgAddress>
              </Affiliation>
            </AuthorGroup>
            <Abstract ID="Abs00231" Language="En" OutputMedium="Online">
              <Heading>Abstract</Heading>
              <Para TextBreak="No">As Americans continue to grow ever sicker from chronic illnesses, such as heart disease, cancer, and stroke, the need for comprehensive interventions has also grown (Kung, Hoyert, Xu, &amp; Murphy, 2008). An estimated 70 % of deaths each year are attributable to chronic illness, with heart disease, cancer, and stroke accounting for 50 % of those deaths (Kung et al., 2008). Americans spend a large majority of their time in the workplace and, as can be seen in other chapters of this Handbook, that fact has made occupational settings an ideal place for prevention and health intervention programs for many years (Haskell &amp; Blair, 1980). However, the effectiveness of these programs has to be measured and evaluated if they are to survive. The importance of <Emphasis Type="Italic">program evaluation</Emphasis>, therefore, cannot be underestimated. The purview of <Emphasis Type="Italic">program evaluation</Emphasis> exists within the social sciences, but also within the private business sector and governmental programs. In fact, The Health Communication Unit at the University of Toronto (2007) goes as far as to define a program as follows: “…any group of related complementary activities intended to achieve specific outcomes or results. For example, community gardens, shopping skill classes and health cooking demonstrations…” (p. 5). Furthermore, they define a program evaluation as, “…the systematic gathering, analysis and reporting of data about a program to assist decision making” (p. 6).</Para>
            </Abstract>
          </ChapterHeader>
          <Body>
            <Section1 ID="Sec00231">
              <Heading>Overview</Heading>
              <Para TextBreak="No">As Americans continue to grow ever sicker from chronic illnesses, such as heart disease, cancer, and stroke, the need for comprehensive interventions has also grown (Kung, Hoyert, Xu, &amp; Murphy, <CitationRef CitationID="CR002331">2008</CitationRef>). An estimated 70% of deaths each year are attributable to chronic illness, with heart disease, cancer, and stroke accounting for 50% of those deaths (Kung et al., <CitationRef CitationID="CR002331">2008</CitationRef>). Americans spend a large majority of their time in the workplace and, as can be seen in other chapters of this Handbook, that fact has made occupational settings an ideal place for prevention and health intervention programs for many years (Haskell &amp; Blair, <CitationRef CitationID="CR002324">1980</CitationRef>). However, the effectiveness of these programs has to be measured and evaluated if they are to survive. The importance of <Emphasis Type="Italic">program evaluation</Emphasis>, therefore, cannot be underestimated. The purview of <Emphasis Type="Italic">program evaluation</Emphasis> exists within the social sciences, but also within the private business sector and governmental programs. In fact, The Health Communication Unit at the University of Toronto (<CitationRef CitationID="CR002355">2007</CitationRef>) goes as far as to define a program as follows: “…any group of related complementary activities intended to achieve specific outcomes or results. For example, community gardens, shopping skill classes and health cooking demonstrations…” (p. 5). Furthermore, they define a program evaluation as, “…the systematic gathering, analysis and reporting of data about a program to assist decision making” (p. 6).</Para>
              <Para TextBreak="No">
                <Emphasis Type="Italic">Program evaluation</Emphasis> consists of a trajectory of methods for collecting, assessing, and evaluating outcomes of interventions; in this case, applied in the workplace to improve health and wellness (Valente, <CitationRef CitationID="CR002356">2002</CitationRef>). <Emphasis Type="Italic">Program evaluation</Emphasis> allows interventions that prove ineffectual, or deliver results through unsustainable costs, to be modified or eliminated. In essence, <Emphasis Type="Italic">program evaluation</Emphasis> makes good programs stronger and bad programs crumble. Table <InternalRef RefID="Tab00231">23.1</InternalRef> provides a summary of the benefits of <Emphasis Type="Italic">program evaluation</Emphasis>.<Table Float="Yes" ID="Tab00231">
                  <Caption Language="En">
                    <CaptionNumber>Table 23.1</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>Importance of evaluating health promotion programs (Valente, <CitationRef CitationID="CR002356">2002</CitationRef>)</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <tgroup cols="1">
                    <colspec colname="c1" colnum="1"/>
                    <tbody>
                      <row>
                        <entry colname="c1">
                          <SimplePara>•Improves chance of developing an effective program</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry colname="c1">
                          <SimplePara>•Allows researchers and organizations to understand the effect of intervention</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry colname="c1">
                          <SimplePara>•Evaluation plan encourages planners to establish clear goals</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry colname="c1">
                          <SimplePara>•Allows researchers and organizations to understand how a program worked</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry colname="c1">
                          <SimplePara>•Information from evaluation can be used for future planning</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry colname="c1">
                          <SimplePara>•Provides an opportunity for research on human behavior</SimplePara>
                        </entry>
                      </row>
                    </tbody>
                  </tgroup>
                </Table>
              </Para>
            </Section1>
            <Section1 ID="Sec00232">
              <Heading>Research Methods/Concepts</Heading>
              <Para TextBreak="No">Federal agencies played an instrumental role in developing the concept of program evaluation by applying the scientific method to evaluate the outcomes of educational and public health initiatives (Rossi &amp; Freeman, <CitationRef CitationID="CR002346">1993</CitationRef>). Shadish, Cook, and Leviton (<CitationRef CitationID="CR002351">1991</CitationRef>) described the three stages that program evaluation has undergone since World War II. The <Emphasis Type="Italic">first stage</Emphasis> emphasized a stringent application of the scientific method to program evaluation. The <Emphasis Type="Italic">second stage</Emphasis> focused on the inherent barriers and limitations to program evaluation, using a traditional scientific approach and emphasized the development of other methodologies. An emphasis was placed on more observational and informal methods of assessment, but these methods were not without structure. The final, and current, <Emphasis Type="Italic">third stage</Emphasis> attempted to synthesize the first two stages by employing multiple theories and techniques, as well as considering the context of the program (Shadish et al., <CitationRef CitationID="CR002351">1991</CitationRef>). For instance, empirically-sound research designs and valid outcome measures remain useful in this third stage, but an appreciation for stakeholder feedback has been seen as crucial for the successful implementation of health promotion programs.</Para>
              <Para TextBreak="No">Valente (<CitationRef CitationID="CR002356">2002</CitationRef>) cogently describes the difficulty with research design in program evaluation: “The central challenge to designing studies is that they must be rigorous enough to make conclusions about program impact, yet face constraints of time, resources, program staff, and the willingness and protection of human subjects” (p. 88). For a program to be effective, a change in perception, attitude, knowledge, behavior, or health outcome must occur. Research designs are intended to provide evidence for “causality” (Henry, <CitationRef CitationID="CR002325">2010</CitationRef>). In the area of occupational health psychology, an evaluator wants to demonstrate that a program caused, or contributed to, a positive health-related change (Valente, <CitationRef CitationID="CR002356">2002</CitationRef>).</Para>
              <Para TextBreak="No">As mentioned, historically, rigorous quantitative research methods were employed for program evaluation. <Emphasis Type="Italic">Quantitative</Emphasis> research involves the systematic examination of an independent variable upon a dependent variable or variables (Creswell, <CitationRef CitationID="CR002315">2003</CitationRef>). For instance, when attempting to decrease heart disease, a health intervention may randomize individuals into health and unhealthy diets (independent variable) and a measure of heart disease would be the dependent variable. It is understandable how practical limitations of a purely quantitative approach led to the development and use of qualitative methodology in program evaluation. <Emphasis Type="Italic">Qualitative</Emphasis> methods are frequently used in the social sciences, but are often poorly understood by business organizations (Denzin &amp; Lincoln, <CitationRef CitationID="CR002316">2000</CitationRef>). A qualitative approach relies upon more naturalistic and less structured data-gathering techniques, but does not lack systematic observation and coding (Creswell, <CitationRef CitationID="CR002315">2003</CitationRef>). Involvement of the stakeholders through the widely-used qualitative methods of interviews or focus groups at the initial, middle, and end-phases of program evaluation increases the likelihood of acceptance and positive results (Bryson &amp; Patton, <CitationRef CitationID="CR00238">2010</CitationRef>). Unfortunately, focus groups and interviews, if haphazardly applied, face the risk of collecting information in an unsystematic way, and the results will offer limited value (Holliday, <CitationRef CitationID="CR002326">2007</CitationRef>).</Para>
              <Para TextBreak="No">Differences exist between program evaluation and other traditional forms of research in the social sciences. In addition to understanding differences between qualitative and quantitative research, an understanding of research methods in general is necessary to understand the complexities of program evaluation. First, a distinction must be made between <Emphasis Type="Italic">efficacy</Emphasis> and <Emphasis Type="Italic">effectiveness</Emphasis> studies. An intervention is said to be “efficacious” if it produces desired results within the context of a controlled ­environment in which error variance is minimized. The term “effectiveness” refers to the ­implementation of an intervention in a real-world environment. However, systematic controls are in place within an effectiveness trial, but the ability to control extraneous variables is minimal when compared to an efficacy trial. As can be seen, program evaluation is more accurately viewed as an effectiveness trial rather than a process that leads to the determination of efficacy (Hallfors et al., <CitationRef CitationID="CR002322">2006</CitationRef>). In other words, the general evaluation of the efficacy and effectiveness of interventions in the workplace relies on systematic observation and quantitative, as well as qualitative, research, and analytic methods.</Para>
              <Para TextBreak="No">Study validity is defined as the accuracy of the evaluation of the program’s impact. A further ­distinction can be made between external and internal validity. <Emphasis Type="Italic">External validity</Emphasis> is closely tied to the concept of “generalizability.” A study is determined to be externally valid and generalizable if it can be applied to a larger population. Generalizability is determined by the context of the intervention and the sample drawn upon for the evaluation (Valente, <CitationRef CitationID="CR002356">2002</CitationRef>). For instance, a program to improve sleep hygiene among a large, culturally diverse sample of United States (US) based factory employees would likely be generalizable to other factory shift workers in the United States. An intervention that targets Laotian immigrants, while important, would not be considered as generalizable and would have lower external validity. A study’s <Emphasis Type="Italic">internal validity</Emphasis> refers to a researcher’s confidence that the study evaluates what it intended to evaluate, and that the results are accurate in that the manipulation of one variable resulted in the change of another. Numerous threats to internal validity exist. First, uncontrollable events or factors can threaten internal validity (Valente, <CitationRef CitationID="CR002356">2002</CitationRef>). By definition, these factors cannot be controlled, only monitored and statistically controlled for at a later date. For instance, a program designed to change attitudes regarding stress management may be impacted by the sudden departure of a manager. Maturation or development of a subject over time is another potential threat to internal validity, but is typically of minimal concern for health promotion interventions, which are usually limited in time (Valente, <CitationRef CitationID="CR002356">2002</CitationRef>).</Para>
              <Para TextBreak="No">Internal threats to validity that can be controlled include <Emphasis Type="Italic">testing</Emphasis>, <Emphasis Type="Italic">instrumentation</Emphasis>, and <Emphasis Type="Italic">sensitization</Emphasis>. With regard to <Emphasis Type="Italic">testing</Emphasis>, certain practice effects can impact the true meaning of test results when testing is given in a repeated manner. Alternative forms of a test or use of a control groups diminish this threat to validity. <Emphasis Type="Italic">Instrumentation</Emphasis> or the impact that observation has on outcomes should be considered (Valente, <CitationRef CitationID="CR002356">2002</CitationRef>). Asking employees to simply record the number of vegetables they eat daily may be overly burdensome and unreliable and can impact the results. Finally, <Emphasis Type="Italic">sensitization</Emphasis> is the impact that the knowledge of the intervention will have on the baseline findings (Valente, <CitationRef CitationID="CR002356">2002</CitationRef>). For example, if a company announces a program to increase exercise, an employee may begin to exercise prior to the program.</Para>
              <Section2 ID="Sec00233">
                <Heading>Statistics Used in Program Evaluation</Heading>
                <Para TextBreak="No">The application and correct use of statistics can appear to be a daunting task for many organizations. However, the understanding of basic statistical concepts can strengthen the validity of the program evaluation. Furthermore, a facile understanding of methodology and research design will allow program evaluators to more effectively communicate and generate support from stakeholders and managers as to the importance of certain program elements necessary for program evaluation. Therefore, a brief review of basic statistical concepts is warranted. Consideration of the data generated by the program evaluation influences the choice of statistics required. Data can be classified into two broad groups—categorical or continuous. Categorical variables have a limited number of response values, while continuous variables have a potentially infinite number of values and equal intervals between values. Further classification of data results in three broad categories of levels of measurement: nominal, ordinal, and interval-ratio. Nominal data are categorical and values are unordered or equivalent, such as gender or race. Ordinal data are also categorical, but values exist in relationship with one another in a ranked fashion (Stevens, <CitationRef CitationID="CR002353">1946</CitationRef>). For instance, an ordinal scale may ask employees how often they exercise with choices as <Emphasis Type="Italic">never</Emphasis>, <Emphasis Type="Italic">rarely</Emphasis>, <Emphasis Type="Italic">sometimes</Emphasis>, and <Emphasis Type="Italic">often</Emphasis>. Demographic data may also be ordinal, such as socioeconomic status, which does not reflect a greater inherent value, but a rank-order value within the dataset. Finally, interval-ratio data are continuous, ranked, and equidistant between values. Although there is a distinction between interval and ratio data, (i.e., ratio data have a true-zero and interval data do not), statistically they are treated essentially the same (Valente, <CitationRef CitationID="CR002356">2002</CitationRef>).</Para>
                <Para TextBreak="No">Choice of independent and dependent variables also influence the selection of statistics to be utilized. As mentioned, independent variables are considered to influence the dependent variable and be uninfluenced by other variables (Given, <CitationRef CitationID="CR002318">2008</CitationRef>). Program evaluators <Emphasis Type="Italic">hypothesize</Emphasis> that a program (the independent variable) will result in a change in an attitude, knowledge, or behavior in a health outcome (the dependent variable). More sophisticated hypotheses regarding behavior change also take into consideration moderating and mediating variables. <Emphasis Type="Italic">Moderating variables</Emphasis> weaken/strengthen the relationship between independent and dependent variables (Baron &amp; Kenny, <CitationRef CitationID="CR00235">1986</CitationRef>). Gym membership (moderating variable) may strengthen the relationship between a work program to increase exercise (independent variable) and number of hours exercising a week (dependent variable.) <Emphasis Type="Italic">Mediating variables</Emphasis> play a more direct role in the relationship between the independent and dependent variables, and typically occur between the independent and dependent variable (Baron &amp; Kenny, <CitationRef CitationID="CR00235">1986</CitationRef>). In our last example, gym membership existed prior to the independent variable. An onsite gym added to our fictitious organization, as part of a campaign to increase exercise, would be considered a mediating variable. The measures used to assess outcomes (dependent variables) must also be psychometrically-sound in order to begin to establish causality and impact. A measure is <Emphasis Type="Italic">reliable</Emphasis> if it consistently measures what it intends to measure. Although related to accuracy, reliability is synonymous with consistency. Validity refers to the extent that a measure assesses what it purports to assess. Validity is impacted by a measures reliability, or consistency, but is best understood as being synonymous with accuracy (Lambert et al., <CitationRef CitationID="CR002333">1996</CitationRef>).</Para>
                <Para TextBreak="No">In addition to reliability and validity, the concepts of <Emphasis Type="Italic">sensitivity</Emphasis> and <Emphasis Type="Italic">specificity</Emphasis> should also be considered. A measure is <Emphasis Type="Italic">sensitive</Emphasis> if it accurately measures the proportion of people who are positive for a state or trait. For instance, a measure of anxiety correctly identifies all those who are anxious. <Emphasis Type="Italic">Specificity</Emphasis> refers to the percentage of negatives that are accurately identified and, in our example, a measure with high specificity would have a low number of individuals who would score in the anxious range when not anxious. As can be seen, there is a balance between sensitivity and specific for most measures (Altman &amp; Bland, <CitationRef CitationID="CR00231">1994</CitationRef>).</Para>
              </Section2>
            </Section1>
            <Section1 ID="Sec00234">
              <Heading>Types of Program Evaluation</Heading>
              <Para TextBreak="No">Prior to implementation of an occupational health intervention, an assessment period is needed to increase the likelihood that an impactful and cost-effective intervention can be implemented. Rossi, Lipsey, and Freeman (<CitationRef CitationID="CR002347">2004</CitationRef>) highlight the different types of assessment that can be offered at different stages of the process. These assessments include the following: (1) evaluating the program’s cost and efficiency; (2) evaluating the program’s outcome; (3) examining the fidelity of program implementation; (4) assessment of program design and theory; and (5) assessment of the need for the program. Jacobs (<CitationRef CitationID="CR002329">2003</CitationRef>) developed a similar <Emphasis Type="Italic">Five-Tiered Approach</Emphasis> to program evaluation. The five tiers are described as incremental in this approach, and include the following: (1) needs assessment; (2) monitoring and accountability; (3) quality review and program clarification; (4) achieving outcomes; and (5) establishing impact.</Para>
              <Para TextBreak="No">Potter (<CitationRef CitationID="CR002343">2006</CitationRef>) also describes three paradigms regarding program evaluation, which the author refers to as the <Emphasis Type="Italic">positivist</Emphasis>, <Emphasis Type="Italic">interpretive</Emphasis>, and <Emphasis Type="Italic">critical-emancipatory</Emphasis> approaches. The <Emphasis Type="Italic">positivist approach</Emphasis> appears to be the most common, and relies on objective results derived primarily from quantitative data. Unfortunately, while ideal in the mind of many social scientists, the reality of the workplace and its naturally occurring obstacles may limit the effective implementation of a positivist design. The <Emphasis Type="Italic">interpretive approach</Emphasis> relies heavily on the perspectives and expectation of the stakeholders involved in the intervention. Emphasis is placed on more observation and information derived from qualitative methods, such as focus groups. The <Emphasis Type="Italic">critical-emancipatory approach</Emphasis> is a more ideologically-driven approach to program evaluation, and it is heavily influenced by social activism (Potter, <CitationRef CitationID="CR002343">2006</CitationRef>). The positivist and interpretive approaches appear most suited for program evaluation in occupational health promotion programs.</Para>
              <Para TextBreak="No">Program evaluation, within both the social sciences and business consulting literatures, are replete with steps and guidelines for its development. Traditionally, program evaluation can be separated into formative evaluations, process evaluations, and summative evaluations (Valente, <CitationRef CitationID="CR002356">2002</CitationRef>; see Fig. <InternalRef RefID="Fig00231">23.1</InternalRef>). Many organizations equate the summative evaluation with program evaluation, or evaluating the impact of a program once it has been completed. As will be seen, program evaluation is critical in all stages of the planning, implementation, and summation of a program.<Figure Category="Standard" Float="Yes" ID="Fig00231">
                  <Caption Language="En">
                    <CaptionNumber>Fig. 23.1</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>Types of program evaluation</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <MediaObject ID="MO00231">
                    <ImageObject Color="BlackWhite" FileRef="MediaObjects/215798_1_En_23_Fig1_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </MediaObject>
                </Figure>
              </Para>
              <Section2 ID="Sec00235">
                <Heading>Formative Evaluation</Heading>
                <Para TextBreak="No">Formative evaluation refers to the evaluation of steps needed to undertake an effective program. Information is needed regarding the population, work-environment, and culture in which a program will be enacted. A review of the literature on the topic of interest, and a careful consideration of the theoretical basis for the intervention, are also required. Furthermore, area experts and potential ­stakeholders should be convened to further assess the problem area to be addressed in order to ensure that the problem is significant (Bartholomew, Parcel, Kok, &amp; Gottlieb, <CitationRef CitationID="CR00236">2001</CitationRef>). Often, a needs assessment is undertaken, as well as pretesting of materials, tests, or technologies to be employed (Itschuld &amp; Kumar, <CitationRef CitationID="CR002328">2010</CitationRef>). Programs fail for many reasons, but beginning with the end in mind, and understanding that program evaluation is needed from start to finish, increases the chance of program ­success. In other words, an appreciation for the trajectory of program evaluation and the questions that are being asked from this process are paramount.</Para>
                <Para TextBreak="No">Planning and implementation of health interventions in the workplace should include program evaluation at the outset. Decisions about the need for a program, scope of a problem, and ways in which a problem will be addressed should include consideration of evaluation along each step of the way. An exciting idea to improve health, but lacking a systematic way to assess the outcomes, is doomed to a relatively brief life-span. Furthermore, adding outcomes after an intervention has been planned or implemented will either interfere with the potential effectiveness of the program or provide uninterpretable outcomes.</Para>
                <Para TextBreak="No">Formative evaluations often rely heavily on qualitative, rather than quantitative, methods and techniques (Huhta, <CitationRef CitationID="CR002327">2010</CitationRef>). As discussed, qualitative data are gathered in a more observational, naturalistic, and unstructured way compared to quantitative data. As one would imagine, understanding the needs and wishes of the employees is critical to designing an effective program, and can often be more effectively gathered through qualitative techniques such as in-depth interviews and focus groups (Armenakis, Harris, &amp; Mossholder, <CitationRef CitationID="CR00232">1993</CitationRef>). Although less structured than quantitative research, qualitative research requires a systematic approach. Miles and Huberman (<CitationRef CitationID="CR002339">1994</CitationRef>) argue for the following criteria of strong qualitative research:
<UnorderedList Mark="Bullet">
                    <ItemContent>
                      <Para TextBreak="No">
                        <Emphasis Type="Italic">Systematic</Emphasis>. The same processes and procedures are delineated and applied to multiple settings.</Para>
                    </ItemContent>
                    <ItemContent>
                      <Para TextBreak="No">
                        <Emphasis Type="Italic">Iterative</Emphasis>. Data collection is ongoing and builds upon previous findings.</Para>
                    </ItemContent>
                    <ItemContent>
                      <Para TextBreak="No">
                        <Emphasis Type="Italic">Flexible</Emphasis>. Adaptations are made as more information is collected.</Para>
                    </ItemContent>
                    <ItemContent>
                      <Para TextBreak="No">
                        <Emphasis Type="Italic">Triangulate</Emphasis>. Use of multiple (at least three) methods of measurement.</Para>
                    </ItemContent>
                  </UnorderedList>
                </Para>
                <Para TextBreak="No">As part of a formative evaluation, the overall impact of the assessment needs to be addressed and entails answering the question, “What impact will have intervention have on the population of interest?” For instance, improving sleep and decreasing fatigue among emergency responders has the potential of having a significant overall impact. However, decreasing the risk of skin cancer among shift workers, while an important health outcome, would be judged to have a relatively low overall impact.</Para>
                <Para TextBreak="No">Although many observational and interview formats are considered qualitative research, <Emphasis Type="Italic">focus groups</Emphasis> provide a unique opportunity to efficiently collect reliable, valid, qualitative data. In a classic work, Merton and colleagues (<CitationRef CitationID="CR002338">1956</CitationRef>) developed the basic principles of focus-group testing that still guides researchers today. An ideal focus group should be comprised of 8–10 similar participants, and potential attrition (cancelation by participants) requires recruiting more subjects than needed (Krueger, <CitationRef CitationID="CR002330">1994</CitationRef>). A trained moderator asks scripted, but open-ended, questions in a manner that allows for the open exchange of ideas, values, and preferences for the potential program. The results of the focus group discussion can be recorded, transcribed, and coded through various techniques (e.g., word-clouds or concept mapping). Valente (<CitationRef CitationID="CR002356">2002</CitationRef>), though, describes three ways in which focus groups may produce misleading information. First, the group discussion may be overly influenced by a subset of individuals. Second, individuals may be reluctant to diverge from what is considered normal. Finally, the topic may lack interest to the participants and, thus, limited information is gathered (Valente, <CitationRef CitationID="CR002356">2002</CitationRef>).</Para>
                <Para TextBreak="No">Over the last two decades, interactive, internet-based interventions have been developed to improve health in occupational settings (Chumley-Jones, Dobbie, &amp; Alfor, <CitationRef CitationID="CR002312">2002</CitationRef>). Multiple platforms have evolved through the last two decades, from CD-ROMS, to Web-based to applications for smart phones. However, with technologically advanced health promotion delivery systems, come additional ­opportunities for formative evaluations. Specifically, <Emphasis Type="Italic">usability testing</Emphasis> has developed as a qualitative technique to evaluate the design, ease-of-use, and acceptability of internet-based interventions, as well as other computer interfaces and devices with potential end-users (Nielsen, <CitationRef CitationID="CR002340">1994</CitationRef>). Usability testing is atheoretical and, thus, a <Emphasis Type="Italic">black box</Emphasis> evaluation that focuses upon efficiency, accuracy, recall, and emotional response (Nielsen, <CitationRef CitationID="CR002340">1994</CitationRef>). Virzi (<CitationRef CitationID="CR002357">1992</CitationRef>) established that a sample size of 5 users was sufficient to identify potential problems with usability during initial stages of testing.</Para>
                <Para TextBreak="No">The theoretical basis of the intervention is needed to assess whether the intervention is achieving results in the manner expected. For example, simple educational methods to improve health and wellness have underperformed compared to other models, such as the <Emphasis Type="Italic">Health Belief Model</Emphasis> (Nutbeam, <CitationRef CitationID="CR002341">2000</CitationRef>). The theoretical model also guides the determination of outcomes to be chosen to examine. However, certain health outcomes occur distally and are affected by a variety of factors. A clear, sound theoretical model allows for the assessment of shorter-term moderating and mediator variables. For instance, exercise has been shown to decrease risk of heart disease. Prospectively following an intervention over a long period of time in order to determine which employees have a lower incidence of heart disease is impractical, but assessing degree of knowledge and increased exercise are noteworthy outcomes.</Para>
                <Para TextBreak="No">In the past, interventionists would design a program and independent researchers would evaluate its impact and effectiveness (Valente, <CitationRef CitationID="CR002356">2002</CitationRef>). This process helped to promote evaluator objectivity, and is referred to as <Emphasis Type="Italic">black box evaluation</Emphasis> (Chen &amp; Rossi, <CitationRef CitationID="CR002311">1983</CitationRef>). This approach appears to have been largely abandoned and replaced by <Emphasis Type="Italic">theory-based evaluations</Emphasis> (Valente, <CitationRef CitationID="CR002356">2002</CitationRef>). Valente (<CitationRef CitationID="CR002356">2002</CitationRef>) goes so far as to argue that theory influences all aspects of program evaluation. Table <InternalRef RefID="Tab00232">23.2</InternalRef> provides a list of major theories commonly used in program evaluation today.<Table Float="Yes" ID="Tab00232">
                    <Caption Language="En">
                      <CaptionNumber>Table 23.2</CaptionNumber>
                      <CaptionContent>
                        <SimplePara>Common theoretical models for behavior change in program evaluation</SimplePara>
                      </CaptionContent>
                    </Caption>
                    <tgroup cols="1">
                      <colspec colname="c1" colnum="1"/>
                      <tbody>
                        <row>
                          <entry colname="c1">
                            <SimplePara>•Diffusion of innovations (Ryan &amp; Gross <CitationRef CitationID="CR002348">1943</CitationRef>)</SimplePara>
                          </entry>
                        </row>
                        <row>
                          <entry colname="c1">
                            <SimplePara>•Hierarchy of effects and steps to behavior change (McGuire, <CitationRef CitationID="CR002337">1989</CitationRef>)</SimplePara>
                          </entry>
                        </row>
                        <row>
                          <entry colname="c1">
                            <SimplePara>•Stages of change and the transtheoretical model (Prochaska, Diclemente, &amp; Norcross <CitationRef CitationID="CR002344">1992</CitationRef>)</SimplePara>
                          </entry>
                        </row>
                        <row>
                          <entry colname="c1">
                            <SimplePara>•Theory of reasoned action (Fishbein &amp; Ajzen <CitationRef CitationID="CR002317">1975</CitationRef>)</SimplePara>
                          </entry>
                        </row>
                        <row>
                          <entry colname="c1">
                            <SimplePara>•Social learning theory and self-efficacy (Bandura, <CitationRef CitationID="CR00234">1986</CitationRef>)</SimplePara>
                          </entry>
                        </row>
                        <row>
                          <entry colname="c1">
                            <SimplePara>•Health belief model (Rosenstock, <CitationRef CitationID="CR002345">1974</CitationRef>)</SimplePara>
                          </entry>
                        </row>
                      </tbody>
                    </tgroup>
                  </Table>
                </Para>
                <Para TextBreak="No">Finally, ethical, legal, and regulatory constraints are considered during formative evaluations. In the workplace, the right to privacy requires significant consideration, and difficulties with mental illness, substance abuse, and sexually transmitted diseases are protected information for most employees. For instance, evaluating a work-based program to decrease heavy drinking among employees not directly involved in public safety presents many challenges. Not only may employees be reluctant to report problems or changes in drinking, but such programs, if poorly designed, may violate an employee’s rights.</Para>
                <Para TextBreak="No">Within formative evaluations, the impact of the evaluation is not being directly studied, but the study validity is increased if input from potential end-user is sought and the methods of program deployment are examined. Without this input, interventions may be less than engaging, poorly focused, or potentially offensive. Usability testing decreases the chances of user frustration and poor adherence with overly complex or flawed technologically-based interventions.</Para>
              </Section2>
              <Section2 ID="Sec00236">
                <Heading>Process Evaluation</Heading>
                <Para TextBreak="No">Process evaluation focuses upon programs that are already proceeding, and examines the activities needed to run the program. Furthermore, process evaluation tracks those who are being served and the manner in which they are being served. The theoretical basis of the intervention is needed to assess whether the intervention is achieving results in the manner expected. Valente (<CitationRef CitationID="CR002356">2002</CitationRef>) argues that ­programs should engage in process evaluations, but disproportionately focus on whether a program demonstrates success, rather than <Emphasis Type="Italic">why</Emphasis> it may be successful. Therefore, process evaluation increases internal study validity, and ensures that changes in outcome measures can be attributed to the success or failure of the program.</Para>
                <Para TextBreak="No">Adherence to the original intent and implementation of the program is referred to as <Emphasis Type="Italic">fidelity</Emphasis> (Valente, <CitationRef CitationID="CR002356">2002</CitationRef>). During the process evaluation phase, assessment as to whether the critical components of the intervention are being administered effectively and uniformly is necessary (Saksvik, Nytro, Dahl-Jorgensen, &amp; Mikkelsen, <CitationRef CitationID="CR002349">2002</CitationRef>). Furthermore, technical issues related to interventions using Web-based or other technologically-laden platforms are assessed. Although usability of the platform should have occurred previously, additional changes may be required to ensure effective implementation. Additional training may be required for the individuals implementing the program. As noted, health promotion programs involve the development of protocols for implementation and, over time, a drift from the original protocol can be detected and corrected with process evaluation. Changes in the staff implementing a specific protocol and the passage of time can both threaten the fidelity of a program. Finally, a second variable relevant to process evaluation is <Emphasis Type="Italic">dose.</Emphasis> Dose refers to the amount of exposure an employee has to a program, as well as to the intensity (Valente, <CitationRef CitationID="CR002356">2002</CitationRef>). For example, a Web-based intervention can be evaluated by frequency of visits by end-users.</Para>
              </Section2>
              <Section2 ID="Sec00237">
                <Heading>Challenges to Process Evaluation</Heading>
                <Para TextBreak="No">Saksvik and colleagues (<CitationRef CitationID="CR002349">2002</CitationRef>) performed one of the few studies to evaluate the implementation process of a health promotion program. Griffiths (<CitationRef CitationID="CR002321">1999</CitationRef>) argues that the reason for the sparse body of literature addressing process evaluation is that most of the studies suffer from poor research design and other methodological limitations. Lipsey and Cordray (<CitationRef CitationID="CR002335">2000</CitationRef>) also state, from a review of the literature, that intervention programs are inherently difficult to implement. Unfortunately, there is also often a lack of attention to implementing process evaluations in different environments (i.e., inattention to generalizability). Further hampering effective evaluations are differences among researchers, bot theoretically and methodologically (Burke, <CitationRef CitationID="CR00239">1993</CitationRef>; Colarelli, <CitationRef CitationID="CR002314">1998</CitationRef>; Handy, <CitationRef CitationID="CR002323">1988</CitationRef>). Social and political realities may also hamper process evaluations, and a growing emphasis has been on <Emphasis Type="Italic">contextualization</Emphasis>, which emphasizes the importance of understanding the uniqueness of the political and social realities within organizations. Lastly, a lack of detailed description of the intervention program can often hamper implementation efforts (Lipsey &amp; Cordray, <CitationRef CitationID="CR002335">2000</CitationRef>). Given these barriers to process evaluation, a more “nuanced” definition is required. Nytrø, Saksvik, Mikkelsen, Bohle, and Quinlan (<CitationRef CitationID="CR002342">2000</CitationRef>) define process as, “…individual, collective or management perceptions and actions in implementing any intervention and their influence of the overall result of the intervention.” (p. 214). This definition takes into account the contextual challenges and differences within and among organizations than more traditional definitions of process evaluation. Finally, Sechrest and Figueredo (<CitationRef CitationID="CR002350">1993</CitationRef>) indicated that organizations have focused on formative over process evaluations. As previously discussed, formative evaluations have often been atheoretical <Emphasis Type="Italic">block box</Emphasis> evaluations, rather than evaluation of programs with a strong theoretical basis. As a result, the lack of theoretical basis during formative evaluations has hampered subsequent process and summative evaluations (Saksvik et al., <CitationRef CitationID="CR002349">2002</CitationRef>).</Para>
                <Para TextBreak="No">Despite their systematic nature, theory-driven approaches have been stressed in this present Chapter. Colarelli (<CitationRef CitationID="CR002314">1998</CitationRef>) describes an evolutionary perspective on process evaluation that merits consideration. The evolutionary perspective accurately brings attention to the fact that, within organizations, change processes are often founded on simple heuristics, ecological adaptations, and tacit knowledge rather than on empirical analyses or clear, reasoned rationales (Colarelli, <CitationRef CitationID="CR002314">1998</CitationRef>). Although these factors are important to remember, the methodological considerations to successfully employ a process evaluation from an evolutionary perspective remain elusive (Saksvik et al., <CitationRef CitationID="CR002349">2002</CitationRef>). However, Saksvik and colleagues (<CitationRef CitationID="CR002349">2002</CitationRef>) approached their study on process evaluation from a systematic, theory driven approach while remaining mindful of evolutionary perspective factors by evaluating, “…not only at the shop floor level, but also in a larger context including external constraints from, for example, company top managers or working life partners.” (p. 39).</Para>
                <Para TextBreak="No">Several researchers have outlined and recommended ways to investigate <Emphasis Type="Italic">formal</Emphasis> process factors. Below, Goldenhar, Lamontagne, Katz, Heaney, and Landsbergis (<CitationRef CitationID="CR002320">2001</CitationRef>) provide a summary of five important approaches:
<BlockQuote>
                    <Para TextBreak="No">“1.Gathering background information about former experiences with similar interventions in the organization and finding out what is known about the selected intervention (e.g., application, efficacy, effectiveness and quality of existing data).</Para>
                    <Para TextBreak="No">2.Securing stakeholder involvement and multidisciplinary teams in the evaluation.</Para>
                    <Para TextBreak="No">3.Choosing methods and designs that are related to scientific quality, but also address the practical limitations.</Para>
                    <Para TextBreak="No">4.Identifying implementation barriers, (e.g. changes in participation and factors that confound the measurement).</Para>
                    <Para TextBreak="No">5.Communicating findings to both participants and non-participants who are capable of taking action based on the results.” (Saksvik et al., <CitationRef CitationID="CR002349">2002</CitationRef>; p. 40)</Para>
                  </BlockQuote>
                </Para>
                <Para TextBreak="No">Evaluating formal process factors is a necessary step that is enhanced by gathering information about informal process factors. Informal factors, related to the culture and politics of an organization, while difficult to glean, represent variables that can doom a program, or program evaluation, as well as offer the opportunity for enhanced success. Below, are Nytrø and colleagues’ (<CitationRef CitationID="CR002342">2000</CitationRef>) suggestions of <Emphasis Type="Italic">informal</Emphasis> processes to be considered when developing a process evaluation.
<BlockQuote>
                    <Para TextBreak="No">“1)The establishment of social climate of learning from failure. This means that one has to move beyond merely building on experience if there appear to be strong cultural, social and psychological prohibitions against learning from failure.</Para>
                    <Para TextBreak="No">2)Providing opportunities for multi-level participation and negotiation in the design of interventions. The pitfall here is often that the managers and employees may agree on the objective of the intervention (e.g. lower absenteeism), but have different ideas about what may constitute important methods for achieving this objective. Possible differences in organizational perceptions have to be uncovered early in the project.</Para>
                    <Para TextBreak="No">3)Facilitating cultural maturity. If an organization is immature in its competence to manage change processes, it may be appropriate to pursue greater empowerment of employees. It may be hard to gain insight into such a lack of cultural maturity, and an acceptance that this trait characterizes the organization in question is not always easily archived.</Para>
                    <Para TextBreak="No">4)Reaching awareness of tacit and informal organizational behavior that undermines the objectives of interventions. Both managers and workers can invent their own idiosyncratic justifications for change from their own locally-bound perspectives. Novel insights may create frustrations, suspicions, and norms that, unintentionally, support dysfunctional behavior.</Para>
                    <Para TextBreak="No">5)Monitoring readiness to change. It is wise to monitor employee attitudes towards the interventions introduced in order to asses to what degree these interventions are appreciated as appropriate means towards alleviating the current state of affairs. The most fit and healthy employees may find the intervention attractive, while those who really need to change health behaviors may find it stigmatizing and unattractive.</Para>
                    <Para TextBreak="No">6)Defining roles and responsibilities before and during the intervention period. Different projects of change require different roles from both employers’ and employees’ representatives and different roles require different skills. One person (the manager) cannot (and should not) hold all roles at the same time. In practice, however, this is often the case.” (Saksvik et al., <CitationRef CitationID="CR002349">2002</CitationRef>; p. 40–41)</Para>
                  </BlockQuote>
                </Para>
                <Para TextBreak="No">Finally, Saksvik and colleagues (<CitationRef CitationID="CR002349">2002</CitationRef>) blended formal and informal process factors and identified the following key process factors:
<BlockQuote>
                    <Para TextBreak="No">“1)The ability to learn from failure and to motivate participants.</Para>
                    <Para TextBreak="No">2)Multi-level participation and negotiation, and differences in organizational perception.</Para>
                    <Para TextBreak="No">3)Insight into tacit informational organizational behavior.</Para>
                    <Para TextBreak="No">4)Clarification of roles and responsibilities, especially the role of middle management.</Para>
                    <Para TextBreak="No">5)Competing projects and reorganization.” (Saksvik et al., <CitationRef CitationID="CR002349">2002</CitationRef>; p. 41)</Para>
                  </BlockQuote>
                </Para>
              </Section2>
              <Section2 ID="Sec00238">
                <Heading>Summative Evaluation</Heading>
                <Para TextBreak="No">Summative evaluation can be used to assess programs that are underway, but typically focus on ­programs that have been completed in order to assess their impact. The summative evaluation, sometimes referred to as impact evaluations, examines whether the stated goals and objectives of the program were met (Henry, <CitationRef CitationID="CR002325">2010</CitationRef>). Determining the impact of the intervention is one of the more challenging aspects of effective program evaluation. Without a sound program evaluation design, it will be impossible to determine if the intervention does generate positive outcomes and what factors in concert with the intervention contribute to these positive outcomes. Although several study designs exist that are commonly used for summative evaluations, comparison groups, and randomized control trials are two of the more rigorous designs. A randomized control trial, the “gold standard” of research, is considered the most robust and empirically-sound study design to establish <Emphasis Type="Italic">causality</Emphasis> (the key factor in summative evaluations). Randomization is the best manner in which to ensure that the makeup of the control and experimental groups are essentially the same, and therefore minimize the risk of <Emphasis Type="Italic">selection bias</Emphasis>, where selection into a group influences the outcomes (Torgerson, Torgerson, &amp; Taylor, <CitationRef CitationID="CR002354">2010</CitationRef>). A common constraint among program evaluations involves voluntary participation in a program. It may be that individuals who voluntarily participate in a weight loss program are more interested in their health than those who do not. A comparison between those groups is likely to result in confounded results, which will greatly limit the interpretation of the findings. Moreover, failure to control for potential confounds will limit the generalizability of the results.</Para>
                <Para TextBreak="No">Although the randomized clinical trial represents the most rigorous design, there are several inherent threats to randomization, as well as practical limitations within the workplace setting. <Emphasis Type="Italic">Contamination</Emphasis> occurs when control participants are exposed to, or contaminated by, the interventions intended for the treatment condition. It is often difficult to blind all research staff to an individual’s group assignment; thus, researchers can inadvertently bias the outcomes. Furthermore, control condition employees may be exposed to educational or interventions designed for the treatment condition (Torgerson et al., <CitationRef CitationID="CR002354">2010</CitationRef>). For example, Kwak and colleagues (<CitationRef CitationID="CR002332">2006</CitationRef>) developed a weight prevention program and used prompts at elevators at worksites to promote taking the stairs. As can be seen, it would be impossible to randomize in this example. Finally, <Emphasis Type="Italic">attrition</Emphasis> (i.e., loss of participants prior to conclusion of the program evaluation) and participants’ preference for a specific group allocation, also threaten randomized control trials (Torgerson et al., <CitationRef CitationID="CR002354">2010</CitationRef>). Even though they are less rigorous than randomized control trials, comparison groups offer practical benefits, and will likely fit more easily into the demands of an operational organization. Furthermore, several comparison designs exist that may minimize, but not eliminate, selection bias (Henry, <CitationRef CitationID="CR002325">2010</CitationRef>). The <Emphasis Type="Italic">naïve design</Emphasis> is the simplest of the comparison designs, and it compares those who participated, or could have participated, in a program to those who did not. A <Emphasis Type="Italic">basic value-added design</Emphasis> relies on pre-intervention outcome measures and employs a regression analysis, thus statistically controlling for pre-intervention differences in outcome measures. Similarly, a <Emphasis Type="Italic">regression-adjusted covariate design</Emphasis> also utilizes a regression analysis and controls for multiple known variables that may bias group membership and, ultimately, outcomes (Henry, <CitationRef CitationID="CR002325">2010</CitationRef>). There is significant evidence that this approach reduces, but does not eliminate, bias (Glazerman, Levy, &amp; Meyers, <CitationRef CitationID="CR002319">2003</CitationRef>). Finally, <Emphasis Type="Italic">matched designs</Emphasis> involve matching members in each group on several variables, such as age, race, marital status, that are known, or suspected, to impact the outcome. Unfortunately, matching becomes challenging without a large sample size, and unforeseen relevant participant characteristics may bias the outcomes in unpredicted ways (Henry, <CitationRef CitationID="CR002325">2010</CitationRef>).</Para>
                <Para TextBreak="No">The number of participants needed to provide meaningful results can be determined by one of several methods, including the tabulation method, the proportion method, the differences method, and power analysis. The simplest, <Emphasis Type="Italic">tabulation method</Emphasis>, requires at least 50 participants for each level of an independent variable. However, this method is likely best suited for very initial assessments of sample size. The most rigorous and most common manner to determine sample size is a power analysis. Cohen (<CitationRef CitationID="CR002313">1977</CitationRef>) defines power analysis as: “The power of a statistical test of a null hypothesis is the probability that it will lead to the rejection of the null hypothesis, i.e., the probability that it will result in the conclusion that the phenomenon exists.” (p. 4). Three elements are needed to calculate sample size using a power analysis: (a) effect size; (b) significance level; and (c) power (an estimate of confidence in results).</Para>
              </Section2>
            </Section1>
            <Section1 ID="Sec00239">
              <Heading>Cost-Effectiveness</Heading>
              <Para TextBreak="No">Cost-effectiveness analysis (CEA) and cost-benefit analysis (CBA) are important elements of program evaluation that can occur during formative, process, or summative evaluations. Cellini and Kee (<CitationRef CitationID="CR002310">2010</CitationRef>) define cost-effectiveness analysis as, “…a technique that relates the costs of a program to its key outcomes or benefits.” (p. 536). Cost-benefit analysis, in comparison, takes into account the ratio of costs compared to all of the program’s benefits. The formula for calculating a cost-effective ratio is as follows:
<Equation ID="Equ0023a">
                  <MediaObject>
                    <ImageObject Color="BlackWhite" FileRef="215798_1_En_23_Chapter_Equ0023a.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </MediaObject>
                  <EquationSource Format="TEX"><![CDATA[
$$ \text{Cost - Effectiveness Ratio}=\frac{\text{Total Costs}}{\text{Units of Effectiveness}}$$
]]></EquationSource>
                </Equation>
              </Para>
              <Para TextBreak="No">The units of effectiveness are typically a clear outcome of the program (e.g., number of hours at the gym or percentage of weight decrease). In program evaluations to promote health change, distal health outcomes, such as decreases in the number of employees with heart disease, would likely be outside the scope of a cost-effective analysis. However, assessment of mediating variables, such as hours a week engaged in physical activity, would be extremely relevant (Cellini &amp; Kee, <CitationRef CitationID="CR002310">2010</CitationRef>).</Para>
              <Para TextBreak="No">With cost-benefit analysis, researchers weigh costs against the estimated value of the program benefits. Costs are subtracted from benefits in this analysis in order to obtain the <Emphasis Type="Italic">net benefits.</Emphasis> A further distinction can be made between financial cost analyses and social cost analyses. Within social cost analyses, both financial and nonfinancial costs and benefits are taken into account. A social cost analysis considers negative outcomes of a program a budgetary cost, and positive impacts as budgetary benefits (Cellini &amp; Kee, <CitationRef CitationID="CR002310">2010</CitationRef>). The formula for cost-benefit analysis is presented below:
<Equation ID="Equ0023b">
                  <MediaObject>
                    <ImageObject Color="BlackWhite" FileRef="215798_1_En_23_Chapter_Equ0023b.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </MediaObject>
                  <EquationSource Format="TEX"><![CDATA[
$$ \text{Net Benefits}=\text{Total Benefits}-\text{Total Costs}$$
]]></EquationSource>
                </Equation>
              </Para>
              <Para TextBreak="No">Despite the apparent simplicity for the formulas to determine cost-effectiveness and cost-benefit, these analyses require multiple assumptions and estimations. However, Cellini and Kee (<CitationRef CitationID="CR002310">2010</CitationRef>) adapted a ten step process from Boardman, Greenberg, Vining, and Weimer (<CitationRef CitationID="CR00237">2006</CitationRef>) that take program evaluators through the needed components of a cost-effective or cost-benefit analysis. The outline of the steps that can be use in both cost-effectiveness and cost-benefit analysis are presented below:
<BlockQuote>
                  <Para TextBreak="No">“1.Set the framework for the analysis.</Para>
                  <Para TextBreak="No">2.Decide whose costs and benefits should be recognized.</Para>
                  <Para TextBreak="No">3.Identify and categorize costs and benefits.</Para>
                  <Para TextBreak="No">4.Project costs and benefits of the life of the program, if applicable.</Para>
                  <Para TextBreak="No">5.Monetize costs.</Para>
                  <Para TextBreak="No">6.Quantify benefits in terms of units of effectiveness (for CEA), or monetize benefits (for CBA).</Para>
                  <Para TextBreak="No">7.Discount costs and benefits to obtain present values.</Para>
                  <Para TextBreak="No">8.Compute a cost-effectiveness ratio or net present value.</Para>
                  <Para TextBreak="No">9.Perform sensitivity analysis.</Para>
                  <Para TextBreak="No">10.Make recommendation when appropriate.” (p. 537–538)</Para>
                </BlockQuote>
              </Para>
            </Section1>
            <Section1 ID="Sec002310">
              <Heading>Additional Barriers to Program Evaluation</Heading>
              <Para TextBreak="No">Frequent, real-world limitations to program evaluation include money and time constraints, as well as the methodological and ethical limitations imposed when evaluating outcomes in workplace settings (Bamberger, Rugh, Church, &amp; Fort, <CitationRef CitationID="CR00233">2004</CitationRef>). In fact, most programs do not allocate a budget for program evaluation at the outset (Bamberger et al., <CitationRef CitationID="CR00233">2004</CitationRef>). Therefore, flexibility and relying on both qualitative and quantitative methods, as well as statistically controlling for internal and external threats to study validity, are often the norm for program evaluation. This, though, should not deter researchers from evaluating the impact of the program. Jacobs (Jacobs, <CitationRef CitationID="CR002329">2003</CitationRef>) developed the <Emphasis Type="Italic">Five-Tiered Approach</Emphasis> of program evaluation to manage monetary, data, and time constraints by condensing and simplifying key elements of formative, process, and summative evaluations. It assumes that many program evaluations will not be fully considered or budgeted by the parties interested in developing a program. Therefore, it takes an incremental and iterative approach where program evaluators can build upon previous tiers if possible. The approach could be accurately described as a blend between qualitative and quantitative methods, and techniques that emphasize the unique aspects of an organization and population. The five tiers include the following:
<UnorderedList Mark="Bullet">
                  <ItemContent>
                    <Para TextBreak="No">
                      <Emphasis Type="Italic">Needs Assessment.</Emphasis> The needs assessment is conducted in a contextual manner and gathers information about the problem to be addressed, in the organization to be addressed, and about previous success with similar interventions.</Para>
                  </ItemContent>
                  <ItemContent>
                    <Para TextBreak="No">
                      <Emphasis Type="Italic">Monitoring and Accountability.</Emphasis> At this tier, emphasis is placed on accurate description of clients, services, personnel, and costs.</Para>
                  </ItemContent>
                  <ItemContent>
                    <Para TextBreak="No">
                      <Emphasis Type="Italic">Quality Review and Program Clarification.</Emphasis> Three standards are assessed at this tier, including model standards (or how a program is intended to operate), program-generated standards (process evaluation based on goals set by the program rather than external source), and participant-generated standards (the preferences of the participants).</Para>
                  </ItemContent>
                  <ItemContent>
                    <Para TextBreak="No">
                      <Emphasis Type="Italic">Achieving Outcomes.</Emphasis> Emphasis at this tier is typically on short-term objectives, using systematic, but less than rigorous, designs to provide useful and interesting results to interested parties.</Para>
                  </ItemContent>
                  <ItemContent>
                    <Para TextBreak="No">
                      <Emphasis Type="Italic">Establishing Impact</Emphasis>. This tier is reserved for large, well-funded summative research with rigorous research designs.</Para>
                  </ItemContent>
                </UnorderedList>
              </Para>
              <Para TextBreak="No">Although the above <Emphasis Type="Italic">Five-Tiered Approach</Emphasis> may appear less than ideal from a methodological perspective, it provides a path to develop useful data under common time, data, and budget constraints.</Para>
            </Section1>
            <Section1 ID="Sec002311">
              <Heading>Intervention Mapping</Heading>
              <Para TextBreak="No">Kwak et al., <CitationRef CitationID="CR002332">2006</CitationRef> described an exceptional example of program evaluation that implemented a theory-driven and systematic approach to a health intervention regarding the prevention of weight gain. Prior to program development and design, the researchers conducted a needs assessment (i.e., identified physical and behavioral factors related to weight gain via a thorough review of the literature). Based on the needs assessment, the following clear objective was defined: “…to decrease overall dietary energy intake and/or increase daily routine physical activity in order to prevent weight gain in young adults aged 25–40 years.” (p. 349). The investigator applied a five step <Emphasis Type="Italic">Intervention Mapping</Emphasis> protocol that relies on theoretical, empirical, and practical factors to develop a health promotion program (Bartholomew et al., <CitationRef CitationID="CR00236">2001</CitationRef>). This study’s use of <Emphasis Type="Italic">Intervention Mapping</Emphasis> provides an excellent example and roadmap for how program evaluation can be incorporated at each stage of program development, implementation, and summation.
</Para>
              <Para TextBreak="No">
                <Emphasis Type="Italic">Step 1. Defining Proximal Program Objectives</Emphasis>. Based upon the needs assessment, researchers translate risk behaviors into behaviors demonstrated to impact the outcome. In the example of the Kwak and colleagues (<CitationRef CitationID="CR002332">2006</CitationRef>) study, specific behaviors were identified as targets to increase energy expenditure (walking and biking to work and increase physical activity at work), as well as decreasing calories (decreasing portion size, replacing high-fat foods with low-calorie food, increase fiber, and replacement of saturated fats by unsaturated fats). Furthermore, these goals were further broken down into performance objectives, such as increased knowledge of fiber-rich food, self-assessment of fiber intake, etc. Lastly, in Step 1, relevant, alterable personal, and environmental determinants are selected. In the Kwak et al. (<CitationRef CitationID="CR002332">2006</CitationRef>) study, knowledge, awareness, preferences, attitude, self-efficacy, and habit were identified as personal determinants. Environmental determinants included availability of health food, management commitment and support, social support, company rules, and cost of healthier food.</Para>
              <Para TextBreak="No">
                <Emphasis Type="Italic">Step 2. Selecting Theory-based Intervention Methods and Practical Strategies.</Emphasis> From an intervention mapping perspective, after personal and environmental determinants are selected, an appropriate theoretical model of behavior change is selected. For this program, multiple theories were applied, including self-monitoring, tailoring, and skills training. Next, practical strategies based on the above steps and sub-steps were developed, such as launching a Web site for information and interactive features for tailoring, placing cues at elevators to increase use of stairs, and making healthier food choices available in vending machines.</Para>
              <Para TextBreak="No">
                <Emphasis Type="Italic">Step 3. Producing and Integrating the Intervention Program.</Emphasis> This step refers to the development of the intervention protocol, materials, and measurements. The program developed consisted of two components: an individual and worksite component. With regard to the individual component, materials produced included: information on an individual’s body mass index, fat percentage, and waist size; a kit which contained a pedometer, waist circumference tape, and calorie guide; and a Web site and CD-ROM. The main environmental components were developed into a handbook with clear guidelines for employers.</Para>
              <Para TextBreak="No">
                <Emphasis Type="Italic">Step 4. Adoption and Implementation Plan.</Emphasis> In this stage, it is often useful to develop a plan that facilitates interactions among the program developers and end users. In this study, working groups were formed that contained employees who could influence the stated objectives, such as cafeteria workers and ­managers, and members of the research team. Also, materials were pretested to assess usability and acceptability, thus increasing the chances for success.</Para>
              <Para TextBreak="No">
                <Emphasis Type="Italic">Step 5. Monitoring and Evaluation Plan</Emphasis>. Within <Emphasis Type="Italic">Intervention Mapping</Emphasis>, process evaluation and summative evaluation are the last steps of the Map. The previous four steps could accurately be described as a formative evaluation. It is important to note that all five steps are developed prior to the initiation of the actual program. For the process evaluation, the researchers selected Roger’s <Emphasis Type="Italic">Diffusion of Innovations Model</Emphasis> and examined intervention delivery, participation, comprehension, satisfaction, level of use, fidelity, and institutionalization. Furthermore, measures of success for the process evaluation were divided into indirect, immediate, and outcome categories. For the summative evaluation, 12 worksites with over 500 participants were utilized, and the researchers employed a nonrandomized pretest multiple posttest control group design. The primary outcome was weight loss, and was collected after the year-long intervention and then another year later. However, other physiological measures, such as waist circumference, were also examined. Thoughts, intentions, and physical activity were evaluated with paper and pencil measures, but 10% of the sample underwent more expensive, but valid, physical performance assessments, such as VO2-max test (Kwak et al., <CitationRef CitationID="CR002332">2006</CitationRef>).</Para>
              <Para TextBreak="No">As can be seen, this <Emphasis Type="Italic">Intervention Mapping</Emphasis> approach provides a clear, structured method for program evaluation that takes into consideration all of the factors discussed throughout this Chapter. Although this approach is not always be practical or possible, it represents an ideal that could be applied to many health promotion programs in occupational settings.</Para>
            </Section1>
            <Section1 ID="Sec002312">
              <Heading>Summary and Future Directions</Heading>
              <Para TextBreak="No">As noted throughout this present Handbook, the need for effective interventions in the workplace is growing (Kung et al., <CitationRef CitationID="CR002331">2008</CitationRef>). Program evaluation has undergone several transitions after beginning in the period following World War II. A rigorous method of evaluation of these programs had once been the “gold-standard” for evaluating the impact of interventions. Unfortunately, many rigorous methodologies were impractical, or impossible, to implement in work settings. Worker safety and confidentiality posed barriers for strict intervention protocols, as well as the practical limitations of providing an intervention in an operational worksite. It had been seen as ideal to exclude evaluators from the program design in order to increase evaluator objectiveness. As a result, qualitative methods that allowed the gathering of stakeholder input began to be more widely used in favor of stricter quantitative methods. In most workplace settings, a combination of both quantitative and qualitative methods is required to ensure effective program evaluation while operating within the constraints of a functioning workplace. For instance, in order to evaluate the effectiveness of a weight loss program in the workplace from a quantitative perspective: the sample size of willing participants would need to be large; all participants would be required to participate and randomly assigned to control or experimental condition; and the dependent variable would consist of weight lost. Although ideal from an empirical perspective, this approach cannot realistically be implemented in most work settings.</Para>
              <Para TextBreak="No">With regard to program evaluation of workplace interventions, a blended approach is required that utilizes the structure of empirical techniques and is informed by more qualitative methods. As discussed in this Chapter, randomized control trials provide the most rigorous assessment, but comparison designs with statistical control provide a more realistic avenue for inquiry. In addition to the theme of a blended methodological approach being the most value in program evaluation, a second theme was emphasized in this Chapter—namely, that program evaluation is needed from the outset, including during initial planning stages (formative evaluation), during the implementation of the program (process evaluation), and at the conclusion (summative evaluation). As seen in the study by Kwak and colleagues (<CitationRef CitationID="CR002332">2006</CitationRef>), effective interventions are integrated throughout all stages of program development.</Para>
              <Para TextBreak="No">Thus, the goals for program evaluation are complex. The integration of intervention programs in the work setting continues. However, accompanying an evaluation process, it will need to be seamless and organic to the industry being targeted. The aforementioned budgetary woes and the fiscal climate represent major hurdles in utilizing the worksite in implementing potentially health-enhancing prevention programs. The appreciation for the work environment to implement health-behavior interventions is not novel—the solution to overcome the barriers to the implementation and its evaluation require will require the development of novel research/methodological techniques in the future. It should also be noted that, with the staggering and growing healthcare cost in the USA, the federal government is moving towards funding the best ways to design guidelines for treatment that are both beneficial and cost-effective (Lauer &amp; Collins, <CitationRef CitationID="CR002334">2010</CitationRef>). This has stimulated the need for more Comparative effectiveness research (CER; Manchikanti, Falco, Boswell, &amp; Hirsch, <CitationRef CitationID="CR002336">2010</CitationRef>; Sox, <CitationRef CitationID="CR002352">2010</CitationRef>). One vital aspect of CER is the use of cost-effectiveness analysis in order to help objectively document the most cost-effective intervention methods. Thus, approaches such as CEA and CBA (discussed earlier in this Chapter) will be at the forefront of needs for future program evaluation of prevention and intervention methods in the workplace.</Para>
            </Section1>
          </Body>
          <BodyRef FileRef="978-1-4614-4839-6_Chapter_23.pdf" OutputMedium="Online" PDFType="Typeset" TargetType="OnlinePDF"/>
          <ChapterBackmatter>
            <Bibliography ID="Bib00231">
              <Heading>References</Heading>
              <Citation ID="CR00231">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>DG</Initials>
                    <FamilyName>Altman</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>JM</Initials>
                    <FamilyName>Bland</FamilyName>
                  </BibAuthorName>
                  <Year>1994</Year>
                  <ArticleTitle Language="En">Diagnostic tests. 1: Sensitivity and specificity</ArticleTitle>
                  <JournalTitle>BMJ: Brittish Medical Journal</JournalTitle>
                  <VolumeID>308</VolumeID>
                  <IssueID>6943</IssueID>
                  <FirstPage>1552</FirstPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1136/bmj.308.6943.1552</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Altman, D. G., &amp; Bland, J. M. (1994). Diagnostic tests. 1: Sensitivity and specificity. <Emphasis Type="Italic">BMJ: Brittish Medical Journal, 308</Emphasis>(6943), 1552.</BibUnstructured>
              </Citation>
              <Citation ID="CR00232">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Armenakis</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>SG</Initials>
                    <FamilyName>Harris</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>KW</Initials>
                    <FamilyName>Mossholder</FamilyName>
                  </BibAuthorName>
                  <Year>1993</Year>
                  <ArticleTitle Language="En">Creating readiness for organizational change</ArticleTitle>
                  <JournalTitle>Human Relations</JournalTitle>
                  <VolumeID>46</VolumeID>
                  <FirstPage>681</FirstPage>
                  <LastPage>703</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1177/001872679304600601</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Armenakis, A., Harris, S. G., &amp; Mossholder, K. W. (1993). Creating readiness for organizational change. <Emphasis Type="Italic">Human Relations, 46</Emphasis>, 681–703.</BibUnstructured>
              </Citation>
              <Citation ID="CR00233">
                <BibUnstructured>Bamberger, M., Rugh, J., Church, M., &amp; Fort, L. (2004). Shoestring evaluation: Designing evaluations under budge, time and data constraints. <Emphasis Type="Italic">American Journal of Evaluation, 25</Emphasis>(1).</BibUnstructured>
              </Citation>
              <Citation ID="CR00234">
                <BibBook>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Bandura</FamilyName>
                  </BibAuthorName>
                  <Year>1986</Year>
                  <BookTitle>Social foundations of thought and action: A social cognitive theory</BookTitle>
                  <PublisherName>Prentice-Hall</PublisherName>
                  <PublisherLocation>Englewood Cliffs, NJ</PublisherLocation>
<Occurrence Type="DOI">
<Handle>10.4135/9781446221129.n6</Handle>
</Occurrence>
                </BibBook>
                <BibUnstructured>Bandura, A. (1986). <Emphasis Type="Italic">Social foundations of thought and action: A social cognitive theory</Emphasis>. Englewood Cliffs, NJ: Prentice-Hall.</BibUnstructured>
              </Citation>
              <Citation ID="CR00235">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>RM</Initials>
                    <FamilyName>Baron</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>DA</Initials>
                    <FamilyName>Kenny</FamilyName>
                  </BibAuthorName>
                  <Year>1986</Year>
                  <ArticleTitle Language="En">The moderator-mediator variable distinction in social psychology research: Conceptual, strategic, and statistical considerations</ArticleTitle>
                  <JournalTitle>Journal of Personality and Social Psychology</JournalTitle>
                  <VolumeID>51</VolumeID>
                  <IssueID>6</IssueID>
                  <FirstPage>1173</FirstPage>
                  <LastPage>1182</LastPage>
                  <Occurrence Type="PID">
                    <Handle>3806354</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1037/0022-3514.51.6.1173</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Baron, R. M., &amp; Kenny, D. A. (1986). The moderator-mediator variable distinction in social psychology research: Conceptual, strategic, and statistical considerations. <Emphasis Type="Italic">Journal of Personality and Social Psychology, 51</Emphasis>(6), 1173–1182.</BibUnstructured>
              </Citation>
              <Citation ID="CR00236">
                <BibBook>
                  <BibAuthorName>
                    <Initials>LK</Initials>
                    <FamilyName>Bartholomew</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>GS</Initials>
                    <FamilyName>Parcel</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>G</Initials>
                    <FamilyName>Kok</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>NH</Initials>
                    <FamilyName>Gottlieb</FamilyName>
                  </BibAuthorName>
                  <Year>2001</Year>
                  <BookTitle>Intervention mapping: Designing theory-evidence-based health promotion programs</BookTitle>
                  <PublisherName>McGraw-Hill</PublisherName>
                  <PublisherLocation>New York</PublisherLocation>
                </BibBook>
                <BibUnstructured>Bartholomew, L. K., Parcel, G. S., Kok, G., &amp; Gottlieb, N. H. (2001). <Emphasis Type="Italic">Intervention mapping: Designing theory-evidence-based health promotion programs</Emphasis>. New York: McGraw-Hill.</BibUnstructured>
              </Citation>
              <Citation ID="CR00237">
                <BibBook>
                  <BibAuthorName>
                    <Initials>AA</Initials>
                    <FamilyName>Boardman</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>DH</Initials>
                    <FamilyName>Greenberg</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>AR</Initials>
                    <FamilyName>Vining</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>DL</Initials>
                    <FamilyName>Weimer</FamilyName>
                  </BibAuthorName>
                  <Year>2006</Year>
                  <BookTitle>Cost-benefit analysis: Concepts and practice</BookTitle>
                  <PublisherName>Prentice Hall</PublisherName>
                  <PublisherLocation>Upper Saddle River, NJ</PublisherLocation>
                </BibBook>
                <BibUnstructured>Boardman, A. A., Greenberg, D. H., Vining, A. R., &amp; Weimer, D. L. (2006). <Emphasis Type="Italic">Cost-benefit analysis: Concepts and practice</Emphasis>. Upper Saddle River, NJ: Prentice Hall.</BibUnstructured>
              </Citation>
              <Citation ID="CR00238">
                <BibChapter>
                  <BibAuthorName>
                    <Initials>JM</Initials>
                    <FamilyName>Bryson</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>MQ</Initials>
                    <FamilyName>Patton</FamilyName>
                  </BibAuthorName>
                  <Year>2010</Year>
                  <ChapterTitle Language="En">Analyzing and engaging stakeholders</ChapterTitle>
                  <BibEditorName>
                    <Initials>JS</Initials>
                    <FamilyName>Wholey</FamilyName>
                  </BibEditorName>
                  <BibEditorName>
                    <Initials>HP</Initials>
                    <FamilyName>Hatry</FamilyName>
                  </BibEditorName>
                  <BibEditorName>
                    <Initials>KE</Initials>
                    <FamilyName>Newcomer</FamilyName>
                  </BibEditorName>
                  <Eds/>
                  <BookTitle>Handbook of practical program evaluation</BookTitle>
                  <EditionNumber>3</EditionNumber>
                  <PublisherName>Jossey-Bass</PublisherName>
                  <PublisherLocation>San Francisco, CA</PublisherLocation>
                </BibChapter>
                <BibUnstructured>Bryson, J. M., &amp; Patton, M. Q. (2010). Analyzing and engaging stakeholders. In J. S. Wholey, H. P. Hatry, &amp; K. E. Newcomer (Eds.), <Emphasis Type="Italic">Handbook of practical program evaluation</Emphasis> (3rd ed.). San Francisco, CA: Jossey-Bass.</BibUnstructured>
              </Citation>
              <Citation ID="CR00239">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>RJ</Initials>
                    <FamilyName>Burke</FamilyName>
                  </BibAuthorName>
                  <Year>1993</Year>
                  <ArticleTitle Language="En">Organizational-level interventions to reduce occupational stressors</ArticleTitle>
                  <JournalTitle>Work and Stress</JournalTitle>
                  <VolumeID>7</VolumeID>
                  <FirstPage>77</FirstPage>
                  <LastPage>87</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1080/02678379308257051</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Burke, R. J. (1993). Organizational-level interventions to reduce occupational stressors. <Emphasis Type="Italic">Work and Stress, 7</Emphasis>, 77–87.</BibUnstructured>
              </Citation>
              <Citation ID="CR002310">
                <BibChapter>
                  <BibAuthorName>
                    <Initials>SR</Initials>
                    <FamilyName>Cellini</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>JE</Initials>
                    <FamilyName>Kee</FamilyName>
                  </BibAuthorName>
                  <Year>2010</Year>
                  <ChapterTitle Language="En">Cost-effectiveness and cost-benefit-analysis</ChapterTitle>
                  <BibEditorName>
                    <Initials>JS</Initials>
                    <FamilyName>Wholey</FamilyName>
                  </BibEditorName>
                  <BibEditorName>
                    <Initials>HP</Initials>
                    <FamilyName>Hatry</FamilyName>
                  </BibEditorName>
                  <BibEditorName>
                    <Initials>KE</Initials>
                    <FamilyName>Newcomer</FamilyName>
                  </BibEditorName>
                  <Eds/>
                  <BookTitle>Handbook of practical program evaluation</BookTitle>
                  <EditionNumber>3</EditionNumber>
                  <PublisherName>Jossey-Bass</PublisherName>
                  <PublisherLocation>San Franciscol, CA</PublisherLocation>
                </BibChapter>
                <BibUnstructured>Cellini, S. R., &amp; Kee, J. E. (2010). Cost-effectiveness and cost-benefit-analysis. In J. S. Wholey, H. P. Hatry, &amp; K. E. Newcomer (Eds.), <Emphasis Type="Italic">Handbook of practical program evaluation</Emphasis> (3rd ed.). San Franciscol, CA: Jossey-Bass.</BibUnstructured>
              </Citation>
              <Citation ID="CR002311">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>HT</Initials>
                    <FamilyName>Chen</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>PH</Initials>
                    <FamilyName>Rossi</FamilyName>
                  </BibAuthorName>
                  <Year>1983</Year>
                  <ArticleTitle Language="En">Evaluation with sense: The theory-driven approach</ArticleTitle>
                  <JournalTitle>Evaluation Review</JournalTitle>
                  <VolumeID>7</VolumeID>
                  <IssueID>3</IssueID>
                  <FirstPage>283</FirstPage>
                  <LastPage>302</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1177/0193841X8300700301</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Chen, H. T., &amp; Rossi, P. H. (1983). Evaluation with sense: The theory-driven approach. <Emphasis Type="Italic">Evaluation Review, 7</Emphasis>(3), 283–302.</BibUnstructured>
              </Citation>
              <Citation ID="CR002312">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>HS</Initials>
                    <FamilyName>Chumley-Jones</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Dobbie</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>CL</Initials>
                    <FamilyName>Alfor</FamilyName>
                  </BibAuthorName>
                  <Year>2002</Year>
                  <ArticleTitle Language="En">Web-based learning: Sound educational method or hype? A review of the evaluation literature</ArticleTitle>
                  <JournalTitle>Academic Medicine</JournalTitle>
                  <VolumeID>77</VolumeID>
                  <IssueID>10</IssueID>
                  <FirstPage>S86</FirstPage>
                  <LastPage>S93</LastPage>
                  <Occurrence Type="PID">
                    <Handle>12377715</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1097/00001888-200210001-00028</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Chumley-Jones, H. S., Dobbie, A., &amp; Alfor, C. L. (2002). Web-based learning: Sound educational method or hype? A review of the evaluation literature. <Emphasis Type="Italic">Academic Medicine, 77</Emphasis>(10), S86–S93.</BibUnstructured>
              </Citation>
              <Citation ID="CR002313">
                <BibBook>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Cohen</FamilyName>
                  </BibAuthorName>
                  <Year>1977</Year>
                  <BookTitle>Statistical power analysis for the behavioral sciences (Rev ed.)</BookTitle>
                  <PublisherName>Academic</PublisherName>
                  <PublisherLocation>New York</PublisherLocation>
                </BibBook>
                <BibUnstructured>Cohen, J. (1977). <Emphasis Type="Italic">Statistical power analysis for the behavioral sciences (Rev ed.)</Emphasis>. New York: Academic.</BibUnstructured>
              </Citation>
              <Citation ID="CR002314">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>SM</Initials>
                    <FamilyName>Colarelli</FamilyName>
                  </BibAuthorName>
                  <Year>1998</Year>
                  <ArticleTitle Language="En">Psychological interventions in organizations. An evolutionary perspective</ArticleTitle>
                  <JournalTitle>The American Psychologist</JournalTitle>
                  <VolumeID>53</VolumeID>
                  <FirstPage>1044</FirstPage>
                  <LastPage>1056</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1037/0003-066X.53.9.1044</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Colarelli, S. M. (1998). Psychological interventions in organizations. An evolutionary perspective. <Emphasis Type="Italic">The American Psychologist, 53</Emphasis>, 1044–1056.</BibUnstructured>
              </Citation>
              <Citation ID="CR002315">
                <BibBook>
                  <BibAuthorName>
                    <Initials>JW</Initials>
                    <FamilyName>Creswell</FamilyName>
                  </BibAuthorName>
                  <Year>2003</Year>
                  <BookTitle>Research design: Qualitative, quantitative and mixed method approaches</BookTitle>
                  <PublisherName>Sage</PublisherName>
                  <PublisherLocation>Thousand Oaks, CA</PublisherLocation>
                </BibBook>
                <BibUnstructured>Creswell, J. W. (2003). <Emphasis Type="Italic">Research design: Qualitative, quantitative and mixed method approaches</Emphasis>. Thousand Oaks, CA: Sage.</BibUnstructured>
              </Citation>
              <Citation ID="CR002316">
                <BibBook>
                  <BibAuthorName>
                    <Initials>NK</Initials>
                    <FamilyName>Denzin</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>YS</Initials>
                    <FamilyName>Lincoln</FamilyName>
                  </BibAuthorName>
                  <Year>2000</Year>
                  <BookTitle>Handbook of qualitative research</BookTitle>
                  <PublisherName>Sage</PublisherName>
                  <PublisherLocation>Thousand Oaks, CA</PublisherLocation>
                </BibBook>
                <BibUnstructured>Denzin, N. K., &amp; Lincoln, Y. S. (2000). <Emphasis Type="Italic">Handbook of qualitative research</Emphasis>. Thousand Oaks, CA: Sage.</BibUnstructured>
              </Citation>
              <Citation ID="CR002317">
                <BibBook>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Fishbein</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>I</Initials>
                    <FamilyName>Ajzen</FamilyName>
                  </BibAuthorName>
                  <Year>1975</Year>
                  <BookTitle>Belief, attitude, intention and behavior: An introduction to theory and research</BookTitle>
                  <PublisherName>Addison-Wesley</PublisherName>
                  <PublisherLocation>Boston</PublisherLocation>
                </BibBook>
                <BibUnstructured>Fishbein, M., &amp; Ajzen, I. (1975). <Emphasis Type="Italic">Belief, attitude, intention and behavior: An introduction to theory and research</Emphasis>. Boston: Addison-Wesley.</BibUnstructured>
              </Citation>
              <Citation ID="CR002318">
                <BibBook>
                  <BibAuthorName>
                    <Initials>LM</Initials>
                    <FamilyName>Given</FamilyName>
                  </BibAuthorName>
                  <Year>2008</Year>
                  <BookTitle>The Sage encyclopedia of qualitative research methods</BookTitle>
                  <PublisherName>Sage</PublisherName>
                  <PublisherLocation>Los Angeles, CA</PublisherLocation>
                </BibBook>
                <BibUnstructured>Given, L. M. (2008). <Emphasis Type="Italic">The Sage encyclopedia of qualitative research methods</Emphasis>. Los Angeles, CA: Sage.</BibUnstructured>
              </Citation>
              <Citation ID="CR002319">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>S</Initials>
                    <FamilyName>Glazerman</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>DM</Initials>
                    <FamilyName>Levy</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>D</Initials>
                    <FamilyName>Meyers</FamilyName>
                  </BibAuthorName>
                  <Year>2003</Year>
                  <ArticleTitle Language="En">Nonexperimental versus experimental estimates of earnings impacts</ArticleTitle>
                  <JournalTitle>The Annals of the American Academy of Political and Social Science</JournalTitle>
                  <VolumeID>589</VolumeID>
                  <FirstPage>63</FirstPage>
                  <LastPage>93</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1177/0002716203254879</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Glazerman, S., Levy, D. M., &amp; Meyers, D. (2003). Nonexperimental versus experimental estimates of earnings impacts. <Emphasis Type="Italic">The Annals of the American Academy of Political and Social Science, 589</Emphasis>, 63–93.</BibUnstructured>
              </Citation>
              <Citation ID="CR002320">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>LM</Initials>
                    <FamilyName>Goldenhar</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Lamontagne</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>T</Initials>
                    <FamilyName>Katz</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>C</Initials>
                    <FamilyName>Heaney</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>P</Initials>
                    <FamilyName>Landsbergis</FamilyName>
                  </BibAuthorName>
                  <Year>2001</Year>
                  <ArticleTitle Language="En">The intervention research process in occupational safety and health: An overview from the national occupational research agenda intervention effectiveness team</ArticleTitle>
                  <JournalTitle>Journal of Occupational and Environmental Medicine</JournalTitle>
                  <VolumeID>43</VolumeID>
                  <FirstPage>616</FirstPage>
                  <LastPage>622</LastPage>
                  <Occurrence Type="PID">
                    <Handle>11464392</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1097/00043764-200107000-00008</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Goldenhar, L. M., Lamontagne, A., Katz, T., Heaney, C., &amp; Landsbergis, P. (2001). The intervention research process in occupational safety and health: An overview from the national occupational research agenda intervention effectiveness team. <Emphasis Type="Italic">Journal of Occupational and Environmental Medicine, 43</Emphasis>, 616–622.</BibUnstructured>
              </Citation>
              <Citation ID="CR002321">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Griffiths</FamilyName>
                  </BibAuthorName>
                  <Year>1999</Year>
                  <ArticleTitle Language="En">Organizational interventions. Facing the limits of the natural science paradigm</ArticleTitle>
                  <JournalTitle>Scandinavian Journal of Work, Environment and Health</JournalTitle>
                  <VolumeID>25</VolumeID>
                  <FirstPage>589</FirstPage>
                  <LastPage>596</LastPage>
                  <Occurrence Type="PID">
                    <Handle>10884158</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.5271/sjweh.485</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Griffiths, A. (1999). Organizational interventions. Facing the limits of the natural science paradigm. <Emphasis Type="Italic">Scandinavian Journal of Work, Environment and Health, 25</Emphasis>, 589–596.</BibUnstructured>
              </Citation>
              <Citation ID="CR002322">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>D</Initials>
                    <FamilyName>Hallfors</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>C</Initials>
                    <FamilyName>Hyunsan</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>V</Initials>
                    <FamilyName>Sanchez</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>S</Initials>
                    <FamilyName>Khatapoush</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>HM</Initials>
                    <FamilyName>Kim</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>D</Initials>
                    <FamilyName>Bauer</FamilyName>
                  </BibAuthorName>
                  <Year>2006</Year>
                  <ArticleTitle Language="En">Efficacy vs defectiveness trial results of an indicated “model” substance abuse program: Implications for public health</ArticleTitle>
                  <JournalTitle>American Journal of Public Health</JournalTitle>
                  <VolumeID>96</VolumeID>
                  <IssueID>12</IssueID>
                  <FirstPage>2254</FirstPage>
                  <LastPage>2259</LastPage>
                  <Occurrence Type="PID">
                    <Handle>16809591</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.2105/AJPH.2005.067462</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Hallfors, D., Hyunsan, C., Sanchez, V., Khatapoush, S., Kim, H. M., &amp; Bauer, D. (2006). Efficacy vs defectiveness trial results of an indicated “model” substance abuse program: Implications for public health. <Emphasis Type="Italic">American Journal of Public Health, 96</Emphasis>(12), 2254–2259.</BibUnstructured>
              </Citation>
              <Citation ID="CR002323">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>JA</Initials>
                    <FamilyName>Handy</FamilyName>
                  </BibAuthorName>
                  <Year>1988</Year>
                  <ArticleTitle Language="En">Theoretical and methodological problems within occupational stress and burnout research</ArticleTitle>
                  <JournalTitle>Work and Stress</JournalTitle>
                  <VolumeID>41</VolumeID>
                  <FirstPage>351</FirstPage>
                  <LastPage>369</LastPage>
<Occurrence Type="DOI">
<Handle>10.1177/001872678804100501</Handle>
</Occurrence>
                </BibArticle>
                <BibUnstructured>Handy, J. A. (1988). Theoretical and methodological problems within occupational stress and burnout research. <Emphasis Type="Italic">Work and Stress, 41</Emphasis>, 351–369.</BibUnstructured>
              </Citation>
              <Citation ID="CR002324">
                <BibUnstructured>Haskell, W. L., &amp; Blair, S. N. (1980). The physical activity component of health promotion in occupational settings. <Emphasis Type="Italic">Public Health Reports, 95</Emphasis>(2).</BibUnstructured>
              </Citation>
              <Citation ID="CR002325">
                <BibChapter>
                  <BibAuthorName>
                    <Initials>GT</Initials>
                    <FamilyName>Henry</FamilyName>
                  </BibAuthorName>
                  <Year>2010</Year>
                  <ChapterTitle Language="En">Comparison group designs</ChapterTitle>
                  <BibEditorName>
                    <Initials>JS</Initials>
                    <FamilyName>Wholey</FamilyName>
                  </BibEditorName>
                  <BibEditorName>
                    <Initials>HP</Initials>
                    <FamilyName>Hatry</FamilyName>
                  </BibEditorName>
                  <BibEditorName>
                    <Initials>KE</Initials>
                    <FamilyName>Newcomer</FamilyName>
                  </BibEditorName>
                  <Eds/>
                  <BookTitle>Handbook of practical program evaluation</BookTitle>
                  <EditionNumber>3</EditionNumber>
                  <PublisherName>Jossey-Bass</PublisherName>
                  <PublisherLocation>San Francisco, CA</PublisherLocation>
                </BibChapter>
                <BibUnstructured>Henry, G. T. (2010). Comparison group designs. In J. S. Wholey, H. P. Hatry, &amp; K. E. Newcomer (Eds.), <Emphasis Type="Italic">Handbook of practical program evaluation</Emphasis> (3rd ed.). San Francisco, CA: Jossey-Bass.</BibUnstructured>
              </Citation>
              <Citation ID="CR002326">
                <BibBook>
                  <BibAuthorName>
                    <Initials>AR</Initials>
                    <FamilyName>Holliday</FamilyName>
                  </BibAuthorName>
                  <Year>2007</Year>
                  <BookTitle>Doing and writing qualitative research</BookTitle>
                  <EditionNumber>2</EditionNumber>
                  <PublisherName>Sage</PublisherName>
                  <PublisherLocation>London</PublisherLocation>
                </BibBook>
                <BibUnstructured>Holliday, A. R. (2007). <Emphasis Type="Italic">Doing and writing qualitative research</Emphasis> (2nd ed.). London: Sage.</BibUnstructured>
              </Citation>
              <Citation ID="CR002327">
                <BibChapter>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Huhta</FamilyName>
                  </BibAuthorName>
                  <Year>2010</Year>
                  <ChapterTitle Language="En">Diagnostic and formative assessment</ChapterTitle>
                  <BibEditorName>
                    <Initials>B</Initials>
                    <FamilyName>Spolsky</FamilyName>
                  </BibEditorName>
                  <BibEditorName>
                    <Initials>FM</Initials>
                    <FamilyName>Hult</FamilyName>
                  </BibEditorName>
                  <Eds/>
                  <BookTitle>The handbook of educational linguistics</BookTitle>
                  <PublisherName>Blackwell</PublisherName>
                  <PublisherLocation>Oxford, UK</PublisherLocation>
                  <FirstPage>469</FirstPage>
                  <LastPage>482</LastPage>
                </BibChapter>
                <BibUnstructured>Huhta, A. (2010). Diagnostic and formative assessment. In B. Spolsky &amp; F. M. Hult (Eds.), <Emphasis Type="Italic">The handbook of educational linguistics</Emphasis> (pp. 469–482). Oxford, UK: Blackwell.</BibUnstructured>
              </Citation>
              <Citation ID="CR002328">
                <BibBook>
                  <BibAuthorName>
                    <Initials>JW</Initials>
                    <FamilyName>Itschuld</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>DD</Initials>
                    <FamilyName>Kumar</FamilyName>
                  </BibAuthorName>
                  <Year>2010</Year>
                  <BookTitle>Needs assessment: An overview</BookTitle>
                  <PublisherName>Sage</PublisherName>
                  <PublisherLocation>Thousand Oaks, CA</PublisherLocation>
                </BibBook>
                <BibUnstructured>Itschuld, J. W., &amp; Kumar, D. D. (2010). <Emphasis Type="Italic">Needs assessment: An overview</Emphasis>. Thousand Oaks, CA: Sage.</BibUnstructured>
              </Citation>
              <Citation ID="CR002329">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>FH</Initials>
                    <FamilyName>Jacobs</FamilyName>
                  </BibAuthorName>
                  <Year>2003</Year>
                  <ArticleTitle Language="En">Child and family program evaluation: Learning to enjoy complexity</ArticleTitle>
                  <JournalTitle>Applied Developmental Science</JournalTitle>
                  <VolumeID>7</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>62</FirstPage>
                  <LastPage>75</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1207/S1532480XADS0702_3</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Jacobs, F. H. (2003). Child and family program evaluation: Learning to enjoy complexity. <Emphasis Type="Italic">Applied Developmental Science, 7</Emphasis>(2), 62–75.</BibUnstructured>
              </Citation>
              <Citation ID="CR002330">
                <BibBook>
                  <BibAuthorName>
                    <Initials>RA</Initials>
                    <FamilyName>Krueger</FamilyName>
                  </BibAuthorName>
                  <Year>1994</Year>
                  <BookTitle>Focus groups: A practical guide for applied research</BookTitle>
                  <PublisherName>Sage</PublisherName>
                  <PublisherLocation>Newbury Park, CA</PublisherLocation>
                </BibBook>
                <BibUnstructured>Krueger, R. A. (1994). <Emphasis Type="Italic">Focus groups: A practical guide for applied research</Emphasis>. Newbury Park, CA: Sage.</BibUnstructured>
              </Citation>
              <Citation ID="CR002331">
                <BibUnstructured>Kung, H. C., Hoyert, D. L., Xu, J. Q., &amp; Murphy, S. L. (2008). Deaths: Final data for 2005. <Emphasis Type="Italic">National Vital Statistics Reports, 56</Emphasis>(10).</BibUnstructured>
              </Citation>
              <Citation ID="CR002332">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>L</Initials>
                    <FamilyName>Kwak</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>SPJ</Initials>
                    <FamilyName>Kremers</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Werman</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>TLS</Initials>
                    <FamilyName>Visscher</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>MA</Initials>
                    <FamilyName>Baak</FamilyName>
                    <Particle>van</Particle>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Burg</FamilyName>
                  </BibAuthorName>
                  <Year>2006</Year>
                  <ArticleTitle Language="En">The NHF-NRG IN Balance-project: the application of Intervention Mapping in the development, implementation and evaluation of weight gain prevent at worksite</ArticleTitle>
                  <JournalTitle>Obesity Reviews</JournalTitle>
                  <VolumeID>8</VolumeID>
                  <FirstPage>347</FirstPage>
                  <LastPage>361</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1111/j.1467-789X.2006.00304.x</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Kwak, L., Kremers, S. P. J., Werman, A., Visscher, T. L. S., van Baak, M. A., &amp; Burg, J. (2006). The NHF-NRG IN Balance-project: the application of Intervention Mapping in the development, implementation and evaluation of weight gain prevent at worksite. <Emphasis Type="Italic">Obesity Reviews, 8</Emphasis>, 347–361.</BibUnstructured>
              </Citation>
              <Citation ID="CR002333">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>MJ</Initials>
                    <FamilyName>Lambert</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>GM</Initials>
                    <FamilyName>Burlingame</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>V</Initials>
                    <FamilyName>Umphress</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>NB</Initials>
                    <FamilyName>Hansen</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>DA</Initials>
                    <FamilyName>Vermeersch</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>GC</Initials>
                    <FamilyName>Clouse</FamilyName>
                  </BibAuthorName>
                  <Etal/>
                  <Year>1996</Year>
                  <ArticleTitle Language="En">The reliability and validity of the outcome questionnaire</ArticleTitle>
                  <JournalTitle>Clinical Psychology &amp; Psychotherapy</JournalTitle>
                  <VolumeID>3</VolumeID>
                  <IssueID>4</IssueID>
                  <FirstPage>249</FirstPage>
                  <LastPage>258</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1002/(SICI)1099-0879(199612)3:4&lt;249::AID-CPP106&gt;3.0.CO;2-S</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Lambert, M. J., Burlingame, G. M., Umphress, V., Hansen, N. B., Vermeersch, D. A., Clouse, G. C., et al. (1996). The reliability and validity of the outcome questionnaire. <Emphasis Type="Italic">Clinical Psychology &amp; Psychotherapy, 3</Emphasis>(4), 249–258.</BibUnstructured>
              </Citation>
              <Citation ID="CR002334">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>MS</Initials>
                    <FamilyName>Lauer</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>FS</Initials>
                    <FamilyName>Collins</FamilyName>
                  </BibAuthorName>
                  <Year>2010</Year>
                  <ArticleTitle Language="En">Using science to improve the nation’s health system</ArticleTitle>
                  <JournalTitle>JAMA: The Journal of the American Medical Association</JournalTitle>
                  <VolumeID>303</VolumeID>
                  <IssueID>21</IssueID>
                  <FirstPage>2182</FirstPage>
                  <LastPage>2183</LastPage>
                  <BibArticleDOI>10.1001/jama.2010.726</BibArticleDOI>
                  <Occurrence Type="DOI">
                    <Handle>10.1001/jama.2010.726</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Lauer, M. S., &amp; Collins, F. S. (2010). Using science to improve the nation’s health system. <Emphasis Type="Italic">JAMA: The Journal of the American Medical Association, 303</Emphasis>(21), 2182–2183. doi:<ExternalRef>
                    <RefSource>10.1001/jama.2010.726</RefSource>
                    <RefTarget Address="10.1001/jama.2010.726" TargetType="DOI"/>
                  </ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR002335">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>MV</Initials>
                    <FamilyName>Lipsey</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>DS</Initials>
                    <FamilyName>Cordray</FamilyName>
                  </BibAuthorName>
                  <Year>2000</Year>
                  <ArticleTitle Language="En">Evaluation methods for social intervention</ArticleTitle>
                  <JournalTitle>Annual Review of Psychology</JournalTitle>
                  <VolumeID>51</VolumeID>
                  <FirstPage>345</FirstPage>
                  <LastPage>375</LastPage>
                  <Occurrence Type="PID">
                    <Handle>10751975</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1146/annurev.psych.51.1.345</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Lipsey, M. V., &amp; Cordray, D. S. (2000). Evaluation methods for social intervention. <Emphasis Type="Italic">Annual Review of Psychology, 51</Emphasis>, 345–375.</BibUnstructured>
              </Citation>
              <Citation ID="CR002336">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>L</Initials>
                    <FamilyName>Manchikanti</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>FJ</Initials>
                    <FamilyName>Falco</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>MV</Initials>
                    <FamilyName>Boswell</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>JA</Initials>
                    <FamilyName>Hirsch</FamilyName>
                  </BibAuthorName>
                  <Year>2010</Year>
                  <ArticleTitle Language="En">Facts, fallacies, and politics of comparative effectiveness research: Part I. Basic considerations</ArticleTitle>
                  <JournalTitle>Pain Physician</JournalTitle>
                  <VolumeID>13</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>E23</FirstPage>
                  <LastPage>E54</LastPage>
                  <Occurrence Type="PID">
                    <Handle>20119474</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Manchikanti, L., Falco, F. J., Boswell, M. V., &amp; Hirsch, J. A. (2010). Facts, fallacies, and politics of comparative effectiveness research: Part I. Basic considerations. <Emphasis Type="Italic">Pain Physician, 13</Emphasis>(1), E23–E54.</BibUnstructured>
              </Citation>
              <Citation ID="CR002337">
                <BibChapter>
                  <BibAuthorName>
                    <Initials>WJ</Initials>
                    <FamilyName>McGuire</FamilyName>
                  </BibAuthorName>
                  <Year>1989</Year>
                  <ChapterTitle Language="En">Theoretical foundations of campaigns</ChapterTitle>
                  <BibEditorName>
                    <Initials>RE</Initials>
                    <FamilyName>Rice</FamilyName>
                  </BibEditorName>
                  <BibEditorName>
                    <Initials>CK</Initials>
                    <FamilyName>Atkins</FamilyName>
                  </BibEditorName>
                  <Eds/>
                  <BookTitle>Public communications campaigns</BookTitle>
                  <EditionNumber>3</EditionNumber>
                  <PublisherName>Sage</PublisherName>
                  <PublisherLocation>Thousand Oaks, CA</PublisherLocation>
                </BibChapter>
                <BibUnstructured>McGuire, W. J. (1989). Theoretical foundations of campaigns. In R. E. Rice &amp; C. K. Atkins (Eds.), <Emphasis Type="Italic">Public communications campaigns</Emphasis> (3rd ed.). Thousand Oaks, CA: Sage.</BibUnstructured>
              </Citation>
              <Citation ID="CR002338">
                <BibBook>
                  <BibAuthorName>
                    <Initials>RK</Initials>
                    <FamilyName>Merton</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Fiske</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>P</Initials>
                    <FamilyName>Kendall</FamilyName>
                  </BibAuthorName>
                  <Year>1956</Year>
                  <BookTitle>The focused interview: A manual of problems &amp; procedures</BookTitle>
                  <PublisherName>Free Press</PublisherName>
                  <PublisherLocation>Glencoe, IL</PublisherLocation>
                </BibBook>
                <BibUnstructured>Merton, R. K., Fiske, M., &amp; Kendall, P. (1956). <Emphasis Type="Italic">The focused interview: A manual of problems &amp; procedures</Emphasis>. Glencoe, IL: Free Press.</BibUnstructured>
              </Citation>
              <Citation ID="CR002339">
                <BibBook>
                  <BibAuthorName>
                    <Initials>MB</Initials>
                    <FamilyName>Miles</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>AM</Initials>
                    <FamilyName>Huberman</FamilyName>
                  </BibAuthorName>
                  <Year>1994</Year>
                  <BookTitle>Qualitative data analysis: An expanded sourcebook</BookTitle>
                  <PublisherName>Sage</PublisherName>
                  <PublisherLocation>Newbury Park, CA</PublisherLocation>
                </BibBook>
                <BibUnstructured>Miles, M. B., &amp; Huberman, A. M. (1994). <Emphasis Type="Italic">Qualitative data analysis: An expanded sourcebook</Emphasis>. Newbury Park, CA: Sage.</BibUnstructured>
              </Citation>
              <Citation ID="CR002340">
                <BibChapter>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Nielsen</FamilyName>
                  </BibAuthorName>
                  <Year>1994</Year>
                  <ChapterTitle Language="En">Heuristic evaluation</ChapterTitle>
                  <BibEditorName>
                    <Initials>J</Initials>
                    <FamilyName>Nielsen</FamilyName>
                  </BibEditorName>
                  <BibEditorName>
                    <Initials>RL</Initials>
                    <FamilyName>Mack</FamilyName>
                  </BibEditorName>
                  <Eds/>
                  <BookTitle>Usability inspection methods</BookTitle>
                  <PublisherName>John Wiley &amp; Sons</PublisherName>
                  <PublisherLocation>New York, NY</PublisherLocation>
                </BibChapter>
                <BibUnstructured>Nielsen, J. (1994). Heuristic evaluation. In J. Nielsen &amp; R. L. Mack (Eds.), <Emphasis Type="Italic">Usability inspection methods</Emphasis>. New York, NY: John Wiley &amp; Sons.</BibUnstructured>
              </Citation>
              <Citation ID="CR002341">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>D</Initials>
                    <FamilyName>Nutbeam</FamilyName>
                  </BibAuthorName>
                  <Year>2000</Year>
                  <ArticleTitle Language="En">Health literacy as a public health goal: A challenge for contemporary health education and communication strategies into the 21st century</ArticleTitle>
                  <JournalTitle>Health Promotion International</JournalTitle>
                  <VolumeID>15</VolumeID>
                  <IssueID>3</IssueID>
                  <FirstPage>259</FirstPage>
                  <LastPage>267</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1093/heapro/15.3.259</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Nutbeam, D. (2000). Health literacy as a public health goal: A challenge for contemporary health education and communication strategies into the 21st century. <Emphasis Type="Italic">Health Promotion International, 15</Emphasis>(3), 259–267.</BibUnstructured>
              </Citation>
              <Citation ID="CR002342">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>K</Initials>
                    <FamilyName>Nytrø</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>PO</Initials>
                    <FamilyName>Saksvik</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Mikkelsen</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>P</Initials>
                    <FamilyName>Bohle</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Quinlan</FamilyName>
                  </BibAuthorName>
                  <Year>2000</Year>
                  <ArticleTitle Language="En">An appraisal of key factors in the implementation of occupational stress interventions</ArticleTitle>
                  <JournalTitle>Work and Stress</JournalTitle>
                  <VolumeID>14</VolumeID>
                  <FirstPage>213</FirstPage>
                  <LastPage>225</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1080/02678370010024749</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Nytrø, K., Saksvik, P. O., Mikkelsen, A., Bohle, P., &amp; Quinlan, M. (2000). An appraisal of key factors in the implementation of occupational stress interventions. <Emphasis Type="Italic">Work and Stress, 14</Emphasis>, 213–225.</BibUnstructured>
              </Citation>
              <Citation ID="CR002343">
                <BibChapter>
                  <BibAuthorName>
                    <Initials>C</Initials>
                    <FamilyName>Potter</FamilyName>
                  </BibAuthorName>
                  <Year>2006</Year>
                  <ChapterTitle Language="En">Program evaluation</ChapterTitle>
                  <BibEditorName>
                    <Initials>K</Initials>
                    <FamilyName>Durrheim</FamilyName>
                  </BibEditorName>
                  <BibEditorName>
                    <Initials>D</Initials>
                    <FamilyName>Painter</FamilyName>
                  </BibEditorName>
                  <BibEditorName>
                    <Initials>M</Initials>
                    <FamilyName>Terre Blanche</FamilyName>
                  </BibEditorName>
                  <Eds/>
                  <BookTitle>Research in practice: Applied methods for the social sciences</BookTitle>
                  <EditionNumber>2</EditionNumber>
                  <PublisherName>UCT Press</PublisherName>
                  <PublisherLocation>Cape Town</PublisherLocation>
                </BibChapter>
                <BibUnstructured>Potter, C. (2006). Program evaluation. In K. Durrheim, D. Painter, &amp; M. Terre Blanche (Eds.), <Emphasis Type="Italic">Research in practice: Applied methods for the social sciences</Emphasis> (2nd ed.). Cape Town: UCT Press.</BibUnstructured>
              </Citation>
              <Citation ID="CR002344">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>JO</Initials>
                    <FamilyName>Prochaska</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>CC</Initials>
                    <FamilyName>Diclemente</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>JC</Initials>
                    <FamilyName>Norcross</FamilyName>
                  </BibAuthorName>
                  <Year>1992</Year>
                  <ArticleTitle Language="En">In search of how people change: Applications to addictive behaviors</ArticleTitle>
                  <JournalTitle>The American Psychologist</JournalTitle>
                  <VolumeID>47</VolumeID>
                  <FirstPage>1102</FirstPage>
                  <LastPage>1114</LastPage>
                  <Occurrence Type="PID">
                    <Handle>1329589</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1037/0003-066X.47.9.1102</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Prochaska, J. O., Diclemente, C. C., &amp; Norcross, J. C. (1992). In search of how people change: Applications to addictive behaviors. <Emphasis Type="Italic">The American Psychologist, 47</Emphasis>, 1102–1114.</BibUnstructured>
              </Citation>
              <Citation ID="CR002345">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>IM</Initials>
                    <FamilyName>Rosenstock</FamilyName>
                  </BibAuthorName>
                  <Year>1974</Year>
                  <ArticleTitle Language="En">Historical origins of the health belief model</ArticleTitle>
                  <JournalTitle>Health Education Monographs</JournalTitle>
                  <VolumeID>2</VolumeID>
                  <FirstPage>328</FirstPage>
                  <LastPage>335</LastPage>
<Occurrence Type="DOI">
<Handle>10.1177/109019817400200403</Handle>
</Occurrence>
                </BibArticle>
                <BibUnstructured>Rosenstock, I. M. (1974). Historical origins of the health belief model. <Emphasis Type="Italic">Health Education Monographs, 2</Emphasis>, 328–335.</BibUnstructured>
              </Citation>
              <Citation ID="CR002346">
                <BibBook>
                  <BibAuthorName>
                    <Initials>PH</Initials>
                    <FamilyName>Rossi</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>HE</Initials>
                    <FamilyName>Freeman</FamilyName>
                  </BibAuthorName>
                  <Year>1993</Year>
                  <BookTitle>Evaluation: A systematic approach</BookTitle>
                  <EditionNumber>5</EditionNumber>
                  <PublisherName>Sage</PublisherName>
                  <PublisherLocation>Newbury Park, CA</PublisherLocation>
                </BibBook>
                <BibUnstructured>Rossi, P. H., &amp; Freeman, H. E. (1993). <Emphasis Type="Italic">Evaluation: A systematic approach</Emphasis> (5th ed.). Newbury Park, CA: Sage.</BibUnstructured>
              </Citation>
              <Citation ID="CR002347">
                <BibBook>
                  <BibAuthorName>
                    <Initials>P</Initials>
                    <FamilyName>Rossi</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>MW</Initials>
                    <FamilyName>Lipsey</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>HE</Initials>
                    <FamilyName>Freeman</FamilyName>
                  </BibAuthorName>
                  <Year>2004</Year>
                  <BookTitle>Evaluation: A systematic approach</BookTitle>
                  <EditionNumber>7</EditionNumber>
                  <PublisherName>Sage</PublisherName>
                  <PublisherLocation>Thousand Oaks, CA</PublisherLocation>
                </BibBook>
                <BibUnstructured>Rossi, P., Lipsey, M. W., &amp; Freeman, H. E. (2004). <Emphasis Type="Italic">Evaluation: A systematic approach</Emphasis> (7th ed.). Thousand Oaks, CA: Sage.</BibUnstructured>
              </Citation>
              <Citation ID="CR002348">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>R</Initials>
                    <FamilyName>Ryan</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>N</Initials>
                    <FamilyName>Gross</FamilyName>
                  </BibAuthorName>
                  <Year>1943</Year>
                  <ArticleTitle Language="En">The diffusion of hybrid seed corn in two Iowa communities</ArticleTitle>
                  <JournalTitle>Rural Sociology</JournalTitle>
                  <VolumeID>8</VolumeID>
                  <FirstPage>15</FirstPage>
                  <LastPage>24</LastPage>
                </BibArticle>
                <BibUnstructured>Ryan, R., &amp; Gross, N. (1943). The diffusion of hybrid seed corn in two Iowa communities. <Emphasis Type="Italic">Rural Sociology, 8</Emphasis>, 15–24.</BibUnstructured>
              </Citation>
              <Citation ID="CR002349">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>PO</Initials>
                    <FamilyName>Saksvik</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>K</Initials>
                    <FamilyName>Nytro</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>C</Initials>
                    <FamilyName>Dahl-Jorgensen</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Mikkelsen</FamilyName>
                  </BibAuthorName>
                  <Year>2002</Year>
                  <ArticleTitle Language="En">A process evaluation of individual and organizational occupational stress and health interventions</ArticleTitle>
                  <JournalTitle>Work and Stress</JournalTitle>
                  <VolumeID>16</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>37</FirstPage>
                  <LastPage>57</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1080/02678370110118744</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Saksvik, P. O., Nytro, K., Dahl-Jorgensen, C., &amp; Mikkelsen, A. (2002). A process evaluation of individual and organizational occupational stress and health interventions. <Emphasis Type="Italic">Work and Stress, 16</Emphasis>(1), 37–57.</BibUnstructured>
              </Citation>
              <Citation ID="CR002350">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>L</Initials>
                    <FamilyName>Sechrest</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>AJ</Initials>
                    <FamilyName>Figueredo</FamilyName>
                  </BibAuthorName>
                  <Year>1993</Year>
                  <ArticleTitle Language="En">Program evaluation</ArticleTitle>
                  <JournalTitle>Annual Review of Psychology</JournalTitle>
                  <VolumeID>44</VolumeID>
                  <FirstPage>645</FirstPage>
                  <LastPage>674</LastPage>
                  <Occurrence Type="PID">
                    <Handle>19845457</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1146/annurev.ps.44.020193.003241</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Sechrest, L., &amp; Figueredo, A. J. (1993). Program evaluation. <Emphasis Type="Italic">Annual Review of Psychology, 44</Emphasis>, 645–674.</BibUnstructured>
              </Citation>
              <Citation ID="CR002351">
                <BibBook>
                  <BibAuthorName>
                    <Initials>WR</Initials>
                    <FamilyName>Shadish</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>TD</Initials>
                    <FamilyName>Cook</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>LC</Initials>
                    <FamilyName>Leviton</FamilyName>
                  </BibAuthorName>
                  <Year>1991</Year>
                  <BookTitle>Foundations of program evaluation; Theories of practice</BookTitle>
                  <PublisherName>Sage</PublisherName>
                  <PublisherLocation>Newbury Park, CA</PublisherLocation>
                </BibBook>
                <BibUnstructured>Shadish, W. R., Cook, T. D., &amp; Leviton, L. C. (1991). <Emphasis Type="Italic">Foundations of program evaluation; Theories of practice</Emphasis>. Newbury Park, CA: Sage.</BibUnstructured>
              </Citation>
              <Citation ID="CR002352">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>HC</Initials>
                    <FamilyName>Sox</FamilyName>
                  </BibAuthorName>
                  <Year>2010</Year>
                  <ArticleTitle Language="En">Comparative effectiveness research: A progress report</ArticleTitle>
                  <JournalTitle>Annals of Internal Medicine</JournalTitle>
                  <VolumeID>153</VolumeID>
                  <FirstPage>469</FirstPage>
                  <LastPage>472</LastPage>
                  <Occurrence Type="PID">
                    <Handle>20679544</Handle>
                  </Occurrence>
<Occurrence Type="DOI">
<Handle>10.7326/0003-4819-153-7-201010050-00269</Handle>
</Occurrence>
                </BibArticle>
                <BibUnstructured>Sox, H. C. (2010). Comparative effectiveness research: A progress report. <Emphasis Type="Italic">Annals of Internal Medicine, 153</Emphasis>, 469–472.</BibUnstructured>
              </Citation>
              <Citation ID="CR002353">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>SS</Initials>
                    <FamilyName>Stevens</FamilyName>
                  </BibAuthorName>
                  <Year>1946</Year>
                  <ArticleTitle Language="En">On the theory of scales of measurement</ArticleTitle>
                  <JournalTitle>Science</JournalTitle>
                  <VolumeID>103</VolumeID>
                  <IssueID>2684</IssueID>
                  <FirstPage>677</FirstPage>
                  <LastPage>680</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1126/science.103.2684.677</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Stevens, S. S. (1946). On the theory of scales of measurement. <Emphasis Type="Italic">Science, 103</Emphasis>(2684), 677–680.</BibUnstructured>
              </Citation>
              <Citation ID="CR002354">
                <BibChapter>
                  <BibAuthorName>
                    <Initials>CJ</Initials>
                    <FamilyName>Torgerson</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>DJ</Initials>
                    <FamilyName>Torgerson</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>CA</Initials>
                    <FamilyName>Taylor</FamilyName>
                  </BibAuthorName>
                  <Year>2010</Year>
                  <ChapterTitle Language="En">Randomized controlled trials and nonrandomized designs</ChapterTitle>
                  <BibEditorName>
                    <Initials>JS</Initials>
                    <FamilyName>Wholey</FamilyName>
                  </BibEditorName>
                  <BibEditorName>
                    <Initials>HP</Initials>
                    <FamilyName>Hatry</FamilyName>
                  </BibEditorName>
                  <BibEditorName>
                    <Initials>KE</Initials>
                    <FamilyName>Newcomer</FamilyName>
                  </BibEditorName>
                  <Eds/>
                  <BookTitle>Handbook of practical program evaluation</BookTitle>
                  <EditionNumber>3</EditionNumber>
                  <PublisherName>Jossey-Bass</PublisherName>
                  <PublisherLocation>San Francisco, CA</PublisherLocation>
                </BibChapter>
                <BibUnstructured>Torgerson, C. J., Torgerson, D. J., &amp; Taylor, C. A. (2010). Randomized controlled trials and nonrandomized designs. In J. S. Wholey, H. P. Hatry, &amp; K. E. Newcomer (Eds.), <Emphasis Type="Italic">Handbook of practical program evaluation</Emphasis> (3rd ed.). San Francisco, CA: Jossey-Bass.</BibUnstructured>
              </Citation>
              <Citation ID="CR002355">
                <BibUnstructured>The Health Communication Unit at the Centre for Health Promotion University of Toronto title: Evaluating Heatlh Promotion Programs Year: 2007</BibUnstructured>
              </Citation>
              <Citation ID="CR002356">
                <BibBook>
                  <BibAuthorName>
                    <Initials>T</Initials>
                    <FamilyName>Valente</FamilyName>
                  </BibAuthorName>
                  <Year>2002</Year>
                  <BookTitle>Evaluating health promotion programs</BookTitle>
                  <PublisherName>Oxford University Press</PublisherName>
                  <PublisherLocation>New York</PublisherLocation>
                </BibBook>
                <BibUnstructured>Valente, T. (2002). <Emphasis Type="Italic">Evaluating health promotion programs</Emphasis>. New York: Oxford University Press.</BibUnstructured>
              </Citation>
              <Citation ID="CR002357">
                <BibArticle>
                  <BibAuthorName>
                    <Initials>RA</Initials>
                    <FamilyName>Virzi</FamilyName>
                  </BibAuthorName>
                  <Year>1992</Year>
                  <ArticleTitle Language="En">Refining the test phase of usability evaluation: How many subjects is enough?</ArticleTitle>
                  <JournalTitle>Human Factors</JournalTitle>
                  <VolumeID>34</VolumeID>
                  <IssueID>4</IssueID>
                  <FirstPage>457</FirstPage>
                  <LastPage>468</LastPage>
                </BibArticle>
                <BibUnstructured>Virzi, R. A. (1992). Refining the test phase of usability evaluation: How many subjects is enough? <Emphasis Type="Italic">Human Factors, 34</Emphasis>(4), 457–468.</BibUnstructured>
              </Citation>
            </Bibliography>
          </ChapterBackmatter>
        </Chapter>
      </Part>
    </Book>
  </Series>
</Publisher>
