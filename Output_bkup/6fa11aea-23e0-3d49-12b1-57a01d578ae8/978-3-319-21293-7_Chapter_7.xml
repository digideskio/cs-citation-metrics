<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE Publisher PUBLIC "-//Springer-Verlag//DTD A++ V2.4//EN" "http://devel.springer.de/A++/V2.4/DTD/A++V2.4.dtd">
<Publisher>
  <PublisherInfo>
    <PublisherName>Springer International Publishing</PublisherName>
    <PublisherLocation>Cham</PublisherLocation>
    <PublisherImprintName>Springer</PublisherImprintName>
    <PublisherURL>http://www.springer.com</PublisherURL>
  </PublisherInfo>
  <Book Language="En" OutputMedium="All">
    <BookInfo BookProductType="Professional book" ContainsESM="No" Language="En" MediaType="eBook" NumberingDepth="3" NumberingStyle="ChapterContent" OutputMedium="All" TocLevels="0">
      <BookID>978-3-319-21293-7</BookID>
      <BookTitle>Modern Stroke Rehabilitation through e-Health-based Entertainment</BookTitle>
      <BookDOI>10.1007/978-3-319-21293-7</BookDOI>
      <BookTitleID>327370</BookTitleID>
      <BookPrintISBN>978-3-319-21292-0</BookPrintISBN>
      <BookElectronicISBN>978-3-319-21293-7</BookElectronicISBN>
      <BookEdition>1st ed. 2016</BookEdition>
      <BookChapterCount>10</BookChapterCount>
      <BookCopyright>
        <CopyrightHolderName>Springer International Publishing Switzerland</CopyrightHolderName>
        <CopyrightYear>2016</CopyrightYear>
        <CopyrightStandardText Language="En">This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed.</CopyrightStandardText>
      </BookCopyright>
      <BookSubjectGroup>
        <BookSubject Code="SCT" Type="Primary">Engineering</BookSubject>
        <BookSubject Code="SCT2700X" Priority="1" Type="Secondary">Biomedical Engineering</BookSubject>
        <BookSubject Code="SCT24068" Priority="2" Type="Secondary">Circuits and Systems</BookSubject>
        <BookSubject Code="SCH55006" Priority="3" Type="Secondary">Rehabilitation</BookSubject>
        <BookSubject Code="SCH27002" Priority="4" Type="Secondary">Public Health</BookSubject>
        <BookSubject Code="SCI18059" Priority="5" Type="Secondary">Multimedia Information Systems</BookSubject>
        <SubjectCollection Code="SUCO11647">Engineering</SubjectCollection>
      </BookSubjectGroup>
      <BookstoreLocation>Electrical Engineering</BookstoreLocation>
    </BookInfo>
    <BookHeader>
      <EditorGroup>
        <Editor AffiliationIDS="Aff1">
          <EditorName DisplayOrder="Western">
            <GivenName>Emmanouela</GivenName>
            <FamilyName>Vogiatzaki</FamilyName>
          </EditorName>
          <Contact>
            <Email>emmanouela@rfsat.com</Email>
          </Contact>
        </Editor>
        <Editor AffiliationIDS="Aff2">
          <EditorName DisplayOrder="Western">
            <GivenName>Artur</GivenName>
            <FamilyName>Krukowski</FamilyName>
          </EditorName>
          <Contact>
            <Email>krukowa@intracom-telecom.com</Email>
          </Contact>
        </Editor>
        <Affiliation ID="Aff1">
          <OrgDivision>(RFSAT) Ltd</OrgDivision>
          <OrgName>Research for Science, Art and Technology</OrgName>
          <OrgAddress>
            <City>Sheffield</City>
            <Country>United Kingdom</Country>
          </OrgAddress>
        </Affiliation>
        <Affiliation ID="Aff2">
          <OrgName>Intracom S. A. Telecom Solutions</OrgName>
          <OrgAddress>
            <City>Peania</City>
            <Country>Greece</Country>
          </OrgAddress>
        </Affiliation>
      </EditorGroup>
    </BookHeader>
    <Chapter ID="b978-3-319-21293-7_7" Language="En" OutputMedium="All">
      <ChapterInfo ChapterType="OriginalPaper" ContainsESM="No" Language="En" NumberingDepth="3" NumberingStyle="ChapterContent" OutputMedium="All" TocLevels="0">
        <ChapterID>7</ChapterID>
        <ChapterNumber>Chapter 7</ChapterNumber>
        <ChapterDOI>10.1007/978-3-319-21293-7_7</ChapterDOI>
        <ChapterSequenceNumber>7</ChapterSequenceNumber>
        <ChapterTitle Language="En">Virtual Reality Gaming with Immersive User Interfaces</ChapterTitle>
        <ChapterFirstPage>195</ChapterFirstPage>
        <ChapterLastPage>214</ChapterLastPage>
        <ChapterCopyright>
          <CopyrightHolderName>Springer International Publishing Switzerland</CopyrightHolderName>
          <CopyrightYear>2016</CopyrightYear>
        </ChapterCopyright>
        <ChapterHistory>
          <RegistrationDate>
            <Year>2015</Year>
            <Month>6</Month>
            <Day>11</Day>
          </RegistrationDate>
        </ChapterHistory>
        <ChapterContext>
          <BookID>978-3-319-21293-7</BookID>
          <BookTitle>Modern Stroke Rehabilitation through e-Health-based Entertainment</BookTitle>
        </ChapterContext>
      </ChapterInfo>
      <ChapterHeader>
        <AuthorGroup>
          <Author AffiliationIDS="Aff3" CorrespondingAffiliationID="Aff3">
            <AuthorName DisplayOrder="Western">
              <GivenName>Emmanouela</GivenName>
              <FamilyName>Vogiatzaki</FamilyName>
            </AuthorName>
            <Contact>
              <Email>emmanouela@rfsat.com</Email>
            </Contact>
          </Author>
          <Author AffiliationIDS="Aff4">
            <AuthorName DisplayOrder="Western">
              <GivenName>Artur</GivenName>
              <FamilyName>Krukowski</FamilyName>
            </AuthorName>
          </Author>
          <Affiliation ID="Aff3">
            <OrgName>Research for Science, Art and Technology (RFSAT) Ltd</OrgName>
            <OrgAddress>
              <City>Sheffield</City>
              <Country>UK</Country>
            </OrgAddress>
          </Affiliation>
          <Affiliation ID="Aff4">
            <OrgName>Intracom S. A. Telecom Solutions</OrgName>
            <OrgAddress>
              <City>Peania</City>
              <Country>Greece</Country>
            </OrgAddress>
          </Affiliation>
        </AuthorGroup>
        <Abstract ID="Abs1" Language="En" OutputMedium="All">
          <Heading>Abstract</Heading>
          <Para ID="Par1">Stroke is one of most common diseases of our modern societies with high socio-economic impact. Hence, rehabilitation approach involving patients in their rehabilitation process while lowering costly involvement of specialised human personnel is needed. This chapter describes a novel approach, offering an integrated rehabilitation training for stroke patients using a serious gaming approach based on a Unity3D virtual reality engine combined with a range of advanced technologies and immersive user interfaces. It puts patients and caretakers in control of the rehabilitation protocols, while leading physicians are enabled to supervise the progress of the rehabilitation via Personal Health Record. Possibility to perform training in a familiar home environment directly improves the effectiveness and compliance in performing personal rehabilitation.</Para>
        </Abstract>
      </ChapterHeader>
      <Body>
        <Section1 ID="Sec1">
          <Heading>Introduction</Heading>
          <Para ID="Par2">Stroke affects about 2 Million [<CitationRef CitationID="CR1">1</CitationRef>] people every year in Europe. For these people the effect of stroke is that they lose certain physical and cognitive abilities at least for a certain period. More than one-third of these patients i.e. more than 670,000 people return to their home with some level of permanent disability leading to a significant reduction of quality of life, which affects not only the patients themselves but also their relatives. This also increases costs of the healthcare services associated with hospitalisation, home services and rehabilitation. Therefore, there is a strong need to improve ambulant care model, in particular, at the home settings, involving the patients into the care pathway, for achieving maximal outcome in terms of clinical as well as quality of life.</Para>
        </Section1>
        <Section1 ID="Sec2">
          <Heading>The Concept</Heading>
          <Para ID="Par3">The StrokeBack project addresses both of the indicated problem areas. The goal of the project is the development of a telemedicine system to support ambulant rehabilitation at home settings for the stroke patients with minimal human intervention. With StrokeBack, the patients would be able to perform rehabilitation in their own home where they feel psychologically better than in care centres. In addition, the contact hours with a physiotherapist could be reduced thus leading to a direct reduction of healthcare cost. By ensuring proper execution of physiotherapy trainings in an automated guided way modulated by appropriate clinical knowledge and in supervised way only when necessary, StrokeBack aims to empower and stimulates patients to exercise more while achieving better quality and effectiveness than it would be possible today. This way StrokeBack system is expected to improve rehabilitation speed, while ensuring high quality of life for patients by enabling them to continue rehabilitation in their familiar home environments instead of subjecting them to alien and stressful hospital settings. This offers also means of reducing indirect healthcare cost as well.</Para>
          <Para ID="Par4">The concept of StrokeBack is complemented by a Patient Health Record (PHR) system in which training measurements and vital physiological and personal patient data are stored. Thus, PHR provides all the necessary medical and personal information for the patient that rehabilitation experts might need in order to evaluate the effectiveness and success of the rehabilitation, e.g. to deduce relations between selected exercises and rehabilitation speed of different patients as well as to assess the overall healthiness of the patient. In addition, the PHR can be used to provide the patient with mid-term feedback, e.g. her/his, rehabilitation speed compared to average as well as improvements over last day/weeks, in order to keep patient motivation high.</Para>
          <Para ID="Par5">The StrokeBack project aims at increasing the rehabilitation speed of stroke patients while patients are in their own home. The benefit we expect from our approach is twofold. Most patients feel psychologically better in their own environment than in hospital and rehabilitation speed is improved. Furthermore, we focus on increasing patients’ motivation when exercising with tools similar to a gaming console.</Para>
          <Para ID="Par6">The StrokeBack concept puts the patient into the centre of the rehabilitation process. It aims at exploiting the fact the patients feel better at home, that it has been shown that patients train more if the training is combined with attractive training environments [<CitationRef CitationID="CR2">2</CitationRef>, <CitationRef CitationID="CR3">3</CitationRef>]. First, the patients learn physical rehabilitation exercises from a therapist at the care centre or in a therapists’ practice. Then the patients can exercise at home with the StrokeBack system monitoring their execution and providing a real-time feedback on whether the execution was correct or not. In addition, it records the training results and vital parameters of the patient. This data can be subsequently analysed by the medical experts for assessment of the patient recovery. Furthermore, the patient may also receive midterm feedback on her/his personal recovery process. In order to ensure proper guidance of the patient, the therapist also gets information from the PHR to assess the recovery process enabling him to decide whether other training sequences should be used, which are then introduced to the patient in the practice again.</Para>
        </Section1>
        <Section1 ID="Sec3">
          <Heading>Game-Based Rehabilitation</Heading>
          <Para ID="Par7">The use of virtual, augmented or mixed-reality environments for training and rehabilitation of post-stroke patients opens an attractive avenue in improving various negative effects occurring because of brain traumas. Those include helping in the recovery of the motor skills, limb-eye coordination, orientation in space, everyday tasks, etc. Training may range from simple goal-directed limb movements aimed at achieving a given goal (e.g. putting a coffee cup on a table), improving lost motor skills (e.g. virtual driving), and others. In order to increase the efficiency of the exercises advanced haptic interfaces are developed, allowing direct body stimulation and use of physical objects within virtual settings, supplementing the visual stimulation.</Para>
          <Para ID="Par8">Immersive environments have quickly been found attractive for remote home-based rehabilitation giving raise to both individual and monitored by therapists remotely. Depending on the type of a physical interface, different types of exercises are possible. Interfaces like Cyber Glove [<CitationRef CitationID="CR4">4</CitationRef>] or Rutgers RMII Master [<CitationRef CitationID="CR5">5</CitationRef>] allow the transfer of patient’s limb movement into the virtual gaming environment. They employ a set of pressure-sensing servos, one per finger, combined with motion sensing. This allows therapists to perform, e.g. range of motion, speed, fractionation (e.g. moving individual fingers) and strength (via pressure-sensing) tasks. Games include two categories: physical exercises (e.g. DigiKey, Power Putty) and functional rehabilitation (e.g. Peg Board or Ball Game) ones. They use computer monitors for visual feedback. Cyber Glove has been used by Rehabilitation Institute of Chicago [<CitationRef CitationID="CR2">2</CitationRef>] also for assessing the pattern of finger movements during grasp and movement space determination for diverse stroke conditions. Virtual environments are increasingly used for functional training and simulation of natural environments, e.g. home, work, outdoor. Exercises may range from simple goal-directed movements [<CitationRef CitationID="CR6">6</CitationRef>] to learn/train for execution of everyday tasks.</Para>
          <Para ID="Par9">Current generation of post-stroke rehabilitation systems, although exploiting latest immersive technologies tend to proprietary approaches concentrating on a closed range of exercise types, lacking thoroughly addressing the complete set of disabilities and offering a comprehensive set of rehabilitation scenarios. The use of technologies is also very selective and varies from one system to another. Although there are cases of using avatars for more intuitive feedback to the patient, the use of complicated wearable devices makes it tiresome and decreases the effectiveness of the exercise [<CitationRef CitationID="CR3">3</CitationRef>]. In our approach we have been exploring novel technologies for body tracing that exploit the rich information gathered by combining wearable sensors with visual feedback systems that are already commercially available such as Microsoft Kinect [<CitationRef CitationID="CR7">7</CitationRef>] or Leap Motion [<CitationRef CitationID="CR8">8</CitationRef>] user interfaces and 3D virtual, augmented and mixed-reality visualisation.</Para>
          <Para ID="Par10">The environment we develop aims to provide a full 3D physical and visual feedback through Mixed-Reality interaction and visualisation technologies placing the user inside of the training environment. Considering that detecting muscle activity cannot be done without wearable device support, our partner in the project, IHP GmbH, has been developing a customizable lightweight embedded sensor device allowing short-range wireless transmission of most common parameters including apart from EMG, also other critical medical signs like ECG, Blood Pressure, heart rate, etc. This way the training exercises become much more intuitive in their approach by using exercise templates with feedback showing correctness of performed exercises. Therapists are then able to prescribe a set of the rehabilitation exercises as treatment through the EHR/PHR platform(s) thus offering means of correlating them with changes of patient’s condition, thus improving effectiveness of patients’ recovery.</Para>
        </Section1>
        <Section1 ID="Sec4">
          <Heading>Body Sensing and User Interfaces</Heading>
          <Para ID="Par11">In order to enable the tracking the correctness of performed exercises automatically without the constant assistance of the physicians, an automated means of tracking and comparing patient’s body movement against correct ones (templates) has to be developed. This is an ongoing part of the work due to the changing requirements from our physicians. Although many methods are in existence, most of them employ elaborate sets of wearable sensors and/or costly visual observations. In our approach we initially intended to employ a proprietary approach using visual-light scanning, but the recent availability of new Kinect, Prime Sense and Leap Motion sensors made us change our approach and use existing IR-LED solutions.</Para>
          <Para ID="Par12">When better accuracy is required that offered by 3D scanners then additional microembedded sensor nodes are employed, e.g. gyros (tilt and position calibration) and inertial/accelerometers (speed changes). Such are readily available for us in both EPOC EEG U/I from Emotiv (used currently as a U/I, though intended to be used in the future for seizure risk alerting) and on Shimmer EMG sensor platform that we use for detecting muscle activity during the exercises. Considering very small sizes of such sensors (less than 5 × 5 mm each) a development of lightweight wireless energy-autonomous (employing energy harvesting) may be possible.</Para>
          <Para ID="Par13">Muscle activity poses problems for measurement since it has been well known for many years [<CitationRef CitationID="CR9">9</CitationRef>] that the EMG reflects effort rather than output and so becomes an unreliable indicator of muscle force as the muscle becomes fatigued. Consequently measurement of force, in addition to the EMG activity, would be a considerable step forward in assessing the effectiveness of rehabilitation strategies and could not only indicate that fatigue is occurring, but also whether the mechanism is central or peripheral in origin [<CitationRef CitationID="CR10">10</CitationRef>]. Similarly, conventional surface EMG measurement requires accurate placement of the sensor over the target muscle, which would be inappropriate for a sensor system integrated within a garment for home use. Electrode arrays are, however, now being developed for EMG measurement and signal processing is used to optimise the signal obtained. Several different solutions have been investigated to offer sufficiently reliable, but also economic muscle activity monitoring. Finally, we concluded on using EMG sensors on the 2R sensor platform from Shimmer for system development purposes, while a dedicated solution made by IHP GmbH.</Para>
          <Para ID="Par14">However, EMG is not the only sensor that is needed for home hospitalisation of patients suffering from chronic diseases like stroke. This requires novel approaches to combining building blocks in a body sensor network. Existing commercial systems provide basic information about activity such as speed and direction of movement and postures. Providing precise information about performance, for example relating movement to muscle activity in a given task and detecting deviations from normal, expected patterns or subtle changes associated with recovery, requires a much higher level of sophistication of data acquisition and processing and interpretation. The challenge is therefore to design and develop an integrated multimodal system along with high-level signal processing techniques and optimisation of the data extracted. The Kinect system has potential for use in haptic interfacing [<CitationRef CitationID="CR11">11</CitationRef>] and has already been used in some software projects, and Open Source software libraries are available for browsers like Chrome [<CitationRef CitationID="CR12">12</CitationRef>] and demonstrations of interfaces to Windows 7 [<CitationRef CitationID="CR3">3</CitationRef>] systems have been shown.</Para>
          <Para ID="Par15">The existing techniques for taking measurements on the human body are generally considered to be adequate for the purpose but are often bulky in nature and cumbersome to mount, e.g. electro-goniometers, and they can also be expensive to implement, e.g. VIACON camera system. Their ability to be used in a home environment is therefore very limited. In this context, we have decided to address those deficiencies by extending the state-of-the-art in the areas of:<UnorderedList Mark="Bullet">
              <ItemContent>
                <Para ID="Par16">Extending the application of existing sensor technologies: For example, we tend to use commercially available MEMS accelerometers with integrated wireless modules to measure joint angles on the upper and lower limbs in order to allow wire-free, low-cost sensor nodes that are optimised in terms of their information content and spatial location.</Para>
              </ItemContent>
              <ItemContent>
                <Para ID="Par17">Novel sensing methodologies to reduce the number of sensors worn on the human body, while maintaining good information quality. For example, many homes now have at least one games console (e.g. Xbox, Nintendo Wii, etc.) as part of a typical family home entertainment system. With the advent of the Xbox Kinect system, the position and movement of a human will be possible to be monitored using a low-cost camera mounted on the TV set.</Para>
              </ItemContent>
              <ItemContent>
                <Para ID="Par18">Easy system installation and calibration by non-experts for use in a non-clinical environment, thus making this solution suitable for use at home for the first timer and with support or untrained caretakers and family.</Para>
              </ItemContent>
              <ItemContent>
                <Para ID="Par19">Transparent verification of correct execution of exercises by patients may be based on data recorded by Body Area Networks (BAN), correlation of prescribed therapies with medical condition thus allows to determine their effectiveness on patient’s condition, either it is positive or negative.</Para>
              </ItemContent>
            </UnorderedList>
</Para>
        </Section1>
        <Section1 ID="Sec5">
          <Heading>Prototyping</Heading>
          <Para ID="Par20">The project has reached 2 years of its lifetime already and the prototyping as well as integration of various technologies have already started. This refers to physiological monitoring with Shimmer sensors, gaming user interfaces as well as the games themselves, focussing on the Unity3D engine. A sub-unit assembly diagram of the ‘Patients home training place’ is depicted in Fig. <InternalRef RefID="Fig1">7.1</InternalRef> and shown placed on a patient in Fig. <InternalRef RefID="Fig2">7.2</InternalRef>. The blue and grey rectangles designate respective elements, while green ones are the user interfaces. The PTZ-camera features pan-tilt-zoom. Arrows show the data flow. The description of the user interfaces shown in this diagram follows.<Figure Category="Standard" Float="Yes" ID="Fig1">
              <Caption Language="En">
                <CaptionNumber>Fig. 7.1</CaptionNumber>
                <CaptionContent>
                  <SimplePara>Integration of the overall ‘home’ system</SimplePara>
                </CaptionContent>
              </Caption>
              <MediaObject ID="MO1">
                <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_7_Fig1_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
              </MediaObject>
            </Figure>
<Figure Category="Standard" Float="Yes" ID="Fig2">
              <Caption Language="En">
                <CaptionNumber>Fig. 7.2</CaptionNumber>
                <CaptionContent>
                  <SimplePara>ECG sensing with Shimmer2R</SimplePara>
                </CaptionContent>
              </Caption>
              <MediaObject ID="MO2">
                <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_7_Fig2_HTML.jpg" Format="JPEG" Rendition="HTML" Type="Halftone"/>
              </MediaObject>
            </Figure>
</Para>
          <Para ID="Par21">Since home-based rehabilitation may increase the risk of stroke re-occurrence we have decided to include EEG sensor, a 2 × 7-node Emotiv EPOC EEG, which we use for monitoring of brain signals, looking for ‘flashing’ activity between the two brain spheres, indicated by participating physiotherapists as a sign of a likely pre-event condition (Fig. <InternalRef RefID="Fig3">7.3</InternalRef>).<Figure Category="Standard" Float="Yes" ID="Fig3">
              <Caption Language="En">
                <CaptionNumber>Fig. 7.3</CaptionNumber>
                <CaptionContent>
                  <SimplePara>Emotiv EEG (<Emphasis Type="Bold">a</Emphasis>), sample brain activity (<Emphasis Type="Bold">b</Emphasis>)</SimplePara>
                </CaptionContent>
              </Caption>
              <MediaObject ID="MO3">
                <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_7_Fig3_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
              </MediaObject>
            </Figure>
</Para>
          <Para ID="Par22">This device offers an additional benefit for being used as a supplementary gaming interface, thus shifting patient’s perception from its use as a preventive device to enjoying and using for controlling games with the ‘power of the mind’. It is not without a merit that Emotiv offers also a Unity3D support for its device, not to mention ongoing development of an even more powerful INSIGHT [<CitationRef CitationID="CR13">13</CitationRef>] sensor version.</Para>
          <Para ID="Par23">Currently apart from searching for clues indicating pre-stroke risks and as ‘mouse’-like user interface, we use EPOC for establishing a correlation between the mental intention to move a limb and the physical action. By combining with data from EMG sensors we aim to detect cases when patient’s brain correctly issues a signal to, e.g. to move an arm, but the patient cannot do it, e.g. due to a broken nerve connection.</Para>
          <Section2 ID="Sec6">
            <Heading>‘Kinect Server’ Implementation</Heading>
            <Para ID="Par24">The principal user interface used to control games has been Microsoft Kinect, the Xbox version at first and then the Windows version when it has been first released in early 2012. Its combination of distance sensing with the RGB camera proved perfectly suitable for both full-body exercises (exploring its embedded skeleton recognition) as well as for near-field exercises of upper limbs. However since Kinect has not been designed for short-range scanning of partial bodies, the skeleton tracking could not be used and hence we had to develop our own algorithms that would be able to recognise arms, palms and fingers and distinguish them from the background objects. This has led to the development of the ‘Kinect server’ based on open source algorithms. The first implementation has used Open NI drivers, closed in April 2014 following the acquisition of PrimeSense by Apple [<CitationRef CitationID="CR14">14</CitationRef>], offering the opportunity for our software to be built for both MS Windows and Linux platforms.</Para>
            <Para ID="Par25">The ‘Kinect Server’ has been a custom development from RFSAT Ltd in the StrokeBack project to allow remote connectivity to the Microsoft Kinect sensor and subsequently the use of the data from the sensor on a variety of devices, not normally supported natively by the respective SDK from Microsoft. Initial implementation of the Kinect Server has been based on Open NI drivers for Xbox version of the device, which has been later translated to a more general drivers supplied by Microsoft in their Kinect SDK version 1.7 and the subsequent versions.</Para>
            <Para ID="Par26">The principle of its operation is that it is comprised of two components:<UnorderedList Mark="Bullet">
                <ItemContent>
                  <Para ID="Par27">
<Emphasis Type="Italic">Server</Emphasis>-<Emphasis Type="Italic">side</Emphasis>—operating on a software platform supported by the selected Kinect drivers, having a role of obtaining relevant data from the Kinect sensor and making it available in a suitable form via network to connected clients.</Para>
                </ItemContent>
                <ItemContent>
                  <Para ID="Par28">
<Emphasis Type="Italic">Client</Emphasis>-<Emphasis Type="Italic">side</Emphasis>—operating on any platform where WEB browsers with embedded Java Scripts are supported. This means almost any networked device, including tablets and a variety of smartphones, even Smart TVs, and other devices.</Para>
                </ItemContent>
              </UnorderedList>
</Para>
            <Para ID="Par29">The types of information made available to by the server to clients include both RGB and depth map as images as well as the list of detected objects. Customisations include for example such features as the limitations of the visibility (detection) area, thus allowing to reduce clutter from nearby objects, focus on the selected object (e.g. the central or the closest one) and other ones.</Para>
            <Para ID="Par30">Two modes of operation have been anticipated in order to enable its use for rehabilitation training in the project (refer to Fig. <InternalRef RefID="Fig4">7.4</InternalRef>). In case that the persistent network connectivity can be maintained, the server part could operate on the home gateway, the client on a game console while the game server (game repository and management of game results for each user) remotely on the same server that hosts the PHR platform. In such an approach home client would not need to bother about updating games to latest versions or managing results. However, in case that network connectivity may or may not be secured, then the game server would need to be hosted locally on a home gateway, alongside the Kinect Server.<Figure Category="Standard" Float="Yes" ID="Fig4">
                <Caption Language="En">
                  <CaptionNumber>Fig. 7.4</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>Network operational modes of the ‘Kinect Server’</SimplePara>
                  </CaptionContent>
                </Caption>
                <MediaObject ID="MO4">
                  <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_7_Fig4_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                </MediaObject>
              </Figure>
</Para>
            <Para ID="Par31">Since server has been implemented as a generic enabler, a supplementary implementation of gesture recognition geared to the Kinect Server has been implemented. In order to achieve wide interoperability on a number of devices and operating systems, an approach using Python script ‘palm_controls’ has been selected with a purpose of detecting specific gestures and mapping them to a custom keystrokes and mouse actions. The list of features and calling syntax is shown below:</Para>
            <Para ID="Par32">
<Emphasis Type="Italic">palm</Emphasis>_<Emphasis Type="Italic">controls</Emphasis> &lt;<Emphasis Type="Italic">server IP</Emphasis>&gt; &lt;<Emphasis Type="Italic">server PORT</Emphasis>&gt; <Emphasis Type="Italic">options</Emphasis>
</Para>
            <Para ID="Par33">
<Emphasis Type="Italic">Sever IP</Emphasis>—network address where Kinect Server is hosted</Para>
            <Para ID="Par34">
<Emphasis Type="Italic">Server PORT</Emphasis>—network port on which the server listens for connections</Para>
            <Para ID="Par35">Options:<UnorderedList Mark="Bullet">
                <ItemContent>
                  <Para ID="Par36">‘-lHH –rHH –uHH –dHH -sHH’</Para>
                  <Para ID="Par37">Defines keys pressed for left, right, up, down and click actions</Para>
                  <Para ID="Par38">
<Emphasis Type="Italic">HH</Emphasis> is a hexadecimal number from the Microsoft character table:</Para>
                  <Para ID="Par39">
<ExternalRef>
                      <RefSource>http://msdn.microsoft.com/en-us/library/windows/desktop/dd375731(v=vs.85).aspx</RefSource>
                      <RefTarget Address="http://msdn.microsoft.com/en-us/library/windows/desktop/dd375731(v=vs.85).aspx" TargetType="URL"/>
                    </ExternalRef>.</Para>
                  <Para ID="Par40">Choosing a zero (0) for a given key disables mapping of the given gesture</Para>
                </ItemContent>
                <ItemContent>
                  <Para ID="Par41">‘-x??? –X??? –y??? –Y??? –z??? –Z???’</Para>
                  <Para ID="Par42">This allows the 3D space where objects are detected.</Para>
                  <Para ID="Par43">Parameters define <Emphasis Type="Italic">Xmin</Emphasis>, <Emphasis Type="Italic">Xmax</Emphasis>, <Emphasis Type="Italic">Ymin</Emphasis>, <Emphasis Type="Italic">Ymax</Emphasis>, <Emphasis Type="Italic">Zmin</Emphasis> and <Emphasis Type="Italic">Zmax</Emphasis>, as given below:</Para>
                  <Para ID="Par44">
<Emphasis Type="Italic">Xmin &amp; Xmax</Emphasis> are given in pixels in the range 0–320</Para>
                  <Para ID="Par45">
<Emphasis Type="Italic">Ymin &amp; Ymax</Emphasis> are given in pixels in the range 0–240</Para>
                  <Para ID="Par46">
<Emphasis Type="Italic">Zmin &amp; Zmax</Emphasis> are given in millimeters, refer to a distance from the sensor</Para>
                </ItemContent>
              </UnorderedList>
</Para>
          </Section2>
          <Section2 ID="Sec7">
            <Heading>Embedded Kinect Server</Heading>
            <Para ID="Par47">The limitations of the Kinect in terms of the compatibility with certain Operating Systems, diversity of often-incompatible drivers and restrictiveness to high-end computing platforms have pushed us to investigating alternative ways of interacting with Kinect devices. This has led to the attempt to develop an ‘Embedded Kinect Server’ or EKS. Our idea was to use a microembedded computer like Raspberry PI [<CitationRef CitationID="CR15">15</CitationRef>] or similar and allow the client device that was running the game to access data from the EKS via local wireless (or wired) network. Such an approach would allow us to remove the physical connectivity restriction of the Kinect and allow 3D scanning capability from any device as long as it was connected to a network.</Para>
            <Para ID="Par48">Various embedded platforms were investigated: Raspberry PI, eBox 3350 [<CitationRef CitationID="CR16">16</CitationRef>], Panda board [<CitationRef CitationID="CR17">17</CitationRef>] (Fig. <InternalRef RefID="Fig5">7.5</InternalRef>) and many other ones. Tests have revealed an inherent problem with Kinect physical design that is shared between the Xbox and the subsequent Windows version that is the need to draw high current from USB ports in order to power sensors despite separate power supply still required.<Figure Category="Standard" Float="Yes" ID="Fig5">
                <Caption Language="En">
                  <CaptionNumber>Fig. 7.5</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>Embedded Kinect server deployment: Panda board (<Emphasis Type="Bold">a</Emphasis>) and physical prototype integrated with the MS Kinect for Windows sensor (<Emphasis Type="Bold">b</Emphasis>)</SimplePara>
                  </CaptionContent>
                </Caption>
                <MediaObject ID="MO5">
                  <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_7_Fig5_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                </MediaObject>
              </Figure>
</Para>
            <Para ID="Par49">Hardware modifications of the Raspberry PI aimed to increase the current supplied to its USB ports, use of powered external USB hub and other work-around proved all unsuccessful. To date only the Panda Board proved to be the only embedded computer able to maintain the Kinect connectivity and running our EKS. In our tests we have managed to run the Mario Bros game on an Android smartphone and use Kinect wirelessly to control a game with patient’s wrists.</Para>
          </Section2>
          <Section2 ID="Sec8">
            <Heading>Rehabilitation Games Using the ‘Kinect Server’</Heading>
            <Para ID="Par50">The main features of our implementation offer the capabilities of restricting the visibility window, filtering the background beyond prescribed distance, distinguishing between separate objects, etc. This way we were able to implement the Kinect-based interface where following the requirements of our physiotherapists we replaced the standard keyboard arrows with gestures of the palm (up, down, left, right and open/close to make a click). Such an interface allowed for the first game-based rehabilitation of stroke patients suffering from limited hand control. The tests were first made with Mario Bros game where all controls were achieved purely with movements of a single palm. The algorithm for analysing wrist position and generating respective keyboard clicks has been developed initially in Matlab and then ported to PERL for deployment along with the Kinect server on an embedded hardware.</Para>
            <Para ID="Par51">The algorithm is shown in Fig. <InternalRef RefID="Fig6">7.6</InternalRef>. It is based on the idea that assuming that the wrist is placed steadily on a support (requirements from physiotherapists), the patients palm would always have fingers closer to the Kinect than the rest of a hand, this allowing easily to determine the palm position and direction the fingers point. Under such condition, we did not have to pay much attention to Kinect calibration and could avoid fixing the relative position of the hand support with respect to the Kinect device. This allowed us to remove the background simply by disregarding anything more distant than the average palm length centred on the centre of gravity (i.e. centre of the palm itself).<Figure Category="Standard" Float="Yes" ID="Fig6">
                <Caption Language="En">
                  <CaptionNumber>Fig. 7.6</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>Wrist position detection algorithm</SimplePara>
                  </CaptionContent>
                </Caption>
                <MediaObject ID="MO6">
                  <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_7_Fig6_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                </MediaObject>
              </Figure>
</Para>
            <Para ID="Par52">The line was then interpolated through remaining points in 3D, whereby the closest point detected was indicating the tip of the closest finger. The direction of the line was equivalent to the movement of a hand in a given direction allowing us to generate the correct keystroke combination (w-a-s-d for N-W-S-E). Since the accuracy was, better than 1/8 of the circle it allowed us to determine also diagonal movements (double keystrokes, i.e. wd-wa-sd-sa for NW-NE-SE-SW). A predefined time delay was applied corresponding to control detection ‘speed’.</Para>
            <Para ID="Par53">Since classical rehabilitation required the use of physical objects, like cubes of glasses we have implemented subsequently a ‘cube stacking’ game, where patient had to use the physical cubes and place them carefully onto the placeholders displayed on the computer screen positioned flat on the table (later replaced with overhead projection) as shown in Fig. <InternalRef RefID="Fig7">7.7</InternalRef> where red cubes and placeholders (grey squares) are visible. Here Kinect sensor is placed to scan horizontally at table level, thus allowing to detect XYZ coordinates of the physical cubes. By matching projections with Kinect scan regions allowing detection of correct placement of cubes.<Figure Category="Standard" Float="Yes" ID="Fig7">
                <Caption Language="En">
                  <CaptionNumber>Fig. 7.7</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>Game using real cubes on a virtual board</SimplePara>
                  </CaptionContent>
                </Caption>
                <MediaObject ID="MO7">
                  <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_7_Fig7_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                </MediaObject>
              </Figure>
</Para>
            <Para ID="Par54">The first level starts with one cube and placeholder parallel to screen edge, then as the game progresses more cubes are used and their requested position could be in any direction. At the end, a score was calculated taking into consideration both the time to place the cubes and the accuracy of placing them over the placeholder. The score was reported in the PHR allowing the physician to track the progress of the patient from one exercise to another. Another variant of the game introduced the possibility of stacking cubes one on top of the other.</Para>
            <Para ID="Par55">An alternative gaming approach to mixing virtual and real objects was a game where patients were requested to throw a paper ball at the virtual circles displayed on the screen as shown in Fig. <InternalRef RefID="Fig8">7.8</InternalRef>. The Kinect sensor synchronised with location of projected object detects physical ball reaching the distance of the wall. Combined with its XY coordinated, this allows to detect the collision.<Figure Category="Standard" Float="Yes" ID="Fig8">
                <Caption Language="En">
                  <CaptionNumber>Fig. 7.8</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>Throwing real paper ball at virtual targets</SimplePara>
                  </CaptionContent>
                </Caption>
                <MediaObject ID="MO8">
                  <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_7_Fig8_HTML.jpg" Format="JPEG" Rendition="HTML" Type="Halftone"/>
                </MediaObject>
              </Figure>
</Para>
            <Para ID="Par56">Such a game allows patients to exercise the whole arm, not just the wrist. Hitting the circle that represented a virtual balloon was rewarded with an animated explosion of the balloon and a respective sound. Such a game proved to be very enjoyable for the patients letting them concentrate on perfecting their movements while forgetting about their motor disabilities, increasing effectiveness of their training.</Para>
          </Section2>
          <Section2 ID="Sec9">
            <Heading>Full-Body Games with Avateering</Heading>
            <Para ID="Par57">Subsequently we have investigated more advanced class of games for stroke patients for full-body exercises. In such a case we have chosen to build such games using 3D engine and employ avateering approach, that is patient’s body motion capture and its projection onto a virtual avatar. When we have started our first implementation, the MS Kinect SDK was not available yet and hence we have explored various ‘hacks’ built by the Kinect developer community. The most applicable to our needs appeared to be ZigFu [<CitationRef CitationID="CR18">18</CitationRef>], which was compatible with Open NI drivers and easy to use under Unity3D [<CitationRef CitationID="CR19">19</CitationRef>] editor.</Para>
            <Para ID="Par58">It proved easier than using commercial products, e.g. Brekel [<CitationRef CitationID="CR20">20</CitationRef>] or Autodesk Motion Builder [<CitationRef CitationID="CR21">21</CitationRef>]. A prototype system uses environments ranging from familiar home spaces in photorealistic quality [<CitationRef CitationID="CR22">22</CitationRef>] (Fig. <InternalRef RefID="Fig9">7.9</InternalRef>) to generic hospital environments (Fig. <InternalRef RefID="Fig10">7.10</InternalRef>).<Figure Category="Standard" Float="Yes" ID="Fig9">
                <Caption Language="En">
                  <CaptionNumber>Fig. 7.9</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>Avateering in a ‘home’ like environment</SimplePara>
                  </CaptionContent>
                </Caption>
                <MediaObject ID="MO9">
                  <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_7_Fig9_HTML.jpg" Format="JPEG" Rendition="HTML" Type="Halftone"/>
                </MediaObject>
              </Figure>
<Figure Category="Standard" Float="Yes" ID="Fig10">
                <Caption Language="En">
                  <CaptionNumber>Fig. 7.10</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>Avateering aimed to repeat movements of an instructor in a ‘hospital’ like virtual environment</SimplePara>
                  </CaptionContent>
                </Caption>
                <MediaObject ID="MO10">
                  <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_7_Fig10_HTML.jpg" Format="JPEG" Rendition="HTML" Type="Halftone"/>
                </MediaObject>
              </Figure>
</Para>
            <Para ID="Par59">Scenes with one and two avatars were implemented. The first one was intended as a base for self-training exercises where instruction would be overlaid over the avatar to indicate the movements that the patient would need to perform in order to pass the exercise. A two-avatar scenario was aimed to offer the side-by-side exercise together with a virtual rehabilitator where patient would need to follow the movements of the ‘physician’ seeing him/herself at the same time. In both cases, the score would be corresponding to the accuracy of following the expected movements. The two scenarios are being subject to assessment by physiotherapists and the decision as to which one will be used for the final system implementation will be depended on evaluation results.</Para>
            <Para ID="Par60">An important advantage of Unity3D over other 3D gaming engines like Cry Engine 3 or Unreal Engine is the possibility to compile games to run either as stand-alone or under from inside a WEB page. The latter approach makes it easier for integrating games as therapies within the PHR system accessible and controllable via WEB browser. A use of this feature for exercises with a real patient is shown in Fig. <InternalRef RefID="Fig11">7.11</InternalRef>.<Figure Category="Standard" Float="Yes" ID="Fig11">
                <Caption Language="En">
                  <CaptionNumber>Fig. 7.11</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>Patient playing online Unity3D game using standard WEB browser</SimplePara>
                  </CaptionContent>
                </Caption>
                <MediaObject ID="MO11">
                  <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_7_Fig11_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                </MediaObject>
              </Figure>
</Para>
            <Para ID="Par61">The Kinect Server and wrist controls have been integrated into a number of games under Unity3D in order to evaluate the adopted concepts with end users, such as into an ‘Infinite Runner’ [<CitationRef CitationID="CR23">23</CitationRef>], shown in Fig. <InternalRef RefID="Fig12">7.12</InternalRef>. It is an incentive-based game combining rehabilitation with entertainment. High scores (coins collected) correspond to improvement in recovering hand movement capabilities.<Figure Category="Standard" Float="Yes" ID="Fig12">
                <Caption Language="En">
                  <CaptionNumber>Fig. 7.12</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>Kinect server used in a wrist-controlled ‘Infinite Runner’ game</SimplePara>
                  </CaptionContent>
                </Caption>
                <MediaObject ID="MO12">
                  <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_7_Fig12_HTML.jpg" Format="JPEG" Rendition="HTML" Type="Halftone"/>
                </MediaObject>
              </Figure>
</Para>
            <Para ID="Par62">The game uses embedded implementation of the Unity3D plug-in for Kinect Server, thus avoiding intermediary use of the Python gesture-interpretation scripts. The game allows exercising lower left and right limb, implementing movements like left and right swipe, up and down swipes, fist and fingers spread, being translated to respective movements of the character, as well as jump and duck actions.</Para>
          </Section2>
          <Section2 ID="Sec10">
            <Heading>3D Stereoscopic Visualisation</Heading>
            <Para ID="Par63">In order to enhance the realism of the games developed with Unity3D engine a stereoscopic projection was implemented offering a sense of depth on supported 3D displays. The current approach has been based on the ‘camera-to-texture’ projection feature available in a PRO version of Unity3D.</Para>
            <Para ID="Par64">It has been based on various published experiments [<CitationRef CitationID="CR24">24</CitationRef>, <CitationRef CitationID="CR25">25</CitationRef>]. A dual virtual cameras placed near each other have their images projected onto virtual screens, in turn captured by a single output camera thus creating a side-by-side display (Fig. <InternalRef RefID="Fig13">7.13</InternalRef>). Such an image is then suitable for driving common 3D displays such as of the 3D Smart TV (e.g. Samsung UE46F6400) or 3D projector (e.g. Epson EH-TW5910) with frame-switching 3D glasses. Both of those have been successfully tested with our system. In the future tests we will evaluate also panoramic virtual 3D visors (e.g. Oculus RIFT) and 3D augmented glasses (e.g. VUZIX Star 1200XLD) for virtual and mixed-reality games respectively.<Figure Category="Standard" Float="Yes" ID="Fig13">
                <Caption Language="En">
                  <CaptionNumber>Fig. 7.13</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>Stereoscopic projection using Unity3D camera-to-texture [<CitationRef CitationID="CR24">24</CitationRef>]</SimplePara>
                  </CaptionContent>
                </Caption>
                <MediaObject ID="MO13">
                  <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_7_Fig13_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                </MediaObject>
              </Figure>
</Para>
          </Section2>
          <Section2 ID="Sec11">
            <Heading>Integration of Leap-Motion User Interface</Heading>
            <Para ID="Par65">The latest addition to the portfolio of our user interfaces has been the Leap Motion device. It has proven invaluable for rehabilitation of upper limbs thanks to its superior ability to detect close range distances over the Kinect. Our developments have led us to use features already offered by the Leap Motion SDK and the community applications, e.g. the Touchless for Windows allowing controlling mouse-like control of the Windows applications. This has led us to experiments with standard games used for the rehabilitation of stroke patients like memory board game (as in Fig. <InternalRef RefID="Fig14">7.14a</InternalRef>) and experimenting with operating virtual cubes composing a Stroke word in a virtual 3D space using only your fingers (as in Fig. <InternalRef RefID="Fig14">7.14b</InternalRef>). Both of those have proven very enjoyable for our patients.<Figure Category="Standard" Float="Yes" ID="Fig14">
                <Caption Language="En">
                  <CaptionNumber>Fig. 7.14</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>Board games using Leap Motion, Memory (<Emphasis Type="Bold">a</Emphasis>) and Grasping (<Emphasis Type="Bold">b</Emphasis>)</SimplePara>
                  </CaptionContent>
                </Caption>
                <MediaObject ID="MO14">
                  <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_7_Fig14_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                </MediaObject>
              </Figure>
</Para>
            <Para ID="Par66">Initial implementation of the games in Fig. <InternalRef RefID="Fig15">7.15</InternalRef> have used third-party gesture recognition applications, such as Game WAVE for Leap Motion [<CitationRef CitationID="CR26">26</CitationRef>], which allowed mapping of pre-defined hand movements to specific keystrokes and mouse movements, thus allowing easy control of any application, including our games. In more recent versions the Unity3D plug-ins from leap Motion have been used, supplemented with custom add-ons aimed to improve the detection of the specific gestures that were important for the selected range of rehabilitation exercises.<Figure Category="Standard" Float="Yes" ID="Fig15">
                <Caption Language="En">
                  <CaptionNumber>Fig. 7.15</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>Game WAVE application for Leap Motion [<CitationRef CitationID="CR26">26</CitationRef>]</SimplePara>
                  </CaptionContent>
                </Caption>
                <MediaObject ID="MO15">
                  <ImageObject Color="BlackWhite" FileRef="MediaObjects/327370_1_En_7_Fig15_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                </MediaObject>
              </Figure>
</Para>
          </Section2>
          <Section2 ID="Sec12">
            <Heading>Integration of Myo Gesture Control Armband</Heading>
            <Para ID="Par67">The latest of the sensor that are directly applicable for rehabilitation training of stroke suffering users is an electromyographic (EMG) sensor from Thalmic Labs called ‘Myo’ [<CitationRef CitationID="CR27">27</CitationRef>], launched in early 2015. Such a sensor allows the detection of electrical potentials of the muscles on the affected limbs. In this sense it gives two types of benefits. From one side clinicians can have a direct indication whether signals are correctly sent from the brain to the muscles. On the other hand it offers a possibility of being used as a user interface for controlling rehabilitation games. The manufacturer has released a range of support software, including and SD, Unity3D plug-in and an application manager allowing custom mapping of supported gestures to required keystrokes and mouse actions. A direct access to each of the eight (8) EMG sensors is also readily available by the SDK in order to allow developers to view raw signals from muscles and devising own gesture recognition algorithms.</Para>
            <Para ID="Par68">Our rehabilitation system could not refrain from taking advantage of such a useful interface and a game has been built, adapted from the ‘Amazing Skater’ Unity3D game template from Ace Games [<CitationRef CitationID="CR28">28</CitationRef>], presented in Fig. <InternalRef RefID="Fig16">7.16</InternalRef>.<Figure Category="Standard" Float="Yes" ID="Fig16">
                <Caption Language="En">
                  <CaptionNumber>Fig. 7.16</CaptionNumber>
                  <CaptionContent>
                    <SimplePara>The ‘Skater Game’ adapted from a template by Ace Games [<CitationRef CitationID="CR28">28</CitationRef>]</SimplePara>
                  </CaptionContent>
                </Caption>
                <MediaObject ID="MO16">
                  <ImageObject Color="Color" FileRef="MediaObjects/327370_1_En_7_Fig16_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                </MediaObject>
              </Figure>
</Para>
            <Para ID="Par69">The game can be played using either a keyboard or a Myo sensor. The latter is supported through a Unity3D plug-in, though it can be also operated using custom StrokeBack application profile via Myo Application Manager.</Para>
          </Section2>
        </Section1>
        <Section1 ID="Sec13">
          <Heading>Summary and Observations</Heading>
          <Para ID="Par70">In the frame of the project, most of the technologies have been implemented, including integrated Smart Table as described in Chapter <ExternalRef>
              <RefSource>9</RefSource>
              <RefTarget Address="10.1007/978-3-319-21293-7_9" TargetType="DOI"/>
            </ExternalRef>, integration with PHR systems allowing management of rehabilitation by physicians as well as a range of user interfaces, not only based on a Kinect sensor. The initial technical validation tests have proven the viability of the design approach adopted.</Para>
          <Para ID="Par71">The suitability of Leap Motion for ‘Touch-Screen’-like applications and game development under Unity3D has been confirmed. Following the success of the technical system tests, the clinical trials with real patients are being conducted since September 2014 into early 2015. Primarily the focus is to be made on the motion capture and recording of the real person (therapist) for subsequent use for demonstration of correct exercises by animating his/her avatar as shown earlier in Fig. <InternalRef RefID="Fig10">7.10</InternalRef>.</Para>
          <Para ID="Par72">Furthermore a 3D hand model needs to be developed, rigged and animated in order to allow its use in Unity3D games. Subsequently the overall integration of the gaming system will be performed whereby selection of games and the necessary data exchange mechanism with the PHR system will be developed. The most difficult work will be related to the real-time comparison of avatar movements for providing an accurate scoring of the correctness of exercises, to be achieved in liaison with the physiotherapists.</Para>
        </Section1>
      </Body>
      <BodyRef FileRef="978-3-319-21293-7_Chapter_7.pdf" OutputMedium="Online" PDFType="Typeset" TargetType="OnlinePDF"/>
      <ChapterBackmatter>
        <Bibliography ID="Bib1">
          <Heading>References</Heading>
          <Citation ID="CR1">
            <CitationNumber>1.</CitationNumber>
            <BibUnstructured>P. Kirchhof, et al, How can we avoid a stroke crisis? 2009, ISBN 978-1-903539-09-5</BibUnstructured>
          </Citation>
          <Citation ID="CR2">
            <CitationNumber>2.</CitationNumber>
            <BibUnstructured>Rehabilitation Institute of Chicago [cited: 2014], <ExternalRef>
                <RefSource>http://www.ric.org/conditions/stroke/</RefSource>
                <RefTarget Address="http://www.ric.org/conditions/stroke/" TargetType="URL"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR3">
            <CitationNumber>3.</CitationNumber>
            <BibChapter>
              <BibAuthorName>
                <Initials>A</Initials>
                <FamilyName>Krukowski</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>E</Initials>
                <FamilyName>Vogiatzaki</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>JM</Initials>
                <FamilyName>Rodríguez</FamilyName>
              </BibAuthorName>
              <Year>2013</Year>
              <ChapterTitle Language="En">Patient health record (PHR) system</ChapterTitle>
              <BibEditorName>
                <Initials>K</Initials>
                <FamilyName>Maharatna</FamilyName>
              </BibEditorName>
              <Eds/>
              <Etal/>
              <BookTitle>Next-generation remote healthcare: a practical system design perspective</BookTitle>
              <PublisherName>Springer Science and Business Media</PublisherName>
              <PublisherLocation>New York</PublisherLocation>
              <BibComments>Chapter 6</BibComments>
            </BibChapter>
            <BibUnstructured>A. Krukowski, E. Vogiatzaki, J.M. Rodríguez, Patient health record (PHR) system, in <Emphasis Type="Italic">Next-generation remote healthcare: a practical system design perspective</Emphasis>, ed. by K. Maharatna et al. (Springer Science and Business Media, New York, 2013). Chapter 6</BibUnstructured>
          </Citation>
          <Citation ID="CR4">
            <CitationNumber>4.</CitationNumber>
            <BibUnstructured>Virtual Technologies Inc. [cited: 2014], <ExternalRef>
                <RefSource>http://www.cyberglovesystems.com/all-products</RefSource>
                <RefTarget Address="http://www.cyberglovesystems.com/all-products" TargetType="URL"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR5">
            <CitationNumber>5.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>M</Initials>
                <FamilyName>Bouzit</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>G</Initials>
                <FamilyName>Burdea</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>G</Initials>
                <FamilyName>Popescu</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>R</Initials>
                <FamilyName>Boian</FamilyName>
              </BibAuthorName>
              <Year>2002</Year>
              <ArticleTitle Language="En">The rutgers master II—new design force-feedback glove</ArticleTitle>
              <JournalTitle>IEEE/ASME Trans. Mechatron.</JournalTitle>
              <VolumeID>7</VolumeID>
              <IssueID>2</IssueID>
              <FirstPage>256</FirstPage>
              <LastPage>263</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1109/TMECH.2002.1011262</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>M. Bouzit, G. Burdea, G. Popescu, R. Boian, The rutgers master II—new design force-feedback glove. IEEE/ASME Trans. Mechatron. <Emphasis Type="Bold">7</Emphasis>(2), 256–263 (2002)</BibUnstructured>
          </Citation>
          <Citation ID="CR6">
            <CitationNumber>6.</CitationNumber>
            <BibUnstructured>M. McNeill, et al Immersive virtual reality for upper limb rehabilitation following stroke, in <Emphasis Type="Italic">Systems, IEEE International Conference on Man and Cybernetics</Emphasis>, 2004. ISBN: 0 7803 8566 7</BibUnstructured>
          </Citation>
          <Citation ID="CR7">
            <CitationNumber>7.</CitationNumber>
            <BibUnstructured>Microsoft Kinect [cited: 2014 Aug 17], <ExternalRef>
                <RefSource>http://kinectforwindows.org</RefSource>
                <RefTarget Address="http://kinectforwindows.org" TargetType="URL"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR8">
            <CitationNumber>8.</CitationNumber>
            <BibUnstructured>Leap Motion [cited: 2014], <ExternalRef>
                <RefSource>https://www.leapmotion.com</RefSource>
                <RefTarget Address="https://www.leapmotion.com" TargetType="URL"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR9">
            <CitationNumber>9.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>R</Initials>
                <FamilyName>Edwards</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>O</Initials>
                <FamilyName>Lippold</FamilyName>
              </BibAuthorName>
              <Year>1956</Year>
              <ArticleTitle Language="En">The relation between force and integrated electrical activity in fatigued muscle</ArticleTitle>
              <JournalTitle>J. Physiol.</JournalTitle>
              <VolumeID>28</VolumeID>
              <FirstPage>677</FirstPage>
              <LastPage>681</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1113/jphysiol.1956.sp005558</Handle>
              </Occurrence>
            </BibArticle>
            <BibUnstructured>R. Edwards, O. Lippold, The relation between force and integrated electrical activity in fatigued muscle. J. Physiol. <Emphasis Type="Bold">28</Emphasis>, 677–681 (1956)</BibUnstructured>
          </Citation>
          <Citation ID="CR10">
            <CitationNumber>10.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>R</Initials>
                <FamilyName>Enkona</FamilyName>
              </BibAuthorName>
              <BibAuthorName>
                <Initials>D</Initials>
                <FamilyName>Stuart</FamilyName>
              </BibAuthorName>
              <Year>1985</Year>
              <ArticleTitle Language="En">The contribution of neuroscience to exercise studies</ArticleTitle>
              <JournalTitle>Fed. Proc.</JournalTitle>
              <VolumeID>44</VolumeID>
              <FirstPage>2279</FirstPage>
              <LastPage>2285</LastPage>
            </BibArticle>
            <BibUnstructured>R. Enkona, D. Stuart, The contribution of neuroscience to exercise studies. Fed. Proc. <Emphasis Type="Bold">44</Emphasis>, 2279–2285 (1985)</BibUnstructured>
          </Citation>
          <Citation ID="CR11">
            <CitationNumber>11.</CitationNumber>
            <BibArticle>
              <BibAuthorName>
                <Initials>J</Initials>
                <FamilyName>Giles</FamilyName>
              </BibAuthorName>
              <Year>2010</Year>
              <ArticleTitle Language="En">Inside the race to hack the Kinect</ArticleTitle>
              <JournalTitle>New Sci.</JournalTitle>
              <VolumeID>208</VolumeID>
              <IssueID>2789</IssueID>
              <FirstPage>22</FirstPage>
              <LastPage>23</LastPage>
              <Occurrence Type="DOI">
                <Handle>10.1016/S0262-4079(10)62989-2</Handle>
              </Occurrence>
              <BibComments>ISSN 0262–4079</BibComments>
            </BibArticle>
            <BibUnstructured>J. Giles, Inside the race to hack the Kinect. New Sci. <Emphasis Type="Bold">208</Emphasis>(2789), 22–23 (2010). ISSN 0262–4079</BibUnstructured>
          </Citation>
          <Citation ID="CR12">
            <CitationNumber>12.</CitationNumber>
            <BibUnstructured>MIT media lab hacks the Kinect for browser navigation with gestures [cited: 201z4], <ExternalRef>
                <RefSource>http://www.readwriteweb.com/2010/11/24/kinect_browser_navigation</RefSource>
                <RefTarget Address="http://www.readwriteweb.com/2010/11/24/kinect_browser_navigation" TargetType="URL"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR13">
            <CitationNumber>13.</CitationNumber>
            <BibUnstructured>Emotiv INSIGHT [cited: 2014], <ExternalRef>
                <RefSource>http://emotivinsight.com</RefSource>
                <RefTarget Address="http://emotivinsight.com" TargetType="URL"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR14">
            <CitationNumber>14.</CitationNumber>
            <BibUnstructured>Apple acquires Israeli 3D chip developer prime sense [cited: 2014 Aug 17], <ExternalRef>
                <RefSource>http://www.reuters.com/article/2013/11/25/us-primesense-offer-apple-idUSBRE9AO04C20131125</RefSource>
                <RefTarget Address="http://www.reuters.com/article/2013/11/25/us-primesense-offer-apple-idUSBRE9AO04C20131125" TargetType="URL"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR15">
            <CitationNumber>15.</CitationNumber>
            <BibUnstructured>Raspberry PI [cited: 2014], <ExternalRef>
                <RefSource>http://www.raspberrypi.org</RefSource>
                <RefTarget Address="http://www.raspberrypi.org" TargetType="URL"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR16">
            <CitationNumber>16.</CitationNumber>
            <BibUnstructured>Roby Savvy eBox-3350MX [cited: 2014], <ExternalRef>
                <RefSource>http://robosavvy.com/store/product_info.php/products_id/1704</RefSource>
                <RefTarget Address="http://robosavvy.com/store/product_info.php/products_id/1704" TargetType="URL"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR17">
            <CitationNumber>17.</CitationNumber>
            <BibUnstructured>Panda Board [cited: 2014], <ExternalRef>
                <RefSource>http://www.pandaboard.org</RefSource>
                <RefTarget Address="http://www.pandaboard.org" TargetType="URL"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR18">
            <CitationNumber>18.</CitationNumber>
            <BibUnstructured>ZigFu for Unity3D [cited: 2014], <ExternalRef>
                <RefSource>http://zigfu.com</RefSource>
                <RefTarget Address="http://zigfu.com" TargetType="URL"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR19">
            <CitationNumber>19.</CitationNumber>
            <BibUnstructured>Unity3D game engine [cited: 2014], <ExternalRef>
                <RefSource>http://unity3d.com</RefSource>
                <RefTarget Address="http://unity3d.com" TargetType="URL"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR20">
            <CitationNumber>20.</CitationNumber>
            <BibUnstructured>Kinect marker-less motion capture (Brekel) [cited: 2014], <ExternalRef>
                <RefSource>http://www.brekel.com</RefSource>
                <RefTarget Address="http://www.brekel.com" TargetType="URL"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR21">
            <CitationNumber>21.</CitationNumber>
            <BibUnstructured>Motion Builder [cited: 2014], <ExternalRef>
                <RefSource>http://www.autodesk.com/products/motionbuilder</RefSource>
                <RefTarget Address="http://www.autodesk.com/products/motionbuilder" TargetType="URL"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR22">
            <CitationNumber>22.</CitationNumber>
            <BibUnstructured>Virtual Room [cited: 2014], <ExternalRef>
                <RefSource>https://www.assetstore.unity3d.com/en/#!/content/6468</RefSource>
                <RefTarget Address="https://www.assetstore.unity3d.com/en/#!/content/6468" TargetType="URL"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR23">
            <CitationNumber>23.</CitationNumber>
            <BibUnstructured>3D infinite runner toolkit [cited: 2014], <ExternalRef>
                <RefSource>http://u3d.as/content/dreamdev-studios/3d-infinite-runner-toolkit/5Nh</RefSource>
                <RefTarget Address="http://u3d.as/content/dreamdev-studios/3d-infinite-runner-toolkit/5Nh" TargetType="URL"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR24">
            <CitationNumber>24.</CitationNumber>
            <BibUnstructured>Lloyd Hooson, Unity 3D stereoscopic development [cited: 2014], <ExternalRef>
                <RefSource>http://lloydhooson.co.uk/2010/01/12/unity-3d-stereoscopic-development</RefSource>
                <RefTarget Address="http://lloydhooson.co.uk/2010/01/12/unity-3d-stereoscopic-development" TargetType="URL"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR25">
            <CitationNumber>25.</CitationNumber>
            <BibUnstructured>Human Interface Technology Laboratory Australia (HIT Lab), Stereoscopic 3D, [cited: 2014], <ExternalRef>
                <RefSource>http://www.hitlab.utas.edu.au/wiki/Stereoscopic_3D</RefSource>
                <RefTarget Address="http://www.hitlab.utas.edu.au/wiki/Stereoscopic_3D" TargetType="URL"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR26">
            <CitationNumber>26.</CitationNumber>
            <BibUnstructured>Game WAVE for Leap Motion, <ExternalRef>
                <RefSource>https://apps.leapmotion.com/apps/gamewave</RefSource>
                <RefTarget Address="https://apps.leapmotion.com/apps/gamewave" TargetType="URL"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR27">
            <CitationNumber>27.</CitationNumber>
            <BibUnstructured>Myo Gesture Control Armband [cited: 2015], <ExternalRef>
                <RefSource>https://www.thalmic.com/en/myo</RefSource>
                <RefTarget Address="https://www.thalmic.com/en/myo" TargetType="URL"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR28">
            <CitationNumber>28.</CitationNumber>
            <BibUnstructured>Amazing Skater from Ace Games, <ExternalRef>
                <RefSource>http://www.acegames.in</RefSource>
                <RefTarget Address="http://www.acegames.in" TargetType="URL"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
          <Citation ID="CR29">
            <CitationNumber>29.</CitationNumber>
            <BibUnstructured>Open EMR [cited: 2014], <ExternalRef>
                <RefSource>http://www.open-emr.org</RefSource>
                <RefTarget Address="http://www.open-emr.org" TargetType="URL"/>
              </ExternalRef>
</BibUnstructured>
          </Citation>
        </Bibliography>
      </ChapterBackmatter>
    </Chapter>
  </Book>
</Publisher>
