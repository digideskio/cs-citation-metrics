<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE Publisher PUBLIC "-//Springer-Verlag//DTD A++ V2.4//EN" "http://devel.springer.de/A++/V2.4/DTD/A++V2.4.dtd">
<Publisher>
  <PublisherInfo>
    <PublisherName>Springer-Verlag</PublisherName>
    <PublisherLocation>London</PublisherLocation>
  </PublisherInfo>
  <Journal OutputMedium="All">
    <JournalInfo JournalProductType="ArchiveJournal" NumberingStyle="ContentOnly">
      <JournalID>13735</JournalID>
      <JournalPrintISSN>2192-6611</JournalPrintISSN>
      <JournalElectronicISSN>2192-662X</JournalElectronicISSN>
      <JournalTitle>International Journal of Multimedia Information Retrieval</JournalTitle>
      <JournalAbbreviatedTitle>Int J Multimed Info Retr</JournalAbbreviatedTitle>
      <JournalSubjectGroup>
        <JournalSubject Type="Primary">Computer Science</JournalSubject>
        <JournalSubject Type="Secondary">Image Processing and Computer Vision</JournalSubject>
        <JournalSubject Type="Secondary">Multimedia Information Systems</JournalSubject>
        <JournalSubject Type="Secondary">Computer Science, general</JournalSubject>
        <JournalSubject Type="Secondary">Data Mining and Knowledge Discovery</JournalSubject>
        <JournalSubject Type="Secondary">Information Systems Applications (incl. Internet)</JournalSubject>
        <JournalSubject Type="Secondary">Information Storage and Retrieval</JournalSubject>
      </JournalSubjectGroup>
    </JournalInfo>
    <Volume OutputMedium="All">
      <VolumeInfo TocLevels="0" VolumeType="Regular">
        <VolumeIDStart>1</VolumeIDStart>
        <VolumeIDEnd>1</VolumeIDEnd>
        <VolumeIssueCount>4</VolumeIssueCount>
      </VolumeInfo>
      <Issue IssueType="Regular" OutputMedium="All">
        <IssueInfo IssueType="Regular" TocLevels="0">
          <IssueIDStart>1</IssueIDStart>
          <IssueIDEnd>1</IssueIDEnd>
          <IssueArticleCount>6</IssueArticleCount>
          <IssueHistory>
            <OnlineDate>
              <Year>2012</Year>
              <Month>4</Month>
              <Day>24</Day>
            </OnlineDate>
            <PrintDate>
              <Year>2012</Year>
              <Month>4</Month>
              <Day>23</Day>
            </PrintDate>
            <CoverDate>
              <Year>2012</Year>
              <Month>4</Month>
            </CoverDate>
            <PricelistYear>2012</PricelistYear>
          </IssueHistory>
          <IssueCopyright>
            <CopyrightHolderName>Springer-Verlag London Limited</CopyrightHolderName>
            <CopyrightYear>2012</CopyrightYear>
          </IssueCopyright>
        </IssueInfo>
        <Article ID="s13735-012-0003-7" OutputMedium="All">
          <ArticleInfo ArticleType="OriginalPaper" ContainsESM="No" Language="En" NumberingStyle="ContentOnly" TocLevels="0">
            <ArticleID>3</ArticleID>
            <ArticleDOI>10.1007/s13735-012-0003-7</ArticleDOI>
            <ArticleSequenceNumber>6</ArticleSequenceNumber>
            <ArticleTitle Language="En" OutputMedium="All">Multimedia semantics-aware query-adaptive hashing with bits reconfigurability</ArticleTitle>
            <ArticleCategory>Invited Paper</ArticleCategory>
            <ArticleFirstPage>59</ArticleFirstPage>
            <ArticleLastPage>70</ArticleLastPage>
            <ArticleHistory>
              <RegistrationDate>
                <Year>2012</Year>
                <Month>2</Month>
                <Day>2</Day>
              </RegistrationDate>
              <Received>
                <Year>2011</Year>
                <Month>12</Month>
                <Day>20</Day>
              </Received>
              <Accepted>
                <Year>2012</Year>
                <Month>1</Month>
                <Day>8</Day>
              </Accepted>
              <OnlineDate>
                <Year>2012</Year>
                <Month>3</Month>
                <Day>9</Day>
              </OnlineDate>
            </ArticleHistory>
            <ArticleCopyright>
              <CopyrightHolderName>Springer-Verlag London Limited</CopyrightHolderName>
              <CopyrightYear>2012</CopyrightYear>
            </ArticleCopyright>
            <ArticleGrants Type="Regular">
              <MetadataGrant Grant="OpenAccess"/>
              <AbstractGrant Grant="OpenAccess"/>
              <BodyPDFGrant Grant="OpenAccess"/>
              <BodyHTMLGrant Grant="OpenAccess"/>
              <BibliographyGrant Grant="OpenAccess"/>
              <ESMGrant Grant="OpenAccess"/>
            </ArticleGrants>
          </ArticleInfo>
          <ArticleHeader>
            <AuthorGroup>
              <Author AffiliationIDS="Aff1" CorrespondingAffiliationID="Aff1">
                <AuthorName DisplayOrder="Western">
                  <GivenName>Yadong</GivenName>
                  <FamilyName>Mu</FamilyName>
                </AuthorName>
                <Contact>
                  <Email>muyadong@gmail.com</Email>
                  <Email>ym2372@columbia.edu</Email>
                </Contact>
              </Author>
              <Author AffiliationIDS="Aff2">
                <AuthorName DisplayOrder="Western">
                  <GivenName>Xiangyu</GivenName>
                  <FamilyName>Chen</FamilyName>
                </AuthorName>
                <Contact>
                  <Email>chenxiangyu@nus.edu.sg</Email>
                </Contact>
              </Author>
              <Author AffiliationIDS="Aff3">
                <AuthorName DisplayOrder="Western">
                  <GivenName>Xianglong</GivenName>
                  <FamilyName>Liu</FamilyName>
                </AuthorName>
                <Contact>
                  <Email>xlliu@nlsde.buaa.edu.cn</Email>
                </Contact>
              </Author>
              <Author AffiliationIDS="Aff2">
                <AuthorName DisplayOrder="Western">
                  <GivenName>Tat-Seng</GivenName>
                  <FamilyName>Chua</FamilyName>
                </AuthorName>
                <Contact>
                  <Email>chuats@nus.edu.sg</Email>
                </Contact>
              </Author>
              <Author AffiliationIDS="Aff4">
                <AuthorName DisplayOrder="Western">
                  <GivenName>Shuicheng</GivenName>
                  <FamilyName>Yan</FamilyName>
                </AuthorName>
                <Contact>
                  <Email>eleyans@nus.edu.sg</Email>
                </Contact>
              </Author>
              <Affiliation ID="Aff1">
                <OrgDivision>Electrical Engineering Department</OrgDivision>
                <OrgName>Columbia University</OrgName>
                <OrgAddress>
                  <City>New York</City>
                  <State>NY</State>
                  <Postcode>10027</Postcode>
                  <Country Code="US">USA</Country>
                </OrgAddress>
              </Affiliation>
              <Affiliation ID="Aff2">
                <OrgDivision>School of Computing</OrgDivision>
                <OrgName>National University of Singapore</OrgName>
                <OrgAddress>
                  <City>Singapore</City>
                  <Postcode>117576</Postcode>
                  <Country Code="SG">Singapore</Country>
                </OrgAddress>
              </Affiliation>
              <Affiliation ID="Aff3">
                <OrgDivision>State Key Laboratory of Software Development Environment</OrgDivision>
                <OrgName>Beihang University</OrgName>
                <OrgAddress>
                  <Postcode>100191</Postcode>
                  <City>Beijing</City>
                  <Country Code="CN">People’s Republic of China</Country>
                </OrgAddress>
              </Affiliation>
              <Affiliation ID="Aff4">
                <OrgDivision>Department of Electrical and Computer Engineering</OrgDivision>
                <OrgName>National University of Singapore</OrgName>
                <OrgAddress>
                  <City>Singapore</City>
                  <Postcode>117576</Postcode>
                  <Country Code="SG">Singapore</Country>
                </OrgAddress>
              </Affiliation>
            </AuthorGroup>
            <Abstract ID="Abs1" Language="En" OutputMedium="All">
              <Heading>Abstract</Heading>
              <Para>In the past decade, <Emphasis Type="Italic">locality-sensitive hashing</Emphasis> (LSH) has gained a large amount of attention from both the multimedia and computer vision communities owing to its empirical success and theoretic guarantee in large-scale multimedia indexing and retrieval. Original LSH algorithms are designated for generic metrics such as Cosine similarity, <InlineEquation ID="IEq1"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq1.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _2$$]]></EquationSource></InlineEquation>-norm and Jaccard index, which are later extended to support those metrics learned from user-supplied supervision information. One of the common drawbacks of existing algorithms lies in their incapability to be flexibly adapted to the metric changes, along with the inefficacy when handling diverse semantics (e.g., the large number of semantic object categories in the <Emphasis Type="Italic">ImageNet</Emphasis> database), which motivates our proposed framework toward reconfigurable hashing. The basic idea of the proposed indexing framework is to maintain a large pool of over-complete hashing functions, which are randomly generated and shared when indexing diverse multimedia semantics. For specific semantic category, the algorithm adaptively selects the most relevant hashing bits by maximizing the consistency between semantic distance and hashing-based Hamming distance, thereby achieving reusability of the pre-computed hashing bits. Such a scheme especially benefits the indexing and retrieval of large-scale databases, since it facilitates one-off indexing rather than continuous computation-intensive maintenance toward metric adaptation. In practice, we propose a sequential bit-selection algorithm based on local consistency and global regularization. Extensive studies are conducted on large-scale image benchmarks to comparatively investigate the performance of different strategies for reconfigurable hashing. Despite the vast literature on hashing, to our best knowledge rare endeavors have been spent toward the reusability of hashing structures in large-scale data sets.</Para>
            </Abstract>
            <KeywordGroup Language="En" OutputMedium="All">
              <Heading>Keywords</Heading>
              <Keyword>Locality-sensitive hashing</Keyword>
              <Keyword>Query-adaptive hashing</Keyword>
              <Keyword>Bits reconfigurability</Keyword>
              <Keyword>Shannon entropy</Keyword>
            </KeywordGroup>
            <ArticleNote Type="Misc">
              <SimplePara>This research is supported by the CSIDM Project No. CSIDM-200803 partially funded by a grant from the National Research Foundation (NRF), which is administered by the Media Development Authority (MDA) of Singapore, and also supported by National Major Project of China “Advanced Unstructured Data Repository” (No. 2010ZX01042-002-001-00).</SimplePara>
            </ArticleNote>
          </ArticleHeader>
          <Body>
            <Section1 ID="Sec1">
              <Heading>Introduction</Heading>
              <Para>With the explosive accumulation of multimedia data in these domains such as shared photos or video clips on the Web, various multimedia applications suffer from large data scales and feature dimensions. Usually such databases are represented by uniform-length high-dimensional feature vectors. Defined on the video features, a simple yet essential operation is to efficiently find a set of nearest neighbors for an arbitrary query by comparing pairwise feature proximity. A naive linear-scan implementation involves pairwise computations between the query and all items in the database, which has linear complexity with respect to the data set scale and is time-consuming for large-scale data and high dimensionality. Fortunately, in most applications, there is no need to identify the exact nearest neighbors. Instead, <Emphasis Type="Italic">approximate nearest neighbors</Emphasis> (ANN) [<CitationRef CitationID="CR2">2</CitationRef>, <CitationRef CitationID="CR3">3</CitationRef>] achieve comparable performance in many scenarios, while greatly decreasing the computational cost. It motivates the research on efficient indexing for large-scale image and video data sets.</Para>
              <Para>Recent progress has witnessed the popularity of <Emphasis Type="Italic">locality-sensitive hashing</Emphasis> (LSH) [<CitationRef CitationID="CR2">2</CitationRef>] as an invaluable tool for retrieving approximate nearest neighbors in the aforementioned setting. The basic idea of LSH is to randomly generate a number of “buckets” according to specific hashing scheme and map data into the hashing buckets. Unlike other kinds of hashing algorithms, LSH is characterized by the so-called “locality-sensitive” property. Namely, denote collision probability to be the probability that two data points are mapped into the same bucket. A valid LSH algorithm will guarantee higher collision probability for similar data. The line of work has gained considerable empirical success in a variety of tasks such as image search, near-duplicate image detection [<CitationRef CitationID="CR13">13</CitationRef>], human pose estimation [<CitationRef CitationID="CR26">26</CitationRef>], etc.</Para>
              <Para>The key factor for an LSH algorithm is the underlying metric to measure data similarity. Original LSH algorithms are devised for uniform-length feature vectors equipped with “standard” metrics, including Jaccard Index [<CitationRef CitationID="CR4">4</CitationRef>], Hamming distance [<CitationRef CitationID="CR12">12</CitationRef>], <InlineEquation ID="IEq2"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq2.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _2$$]]></EquationSource></InlineEquation>-norm [<CitationRef CitationID="CR1">1</CitationRef>], Cosine similarity [<CitationRef CitationID="CR5">5</CitationRef>] or general <InlineEquation ID="IEq3"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq3.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _p$$]]></EquationSource></InlineEquation>-norm (<InlineEquation ID="IEq4"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq4.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p \in (0,2]$$]]></EquationSource></InlineEquation>) [<CitationRef CitationID="CR6">6</CitationRef>]. Although strict collision-bound analysis is presented, unfortunately it is seldom the case that in real-world multimedia applications the pairwise similarity between visual identities (e.g., images, three-dimensional shapes, video clips) are gauged using aforementioned metrics. It is essentially caused by the well-known semantic gap between low-level features and high-level multimedia semantics. Instead, the so-called Mercer kernels [<CitationRef CitationID="CR25">25</CitationRef>] provide more flexibility by implicitly embedding original features into high-dimensional Hilbert spaces. Representative Mercer kernels widely used by multimedia practitioners include the Radial Basis Function (RBF) kernel [<CitationRef CitationID="CR25">25</CitationRef>] and Pyramid Match Kernel (PMK) [<CitationRef CitationID="CR8">8</CitationRef>]. Previous study [<CitationRef CitationID="CR14">14</CitationRef>, <CitationRef CitationID="CR18">18</CitationRef>, <CitationRef CitationID="CR9">9</CitationRef>] shows that the extension of LSH algorithms to the kernelized case is feasible.</Para>
              <Para>Note that all of the aforementioned metrics (including those induced from Mercer kernels) are explicitly predefined. More complications stem from the ambiguous metrics implicitly defined by a bunch of pairwise similarity (or dissimilarity) constraints, which frequently occur in the research field of metric learning [<CitationRef CitationID="CR32">32</CitationRef>]. Hashing with this kind of partial supervision is challenging. Previous efforts address this task toward two directions: (1) hashing with learned metric [<CitationRef CitationID="CR14">14</CitationRef>], which transfigures the original metrics (typically via the modulation of Mahalonobius matrix) and then applies standard hashing techniques, and (2) data-dependent hashing with weak supervision [<CitationRef CitationID="CR28">28</CitationRef>, <CitationRef CitationID="CR18">18</CitationRef>], which seeks most consistent hashing hyperplanes by constrained optimization. The methods from the first category are computationally efficient, since it decouples the overall complicated problem into two sub-problems, each of which is relatively easier. However, when the similarity (or dissimilarity) constraints are given in a very sparse manner, the input will be insufficient to learn a high-quality metric; therefore, they are probably not applicable. The methods from the second category are more tightly related to the final performance, since they simultaneously optimize the hashing hyperplanes and discriminative functions. Their advantages lie in the high complexity in non-convex optimization [<CitationRef CitationID="CR18">18</CitationRef>] or eigen-decomposition [<CitationRef CitationID="CR28">28</CitationRef>]. However, despite their success, existing techniques fail to handle the diverse semantics in real-world multimedia applications. The cruxes of the dilemma originate from two factors:<UnorderedList Mark="Dash"><ItemContent><Para>The ambiguity and inconstancy of the multimedia semantics. An example is the visual semantics induced from pairwise affinity relationship, which is either constructed from manual specification or community-contributed noisy tags. Unfortunately, both information sources are usually subject to frequent update, which potentially causes semantic drifting. Since both the hashing scheme and the resultant indexing structure are seriously hinged on the underlying semantics or metric, one-off data indexing is unfeasible under such circumstance of unstable semantics, which triggers unnecessary labors spent on indexing structure maintenance. </Para></ItemContent><ItemContent><Para>The diversity of the semantics [<CitationRef CitationID="CR30">30</CitationRef>]. Most of previous studies assume that data are associated with a small number of distinct semantics, which is usually not the true case in real-world benchmarks. For example, the hand-labeled ImageNet data set<Footnote ID="Fn1"><Para><ExternalRef><RefSource>http://www.image-net.org/challenges/LSVRC/2010</RefSource><RefTarget Address="http://www.image-net.org/challenges/LSVRC/2010" TargetType="URL"/></ExternalRef></Para></Footnote> contains more than ten million images that depict 10,000+ object categories. Facing such input, one possible solution is to simultaneously pursue the optimal hashing functions for all categories. However, it is unwise considering the unknown and complex intrinsic data structures. Another possible solution is to separately conduct hashing for each unique category and concatenate all to form the final indexing structure, which unfortunately is uneconomic in terms of storage (actually the overlapped semantic subspace between two categories implies that several hashing bits can be shared to save the storage) and vulnerable to semantic changes and new emerging categories due to the expensive re-indexing effort for the large-scale data set. </Para></ItemContent></UnorderedList>The above-mentioned drawbacks of existing methods motivate “reconfigurable hashing” proposed in this paper. Figure <InternalRef RefID="Fig1">1</InternalRef> illustrates the basic idea of reconfigurable hashing, whose basic operation is to generate a set of over-complete hash functions and perform one-off data indexing. When the semantic annotations or constraints are available, the algorithm optimally chooses a small portion of relevant hashing bits from the pool and re-weight them to best fit the target semantic metrics. Obviously, the so-called reconfigurable hashing is in hash bit level.</Para>
              <Para>
                <Figure Category="Standard" Float="Yes" ID="Fig1">
                  <Caption Language="En">
                    <CaptionNumber>Fig. 1</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>Illustration of the proposed hashing scheme with bit reconfigurability. The <Emphasis Type="Italic">left subfigure</Emphasis> shows a toy data set and the pre-computed redundant hashing functions, while the contents on the <Emphasis Type="Italic">right panel sketch</Emphasis> the idea of optimal hashing bit selection toward specific semantic category</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <MediaObject ID="MO1">
                    <ImageObject Color="Color" FileRef="MediaObjects/13735_2012_3_Fig1_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                  </MediaObject>
                </Figure>
              </Para>
              <Para>Figure <InternalRef RefID="Fig2">2</InternalRef> presents the processing pipeline of the image retrieval system. The images in the database are indexed according to a large number of hashing functions. In the retrieval stage, a new image is introduced as the query. We assume that the semantic category associated with the query image is also known. Based on the semantic category, the algorithms discussed in this paper are capable of selecting category-adaptive hashing functions, which is a small subset of the overall hashing pool. Low-level features are extracted from the query image and indexed to obtain the binary hashing code, which is afterward compared with those stored in the image database to find the nearest images.</Para>
              <Para>
                <Figure Category="Standard" Float="Yes" ID="Fig2">
                  <Caption Language="En">
                    <CaptionNumber>Fig. 2</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>Illustration of the proposed image retrieval system</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <MediaObject ID="MO2">
                    <ImageObject Color="Color" FileRef="MediaObjects/13735_2012_3_Fig2_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                  </MediaObject>
                </Figure>
              </Para>
              <Para>In this paper, the goal is to develop a novel indexing framework that supports an unlimited number of diverse semantics based on one-off indexing, and also admits the adaptation to the metric changes at very low computational cost and zero re-indexing effort. In detail, our contributions in this paper can be summarized as follows:<UnorderedList Mark="Dash"><ItemContent><Para>A novel hashing algorithm named <Emphasis Type="Italic">random-anchor-random-projection</Emphasis> (RARP), which is equivalent to redundant random partition of the ambient feature space and proves superior to other candidate LSH algorithms. Strict collision analysis for RARP is supplied. </Para></ItemContent><ItemContent><Para>We discuss different strategies for optimal hash function selection and further propose a sequential algorithm based on local consistence and global regularization. </Para></ItemContent><ItemContent><Para>The idea of reconfigurable hashing is content agnostic and consequently domain independent, but the performances of different selection strategies vary. Comparative investigation of the proposed and other candidate strategies is provided on four popular multiple-semantics image benchmarks, which validates the effectiveness of reconfigurable hashing and its scalability to large-scale data set. </Para></ItemContent></UnorderedList>The rest of the paper is organized as follows. Section <InternalRef RefID="Sec2">2</InternalRef> provides a brief survey of relevant literature. Section <InternalRef RefID="Sec3">3</InternalRef> defines the notations used in this paper and formally states the problem to be solved. Sections <InternalRef RefID="Sec4">4</InternalRef> and <InternalRef RefID="Sec7">5</InternalRef> elaborate on the proposed formulation and also other alternative strategies. More details of the hashing collision analysis are found in Sect. <InternalRef RefID="Sec8">6</InternalRef>. Extensive experiments are conducted on four real-world benchmarks in Sect. <InternalRef RefID="Sec9">7</InternalRef> and in Sect. <InternalRef RefID="Sec12">8</InternalRef> we give the concluding remarks and point out several directions for future work.</Para>
            </Section1>
            <Section1 ID="Sec2">
              <Heading>Related work</Heading>
              <Para>In this section, we provide a brief review of various <Emphasis Type="Italic">locality-sensitive hashing</Emphasis> (LSH) [<CitationRef CitationID="CR2">2</CitationRef>, <CitationRef CitationID="CR6">6</CitationRef>, <CitationRef CitationID="CR11">11</CitationRef>, <CitationRef CitationID="CR17">17</CitationRef>] methods that are recently proposed to tackle the large-scale retrieval problem.</Para>
              <Para>Let <InlineEquation ID="IEq5"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq5.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ H} $$]]></EquationSource></InlineEquation> be a family of hashing functions mapping <InlineEquation ID="IEq6"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq6.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbb{ R} ^d$$]]></EquationSource></InlineEquation> to some universe <InlineEquation ID="IEq7"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq7.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$U$$]]></EquationSource></InlineEquation>. The family <InlineEquation ID="IEq8"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq8.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ H} $$]]></EquationSource></InlineEquation> is called <Emphasis Type="Italic">locality sensitive</Emphasis> if it satisfies the following conditions:</Para>
              <FormalPara RenderingStyle="Style1">
                <Heading>Definition 1</Heading>
                <Para> (<Emphasis Type="Italic">Locality-sensitive hashing</Emphasis> [<CitationRef CitationID="CR2">2</CitationRef>]) A hashing family <InlineEquation ID="IEq9"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq9.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ H} $$]]></EquationSource></InlineEquation> is called <InlineEquation ID="IEq10"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq10.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$(1,c,p_1,p_2)$$]]></EquationSource></InlineEquation>-sensitive if the following properties hold for any two samples <InlineEquation ID="IEq11"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq11.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x,y \in \mathbb{ R} ^d$$]]></EquationSource></InlineEquation>, i.e.,<Equation ID="Equa1"><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_Equa1.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} (K1)&\text{ If} \Vert x-y\Vert \le 1 \text{ then} \text{ Pr}_{\mathcal{ H} } (h(x) = h(y)) \ge p_1. \\ (K1)&\text{ If} \Vert x-y\Vert \ge c \text{ then} \text{ Pr}_{\mathcal{ H} } (h(x) = h(y)) \le p_2. \end{aligned}$$]]></EquationSource></Equation>
                </Para>
              </FormalPara>
              <Para>To guarantee that the hashing functions from <InlineEquation ID="IEq12"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq12.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ H} $$]]></EquationSource></InlineEquation> are meaningful, typically we have <InlineEquation ID="IEq13"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq13.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$c > 1$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq14"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq14.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p_1 > p_2$$]]></EquationSource></InlineEquation>. Other alternative definition exists, such as <InlineEquation ID="IEq15"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq15.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\forall h \in \mathcal{ H} , \text{ Pr}[h(x) = h(y)] = \kappa (x, y)$$]]></EquationSource></InlineEquation>, where <InlineEquation ID="IEq16"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq16.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\kappa (\cdot ,\cdot )$$]]></EquationSource></InlineEquation> denotes the similarity measure between samples <InlineEquation ID="IEq17"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq17.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq18"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq18.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$y$$]]></EquationSource></InlineEquation>. In other words, <InlineEquation ID="IEq19"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq19.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq20"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq20.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$y$$]]></EquationSource></InlineEquation>’s collision probability (i.e., being mapped to the same hash bucket) monotonically increases with respect to their similarity value, which is known as the <Emphasis Type="Italic">locality-sensitive</Emphasis> property.</Para>
              <Para>Existing LSH algorithms can be roughly cast into the following categories:<UnorderedList Mark="Dash"><ItemContent><Para><Emphasis Type="Italic">Element sampling or permutation</Emphasis> Well-known examples include the hashing algorithms developed for the Hamming distance [<CitationRef CitationID="CR12">12</CitationRef>] and Jaccard Index [<CitationRef CitationID="CR4">4</CitationRef>]. For example, in the Hamming case, feature vectors are all binary valued. The work in [<CitationRef CitationID="CR12">12</CitationRef>] presents a hashing scheme <InlineEquation ID="IEq21"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq21.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$h(x) = x_i$$]]></EquationSource></InlineEquation>, where <InlineEquation ID="IEq22"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq22.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$i$$]]></EquationSource></InlineEquation> is randomly sampled from the dimension index set <InlineEquation ID="IEq23"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq23.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\{1, \ldots , d\}$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq24"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq24.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x_i$$]]></EquationSource></InlineEquation> is the binary value of the <InlineEquation ID="IEq25"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq25.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$i$$]]></EquationSource></InlineEquation>-th dimension. The guarantee of the locality-sensitive property is also given in [<CitationRef CitationID="CR12">12</CitationRef>]. </Para></ItemContent><ItemContent><Para><Emphasis Type="Italic">Project-shift-segment</Emphasis> The idea is to map a feature point in <InlineEquation ID="IEq26"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq26.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbb{ R} ^d$$]]></EquationSource></InlineEquation> onto <InlineEquation ID="IEq27"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq27.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbb{ R} ^1$$]]></EquationSource></InlineEquation> along a random projection direction in <InlineEquation ID="IEq28"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq28.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbb{ R} ^d$$]]></EquationSource></InlineEquation>, and then randomly shift the projection values. Finally, the range of projection values is partitioned into several intervals of length <InlineEquation ID="IEq29"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq29.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$l_w$$]]></EquationSource></InlineEquation> (<InlineEquation ID="IEq30"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq30.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$l_w$$]]></EquationSource></InlineEquation> is a data-dependent parameter and needs fine tuning). In the extreme case, there are only two partitions and the output is binary bit. Examples include the algorithm for <InlineEquation ID="IEq31"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq31.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _1$$]]></EquationSource></InlineEquation> norm [<CitationRef CitationID="CR1">1</CitationRef>], for Cosine similarity [<CitationRef CitationID="CR5">5</CitationRef>, <CitationRef CitationID="CR7">7</CitationRef>], for <InlineEquation ID="IEq32"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq32.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _p$$]]></EquationSource></InlineEquation> norm [<CitationRef CitationID="CR6">6</CitationRef>] and for kernel-based metrics or semi-metrics [<CitationRef CitationID="CR14">14</CitationRef>, <CitationRef CitationID="CR20">20</CitationRef>]. </Para></ItemContent></UnorderedList>Here are two representative examples:</Para>
              <Para>(1) Arccos distance: for real-valued feature vectors lying on hypersphere <InlineEquation ID="IEq33"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq33.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$S^{d-1} = \{ x \in \mathbb{ R} ^d \mid \Vert x\Vert _2 = 1 \}$$]]></EquationSource></InlineEquation>, an angle-oriented distance can be defined as <InlineEquation ID="IEq34"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq34.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\Theta (x, y) = \text{ arccos} (\frac{\langle x, y \rangle }{\Vert x\Vert \Vert y\Vert })=\text{ arccos} (\langle x, y \rangle ) $$]]></EquationSource></InlineEquation>. Charikar et al. [<CitationRef CitationID="CR5">5</CitationRef>] propose the following LSH family:<Equation ID="Equ1"><EquationNumber>1</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_Equ1.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} h(x) = {\left\{ \begin{array}{ll}0,&\text{ if}\; \omega ^\top x < 0 \\ 1,&\text{ if}\; \omega ^\top x \ge 0 \end{array}\right.} \end{aligned}$$]]></EquationSource></Equation>where the hashing vector <InlineEquation ID="IEq35"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq35.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\omega $$]]></EquationSource></InlineEquation> is uniformly sampled from the unit hypersphere <InlineEquation ID="IEq36"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq36.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$S^{d-1}$$]]></EquationSource></InlineEquation>. The collision probability is <InlineEquation ID="IEq37"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq37.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\text{ Pr}[h(x) = h(y)] = 1 - \Theta (x,y)/\pi $$]]></EquationSource></InlineEquation>.</Para>
              <Para>(2) <InlineEquation ID="IEq38"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq38.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _p$$]]></EquationSource></InlineEquation> distance with <InlineEquation ID="IEq39"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq39.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p \in (0,2]$$]]></EquationSource></InlineEquation>: for linear vector spaces equipped with the <InlineEquation ID="IEq40"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq40.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _p$$]]></EquationSource></InlineEquation> metric, i.e., <InlineEquation ID="IEq41"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq41.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$D_{\ell _p}(x, y) = (\sum\nolimits _{i=1}^d |x_i - y_i|^p)^{\frac{1}{p}}$$]]></EquationSource></InlineEquation>, Datar et al. [<CitationRef CitationID="CR6">6</CitationRef>] propose a hashing algorithm based on linear projections onto a one-dimensional line and chopping the line into equal-length segments, as below:<Equation ID="Equ2"><EquationNumber>2</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_Equ2.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} h(x) = \left\lfloor \frac{\omega ^\top x + b}{W} \right\rfloor , \end{aligned}$$]]></EquationSource></Equation>where the hashing vector <InlineEquation ID="IEq42"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq42.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\omega \in \mathbb{ R} ^d$$]]></EquationSource></InlineEquation> is randomly sampled from the <Emphasis Type="Italic">p-stable distribution</Emphasis> and <InlineEquation ID="IEq43"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq43.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\lfloor \cdot \rfloor $$]]></EquationSource></InlineEquation> is the flooring function for rounding. <InlineEquation ID="IEq44"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq44.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$W$$]]></EquationSource></InlineEquation> is the data-dependent window size and <InlineEquation ID="IEq45"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq45.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$b$$]]></EquationSource></InlineEquation> is sampled from the uniform distribution <InlineEquation ID="IEq46"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq46.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$U[0,W)$$]]></EquationSource></InlineEquation>.<UnorderedList Mark="Dash"><ItemContent><Para><Emphasis Type="Italic">Prototype-based methods</Emphasis> Another LSH family uses predefined prototypes, such as polytopes on 24-D Leech lattice in <InlineEquation ID="IEq47"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq47.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _2$$]]></EquationSource></InlineEquation> space [<CitationRef CitationID="CR1">1</CitationRef>] (i.e., E2LSH) or 8-D lattice [<CitationRef CitationID="CR23">23</CitationRef>]. </Para></ItemContent><ItemContent><Para><Emphasis Type="Italic">Learning-based methods</Emphasis> Assisted with semantic annotations or labels, LSH can be adapted via various learning methods like the classic SpectralHash [<CitationRef CitationID="CR31">31</CitationRef>] and SemanticHash [<CitationRef CitationID="CR24">24</CitationRef>]. Recent progress has also been made on hashing with weak supervision [<CitationRef CitationID="CR18">18</CitationRef>, <CitationRef CitationID="CR28">28</CitationRef>] and sequential optimization [<CitationRef CitationID="CR29">29</CitationRef>]. </Para></ItemContent></UnorderedList>From the brief survey in this section, it is observed that prior research has mainly focused on designing LSH algorithms for specific metrics, while the task of our work aims to provide a meta-hashing method applicable to the existence of scalable diverse semantics and adaptive metrics. To our best knowledge, few related work can be found. Study on this topic still lacks in-depth exploration and remains an open problem.</Para>
            </Section1>
            <Section1 ID="Sec3">
              <Heading>Notations and problem setting</Heading>
              <Para>Before continuing, let us formally establish the notations and the problem setting. Denote <InlineEquation ID="IEq48"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq48.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ X} = \{x_1,\ldots ,x_n \}$$]]></EquationSource></InlineEquation> as the set of feature vectors in <InlineEquation ID="IEq49"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq49.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathbb{ R} ^d$$]]></EquationSource></InlineEquation>. Let <InlineEquation ID="IEq50"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq50.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$h_i: \mathbb{ R} ^d \mapsto \{0,1\}, i = 1 \ldots m$$]]></EquationSource></InlineEquation> be <InlineEquation ID="IEq51"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq51.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$m$$]]></EquationSource></InlineEquation> independently generated hashing functions, where <InlineEquation ID="IEq52"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq52.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$m$$]]></EquationSource></InlineEquation> is large enough to form an over-complete hashing pool. All samples in <InlineEquation ID="IEq53"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq53.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ X} $$]]></EquationSource></InlineEquation> are hashed to obtain binary bits according to the collection of hashing functions <InlineEquation ID="IEq54"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq54.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\{ h_i \}$$]]></EquationSource></InlineEquation>. The hashing operation is performed only once and not required to be redone any more. The aim of reconfigurable hashing is to select compact hashing bit configuration from the pool to approximate any unknown metrics in terms of Hamming distance.</Para>
              <Para>It is reasonable to assume that the maximum number of active hashing functions for each semantic category is budgeted. Denote it as <InlineEquation ID="IEq55"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq55.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$l$$]]></EquationSource></InlineEquation> and assume <InlineEquation ID="IEq56"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq56.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$l \ll m$$]]></EquationSource></InlineEquation>. To explicitly define the target semantics (or equivalently, metrics), assume that a fraction of data in <InlineEquation ID="IEq57"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq57.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ X} $$]]></EquationSource></InlineEquation> are associated with side information. Specifically, we focus on the widely used pairwise relationship [<CitationRef CitationID="CR18">18</CitationRef>, <CitationRef CitationID="CR28">28</CitationRef>] throughout this paper, which reveals the proximal extent of the two samples.</Para>
              <Para>Define two sets <InlineEquation ID="IEq58"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq58.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ M} $$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq59"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq59.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ C} $$]]></EquationSource></InlineEquation>. For any sample pair <InlineEquation ID="IEq60"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq60.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$(x_i,x_j) \in \mathcal{ M} $$]]></EquationSource></InlineEquation>, it reflects the acknowledgement from the annotators that <InlineEquation ID="IEq61"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq61.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x_i,x_j$$]]></EquationSource></InlineEquation> semantically form a neighbor pair in the context of target category. Similarly, <InlineEquation ID="IEq62"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq62.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$(x_i,x_j) \in \mathcal{ C} $$]]></EquationSource></InlineEquation> implies that they are far away in the unknown metric space or have different class labels. Note that the manual annotation is typically labor intensive; therefore, normally we assume that the labeled samples only cover a small portion of the whole data set. Also for large-scale data set associated with diverse semantics, the annotation is heavily unbalanced. In other words, the cardinality of <InlineEquation ID="IEq63"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq63.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ M} $$]]></EquationSource></InlineEquation> is far less than that of <InlineEquation ID="IEq64"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq64.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ C} $$]]></EquationSource></InlineEquation>, which mainly follows from the fact that <InlineEquation ID="IEq65"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq65.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ C} $$]]></EquationSource></InlineEquation> is the amalgamation of all other non-target categories. A qualified algorithm on reconfigurable hashing is expected to survive in such settings.</Para>
              <Para>Generally, we can regard the hashing function <InlineEquation ID="IEq66"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq66.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$h_i$$]]></EquationSource></InlineEquation> as a black box and only visit the binary hashing bits during the optimization. Different hashing schemes notably affect the retrieval quality given budgeted hashing bits. Ideally, most hashing functions are expected to be relevant to a target semantic category and complementary to each other. In this paper, we target the data lying in the <InlineEquation ID="IEq67"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq67.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _p$$]]></EquationSource></InlineEquation>-normed spaces (<InlineEquation ID="IEq68"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq68.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$0 < p \le 2$$]]></EquationSource></InlineEquation>) since it covers most of the feature representations used in multimedia applications. Most of the traditional hashing approaches [(e.g., the one presented in Eq. (<InternalRef RefID="Equ1">1</InternalRef>)] often ignore the data distribution, which potentially results in lower efficiency for unevenly distributed data. For example, the well-known SIFT feature [<CitationRef CitationID="CR15">15</CitationRef>] resides only within one of the quadrants. When applying the hashing algorithm in (<InternalRef RefID="Equ1">1</InternalRef>), more empty hashing buckets will be found. To attack this issue, we propose a hashing scheme named <Emphasis Type="Italic">random-anchor-random-projection</Emphasis> (called RARP hereafter), which belongs to the random projection-based hash family, yet differentiates itself from others by taking data distribution into account.</Para>
              <Para>In the proposed method, to generate a hashing function, a sample <InlineEquation ID="IEq69"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq69.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x^o$$]]></EquationSource></InlineEquation> is randomly sampled from the data set to serve as the so-called “anchor point”. Also, a random vector <InlineEquation ID="IEq70"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq70.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\omega $$]]></EquationSource></InlineEquation> is sampled uniformly from the <InlineEquation ID="IEq71"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq71.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p$$]]></EquationSource></InlineEquation>-stable distribution [<CitationRef CitationID="CR6">6</CitationRef>, <CitationRef CitationID="CR11">11</CitationRef>]. The projection value can be evaluated as <InlineEquation ID="IEq72"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq72.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\langle \omega , x - x^o \rangle = \langle \omega , x \rangle - b_{\omega , x^o}$$]]></EquationSource></InlineEquation>, where <InlineEquation ID="IEq73"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq73.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$b_{\omega , x^o}= \langle \omega , x^o \rangle $$]]></EquationSource></InlineEquation> is used as the hashing threshold, i.e.,<Equation ID="Equ3"><EquationNumber>3</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_Equ3.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} h(x) = {\left\{ \begin{array}{ll}0,&\text{ if}\; \langle \omega , x \rangle < b_{\omega , x^o} \\ 1,&\text{ if}\; \langle \omega , x \rangle \ge b_{\omega , x^o} \end{array}\right.} \end{aligned}$$]]></EquationSource></Equation>where <InlineEquation ID="IEq74"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq74.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\langle \omega , x \rangle $$]]></EquationSource></InlineEquation> denotes the inner product between <InlineEquation ID="IEq75"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq75.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\omega $$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq76"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq76.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x$$]]></EquationSource></InlineEquation>. The collision analysis for RARP is discussed in Sect. <InternalRef RefID="Sec8">6</InternalRef>.</Para>
              <Para>In the hashing literature, it is common to utilize Hamming distance as a proxy of the distance or similarity in the original feature space, which is defined as:<Equation ID="Equ4"><EquationNumber>4</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_Equ4.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} H(x, x^\prime ) = \sum _{b=1}^B (h_b(x) \oplus h_b(x^\prime )), \end{aligned}$$]]></EquationSource></Equation>where <InlineEquation ID="IEq77"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq77.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\oplus $$]]></EquationSource></InlineEquation> denotes the logical XOR operation (the output of XOR will be one if two input binary bits are different, and otherwise zero). Recall that the range of each hashing function is <InlineEquation ID="IEq78"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq78.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\{0,1\}$$]]></EquationSource></InlineEquation>. Equation (<InternalRef RefID="Equ4">4</InternalRef>) can be expressed in a more tractable form:<Equation ID="Equ5"><EquationNumber>5</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_Equ5.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \Vert h(x)- h(x^\prime )\Vert =(h(x) - h(x^\prime ))^\mathrm{ T} (h(x) - h(x^\prime )). \end{aligned}$$]]></EquationSource></Equation>Here, we adopt a generalized Hamming distance to ease numerical optimization. Specifically, we introduce the parametric Mahalonoisbu matrix <InlineEquation ID="IEq79"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq79.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$M$$]]></EquationSource></InlineEquation> for modulating purpose. To ensure the positiveness of the resulting measure, <InlineEquation ID="IEq80"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq80.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$M$$]]></EquationSource></InlineEquation> is required to reside in the positive semi-definite (p.s.d.) cone, or mathematically <InlineEquation ID="IEq81"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq81.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$M \succeq 0$$]]></EquationSource></InlineEquation>. The distance under specific <InlineEquation ID="IEq82"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq82.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$M$$]]></EquationSource></InlineEquation> can be written as follows:<Equation ID="Equ6"><EquationNumber>6</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_Equ6.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \Vert h(x) - h(x^\prime ) \Vert _M = \left( h(x) - h(x^\prime ) \right)^\mathrm{ T} \cdot M \cdot \left( h(x) - h(x^\prime ) \right).\nonumber \\ \end{aligned}$$]]></EquationSource></Equation>
              </Para>
            </Section1>
            <Section1 ID="Sec4">
              <Heading>The proposed algorithm</Heading>
              <Para>As a meta-hashing framework, the ultimate goal of reconfigurable hashing is the selection of hashing bits from a pre-built large pool. In this section, we first present a novel algorithm based on the idea of <Emphasis Type="Italic">averaged margin</Emphasis> and <Emphasis Type="Italic">global regularization</Emphasis>. Moreover, we also describe the other algorithm that simultaneously optimizes the hashing functions and bit weights.</Para>
              <Para>Later, we also present four more baseline algorithms for the same task, based on random selection, maximum variance, maximum local margin and Shannon information entropy, respectively. The empirical evaluation of the above methods is postponed to the experimental section.</Para>
              <Section2 ID="Sec5">
                <Heading>Formulation</Heading>
                <Para>As stated above, we rely on sets <InlineEquation ID="IEq83"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq83.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ M} $$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq84"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq84.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ C} $$]]></EquationSource></InlineEquation> to determine the underlying semantics. However, the construction of pairwise relationship has quadratic complexity of the sample number. To mitigate the annotation burden, a practical solution is instead to build two sets <InlineEquation ID="IEq85"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq85.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ L} _+$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq86"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq86.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ L} _-$$]]></EquationSource></InlineEquation>. The former set consists of the samples assigned to the target semantic label, and <InlineEquation ID="IEq87"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq87.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ L} _-$$]]></EquationSource></InlineEquation> collects the rest samples. We further generate <Emphasis Type="Italic">random homogeneous pair</Emphasis> and <Emphasis Type="Italic">random heterogeneous pair</Emphasis> to enrich <InlineEquation ID="IEq88"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq88.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ M} $$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq89"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq89.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ C} $$]]></EquationSource></InlineEquation>, respectively. For each sample <InlineEquation ID="IEq90"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq90.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x_i \in \mathcal{ L} _+$$]]></EquationSource></InlineEquation>, we randomly select <InlineEquation ID="IEq91"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq91.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x_j \in \mathcal{ L} _+$$]]></EquationSource></InlineEquation> with the guarantee <InlineEquation ID="IEq92"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq92.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$i \ne j$$]]></EquationSource></InlineEquation>. The pair <InlineEquation ID="IEq93"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq93.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$(x_i,x_j)$$]]></EquationSource></InlineEquation> is called <Emphasis Type="Italic">random homogeneous pair</Emphasis>. Likewise, given <InlineEquation ID="IEq94"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq94.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x_k \in \mathcal{ L} _-$$]]></EquationSource></InlineEquation>, <InlineEquation ID="IEq95"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq95.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$(x_i,x_k)$$]]></EquationSource></InlineEquation> constitutes a <Emphasis Type="Italic">random heterogeneous pair</Emphasis>. Therefore, the construction of <InlineEquation ID="IEq96"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq96.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ M} $$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq97"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq97.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ C} $$]]></EquationSource></InlineEquation> is efficient.</Para>
                <Para>Matrix <InlineEquation ID="IEq98"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq98.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$M$$]]></EquationSource></InlineEquation> in Eq. (<InternalRef RefID="Equ6">6</InternalRef>) can be eigen-decomposed to obtain <InlineEquation ID="IEq99"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq99.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$M = \sum\nolimits _{k=1}^K \sigma _k w_k w_k^\mathrm{ T}$$]]></EquationSource></InlineEquation>. To simplify numerical optimization, we impose <InlineEquation ID="IEq100"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq100.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\sigma _k = 1$$]]></EquationSource></InlineEquation> such that <InlineEquation ID="IEq101"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq101.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$M = W W^\mathrm{ T}$$]]></EquationSource></InlineEquation> where <InlineEquation ID="IEq102"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq102.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$W = [w_1,\ldots ,w_K]$$]]></EquationSource></InlineEquation>. Denote the index set <InlineEquation ID="IEq103"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq103.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ I} $$]]></EquationSource></InlineEquation> to be the collection of selected hashing bits at the current iteration. Let <InlineEquation ID="IEq104"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq104.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$h_\mathcal{ I} (x_i)$$]]></EquationSource></InlineEquation> be the vectorized hashing bits for <InlineEquation ID="IEq105"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq105.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x_i$$]]></EquationSource></InlineEquation>. Two margin-oriented data matrices can be calculated by traversing <InlineEquation ID="IEq106"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq106.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ M} ,\mathcal{ C} $$]]></EquationSource></InlineEquation>, respectively, and piling the difference column vectors, i.e.,<Equation ID="Equa2"><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_Equa2.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} X_m&=\{ h_\mathcal{ I} (x_i) - h_\mathcal{ I} (x_j)\}_{(x_i,x_j) \in \mathcal{ M} } \\ X_c&=\{ h_\mathcal{ I} (x_i) - h_\mathcal{ I} (x_j) \}_{(x_i,x_j) \in \mathcal{ C} } \end{aligned}$$]]></EquationSource></Equation>We adopt the <Emphasis Type="Italic">averaged local margin</Emphasis> [<CitationRef CitationID="CR27">27</CitationRef>] based criterion to measure the empirical gain of <InlineEquation ID="IEq107"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq107.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ I} $$]]></EquationSource></InlineEquation>, which is defined as:<Equation ID="Equ7"><EquationNumber>7</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_Equ7.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} J(W)= \frac{1}{n_c} \text{ tr} \{ W^\mathrm{ T} X_c X_c^\mathrm{ T} W\} - \frac{1}{n_m} \text{ tr} \{ W^\mathrm{ T} X_m X_m^\mathrm{ T} W\}, \end{aligned}$$]]></EquationSource></Equation>where <InlineEquation ID="IEq108"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq108.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$n_c$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq109"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq109.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$n_m$$]]></EquationSource></InlineEquation> are cardinalities of <InlineEquation ID="IEq110"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq110.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ C} $$]]></EquationSource></InlineEquation>, <InlineEquation ID="IEq111"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq111.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ M} $$]]></EquationSource></InlineEquation>, respectively. Intuitively, <InlineEquation ID="IEq112"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq112.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$J(W)$$]]></EquationSource></InlineEquation> maximizes the difference between <Emphasis Type="Italic">random heterogeneous pair</Emphasis> and <Emphasis Type="Italic">random homogeneous pair</Emphasis> in terms of averaged Hamming distances, analogous to the concept of margin in kernel-based learning [<CitationRef CitationID="CR25">25</CitationRef>].</Para>
                <Para>Moreover, prior work such as the well-known <Emphasis Type="Italic">spectral hashing</Emphasis> [<CitationRef CitationID="CR31">31</CitationRef>] observes an interesting phenomena, i.e., hashing functions with balanced bit distribution tend to bring superior performance. In other words, the entire data set is split into two equal-size partitions. Intuitively, balanced hashing function separates more nearest neighbor pairs. Coupling the independence condition of different bits, such a scheme results in more buckets. Consequently, the collisions of heterogeneous pairs are reduced with high probability. Motivated by this observation, we introduce a global regularizer regarding bit distribution, i.e.,<Equation ID="Equ8"><EquationNumber>8</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_Equ8.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \mathcal{ R} (W) = \mathbb{ E} (\Vert W^{\mathrm{ T} (h_\mathcal{ I} }(x_i) - \mu )\Vert _2^2), \end{aligned}$$]]></EquationSource></Equation>where <InlineEquation ID="IEq113"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq113.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mu $$]]></EquationSource></InlineEquation> represents the statistical mean of all hashing-bit vectors. In practice, a small subset <InlineEquation ID="IEq114"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq114.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ X} _s$$]]></EquationSource></InlineEquation> with cardinality <InlineEquation ID="IEq115"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq115.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$n_s$$]]></EquationSource></InlineEquation> is sampled and serves as a statistical surrogate. Equation (<InternalRef RefID="Equ8">8</InternalRef>) can be rewritten as:<Equation ID="Equ9"><EquationNumber>9</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_Equ9.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \mathcal{ R} (W) = \frac{1}{n_s} \text{ tr}(W^\mathrm{ T} X_s X_s^\mathrm{ T} W) - \text{ tr}(W^\mathrm{ T} \mu \mu ^\mathrm{ T} W). \end{aligned}$$]]></EquationSource></Equation>For brevity, denote <InlineEquation ID="IEq116"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq116.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$L_J = X_c X_c^\mathrm{ T} / n_c - X_m X_m^\mathrm{ T} / n_m$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq117"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq117.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$L_R = X_s X_s^\mathrm{ T} / n_s - \mu \mu ^\mathrm{ T}$$]]></EquationSource></InlineEquation>. Considering all together, finally we get the regularized objective function:<Equation ID="Equ10"><EquationNumber>10</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_Equ10.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} F(W) = \text{ tr}(W^\mathrm{ T} L_J W) + \eta \cdot \text{ tr}(W^\mathrm{ T} L_R W), \end{aligned}$$]]></EquationSource></Equation>where <InlineEquation ID="IEq118"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq118.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\eta > 0$$]]></EquationSource></InlineEquation> is a free parameter to control the regularizing strength. It is easily verified that<Equation ID="Equ11"><EquationNumber>11</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_Equ11.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \max F(W) = \sum _{k=1}^K \lambda _k, \end{aligned}$$]]></EquationSource></Equation>where <InlineEquation ID="IEq119"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq119.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\{ \lambda _k \}$$]]></EquationSource></InlineEquation> comprise the non-negative eigenvalues of matrix <InlineEquation ID="IEq120"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq120.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$L_J + \eta \cdot L_R$$]]></EquationSource></InlineEquation> (the negative eigenvalues stem from the indefinite property of <InlineEquation ID="IEq121"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq121.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$L_J$$]]></EquationSource></InlineEquation>) and the value of <InlineEquation ID="IEq122"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq122.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$K$$]]></EquationSource></InlineEquation> is thereby automatically determined.</Para>
                <Para>Due to the large number of the hashing pool, global optimization is computationally forbidden. Here, we employ a greedy strategy for sequential bit selection. In the <InlineEquation ID="IEq123"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq123.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$t$$]]></EquationSource></InlineEquation>-th iteration, each unselected hashing function <InlineEquation ID="IEq124"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq124.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$h_p$$]]></EquationSource></InlineEquation> is individually added into current index set <InlineEquation ID="IEq125"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq125.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ I} ^{(t)}$$]]></EquationSource></InlineEquation> and the optimum of <InlineEquation ID="IEq126"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq126.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$F(W)$$]]></EquationSource></InlineEquation> under <InlineEquation ID="IEq127"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq127.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ I} ^{(t)} \cup \{ p \}$$]]></EquationSource></InlineEquation> is computed. The hashing function that maximizes the gain will be eventually added into <InlineEquation ID="IEq128"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq128.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ I} ^{(t)}$$]]></EquationSource></InlineEquation>. The procedure iterates until the hashing bit budget is reached.</Para>
                <Para>Unfortunately, one potential selection bias is rooted in the term <InlineEquation ID="IEq129"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq129.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$tr \{ W^\mathrm{ T} X_c X_c^\mathrm{ T} W^\mathrm{ T}\}$$]]></EquationSource></InlineEquation> in Eq. (<InternalRef RefID="Equ7">7</InternalRef>), which can be equivalently expressed as <InlineEquation ID="IEq130"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq130.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\sum _{(x_i,x_j) \in \mathcal{ C} } W^{\mathrm{ T} } h_{ij} h_{ij}^{\mathrm{ T} } W^{\mathrm{ T} }$$]]></EquationSource></InlineEquation> with <InlineEquation ID="IEq131"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq131.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$h_{ij} = h_{\mathcal{ I} }(x_i) - h_{\mathcal{ I} }(x_j)$$]]></EquationSource></InlineEquation>. Owing to the summation operation over the constraint set <InlineEquation ID="IEq132"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq132.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ C} $$]]></EquationSource></InlineEquation>, the estimation is smooth and robust. However, recall that <InlineEquation ID="IEq133"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq133.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ C} $$]]></EquationSource></InlineEquation> is randomly rendered. In some extreme case, the selected optimal hashing functions may be trapped in the regions where the density of <InlineEquation ID="IEq134"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq134.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$(x_i,x_j)$$]]></EquationSource></InlineEquation> is relatively high, resulting the zero-norm values of some difference vectors (i.e., <InlineEquation ID="IEq135"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq135.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\Vert h_{ij}\Vert _0$$]]></EquationSource></InlineEquation>) are extremely high.</Para>
                <Para>To mitigate this selection bias, we truncate too-high zero-norm to avoid over-penalizing. Given a predefined threshold <InlineEquation ID="IEq136"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq136.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\theta $$]]></EquationSource></InlineEquation> (in implementation we set <InlineEquation ID="IEq137"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq137.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\theta =5$$]]></EquationSource></InlineEquation>, which is a conservative parameter since hashing buckets with distances larger than five are rarely visited in approximate nearest neighbor retrieval), we re-scale the difference vector via the following formula:<Equation ID="Equ12"><EquationNumber>12</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_Equ12.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} h_{ij} = \frac{\min (\Vert h_{ij}\Vert _0,\theta )}{\Vert h_{ij}\Vert _0} \cdot h_{ij}. \end{aligned}$$]]></EquationSource></Equation>
                </Para>
              </Section2>
              <Section2 ID="Sec6">
                <Heading>Hashing function refinement</Heading>
                <Para>Note that the proposed algorithm in the previous section is intrinsically a meta-hashing algorithm, since it does not take the construction of hashing functions into account. Instead, it directly works with the binary codes produced by the random hashing functions. An interesting problem is that whether it helps or not if we further refine the selected hashing functions. As a tentative attempt, we propose another formulation that makes refinement to the hashing functions.</Para>
                <Para>Suppose we have obtained the hashing vectors <InlineEquation ID="IEq138"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq138.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$F_0 \in \mathbb{ R} ^{d \times l}$$]]></EquationSource></InlineEquation> for the <InlineEquation ID="IEq139"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq139.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$k$$]]></EquationSource></InlineEquation>-th category. To further refine the hashing vectors, a natural solution is to jointly optimize the bit re-weighting parameter and hashing vectors. For ease of optimization, here we abandon the transform matrix <InlineEquation ID="IEq140"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq140.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$W$$]]></EquationSource></InlineEquation> in the previous section and introduce the vector <InlineEquation ID="IEq141"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq141.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\alpha _k \in \mathbb{ R} ^{l \times 1}$$]]></EquationSource></InlineEquation> for bit re-weighting purpose. Denote the hashing vectors after refinement to be <InlineEquation ID="IEq142"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq142.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$F$$]]></EquationSource></InlineEquation>. The key idea is akin to the idea of supervised locality-preserving method based on graph Laplacian [<CitationRef CitationID="CR10">10</CitationRef>]. Specifically, the <InlineEquation ID="IEq143"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq143.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$(i,j)$$]]></EquationSource></InlineEquation>-th element (<InlineEquation ID="IEq144"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq144.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$i \ne j$$]]></EquationSource></InlineEquation>) in Laplacian matrix <InlineEquation ID="IEq145"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq145.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$L_k$$]]></EquationSource></InlineEquation> is only non-zero when <InlineEquation ID="IEq146"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq146.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x_j$$]]></EquationSource></InlineEquation> belongs to the <InlineEquation ID="IEq147"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq147.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$k$$]]></EquationSource></InlineEquation>-NN of <InlineEquation ID="IEq148"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq148.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x_i$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq149"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq149.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x_i,x_j$$]]></EquationSource></InlineEquation> are from the same semantic category. The overall formulation is as follows:<Equation ID="Equ13"><EquationNumber>13</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_Equ13.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \arg \min \limits _{F,\alpha _k}&\alpha _k^\mathrm{ T} F^\mathrm{ T} X L_k X^\mathrm{ T} F \alpha _k + \beta \Vert F - F_0 \Vert _F^2, \\ \text{ s.t.}&\forall \ i, \Vert f_i \Vert _2 \le 1, \nonumber \\&\alpha _k \ge 0, \ \Vert \alpha _k \Vert _1= 1, \nonumber \end{aligned}$$]]></EquationSource></Equation>where <InlineEquation ID="IEq150"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq150.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\Vert \cdot \Vert _F$$]]></EquationSource></InlineEquation> denotes the matrix Frobenius norm. <InlineEquation ID="IEq151"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq151.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\beta $$]]></EquationSource></InlineEquation> is a free parameter to control the proximity of the final solution to the initial value <InlineEquation ID="IEq152"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq152.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$F_0$$]]></EquationSource></InlineEquation>.</Para>
                <Para>The overall formulation is non-convex. However, it becomes convex when fixing one of the variables (either <InlineEquation ID="IEq153"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq153.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$F$$]]></EquationSource></InlineEquation> or <InlineEquation ID="IEq154"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq154.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\alpha _k$$]]></EquationSource></InlineEquation>). When <InlineEquation ID="IEq155"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq155.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$F$$]]></EquationSource></InlineEquation> is known, it is a quadratic programming with linear constraints. When <InlineEquation ID="IEq156"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq156.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\alpha _k$$]]></EquationSource></InlineEquation> is fixed, <InlineEquation ID="IEq157"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq157.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$F$$]]></EquationSource></InlineEquation> can be updated by the projected gradient method. This alternating minimization algorithm guarantees a fixed point for <InlineEquation ID="IEq158"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq158.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$(F,\alpha _k)$$]]></EquationSource></InlineEquation>.</Para>
              </Section2>
            </Section1>
            <Section1 ID="Sec7">
              <Heading>Baseline algorithms</Heading>
              <Para>Besides our proposed hashing bit selection strategy, we also explore other alternatives. In detail, we choose the following:</Para>
              <Para><Emphasis Type="Italic">Method-I: random selection (RS).</Emphasis> In each iteration, select a hashing bit from the pool by uniform sampling. The procedure terminates when maximum budgeted number of hashing functions is reached.</Para>
              <Para><Emphasis Type="Italic">Method-II: maximum unfolding (MU).</Emphasis> As previously mentioned, previous research has revealed the superior performance of balanced (or max-variance) hashing function. In other words, it prefers hashing schemes with maximum unfolding. This strategy selects top-ranked maximum-variance hashing bits from the pool.</Para>
              <Para><Emphasis Type="Italic">Method-III: maximum averaged margin (MAM).</Emphasis> Similar to Eq. (<InternalRef RefID="Equ7">7</InternalRef>), we can compute the <Emphasis Type="Italic">averaged margin</Emphasis> of each hashing function in the pool according to the formula and keep top-scored hashing bits via greedy selection.<Equation ID="Equ14"><EquationNumber>14</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_Equ14.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \text{ score}(h_p)&=\mathbb{ E} _{(x_i,x_j)\in \mathcal{ C} } (h_p(x_i) \oplus h_p(x_j)) \nonumber \\&-\mathbb{ E} _{(x_i,x_j)\in \mathcal{ M} } (h_p(x_i) \oplus h_p(x_j)). \end{aligned}$$]]></EquationSource></Equation><Emphasis Type="Italic">Method-IV: weighted Shannon entropy (WSE).</Emphasis> For each candidate in the pool, we calculate a score based on the Shannon entropy [<CitationRef CitationID="CR16">16</CitationRef>]. For completeness, we give its definition. Assume the index set of data as <InlineEquation ID="IEq159"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq159.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$L$$]]></EquationSource></InlineEquation>, two disjoint subsets <InlineEquation ID="IEq160"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq160.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$L_l$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq161"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq161.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$L_r$$]]></EquationSource></InlineEquation> can be created by a Boolean test <InlineEquation ID="IEq162"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq162.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ T} $$]]></EquationSource></InlineEquation> induced by a hashing function <InlineEquation ID="IEq163"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq163.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$h(\cdot )$$]]></EquationSource></InlineEquation>. The Shannon entropy is computed as:<Equation ID="Equ15"><EquationNumber>15</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_Equ15.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} S_C(L, \mathcal{ T} ) = \frac{2 \cdot I_{C,\mathcal{ T} } (L)}{H_C(L) + H_\mathcal{ T} (L)}, \end{aligned}$$]]></EquationSource></Equation>where <InlineEquation ID="IEq164"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq164.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$H_C$$]]></EquationSource></InlineEquation> denotes the entropy of the category distribution in L. Formally,<Equation ID="Equ16"><EquationNumber>16</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_Equ16.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} H_C(L) = - \sum \limits _c \frac{n_c}{n} \log _2 \frac{n_c}{n}, \end{aligned}$$]]></EquationSource></Equation>where <InlineEquation ID="IEq165"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq165.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$n$$]]></EquationSource></InlineEquation> is the cardinality of <InlineEquation ID="IEq166"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq166.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$L$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq167"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq167.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$n_c$$]]></EquationSource></InlineEquation> is the number of samples in the category with index <InlineEquation ID="IEq168"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq168.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$c$$]]></EquationSource></InlineEquation>. Maximal value is achieved when all <InlineEquation ID="IEq169"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq169.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$n_c$$]]></EquationSource></InlineEquation> are the same. Similarly, the <Emphasis Type="Italic">split entropy</Emphasis><InlineEquation ID="IEq170"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq170.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$H_\mathcal{ T} $$]]></EquationSource></InlineEquation> is defined for the test <InlineEquation ID="IEq171"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq171.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ T} $$]]></EquationSource></InlineEquation>, which splits the data into two partitions:<Equation ID="Equ17"><EquationNumber>17</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_Equ17.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} H_\mathcal{ T} (L) = - \sum \limits _{p=1}^2 \frac{n_p}{n} \log _2 \frac{n_p}{n}, \end{aligned}$$]]></EquationSource></Equation>where <InlineEquation ID="IEq172"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq172.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$n_p$$]]></EquationSource></InlineEquation> (<InlineEquation ID="IEq173"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq173.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p$$]]></EquationSource></InlineEquation> = 1 or 2) denotes the sample number in <InlineEquation ID="IEq174"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq174.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$L_l$$]]></EquationSource></InlineEquation> or <InlineEquation ID="IEq175"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq175.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$L_r$$]]></EquationSource></InlineEquation>. The maximum of <InlineEquation ID="IEq176"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq176.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$H_\mathcal{ T} (L)$$]]></EquationSource></InlineEquation> is reached when the two partitions have equal sizes. Based on the entropy of <InlineEquation ID="IEq177"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq177.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$L$$]]></EquationSource></InlineEquation>, the <Emphasis Type="Italic">impurity</Emphasis> of <InlineEquation ID="IEq178"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq178.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ T} $$]]></EquationSource></InlineEquation> can be calculated by the mutual information of the split, i.e.,<Equation ID="Equ18"><EquationNumber>18</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_Equ18.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} I_{C,\mathcal{ T} }(L) = H_C(L) - \sum \limits _{p=1}^2 \frac{n_p}{n} H_C(L_p). \end{aligned}$$]]></EquationSource></Equation>Intuitively, <InlineEquation ID="IEq179"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq179.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$S_C(L, \mathcal{ T} )$$]]></EquationSource></InlineEquation> prefers <InlineEquation ID="IEq180"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq180.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ T} $$]]></EquationSource></InlineEquation> that is as balanced as possible and meanwhile separates different categories. As aforementioned, in the setting of reconfigurable hashing, the numbers of labeled samples from target category and non-target categories are heavily unbalanced, therefore we re-scale the sample weights such that the summed weights for the target category and non-target categories are equal. Finally, the hashing functions with the highest scores are kept.</Para>
            </Section1>
            <Section1 ID="Sec8">
              <Heading>Hashing collision probability</Heading>
              <Para>Before delving into the experimental results, we would like to highlight the asymptotic property of the proposed <Emphasis Type="Italic">random-anchor-random-projection</Emphasis> (RARP) hashing functions.</Para>
              <Para>For two samples <InlineEquation ID="IEq181"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq181.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x_1$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq182"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq182.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x_2$$]]></EquationSource></InlineEquation>, let <InlineEquation ID="IEq183"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq183.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$c = \Vert x_1-x_2\Vert _p$$]]></EquationSource></InlineEquation>. In the hashing literature, it is well acknowledged [<CitationRef CitationID="CR12">12</CitationRef>] that the computational complexity of a hashing algorithm is dominated by <InlineEquation ID="IEq184"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq184.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ O} (n^\rho )$$]]></EquationSource></InlineEquation>, where <InlineEquation ID="IEq185"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq185.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$n$$]]></EquationSource></InlineEquation> is the data set size and <InlineEquation ID="IEq186"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq186.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\rho <1$$]]></EquationSource></InlineEquation> is dependent on algorithm choice and <InlineEquation ID="IEq187"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq187.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$c$$]]></EquationSource></InlineEquation>. Suppose <InlineEquation ID="IEq188"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq188.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\omega $$]]></EquationSource></InlineEquation> determines the parametric random hashing hyperplane. It is known that <InlineEquation ID="IEq189"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq189.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\langle \omega , x_1-x_2 \rangle $$]]></EquationSource></InlineEquation> is distributed as <InlineEquation ID="IEq190"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq190.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$c X$$]]></EquationSource></InlineEquation>, where <InlineEquation ID="IEq191"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq191.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$X$$]]></EquationSource></InlineEquation> is drawn from the <InlineEquation ID="IEq192"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq192.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p$$]]></EquationSource></InlineEquation>-stable distribution. Denote the range of projected values as <InlineEquation ID="IEq193"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq193.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$R = max _i \langle \omega ,x_i \rangle - min _i \langle \omega ,x_i\rangle $$]]></EquationSource></InlineEquation> and let <InlineEquation ID="IEq194"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq194.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\eta = \frac{\langle \omega ,x^o \rangle - \min _i \langle \omega ,x_i \rangle }{R}$$]]></EquationSource></InlineEquation> (<InlineEquation ID="IEq195"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq195.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x^o$$]]></EquationSource></InlineEquation> is the random anchor). Without loss of generality, we assume <InlineEquation ID="IEq196"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq196.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\eta > 0.5$$]]></EquationSource></InlineEquation>. Let <InlineEquation ID="IEq197"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq197.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$g_p(t)$$]]></EquationSource></InlineEquation> be the probability density function of the absolute value of the <InlineEquation ID="IEq198"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq198.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p$$]]></EquationSource></InlineEquation>-stable distribution. The collision probability of RARP can be written as<Equation ID="Equ19"><EquationNumber>19</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_Equ19.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \text{ Pr} ( h_{\omega ,x^o}(x_1)&=h_{\omega ,x^o}(x_2)) \approx \int\limits _{0}^{\eta R} \frac{1}{c} g_p\left(\frac{t}{c}\right) \left(\eta -\frac{t}{R}\right)\,\text{ d}t \nonumber \\&+ \int\limits _{0}^{(1-\eta ) R} \frac{1}{c} g_p\left(\frac{t}{c}\right) \left(1-\eta -\frac{t}{R}\right)\,\text{ d}t \end{aligned}$$]]></EquationSource></Equation>The two terms in Eq. (<InternalRef RefID="Equ19">19</InternalRef>) reflect the chances that <InlineEquation ID="IEq199"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq199.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x_1,x_2$$]]></EquationSource></InlineEquation> collides in the two sides of <InlineEquation ID="IEq200"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq200.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x^o$$]]></EquationSource></InlineEquation>, respectively. Note that the equality relationship only approximately holds in (<InternalRef RefID="Equ19">19</InternalRef>) due to the uneven data distribution (computing the accurate probability involves double integrals along <InlineEquation ID="IEq201"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq201.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\omega $$]]></EquationSource></InlineEquation>), and rigorously holds in case of uniform distribution. Moreover, when <InlineEquation ID="IEq202"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq202.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$R$$]]></EquationSource></InlineEquation> is large enough and the uniformity holds, analytic bound for <InlineEquation ID="IEq203"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq203.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\rho $$]]></EquationSource></InlineEquation> exists. Analysis in this section follows closely [<CitationRef CitationID="CR6">6</CitationRef>], and therefore the detailed proofs are omitted.</Para>
              <FormalPara RenderingStyle="Style1">
                <Heading>Theorem 1</Heading>
                <Para> For any <InlineEquation ID="IEq204"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq204.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$p \in (0,2]$$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq205"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq205.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$c>1$$]]></EquationSource></InlineEquation>, there exists hashing family <InlineEquation ID="IEq206"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq206.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ H} $$]]></EquationSource></InlineEquation> for <InlineEquation ID="IEq207"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq207.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _p$$]]></EquationSource></InlineEquation>-norm such that for any scalar <InlineEquation ID="IEq208"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq208.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\gamma > 0$$]]></EquationSource></InlineEquation>,<Equation ID="Equ20"><EquationNumber>20</EquationNumber><MediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_Equ20.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject><EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \lim _{R \rightarrow \infty } \rho \le (1+\gamma ) \cdot \max \left( \frac{1}{c}, \frac{1}{c^p} \right). \end{aligned}$$]]></EquationSource></Equation>
                </Para>
              </FormalPara>
            </Section1>
            <Section1 ID="Sec9">
              <Heading>Experiments</Heading>
              <Para>
                <Figure Category="Standard" Float="Yes" ID="Fig3">
                  <Caption Language="En">
                    <CaptionNumber>Fig. 3</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>Exemplar images on selected benchmarks</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <MediaObject ID="MO25">
                    <ImageObject Color="Color" FileRef="MediaObjects/13735_2012_3_Fig3_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                  </MediaObject>
                </Figure>
              </Para>
              <Para>In this section, we justify the effectiveness of the proposed reconfigurable hashing through empirical evaluations on four benchmarks: Caltech-101<Footnote ID="Fn2"><Para><ExternalRef><RefSource>http://www.vision.caltech.edu/Image_Datasets/Caltech101/</RefSource><RefTarget Address="http://www.vision.caltech.edu/Image_Datasets/Caltech101/" TargetType="URL"/></ExternalRef></Para></Footnote>, MNIST-Digit<Footnote ID="Fn3"><Para><ExternalRef><RefSource>http://yann.lecun.com/exdb/mnist/</RefSource><RefTarget Address="http://yann.lecun.com/exdb/mnist/" TargetType="URL"/></ExternalRef></Para></Footnote>, CIFAR-10 and CIFAR-100<Footnote ID="Fn4"><Para><ExternalRef><RefSource>http://www.cs.utoronto.ca/~kriz/cifar.html</RefSource><RefTarget Address="http://www.cs.utoronto.ca/~kriz/cifar.html" TargetType="URL"/></ExternalRef></Para></Footnote>. In the experiments, we compare the proposed hashing bit selecting strategy with other alternatives presented in Sect. <InternalRef RefID="Sec7">5</InternalRef>. To reduce the effect of randomness, all experiments are iterated 30 times to get the statistical average. By default, we set <InlineEquation ID="IEq210"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq210.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\eta =0.5$$]]></EquationSource></InlineEquation> and choose both four samples from target category and non-target categories to construct <InlineEquation ID="IEq211"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq211.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ M} $$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq212"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq212.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ C} $$]]></EquationSource></InlineEquation>. The size of the hashing pool is fixed to be 10K in all experiments unless otherwise mentioned. Figure <InternalRef RefID="Fig3">3</InternalRef> shows selected images in the adopted benchmarks.</Para>
              <Section2 ID="Sec10">
                <Heading>Caltech-101 and CIFAR-100</Heading>
                <Para>
                  <Figure Category="Standard" Float="Yes" ID="Fig4">
                    <Caption Language="En">
                      <CaptionNumber>Fig. 4</CaptionNumber>
                      <CaptionContent>
                        <SimplePara>The evolution of accuracies of the first 300 retrieved samples on randomly selected Caltech-101 categories</SimplePara>
                      </CaptionContent>
                    </Caption>
                    <MediaObject ID="MO26">
                      <ImageObject Color="Color" FileRef="MediaObjects/13735_2012_3_Fig4_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                    </MediaObject>
                  </Figure>
                </Para>
                <Para>
                  <Figure Category="Standard" Float="Yes" ID="Fig5">
                    <Caption Language="En">
                      <CaptionNumber>Fig. 5</CaptionNumber>
                      <CaptionContent>
                        <SimplePara>The evolution of accuracies of the first 1,000 retrieved samples on randomly selected CIFAR-100 categories</SimplePara>
                      </CaptionContent>
                    </Caption>
                    <MediaObject ID="MO27">
                      <ImageObject Color="Color" FileRef="MediaObjects/13735_2012_3_Fig5_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                    </MediaObject>
                  </Figure>
                </Para>
                <Para>Caltech-101 is constructed to test object recognition algorithms for semantic categories of images. The data set contains 101 object categories and one background category, with 40–800 images per category. As preprocessing, the maximum dimension of each image is normalized to be 480 pixels. We extract 5,000 SIFT descriptors from each image whose locations and scales are determined in a random manner (see [<CitationRef CitationID="CR21">21</CitationRef>] for more details). For the visual vocabulary construction, we employ the recently proposed <Emphasis Type="Italic">randomized locality-sensitive vocabularies</Emphasis> (RLSV) [<CitationRef CitationID="CR19">19</CitationRef>] to build 20 independent bag-of-words feature, each of which consists of roughly 1K visual words. Finally, they are concatenated to form a single feature vector and reduced to be 1000-dimensional by dimensionality reduction.</Para>
                <Para>CIFAR-100 comprises 60,000 images selected from 80M Tiny-Image data set<Footnote ID="Fn5"><Para><ExternalRef><RefSource>http://people.csail.mit.edu/torralba/tinyimages/</RefSource><RefTarget Address="http://people.csail.mit.edu/torralba/tinyimages/" TargetType="URL"/></ExternalRef></Para></Footnote>. This data set is just like the CIFAR-10, except that it has 100 classes containing 600 images each. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a “fine” label (the class to which it belongs) and a “coarse” label (the superclass to which it belongs). Table <InternalRef RefID="Tab1">1</InternalRef> presents some examples of these two-granularity categories. For the <InlineEquation ID="IEq213"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq213.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$32 \times 32$$]]></EquationSource></InlineEquation> pixel images, we extract <InlineEquation ID="IEq214"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq214.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _2$$]]></EquationSource></InlineEquation>-normalized 384-D GIST features [<CitationRef CitationID="CR22">22</CitationRef>].</Para>
                <Table Float="Yes" ID="Tab1">
                  <Caption Language="En">
                    <CaptionNumber>Table 1</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>Exemplar labels in the CIFAR-100 data set</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <tgroup cols="2">
                    <colspec align="left" colname="c1" colnum="1"/>
                    <colspec align="left" colname="c2" colnum="2"/>
                    <thead>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>Coarse labels</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>Fine labels</SimplePara>
                        </entry>
                      </row>
                    </thead>
                    <tbody>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>Fish</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>Aquarium fish, flatfish, ray, shark, trout</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>Flowers</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>Orchids, poppies, roses, sunflowers, tulips</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>Food containers</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>Bottles, bowls, cans, cups, plates</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>Trees</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>Maple, oak, palm, pine, willow</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>People</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>Baby, boy, girl, man, woman</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>Vehicles 1</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>Bicycle, bus, motorcycle, pickup truck, train</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>Vehicles 2</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>Lawn mower, rocket, streetcar, tank, tractor</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>Reptiles</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>Crocodile, dinosaur, lizard, snake, turtle</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>Insects</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>Bee, beetle, butterfly, caterpillar, cockroach</SimplePara>
                        </entry>
                      </row>
                    </tbody>
                  </tgroup>
                </Table>
                <Para>We randomly generate 15 samples from each category in Caltech-101 and 30 samples for CIFAR-100 respectively, used for hashing bit selection. For each category, a unique hashing scheme is learned either by our proposed method or other methods mentioned in Sect. <InternalRef RefID="Sec7">5</InternalRef> with hashing bit budget equal to 14 on Caltech-100 or 16 on CIFAR-100, therefore in total 102 different hashing schemes for Caltech-101 and 100 for CIFAR-100. During the learning on specific category, the ensemble of the rest categories serves as the negative class. For each training sample from the target category, four random homogenous pairs and four random heterogenous pairs are generated by uniform sampling, forming the constraint sets <InlineEquation ID="IEq215"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq215.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ M} $$]]></EquationSource></InlineEquation> and <InlineEquation ID="IEq216"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq216.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ C} $$]]></EquationSource></InlineEquation>, respectively.</Para>
                <Para>
                  <Figure Category="Standard" Float="Yes" ID="Fig6">
                    <Caption Language="En">
                      <CaptionNumber>Fig. 6</CaptionNumber>
                      <CaptionContent>
                        <SimplePara>Comparison of conventional random projection-based LSH (RP) and our proposed RARP on Caltech-101. Both utilize 14 bits in total. The accuracies for RARP and RP is <InlineEquation ID="IEq217"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq217.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$3.99$$]]></EquationSource></InlineEquation> versus <InlineEquation ID="IEq218"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq218.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$3.61\%$$]]></EquationSource></InlineEquation> averaged on 102 categories, obtained by 30 independent runs. Note that the accuracies are transformed by logarithm function for better viewing</SimplePara>
                      </CaptionContent>
                    </Caption>
                    <MediaObject ID="MO28">
                      <ImageObject Color="Color" FileRef="MediaObjects/13735_2012_3_Fig6_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                    </MediaObject>
                  </Figure>
                </Para>
                <Para>We report the averaged results over 30 runs of our proposed method, along with the results obtained by four baselines, i.e., random selection (RS), maximum unfolding (MU), maximum averaged margin (MAM) and weighted Shannon entropy (WSE). The results of naive linear scan (NLS) are also reported. However, recall that NLS utilizes no side information. There is no guarantee that NLS provides the upper bound of the performance, as illustrated in the cases of Caltech-101 and CIFAR-100. We collect the proportions of “good neighbors” (samples belonging to the same category) in the first hundreds of retrieved samples (300 for Caltech-101, and 1000 for CIFAR-100). The samples within every bucket are randomly shuffled, and multiple candidate buckets with the same Hamming distance are also shuffled, so that the evaluation will not be affected by the order of the first retrieved samples (this operation is usually ignored in the evaluations of previous work). See Table <InternalRef RefID="Tab2">2</InternalRef> for the detailed experimental results, where the winning counts of each algorithm are also compared. To better illustrate the evolving tendencies of reconfigurable hashing, Figs. <InternalRef RefID="Fig4">4</InternalRef> and <InternalRef RefID="Fig5">5</InternalRef> plot the accuracies of selected categories from Caltech-101 and CIFAR-100, respectively. It is observed that the plotted curves on CIFAR-100 have more gentle slopes compared with Caltech-101’s, which reveals the different characteristics of underlying data distributions, i.e., the samples from the same category in Caltech-101 gather more closely.</Para>
                <Para>Although reconfigurable hashing is a meta-hashing framework, the ground hashing algorithms seriously affect the final performance. In Fig. <InternalRef RefID="Fig6">6</InternalRef>, we plot the logarithm of the accuracy for each category on Caltech-101, employing either our proposed RARP or conventional LSH as described in Eqs. (<InternalRef RefID="Equ3">3</InternalRef>) and (<InternalRef RefID="Equ1">1</InternalRef>), respectively. RARP shows superior performance, which indicates that data-dependent hashing algorithms such as RARP are promising for future exploration.</Para>
                <Table Float="Yes" ID="Tab2">
                  <Caption Language="En">
                    <CaptionNumber>Table 2</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>Reconfigurable hashing on multi-category benchmarks Caltech-101 and CIFAR-100</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <tgroup cols="7">
                    <colspec align="left" colname="c1" colnum="1"/>
                    <colspec align="left" colname="c2" colnum="2"/>
                    <colspec align="left" colname="c3" colnum="3"/>
                    <colspec align="left" colname="c4" colnum="4"/>
                    <colspec align="left" colname="c5" colnum="5"/>
                    <colspec align="left" colname="c6" colnum="6"/>
                    <colspec align="left" colname="c7" colnum="7"/>
                    <thead>
                      <row>
                        <entry align="left" colname="c1"/>
                        <entry align="left" colname="c2">
                          <SimplePara>RS</SimplePara>
                        </entry>
                        <entry align="left" colname="c3">
                          <SimplePara>MU</SimplePara>
                        </entry>
                        <entry align="left" colname="c4">
                          <SimplePara>MAM</SimplePara>
                        </entry>
                        <entry align="left" colname="c5">
                          <SimplePara>WSE</SimplePara>
                        </entry>
                        <entry align="left" colname="c6">
                          <SimplePara>Ours</SimplePara>
                        </entry>
                        <entry align="left" colname="c7">
                          <SimplePara>NLS</SimplePara>
                        </entry>
                      </row>
                    </thead>
                    <tbody>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>Caltech-101</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>3.99</SimplePara>
                        </entry>
                        <entry align="left" colname="c3">
                          <SimplePara>4.92</SimplePara>
                        </entry>
                        <entry align="left" colname="c4">
                          <SimplePara>10.31</SimplePara>
                        </entry>
                        <entry align="left" colname="c5">
                          <SimplePara>10.15</SimplePara>
                        </entry>
                        <entry align="left" colname="c6">
                          <SimplePara>11.08</SimplePara>
                        </entry>
                        <entry align="left" colname="c7">
                          <SimplePara>10.20</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>CIFAR-100</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>1.75</SimplePara>
                        </entry>
                        <entry align="left" colname="c3">
                          <SimplePara>2.01</SimplePara>
                        </entry>
                        <entry align="left" colname="c4">
                          <SimplePara>3.93</SimplePara>
                        </entry>
                        <entry align="left" colname="c5">
                          <SimplePara>3.93</SimplePara>
                        </entry>
                        <entry align="left" colname="c6">
                          <SimplePara>4.26</SimplePara>
                        </entry>
                        <entry align="left" colname="c7">
                          <SimplePara>4.09</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>Caltech-101</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>0</SimplePara>
                        </entry>
                        <entry align="left" colname="c3">
                          <SimplePara>0</SimplePara>
                        </entry>
                        <entry align="left" colname="c4">
                          <SimplePara>2</SimplePara>
                        </entry>
                        <entry align="left" colname="c5">
                          <SimplePara>1</SimplePara>
                        </entry>
                        <entry align="left" colname="c6">
                          <SimplePara>99</SimplePara>
                        </entry>
                        <entry align="left" colname="c7"/>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>CIFAR-100</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>0</SimplePara>
                        </entry>
                        <entry align="left" colname="c3">
                          <SimplePara>0</SimplePara>
                        </entry>
                        <entry align="left" colname="c4">
                          <SimplePara>4</SimplePara>
                        </entry>
                        <entry align="left" colname="c5">
                          <SimplePara>5</SimplePara>
                        </entry>
                        <entry align="left" colname="c6">
                          <SimplePara>91</SimplePara>
                        </entry>
                        <entry align="left" colname="c7"/>
                      </row>
                    </tbody>
                  </tgroup>
                  <tfooter>
                    <SimplePara>The top table illustrates the averaged accuracies (<InlineEquation ID="IEq219"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq219.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\%$$]]></EquationSource></InlineEquation>) of the first <InlineEquation ID="IEq220"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq220.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$k$$]]></EquationSource></InlineEquation> retrieved samples (<InlineEquation ID="IEq221"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq221.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$k=300$$]]></EquationSource></InlineEquation> for Caltech-101 and <InlineEquation ID="IEq222"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq222.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$k=1,000$$]]></EquationSource></InlineEquation> for CIFAR-100). We also count the number of categories on which an algorithm beats all the others (102 or 100 in total on these two benchmarks). The results are reported in the bottom table</SimplePara>
                  </tfooter>
                </Table>
              </Section2>
              <Section2 ID="Sec11">
                <Heading>MNIST-digit and CIFAR-10</Heading>
                <Para>The sample number of each category on Caltech-101 and CIFAR-100 is relatively small, ranging from 31 to 800. To complement the study in Sect. <InternalRef RefID="Sec10">7.1</InternalRef>, we also conduct experiments on the benchmarks MNIST-Digit and CIFAR-10, which have larger sample number (6K or 7K) per category.</Para>
                <Para>MNIST-Digit is constructed for handwritten digits recognition. It consists of totally 70,000 digit images, 7,000 images for each digit in <InlineEquation ID="IEq223"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq223.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$0 \sim 9$$]]></EquationSource></InlineEquation>. The digits have been size-normalized to be <InlineEquation ID="IEq224"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq224.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$28 \times 28$$]]></EquationSource></InlineEquation> pixels. In our study, each digit image is transformed by matrix-to-vector concatenation and normalized to be unit-length feature. These raw grayscale vectors directly serve as the low-level feature for recognition purpose.</Para>
                <Para>Similar to CIFAR-100, CIFAR-10 is also a labeled subset of the 80 million tiny images data set, containing 60K <InlineEquation ID="IEq225"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq225.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$32 \times 32$$]]></EquationSource></InlineEquation> color images in ten classes (6K images for each class). The data set is constructed to learn meaningful recognition-related image filters whose responses resemble the behavior of human visual cortex. In the experiment we use the 387-d GIST image feature.</Para>
                <Para>We learn category-dependent hashing schemes with 16 hashing bit budget. The experimental settings are identical to those on CIFAR-100, except that in the testing stage, only a portion of testing samples (300 in our implementation) are chosen for evaluation. Table <InternalRef RefID="Tab3">3</InternalRef> presents the results in terms of accuracy and winning count.</Para>
                <Para>It is meaningful to investigate the correlation of the bucket number and the final performance. In Fig. <InternalRef RefID="Fig7">7</InternalRef>, we plot the bucket number for each of the ten categories averaged over 30 independent runs. It is observed that MU results in the largest bucket numbers, which is consistent with its design principle. However, the retrieval performance of MU is only slightly better than random selection (RS), which negates the hypothesis that increasing bucket number will promote the performance with high probability. In contrast, WSE has the fewest buckets compared with the other three non-random algorithms, yet the performance is amazingly excellent (see Table <InternalRef RefID="Tab3">3</InternalRef>). Intuitively, the Shannon entropy adopted in WSE favors hashing hyperplanes that cross the boundary between target category and its complemental categories. Such a strategy tends to keep the samples from target category stay closely in terms of Hamming distance and reduces unnecessary bucket creation. The high contrast between the small bucket number and high effectiveness suggests that the intelligent category-aware bucket creation is crucial for reconfigurable hashing. On the other hand, although both MAM and our proposed strategy utilize the idea of averaged margin, the latter brings slightly larger bucket number, which is supposed to stem from the regularization term <InlineEquation ID="IEq226"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq226.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\mathcal{ R} (W)$$]]></EquationSource></InlineEquation> defined in Eq. (<InternalRef RefID="Equ8">8</InternalRef>). It is observed that the combination of averaged margin and maximum unfolding improves the hashing quality.</Para>
                <Table Float="Yes" ID="Tab3">
                  <Caption Language="En">
                    <CaptionNumber>Table 3</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>Reconfigurable hashing on two 10-category image benchmarks MNIST-Digit and CIFAR-10</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <tgroup cols="7">
                    <colspec align="left" colname="c1" colnum="1"/>
                    <colspec align="left" colname="c2" colnum="2"/>
                    <colspec align="left" colname="c3" colnum="3"/>
                    <colspec align="left" colname="c4" colnum="4"/>
                    <colspec align="left" colname="c5" colnum="5"/>
                    <colspec align="left" colname="c6" colnum="6"/>
                    <colspec align="left" colname="c7" colnum="7"/>
                    <thead>
                      <row>
                        <entry align="left" colname="c1"/>
                        <entry align="left" colname="c2">
                          <SimplePara>RS</SimplePara>
                        </entry>
                        <entry align="left" colname="c3">
                          <SimplePara>MU</SimplePara>
                        </entry>
                        <entry align="left" colname="c4">
                          <SimplePara>MAM</SimplePara>
                        </entry>
                        <entry align="left" colname="c5">
                          <SimplePara>WSE</SimplePara>
                        </entry>
                        <entry align="left" colname="c6">
                          <SimplePara>Ours</SimplePara>
                        </entry>
                        <entry align="left" colname="c7">
                          <SimplePara>NLS</SimplePara>
                        </entry>
                      </row>
                    </thead>
                    <tbody>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>CIFAR-10</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>14.23</SimplePara>
                        </entry>
                        <entry align="left" colname="c3">
                          <SimplePara>15.58</SimplePara>
                        </entry>
                        <entry align="left" colname="c4">
                          <SimplePara>21.06</SimplePara>
                        </entry>
                        <entry align="left" colname="c5">
                          <SimplePara>20.51</SimplePara>
                        </entry>
                        <entry align="left" colname="c6">
                          <SimplePara>21.92</SimplePara>
                        </entry>
                        <entry align="left" colname="c7">
                          <SimplePara>25.14</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>MNIST-Digit</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>28.24</SimplePara>
                        </entry>
                        <entry align="left" colname="c3">
                          <SimplePara>34.96</SimplePara>
                        </entry>
                        <entry align="left" colname="c4">
                          <SimplePara>60.97</SimplePara>
                        </entry>
                        <entry align="left" colname="c5">
                          <SimplePara>60.00</SimplePara>
                        </entry>
                        <entry align="left" colname="c6">
                          <SimplePara>63.60</SimplePara>
                        </entry>
                        <entry align="left" colname="c7">
                          <SimplePara>74.74</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>CIFAR-10</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>0</SimplePara>
                        </entry>
                        <entry align="left" colname="c3">
                          <SimplePara>0</SimplePara>
                        </entry>
                        <entry align="left" colname="c4">
                          <SimplePara>0</SimplePara>
                        </entry>
                        <entry align="left" colname="c5">
                          <SimplePara>0</SimplePara>
                        </entry>
                        <entry align="left" colname="c6">
                          <SimplePara>10</SimplePara>
                        </entry>
                        <entry align="left" colname="c7"/>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>MNIST-Digit</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>0</SimplePara>
                        </entry>
                        <entry align="left" colname="c3">
                          <SimplePara>0</SimplePara>
                        </entry>
                        <entry align="left" colname="c4">
                          <SimplePara>0</SimplePara>
                        </entry>
                        <entry align="left" colname="c5">
                          <SimplePara>0</SimplePara>
                        </entry>
                        <entry align="left" colname="c6">
                          <SimplePara>10</SimplePara>
                        </entry>
                        <entry align="left" colname="c7"/>
                      </row>
                    </tbody>
                  </tgroup>
                  <tfooter>
                    <SimplePara>The implications of the top and bottom tables are the same as in Table <InternalRef RefID="Tab2">2</InternalRef>. Note that our proposed strategy wins on all the categories</SimplePara>
                  </tfooter>
                </Table>
              </Section2>
            </Section1>
            <Section1 ID="Sec12">
              <Heading>Conclusions</Heading>
              <Para>
                <Figure Category="Standard" Float="Yes" ID="Fig7">
                  <Caption Language="En">
                    <CaptionNumber>Fig. 7</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>Comparison of bucket numbers on CIFAR-10</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <MediaObject ID="MO29">
                    <ImageObject Color="Color" FileRef="MediaObjects/13735_2012_3_Fig7_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                  </MediaObject>
                </Figure>
              </Para>
              <Para>In this paper, we investigate the possibility of effective hashing in the existence of diverse semantics and metric adaptation. We propose a novel meta-hashing framework based on the idea of reconfigurable hashing. Unlike directly optimizing the parameters of hashing functions in conventional methods, reconfigurable hashing constructs a large hash pool by one-off data indexing and then selects the most effective hashing-bit combination at runtime. The contributions in this paper include a novel RARP-based hashing algorithm for <InlineEquation ID="IEq227"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_3_Article_IEq227.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\ell _p$$]]></EquationSource></InlineEquation> norm, a novel bit-selection algorithm based on averaged margin and global unfolding-based regularization, and a comparative study of various bit-selection strategies. For the future research direction, we are working toward two directions:<UnorderedList Mark="Dash"><ItemContent><Para>How to identify the correlation of different hashing bits and then mitigate its adverse effect is still an open problem in reconfigurable hashing. The current techniques are far from satisfactory. We believe that some tools developed in the information theory community are helpful. </Para></ItemContent><ItemContent><Para>The effectiveness of a hashing algorithm heavily hinges on the characteristics of underlying data distributions. Developing a taxonomy about data distribution in the hashing context is especially useful. </Para></ItemContent></UnorderedList>
              </Para>
            </Section1>
          </Body>
          <BodyRef FileRef="BodyRef/PDF/13735_2012_Article_3.pdf" TargetType="OnlinePDF"/>
          <BodyRef FileRef="BodyRef/PDF/13735_2012_3_TEX.zip" TargetType="TEX"/>
          <ArticleBackmatter>
            <Bibliography ID="Bib1">
              <Heading>References</Heading>
              <Citation ID="CR1">
                <CitationNumber>1.</CitationNumber>
                <BibUnstructured>Andoni A, Indyk P (2006) Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. In: FOCS. doi:<ExternalRef><RefSource>10.1109/FOCS.2006.49</RefSource><RefTarget Address="10.1109/FOCS.2006.49" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR2">
                <CitationNumber>2.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Andoni</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>P</Initials>
                    <FamilyName>Indyk</FamilyName>
                  </BibAuthorName>
                  <Year>2008</Year>
                  <ArticleTitle Language="En">Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions</ArticleTitle>
                  <JournalTitle>Commun ACM</JournalTitle>
                  <VolumeID>51</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>117</FirstPage>
                  <LastPage>122</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1145/1327452.1327494</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Andoni A, Indyk P (2008) Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. Commun ACM 51(1):117–122</BibUnstructured>
              </Citation>
              <Citation ID="CR3">
                <CitationNumber>3.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Bentley</FamilyName>
                  </BibAuthorName>
                  <Year>1975</Year>
                  <ArticleTitle Language="En">Multidimensional binary search trees used for associative searching</ArticleTitle>
                  <JournalTitle>Commun ACM</JournalTitle>
                  <VolumeID>18</VolumeID>
                  <IssueID>9</IssueID>
                  <FirstPage>509</FirstPage>
                  <LastPage>517</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>904133</Handle>
                  </Occurrence>
                  <Occurrence Type="ZLBID">
                    <Handle>0306.68061</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1145/361002.361007</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Bentley J (1975) Multidimensional binary search trees used for associative searching. Commun ACM 18(9):509–517</BibUnstructured>
              </Citation>
              <Citation ID="CR4">
                <CitationNumber>4.</CitationNumber>
                <BibUnstructured>Broder AZ, Charikar M, Frieze AM, Mitzenmacher M (1998) Minwise independent permutations. In: Proceedings of the thirtieth annual ACM symposium on theory of computing (STOC)</BibUnstructured>
              </Citation>
              <Citation ID="CR5">
                <CitationNumber>5.</CitationNumber>
                <BibUnstructured>Charikar M (2002) Similarity estimation techniques from rounding algorithms. In: STOC. doi:<ExternalRef><RefSource>10.1145/509907.509965</RefSource><RefTarget Address="10.1145/509907.509965" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR6">
                <CitationNumber>6.</CitationNumber>
                <BibUnstructured>Datar M, Immorlica N, Indyk P, Mirrokni V (2004) Locality-sensitive hashing scheme based on p-stable distributions. In: SCG. doi:<ExternalRef><RefSource>10.1145/997817.997857</RefSource><RefTarget Address="10.1145/997817.997857" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR7">
                <CitationNumber>7.</CitationNumber>
                <BibUnstructured>Dong W, Wang Z, Charikar M, Li K (2008) Efficiently matching sets of features with random histograms. In: ACM Multimedia. doi:<ExternalRef><RefSource>10.1145/1459359.1459384</RefSource><RefTarget Address="10.1145/1459359.1459384" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR8">
                <CitationNumber>8.</CitationNumber>
                <BibUnstructured>Grauman K, Darrell T (2005) The pyramid match kernel: discriminative classification with sets of image features. In: ICCV. doi:<ExternalRef><RefSource>10.1109/ICCV.2005.239</RefSource><RefTarget Address="10.1109/ICCV.2005.239" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR9">
                <CitationNumber>9.</CitationNumber>
                <BibUnstructured>He J, Liu W, Chang SF (2010) Scalable similarity search with optimized kernel hashing. In: SIGKDD. doi:<ExternalRef><RefSource>10.1145/1835804.1835946</RefSource><RefTarget Address="10.1145/1835804.1835946" TargetType="DOI"/></ExternalRef>. </BibUnstructured>
              </Citation>
              <Citation ID="CR10">
                <CitationNumber>10.</CitationNumber>
                <BibUnstructured>He X, Niyogi P (2003) Locality preserving projections. In: NIPS</BibUnstructured>
              </Citation>
              <Citation ID="CR11">
                <CitationNumber>11.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>P</Initials>
                    <FamilyName>Indyk</FamilyName>
                  </BibAuthorName>
                  <Year>2006</Year>
                  <ArticleTitle Language="En">Stable distributions, pseudorandom generators, embeddings, and data stream computation</ArticleTitle>
                  <JournalTitle>J ACM</JournalTitle>
                  <VolumeID>53</VolumeID>
                  <IssueID>3</IssueID>
                  <FirstPage>307</FirstPage>
                  <LastPage>323</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>2238947</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1145/1147954.1147955</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Indyk P (2006) Stable distributions, pseudorandom generators, embeddings, and data stream computation. J ACM 53(3):307–323</BibUnstructured>
              </Citation>
              <Citation ID="CR12">
                <CitationNumber>12.</CitationNumber>
                <BibUnstructured>Indyk P, Motwani R (1998) Approximate nearest neighbors: towards removing the curse of dimensionality. In: STOC. doi:<ExternalRef><RefSource>10.1145/276698.276876</RefSource><RefTarget Address="10.1145/276698.276876" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR13">
                <CitationNumber>13.</CitationNumber>
                <BibUnstructured>Ke Y, Sukthankar R, Huston L (2004) An efficient parts-based near-duplicate and sub-image retrieval system. In: ACM Multimedia. doi:<ExternalRef><RefSource>10.1145/1027527.1027729</RefSource><RefTarget Address="10.1145/1027527.1027729" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR14">
                <CitationNumber>14.</CitationNumber>
                <BibUnstructured>Kulis B, Grauman K (2009) Kernelized locality-sensitive hashing for scalable image search. In: ICCV. doi:<ExternalRef><RefSource>10.1109/ICCV.2009.5459466</RefSource><RefTarget Address="10.1109/ICCV.2009.5459466" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR15">
                <CitationNumber>15.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>DG</Initials>
                    <FamilyName>Lowe</FamilyName>
                  </BibAuthorName>
                  <Year>2004</Year>
                  <ArticleTitle Language="En">Distinctive image features from scale-invariant keypoints</ArticleTitle>
                  <JournalTitle>Int J Comput Vis</JournalTitle>
                  <VolumeID>60</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>91</FirstPage>
                  <LastPage>110</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1023/B:VISI.0000029664.99615.94</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Lowe DG (2004) Distinctive image features from scale-invariant keypoints. Int J Comput Vis 60(2):91–110</BibUnstructured>
              </Citation>
              <Citation ID="CR16">
                <CitationNumber>16.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>F</Initials>
                    <FamilyName>Moosmann</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>E</Initials>
                    <FamilyName>Nowak</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>F</Initials>
                    <FamilyName>Jurie</FamilyName>
                  </BibAuthorName>
                  <Year>2008</Year>
                  <ArticleTitle Language="En">Randomized clustering forests for image classification</ArticleTitle>
                  <JournalTitle>IEEE Trans PAMI</JournalTitle>
                  <VolumeID>30</VolumeID>
                  <IssueID>9</IssueID>
                  <FirstPage>1632</FirstPage>
                  <LastPage>1646</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TPAMI.2007.70822</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Moosmann F, Nowak E, Jurie F (2008) Randomized clustering forests for image classification. IEEE Trans PAMI 30(9):1632–1646</BibUnstructured>
              </Citation>
              <Citation ID="CR17">
                <CitationNumber>17.</CitationNumber>
                <BibUnstructured>Motwani R, Naor A, Panigrahi R (2006) Lower bounds on locality sensitive hashing. In: SCG. doi:<ExternalRef><RefSource>10.1145/1137856.1137881</RefSource><RefTarget Address="10.1145/1137856.1137881" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR18">
                <CitationNumber>18.</CitationNumber>
                <BibUnstructured>Mu Y, Shen J, Yan S (2010) Weakly-supervised hashing in kernel space. In: CVPR. doi:<ExternalRef><RefSource>10.1109/CVPR.2010.5540024</RefSource><RefTarget Address="10.1109/CVPR.2010.5540024" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR19">
                <CitationNumber>19.</CitationNumber>
                <BibUnstructured>Mu Y, Sun J, Han TX, Cheong LF, Yan S (2010) Randomized locality sensitive vocabularies for bag-of-features model. In: ECCV. doi:<ExternalRef><RefSource>10.1007/978-3-642-15558-1_54</RefSource><RefTarget Address="10.1007/978-3-642-15558-1_54" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR20">
                <CitationNumber>20.</CitationNumber>
                <BibUnstructured>Mu Y, Yan S (2010) Non-metric locality-sensitive hashing. In: AAAI</BibUnstructured>
              </Citation>
              <Citation ID="CR21">
                <CitationNumber>21.</CitationNumber>
                <BibUnstructured>Nowak E, Jurie F, Triggs B (2006) Sampling strategies for bag-of-features image classification. In: ECCV. doi:<ExternalRef><RefSource>10.1007/11744085_38</RefSource><RefTarget Address="10.1007/11744085_38" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR22">
                <CitationNumber>22.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Oliva</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Torralba</FamilyName>
                  </BibAuthorName>
                  <Year>2001</Year>
                  <ArticleTitle Language="En">Modeling the shape of the scene: a holistic representation of the spatial envelope</ArticleTitle>
                  <JournalTitle>Int J Comput Vis</JournalTitle>
                  <VolumeID>42</VolumeID>
                  <IssueID>3</IssueID>
                  <FirstPage>145</FirstPage>
                  <LastPage>175</LastPage>
                  <Occurrence Type="ZLBID">
                    <Handle>0990.68601</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1023/A:1011139631724</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Oliva A, Torralba A (2001) Modeling the shape of the scene: a holistic representation of the spatial envelope. Int J Comput Vis 42(3):145–175</BibUnstructured>
              </Citation>
              <Citation ID="CR23">
                <CitationNumber>23.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>L</Initials>
                    <FamilyName>Paulevé</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>H</Initials>
                    <FamilyName>Jégou</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>L</Initials>
                    <FamilyName>Amsaleg</FamilyName>
                  </BibAuthorName>
                  <Year>2010</Year>
                  <ArticleTitle Language="En">Locality sensitive hashing: a comparison of hash function types and querying mechanisms</ArticleTitle>
                  <JournalTitle>Pattern Recognit Lett</JournalTitle>
                  <VolumeID>31</VolumeID>
                  <IssueID>11</IssueID>
                  <FirstPage>1348</FirstPage>
                  <LastPage>1358</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1016/j.patrec.2010.04.004</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Paulevé L, Jégou H, Amsaleg L (2010) Locality sensitive hashing: a comparison of hash function types and querying mechanisms. Pattern Recognit Lett 31(11):1348–1358</BibUnstructured>
              </Citation>
              <Citation ID="CR24">
                <CitationNumber>24.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>R</Initials>
                    <FamilyName>Salakhutdinov</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>G</Initials>
                    <FamilyName>Hinton</FamilyName>
                  </BibAuthorName>
                  <Year>2009</Year>
                  <ArticleTitle Language="En">Semantic hashing</ArticleTitle>
                  <JournalTitle>Int J Approx Reason</JournalTitle>
                  <VolumeID>50</VolumeID>
                  <IssueID>7</IssueID>
                  <FirstPage>969</FirstPage>
                  <LastPage>978</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1016/j.ijar.2008.11.006</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Salakhutdinov R, Hinton G (2009) Semantic hashing. Int J Approx Reason 50(7):969–978</BibUnstructured>
              </Citation>
              <Citation ID="CR25">
                <CitationNumber>25.</CitationNumber>
                <BibBook>
                  <BibAuthorName>
                    <Initials>B</Initials>
                    <FamilyName>Scholkopf</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>AJ</Initials>
                    <FamilyName>Smola</FamilyName>
                  </BibAuthorName>
                  <Year>2001</Year>
                  <BookTitle>Learning with kernels: support vector machines, regularization, optimization, and beyond</BookTitle>
                  <PublisherName>MIT Press</PublisherName>
                  <PublisherLocation>Cambridge</PublisherLocation>
                </BibBook>
                <BibUnstructured>Scholkopf B, Smola AJ (2001) Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT Press, Cambridge</BibUnstructured>
              </Citation>
              <Citation ID="CR26">
                <CitationNumber>26.</CitationNumber>
                <BibUnstructured>Shakhnarovich G, Viola PA, Darrell T (2003) Fast pose estimation with parameter-sensitive hashing. In: ICCV. doi:<ExternalRef><RefSource>10.1109/ICCV.2003.1238424</RefSource><RefTarget Address="10.1109/ICCV.2003.1238424" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR27">
                <CitationNumber>27.</CitationNumber>
                <BibUnstructured>Wang F, Zhang C (2007) Feature extraction by maximizing the average neighborhood margin. In: CVPR. doi:<ExternalRef><RefSource>10.1109/CVPR.2007.383124</RefSource><RefTarget Address="10.1109/CVPR.2007.383124" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR28">
                <CitationNumber>28.</CitationNumber>
                <BibUnstructured>Wang J, Kumar S, Chang SF (2010) Semi-supervised hashing for scalable image retrieval. In: CVPR. doi:<ExternalRef><RefSource>10.1109/CVPR.2010.5539994</RefSource><RefTarget Address="10.1109/CVPR.2010.5539994" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR29">
                <CitationNumber>29.</CitationNumber>
                <BibUnstructured>Wang J, Kumar S, Chang SF (2010) Sequential projection learning for hashing with compact codes. In: ICML</BibUnstructured>
              </Citation>
              <Citation ID="CR30">
                <CitationNumber>30.</CitationNumber>
                <BibUnstructured>Wang M, Song Y, Hua XS (2009) Concept representation based video indexing. In: SIGIR. doi:<ExternalRef><RefSource>10.1145/1571941.1572062</RefSource><RefTarget Address="10.1145/1571941.1572062" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR31">
                <CitationNumber>31.</CitationNumber>
                <BibUnstructured>Weiss Y, Torralba A, Fergus R (2008) Spectral hashing. In: NIPS</BibUnstructured>
              </Citation>
              <Citation ID="CR32">
                <CitationNumber>32.</CitationNumber>
                <BibUnstructured>Xing EP, Ng AY, Jordan MI, Russell SJ (2002) Distance metric learning with application to clustering with side-information. In: NIPS</BibUnstructured>
              </Citation>
            </Bibliography>
          </ArticleBackmatter>
        </Article>
      </Issue>
    </Volume>
  </Journal>
</Publisher>
