<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE Publisher PUBLIC "-//Springer-Verlag//DTD A++ V2.4//EN" "http://devel.springer.de/A++/V2.4/DTD/A++V2.4.dtd">
<Publisher>
  <PublisherInfo>
    <PublisherName>Springer-Verlag</PublisherName>
    <PublisherLocation>London</PublisherLocation>
  </PublisherInfo>
  <Journal OutputMedium="All">
    <JournalInfo JournalProductType="ArchiveJournal" NumberingStyle="ContentOnly">
      <JournalID>13735</JournalID>
      <JournalPrintISSN>2192-6611</JournalPrintISSN>
      <JournalElectronicISSN>2192-662X</JournalElectronicISSN>
      <JournalTitle>International Journal of Multimedia Information Retrieval</JournalTitle>
      <JournalAbbreviatedTitle>Int J Multimed Info Retr</JournalAbbreviatedTitle>
      <JournalSubjectGroup>
        <JournalSubject Type="Primary">Computer Science</JournalSubject>
        <JournalSubject Type="Secondary">Data Mining and Knowledge Discovery</JournalSubject>
        <JournalSubject Type="Secondary">Computer Science, general</JournalSubject>
        <JournalSubject Type="Secondary">Information Storage and Retrieval</JournalSubject>
        <JournalSubject Type="Secondary">Image Processing and Computer Vision</JournalSubject>
        <JournalSubject Type="Secondary">Multimedia Information Systems</JournalSubject>
        <JournalSubject Type="Secondary">Information Systems Applications (incl. Internet)</JournalSubject>
        <SubjectCollection Code="Computer Science">SC6</SubjectCollection>
      </JournalSubjectGroup>
    </JournalInfo>
    <Volume OutputMedium="All">
      <VolumeInfo TocLevels="0" VolumeType="Regular">
        <VolumeIDStart>1</VolumeIDStart>
        <VolumeIDEnd>1</VolumeIDEnd>
        <VolumeIssueCount>4</VolumeIssueCount>
      </VolumeInfo>
      <Issue IssueType="Regular" OutputMedium="All">
        <IssueInfo IssueType="Regular" TocLevels="0">
          <IssueIDStart>3</IssueIDStart>
          <IssueIDEnd>3</IssueIDEnd>
          <IssueArticleCount>4</IssueArticleCount>
          <IssueHistory>
            <OnlineDate>
              <Year>2012</Year>
              <Month>9</Month>
              <Day>18</Day>
            </OnlineDate>
            <PrintDate>
              <Year>2012</Year>
              <Month>9</Month>
              <Day>17</Day>
            </PrintDate>
            <CoverDate>
              <Year>2012</Year>
              <Month>10</Month>
            </CoverDate>
            <PricelistYear>2012</PricelistYear>
          </IssueHistory>
          <IssueCopyright>
            <CopyrightHolderName>Springer-Verlag London Limited</CopyrightHolderName>
            <CopyrightYear>2012</CopyrightYear>
          </IssueCopyright>
        </IssueInfo>
        <Article ID="s13735-012-0016-2" OutputMedium="All">
          <ArticleInfo ArticleType="OriginalPaper" ContainsESM="No" Language="En" NumberingStyle="ContentOnly" TocLevels="0">
            <ArticleID>16</ArticleID>
            <ArticleDOI>10.1007/s13735-012-0016-2</ArticleDOI>
            <ArticleSequenceNumber>2</ArticleSequenceNumber>
            <ArticleTitle Language="En" OutputMedium="All">A study on video data mining</ArticleTitle>
            <ArticleCategory>Trends and Surveys</ArticleCategory>
            <ArticleFirstPage>153</ArticleFirstPage>
            <ArticleLastPage>172</ArticleLastPage>
            <ArticleHistory>
              <RegistrationDate>
                <Year>2012</Year>
                <Month>8</Month>
                <Day>4</Day>
              </RegistrationDate>
              <Received>
                <Year>2011</Year>
                <Month>10</Month>
                <Day>17</Day>
              </Received>
              <Revised>
                <Year>2012</Year>
                <Month>7</Month>
                <Day>31</Day>
              </Revised>
              <Accepted>
                <Year>2012</Year>
                <Month>8</Month>
                <Day>3</Day>
              </Accepted>
              <OnlineDate>
                <Year>2012</Year>
                <Month>8</Month>
                <Day>25</Day>
              </OnlineDate>
            </ArticleHistory>
            <ArticleCopyright>
              <CopyrightHolderName>Springer-Verlag London Limited</CopyrightHolderName>
              <CopyrightYear>2012</CopyrightYear>
            </ArticleCopyright>
            <ArticleGrants Type="Regular">
              <MetadataGrant Grant="OpenAccess"/>
              <AbstractGrant Grant="OpenAccess"/>
              <BodyPDFGrant Grant="OpenAccess"/>
              <BodyHTMLGrant Grant="OpenAccess"/>
              <BibliographyGrant Grant="OpenAccess"/>
              <ESMGrant Grant="OpenAccess"/>
            </ArticleGrants>
          </ArticleInfo>
          <ArticleHeader>
            <AuthorGroup>
              <Author AffiliationIDS="Aff1 Aff2" CorrespondingAffiliationID="Aff1">
                <AuthorName DisplayOrder="Western">
                  <GivenName>V.</GivenName>
                  <FamilyName>Vijayakumar</FamilyName>
                </AuthorName>
                <Contact>
                  <Email>veluvijay20@gmail.com</Email>
                </Contact>
              </Author>
              <Author AffiliationIDS="Aff3">
                <AuthorName DisplayOrder="Western">
                  <GivenName>R.</GivenName>
                  <FamilyName>Nedunchezhian</FamilyName>
                </AuthorName>
                <Contact>
                  <Email>rajuchezhian@gmail.com</Email>
                </Contact>
              </Author>
              <Affiliation ID="Aff1">
                <OrgDivision>Research and Development Centre</OrgDivision>
                <OrgName>Bharathiar University</OrgName>
                <OrgAddress>
                  <City>Coimbatore</City>
                  <Postcode>641 022</Postcode>
                  <State>Tamil Nadu</State>
                  <Country Code="IN">India</Country>
                </OrgAddress>
              </Affiliation>
              <Affiliation ID="Aff2">
                <OrgDivision>Department of Computer Applications</OrgDivision>
                <OrgName>Sri Ramakrishna Engineering College</OrgName>
                <OrgAddress>
                  <City>Coimbatore</City>
                  <Postcode>641 022</Postcode>
                  <State>Tamil Nadu</State>
                  <Country Code="IN">India</Country>
                </OrgAddress>
              </Affiliation>
              <Affiliation ID="Aff3">
                <OrgDivision>Department of Information and Technology</OrgDivision>
                <OrgName>Sri Ramakrishna Engineering College</OrgName>
                <OrgAddress>
                  <City>Coimbatore</City>
                  <Postcode>641 022</Postcode>
                  <State>Tamil Nadu</State>
                  <Country Code="IN">India</Country>
                </OrgAddress>
              </Affiliation>
            </AuthorGroup>
            <Abstract ID="Abs1" Language="En" OutputMedium="All">
              <Heading>Abstract</Heading>
              <Para>Data mining is a process of extracting previously unknown knowledge and detecting the interesting patterns from a massive set of data. Thanks to the extensive use of information technology and the recent developments in multimedia systems, the amount of multimedia data available to users has increased exponentially. Video is an example of multimedia data as it contains several kinds of data such as text, image, meta-data, visual and audio. It is widely used in many major potential applications like security and surveillance, entertainment, medicine, education programs and sports. The objective of video data mining is to discover and describe interesting patterns from the huge amount of video data as it is one of the core problem areas of the data-mining research community. Compared to the mining of other types of data, video data mining is still in its infancy. There are many challenging research problems existing with video mining. Beginning with an overview of the video data-mining literature, this paper concludes with the applications of video mining.</Para>
            </Abstract>
            <KeywordGroup Language="En" OutputMedium="All">
              <Heading>Keywords</Heading>
              <Keyword>Video processing</Keyword>
              <Keyword>Video information retrieval</Keyword>
              <Keyword>Video mining</Keyword>
              <Keyword>Video association mining</Keyword>
              <Keyword>Movie classification</Keyword>
              <Keyword>Sports mining</Keyword>
            </KeywordGroup>
          </ArticleHeader>
          <Body>
            <Section1 ID="Sec1">
              <Heading>Introduction</Heading>
              <Para>It is the advancement in multimedia acquisition and storage technology that has led to a tremendous growth in multimedia databases. Multimedia mining deals with the extraction of implicit knowledge, multimedia data relationships or other patterns not explicitly stored in the multimedia data [<CitationRef CitationID="CR6">6</CitationRef>, <CitationRef CitationID="CR45">45</CitationRef>, <CitationRef CitationID="CR100">100</CitationRef>]. The management of multimedia data is one of the crucial tasks in the data mining owing to the non-structured nature of the multimedia data. The main challenge is to handle the multimedia data with a complex structure such as images, multimedia text, video and audio data [<CitationRef CitationID="CR21">21</CitationRef>, <CitationRef CitationID="CR60">60</CitationRef>, <CitationRef CitationID="CR65">65</CitationRef>, <CitationRef CitationID="CR72">72</CitationRef>].</Para>
              <Para>Nowadays people have accessibility to a tremendous amount of video both on television and internet. So, there is a great potential for video-based applications in many areas including security and surveillance, personal entertainment, medicine, sports, news video, educational programs and movies and so on. Video data contains several kinds of data such as video, audio and text [<CitationRef CitationID="CR59">59</CitationRef>]. The video consists of a sequence of images with some temporal information. The audio consists of speech, music and various special sounds whereas the textual information represents its linguistic form.</Para>
              <Para>The video content may be classified into three categories, namely [<CitationRef CitationID="CR102">102</CitationRef>]. (i) Low-level feature information that includes features such as color, texture, shape and so on, (ii) Syntactic information that describes the contents of video, including salient objects, their spatial-temporal position and spatial-temporal relations between them, and (iii) semantic information, which describes what is happening in the video along with what is perceived by the users. The semantic information used to identify the video events has two important aspects [<CitationRef CitationID="CR87">87</CitationRef>]. They are: (a) A spatial aspect presented by a video frame, such as the location, characters and objects displayed in the video frame. (b) A temporal aspect presented by a sequence of video frames in time such as the character’s actions and the object’s movements presented in a sequence. The higher-level semantic information of video is extracted by examining the features of the audio, video and text of the video. Multiple cues from different modalities including audio and visual features are fully exploited and used to capture the semantic structure of the video bridging the gap between the high level semantic concepts and the low-level features. Three modalities are identified within a video [<CitationRef CitationID="CR84">84</CitationRef>]. They are: (1) Visual modality containing the scene that can be seen in the video; (2) Auditory modality with the speech, music, and environmental sounds that can be heard along with the video; (3) Textual modality having the textual resources which describes the content of the video document.</Para>
              <Para>Video databases are widespread and video data sets are extremely large. There are tools for managing and searching within such collections, but the need for tools to extract the hidden and useful knowledge embedded within the video data is becoming critical for many decision-making applications.</Para>
              <Section2 ID="Sec2">
                <Heading>Video processing</Heading>
                <Para>Though the acquisition and storage of video data is an easy task the retrieval of information from the video data is a challenging task. One of the most important steps involved is to transform the video data from non-structured data into a structured data set as the processing of the video data with image processing or computer vision techniques demands structured-format features [<CitationRef CitationID="CR73">73</CitationRef>, <CitationRef CitationID="CR75">75</CitationRef>]. Before applying the data-mining techniques on the video key frame, the video, audio and text features are extracted using the image processing techniques, eliminating the digitalization noise and illumination changes to avoid false positive detection.</Para>
                <Para>The video data can be structured in two ways according to the content structure. First, and foremost the scripted video databases [<CitationRef CitationID="CR105">105</CitationRef>, <CitationRef CitationID="CR111">111</CitationRef>] are carefully produced according to a script or plan that is later edited, compiled and distributed for consumption. These video databases have some content structures such as movies, dramas and news. Second, unscripted video databases [<CitationRef CitationID="CR105">105</CitationRef>] have no content structures as “raw” videos like surveillance videos and sports videos have no scene change.</Para>
                <Para>It is the most fundamental task in video processing to partition the long video sequences into a number of shots and find a key frame of each shot for further video information retrieval tasks. Hu et al. [<CitationRef CitationID="CR40">40</CitationRef>] presented several strategies in visual content-based video indexing and retrieval to focusing on the video structure analysis, including shot boundary detection, key frame extraction and scene segmentation, extraction of features and video data mining. Bhatt and Kankanhalli [<CitationRef CitationID="CR6">6</CitationRef>] discussed the video feature extraction, video transformation and representation techniques and the video data-mining techniques.</Para>
                <Section3 ID="Sec3">
                  <Heading>Video data model</Heading>
                  <Para>Since, the relational or object oriented data model does not provide enough facilities for managing and retrieving the video contents efficiently, an appropriate data model is needed to do it. Three main reasons [<CitationRef CitationID="CR79">79</CitationRef>] can be identified for this: (1) lack of facilities for the management of spatiotemporal relations, (2) lack of knowledge-based methods for interpreting raw data into semantic contents and (3) lack of query representations for complex structures.</Para>
                  <Para>A video data model is a representation of video data based on its characteristics and content as well as the applications it is intended for [<CitationRef CitationID="CR44">44</CitationRef>]. It is based on the idea of video segmentation or video annotation. Video data mining requires a good data model for video representation. Various models have been proposed by different authors. Petkovic and Jonker [<CitationRef CitationID="CR79">79</CitationRef>] proposed a content-based retrieval data model with four layers. They are: (i) Raw video data layer with a sequence of frames, as well as some video attributes. (ii) Feature layer consisting of domain-independent features that can be automatically extracted from raw data, characterizing colors, textures, shapes, and motion. (iii) Object layer having the entities, characterized by a prominent spatial dimension and assigned to regions across frames. (iv) Event layer with entities that have a prominent temporal extent describing the movements and interactions of different objects in a spatial-temporal manner.</Para>
                  <Para>Zhu et al. [<CitationRef CitationID="CR111">111</CitationRef>] described a hierarchical video database management framework using video semantic units to construct database indices. They presented a hierarchical video database model [<CitationRef CitationID="CR27">27</CitationRef>] that captures the structures and semantics of video contents in databases. It provides a framework for automatic mapping from the high-level concepts to the low-level representative features. It is exploited by partitioning the video contents into a set of hierarchically manageable units such as clusters, sub clusters, sub regions, shots or objects, frames or video object planes and regions. It supports a more efficient video representation, indexing and video data accessing techniques.</Para>
                </Section3>
                <Section3 ID="Sec4">
                  <Heading>Video segmentation</Heading>
                  <Para>The first step in any video data management system is invariably, the segmentation of the video track into smaller units [<CitationRef CitationID="CR3">3</CitationRef>, <CitationRef CitationID="CR68">68</CitationRef>] enabling the subsequent processing operations on video shots, such as video indexing, semantic representation or tracking of the selected video information and identifying the frames where a transition takes place from one shot to another. The visual-based segmentation identifies the shot boundaries and the motion-based segmentation identifies pans and zooms [<CitationRef CitationID="CR12">12</CitationRef>]. In general, most of the videos from daily life can be represented using a hierarchy of levels [<CitationRef CitationID="CR29">29</CitationRef>] shown in Fig. <InternalRef RefID="Fig1">1</InternalRef>. The following terms are defined [<CitationRef CitationID="CR68">68</CitationRef>, <CitationRef CitationID="CR105">105</CitationRef>] as<Figure Category="Standard" Float="Yes" ID="Fig1">
                      <Caption Language="En">
                        <CaptionNumber>Fig. 1</CaptionNumber>
                        <CaptionContent>
                          <SimplePara>Video hierarchy</SimplePara>
                        </CaptionContent>
                      </Caption>
                      <MediaObject ID="MO1">
                        <ImageObject Color="BlackWhite" FileRef="MediaObjects/13735_2012_16_Fig1_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </MediaObject>
                    </Figure>
                  </Para>
                  <Para>
                    <Emphasis Type="Italic">Video</Emphasis> It refers to multimedia sequences comprised of both sound and a series of images.</Para>
                  <Para>
                    <Emphasis Type="Italic">Scene</Emphasis> It is a collection of semantically related and temporally adjacent groups depicting and conveying a high-level concept with a sequence of shots focusing on the same point or location of interest [<CitationRef CitationID="CR83">83</CitationRef>].</Para>
                  <Para>
                    <Emphasis Type="Italic">Video group</Emphasis> It is an intermediate entity between the physical shots and semantic scenes serving as the bridge between the shot and scene. Two kinds of shots are absorbed into a video group [<CitationRef CitationID="CR116">116</CitationRef>]. (1) Temporally related: Shots related in temporal series where similar shots are shown back and forth. (2) Spatially related: Shots similar in visual perception, where all shots in the group are similar in visual features.</Para>
                  <Para>
                    <Emphasis Type="Italic">Shot</Emphasis> It is defined as a sequence of frames taken by a single camera with no major changes in the visual content [<CitationRef CitationID="CR82">82</CitationRef>]. One of the important initial steps in segmentation and analysis of the video data is the shot-boundary detection. What is challenging in video segmentation is that the shot change detection algorithm must be able to handle all types of scene changes like abrupt changes and gradual changes.</Para>
                  <Para>
                    <Emphasis Type="Italic">Key frame</Emphasis> The frame represents the salient visual contents of a shot. Since a large number of adjacent frames are likely to be similar, one or more key frames can be extracted from the shot depending on the complexity of the content of the shot. The key frames extracted from the video streams are treated as images.</Para>
                </Section3>
                <Section3 ID="Sec5">
                  <Heading>Feature extraction</Heading>
                  <Para>The video segmented and the key frames chosen, the low-level image features can be extracted from the key frames. The low-level visual features such as color, texture, edge and shapes can be extracted and represented as feature descriptors [<CitationRef CitationID="CR43">43</CitationRef>]. A feature is defined as a descriptive parameter extracted from an image or a video stream [<CitationRef CitationID="CR92">92</CitationRef>]. There are two kinds of video features extracted from the video [<CitationRef CitationID="CR11">11</CitationRef>, <CitationRef CitationID="CR45">45</CitationRef>]. (i) The description based features that use metadata, such as keywords, caption, size and time of the creation. (ii) The content based features are based on the content of the object itself. There are two categories of content based features: the global features extracted from a whole image and the local or regional features describing the chosen patches of a given image [<CitationRef CitationID="CR36">36</CitationRef>]. Each region is then processed to extract a set of features characterizing the visual properties including the color, texture, motion and structure of the region. The shot-based features and the object-based features are the two approaches used to access the video sources in the database.</Para>
                  <Para>Fu et al. [<CitationRef CitationID="CR29">29</CitationRef>] extracted the video features such as color histogram, domain color, edge color and texture and stored to XML documents for further video mining process. Choudhary et al. [<CitationRef CitationID="CR18">18</CitationRef>] represented the low-level features like the position of objects, size of objects, color correlogram of objects, and so on extracted from the frames of the video or high level semantically meaningful concepts like the object category and the object trajectory [<CitationRef CitationID="CR18">18</CitationRef>].</Para>
                  <Para>The spatial features (visual), audio features and the features of moving objects (temporal features) can be used to mine the significant patterns in a video [<CitationRef CitationID="CR48">48</CitationRef>]. These resulting patterns are then evaluated and interpreted to obtain the knowledge of the application. The Mining objects, events and scenes in the video are useful for many applications. For instance, they can serve as entry points for retrieval and browsing or they can provide a basis for video summarization [<CitationRef CitationID="CR80">80</CitationRef>]. Liu and He [<CitationRef CitationID="CR56">56</CitationRef>] detected football events by using the multiple feature extraction (visual, auditory features, text and audio keywords) and fusion.</Para>
                </Section3>
              </Section2>
              <Section2 ID="Sec6">
                <Heading>Video information retrieval</Heading>
                <Para>A video retrieval system is essentially a database management system. It is for handling video data is concerning with returning similar video clips (or scenes, shots, and frames) to a user for their video query. For an information retrieval to the efficient, the video data have to be manipulated properly. Basically there are four steps involved in any automatic video information retrieval [<CitationRef CitationID="CR73">73</CitationRef>, <CitationRef CitationID="CR75">75</CitationRef>, <CitationRef CitationID="CR83">83</CitationRef>], namely (1) Shot boundary detection, (2) Key frames selection, (3) Extracting low-level features from key frames and (4) Content-based video information retrieval with a query in the form of input provided by the user and a search carried out through the database on the basis of the input.</Para>
                <Para>There are the two familiar concepts used for the video information retrieval: video abstraction and video annotation.</Para>
                <Section3 ID="Sec7">
                  <Heading>Video abstraction</Heading>
                  <Para>Video Abstraction is a short summary of a long video having a set of stills or moving images selected and reconstructed from an original video with concise information about the content widely used in the video cataloging, indexing and retrieving [<CitationRef CitationID="CR73">73</CitationRef>, <CitationRef CitationID="CR75">75</CitationRef>]. The video abstraction is of two types: video summarization and dynamic video skimming.</Para>
                  <Para>
                    <Emphasis Type="Italic">Video Summarization</Emphasis> Video Summarization is defined as a sequence of stills or moving pictures (with or without audio) presenting the content of a video [<CitationRef CitationID="CR83">83</CitationRef>] in such a way that the respective target group is rapidly provided with concise information about the content preserving the essential message of the original and aiming at generating a series of visual contents for users to browse and understand the whole story of the video efficiently [<CitationRef CitationID="CR10">10</CitationRef>, <CitationRef CitationID="CR51">51</CitationRef>]. Selecting or reconstructing a set of salient key frames from an original video sequence, it discards similar frames preserving the frames, different from one another and drawing the attention of the people. It assumes that the scripted content summary is carefully structured as a sequence of semantic units whereas the unscripted content requires a “highlights” extraction framework that captures only remarkable events that constitute the summary.</Para>
                  <Para>The Video summarization uses two approaches [<CitationRef CitationID="CR107">107</CitationRef>] based on the video features. First, the rule-based approach combines evidences from several types of processing (audio, video, text) to detect certain configuration of events included in the summary using the pattern recognition algorithms to identify elements in the video and finds rules to qualitatively select important elements to be included in the summary. Ngo et al. [<CitationRef CitationID="CR70">70</CitationRef>] proposed a unified approach for summarization based on the analysis of the video structures and video highlights. Second, the mathematically oriented approach that uses similarities within the videos to compute a relevant value of video segments or frames with the mathematical criteria, such as frequency of occurrence to quantitatively evaluate the importance of the video segments. Lu et al. [<CitationRef CitationID="CR51">51</CitationRef>] presented an approach for video summarization based on the graph optimization. The approach has three stages: First, the source video is segmented into video shots selecting a candidate shot set from the video shots according to some video features. Second, a dissimilarity function is defined between the video shots to describe their spatial-temporal relation and the candidate video shot set is modeled into a directional graph. Third, a dynamic programming algorithm is used to search the longest path in the graph as the final video skimming generating a static video simultaneously.</Para>
                  <Para>
                    <Emphasis Type="Italic">Dynamic Video Skimming</Emphasis> Video skimming [<CitationRef CitationID="CR73">73</CitationRef>, <CitationRef CitationID="CR75">75</CitationRef>] consists of a collection of image sequences along with the related audios from an original video possessing a higher level of semantic meaning or preview of an original video than the video summary does and highlight generation is one of the important applications in it with the most interesting and attractive parts of a video [<CitationRef CitationID="CR75">75</CitationRef>]. It is similar to a trailer of a movie showing the most attractive scenes without revealing the ending of a film and used in a film domain frequently as shown in Fig. <InternalRef RefID="Fig2">2</InternalRef>.<Figure Category="Standard" Float="Yes" ID="Fig2">
                      <Caption Language="En">
                        <CaptionNumber>Fig. 2</CaptionNumber>
                        <CaptionContent>
                          <SimplePara>A video highlight</SimplePara>
                        </CaptionContent>
                      </Caption>
                      <MediaObject ID="MO2">
                        <ImageObject Color="BlackWhite" FileRef="MediaObjects/13735_2012_16_Fig2_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </MediaObject>
                    </Figure>
                  </Para>
                </Section3>
                <Section3 ID="Sec8">
                  <Heading>Video annotation</Heading>
                  <Para>Annotation involves attaching keywords from the specialism along with commentaries produced by experts and those obtained from a body of the existing texts [<CitationRef CitationID="CR85">85</CitationRef>]. The text embedded in the video (closed caption) is a powerful keyword resource in building the video annotation and retrieval system enabling text-based querying, video retrieval and content summarization.</Para>
                  <Para>Video annotation [<CitationRef CitationID="CR67">67</CitationRef>] has preceded along three dimensions, namely, supervised annotation (uses machine learning of visual and text features), unsupervised annotation and contexts-based approaches. In supervised learning based on the annotated concepts for each video, the unclassified files are automatically categorized. In unsupervised learning, the video files are clustered and annotators assign key words to each cluster which can be used to extract rules for annotating future documents. The third approach tries to mine concepts by looking at the contextual information such as the text associated with images to derive semantic concepts. Moxley et al. [<CitationRef CitationID="CR67">67</CitationRef>] presented a new approach to automatic video annotation by leveraging search and mining techniques. It employed as a two-step process of search followed by mining. Given a query video consisting of the visual content and speech-recognized transcripts, the similar videos are first ranked in a multimodal search. Then, the transcripts associated with these similar videos are mined to extract keywords for the query. Tseng et al. [<CitationRef CitationID="CR95">95</CitationRef>] proposed an innovative method for the semantic video annotation by integrating visual features, speech features and frequent patterns existing in a video. Wang et al. [<CitationRef CitationID="CR101">101</CitationRef>] presented an approach to automatically annotate a video shot with an adaptive number of annotation key words according to the richness of the video content.</Para>
                  <Para>Annotation [<CitationRef CitationID="CR95">95</CitationRef>] can be classified into three categories: (a) Statistics-Based Annotation that computes the probabilities between visual features and candidate keywords; (b) Rule-Based Annotation that discovers the associated concepts hidden in the sequential images; (c) Hybrid Annotation Methods which integrate the statistics-based and rule-based methods for mining visual features in the video. The annotation problem has received a significant attention, since annotation helps bridge the semantic gap that results from querying using one mode (e.g., text) for the returns of another mode (e.g., images) [<CitationRef CitationID="CR57">57</CitationRef>]. Generating captions or annotations automatically for still video is an arduous task. Aradhye et al. [<CitationRef CitationID="CR5">5</CitationRef>] presented a method for large scale auto-annotation videos without requiring any explicit manual annotation.</Para>
                  <Para>This paper presents a detailed study on video data-mining concepts, techniques, research issues and various application domains are discussed here. The paper is organized as follows. Section <InternalRef RefID="Sec9">2</InternalRef> discusses the concepts of video data mining. Section <InternalRef RefID="Sec15">3</InternalRef> presents the video data-mining techniques and approaches. Section <InternalRef RefID="Sec21">4</InternalRef> explains the application of the video data mining. Section <InternalRef RefID="Sec26">5</InternalRef> covers the key accomplishments of video data mining. The research issues and future directions are discussed in Sect. <InternalRef RefID="Sec27">6</InternalRef> and Sect. <InternalRef RefID="Sec28">7</InternalRef> concludes the paper.</Para>
                </Section3>
              </Section2>
            </Section1>
            <Section1 ID="Sec9">
              <Heading>Video data mining</Heading>
              <Para>It is video data mining that deals with the extraction of implicit knowledge, video data relationships, or other patterns not explicitly stored in the video databases considered as an extension of still image mining by including mining of temporal image sequences [<CitationRef CitationID="CR64">64</CitationRef>]. It is a process which not only automatically extracts content and structure of video, features of moving objects, spatial or temporal correlations of those features, but also discovers patterns of video structure, object activities, video events from vast amounts of video data with a little assumption of their contents.</Para>
              <Section2 ID="Sec10">
                <Heading>Video information retrieval versus video data mining</Heading>
                <Para>It is video information retrieval, not information retrieval that is considered as part of the video data mining of a certain level knowledge discovery such as feature selection, dimensionality reduction and concept discovery.</Para>
                <Para>The dissimilarities of video data mining [<CitationRef CitationID="CR101">101</CitationRef>] with related areas are as follows,<UnorderedList Mark="Bullet">
                    <ItemContent>
                      <Para>Video data mining versus computer vision or video processing: The relationship between video processing and video mining is very subjective. The goal of video data mining is to extract patterns from the video sequences whereas video processing focuses on understanding and/or extracting features from the video database.</Para>
                    </ItemContent>
                    <ItemContent>
                      <Para>Video data mining versus pattern recognition: Both areas share the feature extraction steps but differ in pattern specificity. The objective of pattern recognition is to recognize specific, classification patterns, pattern generation and analysis. Pattern recognition is indulging in a research on classifying special samples with an existing model while video mining is involved in discovering rules and patterns of samples with or without image processing. The objective of video data mining is to generate all significant patterns without prior knowledge of what they are.</Para>
                    </ItemContent>
                    <ItemContent>
                      <Para>Video information retrieval versus video data mining [<CitationRef CitationID="CR73">73</CitationRef>, <CitationRef CitationID="CR75">75</CitationRef>]: The difference is similar to the difference between database management and data mining [<CitationRef CitationID="CR94">94</CitationRef>]. Video mining focuses on finding correlations and patterns previously unknown from large video databases shown in Fig. <InternalRef RefID="Fig3">3</InternalRef>.</Para>
                    </ItemContent>
                  </UnorderedList>
                  <Figure Category="Standard" Float="Yes" ID="Fig3">
                    <Caption Language="En">
                      <CaptionNumber>Fig. 3</CaptionNumber>
                      <CaptionContent>
                        <SimplePara>Video mining</SimplePara>
                      </CaptionContent>
                    </Caption>
                    <MediaObject ID="MO3">
                      <ImageObject Color="BlackWhite" FileRef="MediaObjects/13735_2012_16_Fig3_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                  </Figure>
                </Para>
                <Para>Sanjeevkumar and Praveenkumar [<CitationRef CitationID="CR86">86</CitationRef>] presented an architecture for multimedia data mining to find the patterns in multimedia data. Video mining involves three main tasks [<CitationRef CitationID="CR23">23</CitationRef>]. They are: (1) Video preprocessing with high quality video objects such as blocks of pixels, key frames, segments, scenes, moving objects and description text; (2) The extracting of the features and semantic information of video objects such as physical features, motion features, relation features and semantic descriptions of these features, and (3) Video patterns and knowledge discovery using video, audio and text features.</Para>
              </Section2>
              <Section2 ID="Sec11">
                <Heading>Key problems in video data mining</Heading>
                <Para>Video data mining is an emerging field that can be defined as the unsupervised discovery of patterns in audio visual contents. Mining video data is even more complicated than mining still image data requiring tools for discovering relationships between objects or segments within the video components, such as classifying video images based on their contents, extracting patterns in sound, categorizing speech and music, and recognizing and tracking objects in video streams. The existing data-mining tools pose various problems while applied to video database. They are: (a) Database model problem in which video documents are generally unstructured in semantics and cannot be represented easily via the relational data model demanding a good video database model that is crucial to support more efficient video database management and mining [<CitationRef CitationID="CR111">111</CitationRef>]. To adopt a good model, one needs to address three problems, namely (1) How many levels should be included in the model? (2) What kind of decision rules should be used at each node? (3) Do these nodes make sense to human beings? (b) The retrieval results solely based on the low level feature extraction are mostly unsatisfactory and unpredictable. It is the semantic gap between the low level visual features and the high level user domain that happens to the one of the hurdles for the development of a video data-mining system. Modelling the high level features rather than the low level features is difficult as the former is depending on the semantics whereas the latter is based on the syntactic structure. (c) Maintaining data integrity and security in video database management structure. These challenges have led to a lot of research and development in the area of video data mining. The main objective of video mining is to extract the significant objects, characters and scenes by determining their frequency of re-occurrence.</Para>
                <Para>Most of the existing video mining tools lack the semantic interpretation of the video. Though there are several widely-accepted data-mining techniques, most of them are unsuitable for video data mining because of the semantic gap. Numerous methodologies have been developed and many applications have been investigated including the organizing video data indexing and retrieval [<CitationRef CitationID="CR111">111</CitationRef>, <CitationRef CitationID="CR115">115</CitationRef>] extracting representative features from raw video data before the mining process and integrating features obtained from multiple modalities. But still, it is in need of improved methods for the retrieval and mining process. Some of the current directions in mining video data include [<CitationRef CitationID="CR93">93</CitationRef>] extracting data and/or metadata from the video databases, storing the extracted data in structured databases, and applying data-mining tools to the structured video databases, integrating data-mining techniques with the information retrieval tools and developing data-mining tools to operate directly on the unstructured video databases.</Para>
              </Section2>
              <Section2 ID="Sec12">
                <Heading>Video data mining</Heading>
                <Para>Video mining can be defined as the unsupervised discovery of patterns in an audio-visual content [<CitationRef CitationID="CR23">23</CitationRef>]. The temporal (motion) and spatial (color, texture, shapes and text regions) features of the video can be used for the task mining.</Para>
                <Para>Oh and Bandi [<CitationRef CitationID="CR72">72</CitationRef>] proposed a framework for real time video data mining for the raw videos which is shown in Fig. <InternalRef RefID="Fig4">4</InternalRef>. In the first stage the grouping of input frames takes place to a set of basic units. In the second stage it extracts some of the features from each segment. In the third stage, the decomposed segments are clustered into similar groups. The next two are the actual mining of the raw video sequences and the video data compression for the storage of these raw videos. The knowledge and patterns can discover and detect the object identification, modeling and detection of normal and abnormal events, video summarization, classification and retrieval.<Figure Category="Standard" Float="Yes" ID="Fig4">
                    <Caption Language="En">
                      <CaptionNumber>Fig. 4</CaptionNumber>
                      <CaptionContent>
                        <SimplePara>General framework for video data mining</SimplePara>
                      </CaptionContent>
                    </Caption>
                    <MediaObject ID="MO4">
                      <ImageObject Color="BlackWhite" FileRef="MediaObjects/13735_2012_16_Fig4_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                  </Figure>
                </Para>
                <Para>Oh and Bandi [<CitationRef CitationID="CR72">72</CitationRef>] and Su et al. [<CitationRef CitationID="CR91">91</CitationRef>] proposed a multi- level hierarchical clustering approach to group segments with similar categories at the top level and similar motions at the bottom level using K-Means algorithm and cluster validity method.</Para>
              </Section2>
              <Section2 ID="Sec13">
                <Heading>Audio data mining</Heading>
                <Para>Audio is what plays a significant role in the detection and recognition of events in video. Supplying speech, music and various special sounds and can be used to separate different speeches, detect various audio events, analyze for spoken text, emotions, high-light detection in sports videos and so on. In movies, the audio is often correlated with the scene [<CitationRef CitationID="CR82">82</CitationRef>]. For instance, the shots of fighting and explosions are mostly accompanied by a sudden change in the audio level.</Para>
                <Para>In addition, audio features can be used to characterize the media signals to discriminate between music and speech classes. In general, the audio features [<CitationRef CitationID="CR86">86</CitationRef>] can be categorized into two groups, namely, time domain features which include zero-crossing rates, amplitudes, pitches and the frequency domain features consisting of spectrograms, cepstral coefficients and mel-frequency cepstral coefficients. There are two main approaches to audio data mining [<CitationRef CitationID="CR53">53</CitationRef>]. Firstly, Text-based indexing approach converts speech to text and then identifies words in a dictionary having several hundred thousand entries. If a word or name is not in the dictionary, the Large Vocabulary Continuous Speech Recognizers system will choose the most similar word it can find. Secondly, phoneme-based indexing approach analyzes and identifies sounds in a piece of audio content to create a phonetic-based index. It then uses a dictionary of several dozen phonemes to convert a user’s search term to the correct phoneme string. Finally, the system looks for the search terms in the index.</Para>
                <Para>Zhou et al. [<CitationRef CitationID="CR110">110</CitationRef>] proposed the data-mining models for detecting errors in Dictation Speech Recognition. They presented three popular data-mining techniques for detecting errors, including Naive Bayes, Neural Networks and Support Vector Machines. Doudpota and Guha [<CitationRef CitationID="CR25">25</CitationRef>] proposed a system to automatically locate and extract songs from digitized movies. Subsequently a song grammar was proposed and used to construct a probabilistic timed automaton to differentiate songs. Audio classification is being used in various applications such as musical instrument sound identification, music retrieval or personal recognition. Okada et al. [<CitationRef CitationID="CR76">76</CitationRef>] introduced the rule-based classification method for multi-class audio data. Audio power spectra of multiclasses are transformed into a transaction database that includes a “class label item” in each transaction. Classification rules are extracted by an exhaustive search of the closed itemsets and the greedy rule-selection approach. Chen et al. [<CitationRef CitationID="CR13">13</CitationRef>] proposed an unsupervised technique of discovering commercials by mining repeated sequence in audio stream.</Para>
              </Section2>
              <Section2 ID="Sec14">
                <Heading>Video text mining</Heading>
                <Para>It is the video texts which highlight the important events in the video database such as the speech transcriptions and sports overlay/sliding texts that are the information source and exploiting several information extraction techniques to arrive at the representative semantic information. Being useful in the labeling of the videos [<CitationRef CitationID="CR63">63</CitationRef>] the video text can be obtained from three sources: scene text, superimposed text and automatic speech recognition [<CitationRef CitationID="CR59">59</CitationRef>]. Scene text occurs as a natural part of the actual scene captured by the camera. The examples are, billboards, text on vehicles, and writings on human clothes; Super imposed text is mechanically added text to the video frames in order to supplement the visual and audio content providing additional information for a better understanding of the video and Automatic speech recognition converts speech to text is then mined. Nemrava et al. [<CitationRef CitationID="CR69">69</CitationRef>] presented the semantic multimedia annotation and indexing with the use of the video textual resources. Kucuk and Yacici [<CitationRef CitationID="CR46">46</CitationRef>] proposed a text-based fully automated system for the semantic annotation and retrieval of the news videos exploiting a wide range of information extraction techniques including the named entity recognition, automatic hyper-linking, person entity extraction with co-reference resolution and semantic event extraction.</Para>
              </Section2>
            </Section1>
            <Section1 ID="Sec15">
              <Heading>Video data mining approaches</Heading>
              <Para>Recently, there has been a trend of employing various data-mining approaches [<CitationRef CitationID="CR61">61</CitationRef>, <CitationRef CitationID="CR72">72</CitationRef>, <CitationRef CitationID="CR91">91</CitationRef>] in exploring knowledge from the video database. Consequently, many video mining approaches have been proposed which can be roughly classified into five categories. They are: Video pattern mining, Video clustering and classification, Video association mining, Video content structure mining and Video motion mining.</Para>
              <Section2 ID="Sec16">
                <Heading>Video structure mining</Heading>
                <Para>Since, video data is a kind of unstructured stream an efficient access to video is not an easy task. Therefore the main objective of the video structure mining is the identification of the content structure and patterns to carry out the fast random access of the video database.</Para>
                <Para>As video structure represents the syntactic level composition of the video content, its basic structure [<CitationRef CitationID="CR29">29</CitationRef>] is represented as a hierarchical structure constituted by the video program, scene, shot and key-frame as shown in Fig. <InternalRef RefID="Fig1">1</InternalRef>. Video structure mining is defined as the process of discovering the fundamental logic structure from the preprocessed video program adopting data-mining method such as classification, clustering and association rule.</Para>
                <Para>It is essential to analyze video content semantically and fuse multi-modality information to bridge the gap between human semantic concepts and computer low-level features from both the video sequences and audio streams. Video structure mining gets not only the video content constructing patterns but also the semantic information among the constructive patterns [<CitationRef CitationID="CR20">20</CitationRef>]. Video structure mining is executed in the following steps [<CitationRef CitationID="CR115">115</CitationRef>]: (1) video shot detection, (2) scene detection, (3) scene clustering, and (4) event mining.</Para>
                <Para>Fu et al. [<CitationRef CitationID="CR29">29</CitationRef>] defined two kinds of structural knowledge, namely, video semantic and syntactic structure knowledge leading to the concepts of video semantic and syntactic structure mining. Syntactic structure mining is based on the video basic structure which adopts the methods of data mining according to the similar video units and video unit features. It acquires some syntactic rules in general, including dialogue, interview, news, talk show and so on. These video syntactic rules are structural knowledge triggering the process that mines constructional patterns in the video structure units, and explores relations between video units and features. Semantic structure mining is a process that discovers semantics and events in video basic structure units. The basic structure units explore the relations between video unit features and features such as color and texture pattern in the explosion scene, light and texture pattern in indoor or outdoor scene, audio pattern in highlight scene and so on. These relations are represented by association rules between video unit feature(s) and feature(s).</Para>
                <Para>The current researches on it focus on mining object semantic information and event detection. The video event represents the occurrences of certain semantic concepts. Chen et al. [<CitationRef CitationID="CR12">12</CitationRef>, <CitationRef CitationID="CR14">14</CitationRef>, <CitationRef CitationID="CR15">15</CitationRef>] presented a video event detection framework that is shot-based, following the three-level architecture and proceeding the low-level descriptor extraction, mid-level descriptor extraction, and high-level analysis. Heuristic rules can be used to partially bridge the semantic gap between the low-level features and the high level subjective concepts. The decision tree logic data classification model algorithm is then performed upon the combination of multimodal mid-level descriptors and the low-level feature descriptors for event detection. Zhao et al. [<CitationRef CitationID="CR114">114</CitationRef>] proposed the Hierarchical Markov Model Mediator mechanism to efficiently store, organize, and manage the low-level features, multimedia objects, and semantic events along with the high-level user perceptions such as user preferences in the multimedia database management system.</Para>
              </Section2>
              <Section2 ID="Sec17">
                <Heading>Video clustering and classification</Heading>
                <Para>Video clustering and classification are used to cluster and classify video units into different categories. Therefore clustering is a significant unsupervised learning technique for the discovery of certain knowledge from a dataset. Clustering video sequences in order to infer and extract activities from a single video stream is an extremely important problem and so it has a significant potential in video indexing, surveillance,activity discovery and event recognition [<CitationRef CitationID="CR97">97</CitationRef>, <CitationRef CitationID="CR103">103</CitationRef>]. In the video surveillance systems, it is to find the patterns and groups of moving objects that the clustering analysis is used. Clustering similar shots into one unit eliminates redundancy and as a result, produces a more concise video content summary [<CitationRef CitationID="CR116">116</CitationRef>, <CitationRef CitationID="CR117">117</CitationRef>]. Clustering algorithms are categorized into partitioning methods, hierarchical methods, density-based methods, grid based methods and model-based methods.</Para>
                <Para>Vailaya et al. [<CitationRef CitationID="CR99">99</CitationRef>] proposed a method to cluster the video shots based on the key frames representing the shots. Figure <InternalRef RefID="Fig5">5</InternalRef> shows a block diagram of the general problem of video clustering. It is detecting by the shots from the video frame sequence that the key frames are extracted. Next, a feature vector is computed so that the key frames can be clustered based on the feature vector assigning the semantic interpretations to various clusters at the last stage. These semantic interpretations are used in the retrieval system to index and browse the video database. At the clustering stage, it is desirable to cluster the shots into semantic categories such as presence/absence of buildings, texts, specific texture and so on, so that a higher level abstract (semantic) label can be assigned to the shots (indoor shots, outdoor shots, city shots, beach shots, landscape shots).<Figure Category="Standard" Float="Yes" ID="Fig5">
                    <Caption Language="En">
                      <CaptionNumber>Fig. 5</CaptionNumber>
                      <CaptionContent>
                        <SimplePara>Video clustering</SimplePara>
                      </CaptionContent>
                    </Caption>
                    <MediaObject ID="MO5">
                      <ImageObject Color="BlackWhite" FileRef="MediaObjects/13735_2012_16_Fig5_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                  </Figure>
                </Para>
                <Para>Video classification aims at grouping videos together with similar contents and to disjoin videos with non-similar contents and thus categorizing or assigning class labels to a pattern set under the supervision. It is the primary step for retrieval and the classification approaches are those techniques that split the video into predefined categories. Semantic video classification approaches [<CitationRef CitationID="CR26">26</CitationRef>] can be classified into two categories. First, rule-based approach that uses domain knowledge to define the perceptional rules and achieve semantic video classification and easy to insert, delete and modify the existing rules when the nature of the video classes changes. It is attractive only for the video domains such as news and films that have well-defined story structures for the semantic units (i.e., film and news making rules). Second, the statistical approach that uses statistical machine learning to bridge the semantic gap. This supports more effective semantic video classification by discovering non-obvious correlations (i.e., hidden rules) among different video patterns.</Para>
                <Para>The key features are used to categorize video into predefined genres. Video classifications are based on the spatial and temporal characteristics and necessary for efficient access, understanding and retrieval of the videos. Pan et al. [<CitationRef CitationID="CR77">77</CitationRef>] proposed a video graph tool for video mining and visualizing the structure of the plot of a video sequence. The video graph of the video clip is the directed graph where every node corresponds to a shot group and edges indicate temporal succession. This algorithm is used to “stitch” together similar scenes even if they are not consecutive and automatically derive video graphs. It derives the number of recurrent shot groups for video mining and classification, distinguishing between different video types, e.g., news stories versus commercials.</Para>
                <Para>Pan et al. [<CitationRef CitationID="CR78">78</CitationRef>] presented a video cube tool to classify a video clip into one of ‘n’ given classes (e.g., “news”, “commercials”, etc) which automatically derive a “vocabulary” from each class of the video clips, using the “Independent Component Analysis” incorporating the spatial and temporal information which works on both video and audio information. It creates a vocabulary that describes images, motions and the audio parts of the video and thus provides a way to automatically extract features. The video and audio features reveal the essential characteristics of a genre class and are closely related to the neural signals used in the human perceptual press. VCube algorithm uses the video bases of genre classes to classify a video clip and the audio bases to classify the clips based on their audio information.</Para>
                <Para>Building an activity recognition and classification system is a challenging task because of the variations in the environment, objects and actions. Variations in the environment can be caused by cluttered or moving background, camera motion, occlusion, weather and illumination changes while Variations in the objects are because of the differences in appearance, size or posture of the objects or because of self motion which is not a part of the activity and variations in the action can make it difficult to recognize semantically equivalent actions as such, for example imagine the many ways to jump over an obstacle or different ways to throw a stick.</Para>
                <Para>Nowozin et al. [<CitationRef CitationID="CR71">71</CitationRef>] proposed a classifier for the sequence representations for the action classification in the videos that retains the temporal order in a video. They first proposed the LPBoost classifier for sequential representations, and then, the discriminative PrefixSpan subsequence mining algorithm to find the optimal discriminative subsequent patterns. Brezeale et al. [<CitationRef CitationID="CR7">7</CitationRef>] came out with a survey on video classification. They found that features are drawn from three modalities divided into four groups of automatic classification of the video such as text-based approaches, audio based approaches, visual-based approaches, and the combination of the text, audio, and visual features. Tien et al. [<CitationRef CitationID="CR96">96</CitationRef>] extracted the high-level audiovisual features to describe the video segments which are further transformed to symbolic streams and an efficient mining technique was applied to derive all frequent patterns that characterize tennis events. After mining, they categorized the frequent patterns into several kinds of events and thus achieved event detection for tennis videos by checking the correspondence between mined patterns and events.</Para>
              </Section2>
              <Section2 ID="Sec18">
                <Heading>Video association mining</Heading>
                <Para>Video association mining is the process of discovering associations in a given video. The video knowledge is explored in a two stages, the first being the video content processing in which the video clip is segmented into certain analysis units extracting their representative features and the second being the video association mining that extracts the knowledge from the feature descriptors. Mongy et al. [<CitationRef CitationID="CR66">66</CitationRef>] presented a framework for video usage mining to generate user profiles on a video search engine in the context of movie production that analyzes the user behaviors on a set of video data to create suitable tools to help people in browsing and searching a large amount of video data.</Para>
                <Para>In video association mining, the video processing and the existing data-mining algorithms are seamlessly integrated into mine video knowledge. Zhu et al. [<CitationRef CitationID="CR111">111</CitationRef>] proposed a multilevel sequential association mining to explore the associations between the audio and visual cues and classified the associations by assigning each of them with a class label using their appearances in the video to construct video indices. They integrated the traditional association measures (support and confidence) and the video temporal information to evaluate video associations.</Para>
                <Para>Sivaselvan et al. [<CitationRef CitationID="CR89">89</CitationRef>] presented a video association mining consisting of two key phases. First, the transformation phase converts the original input video into an alternate transactional format, namely, a cluster sequence. Second the frequent temporal pattern mining phase that is concerned with the generation of the patterns subject to the temporal distance and support thresholds.</Para>
                <Para>Lin et al. [<CitationRef CitationID="CR55">55</CitationRef>] developed a video semantic concept discovery framework that utilizes multimodal content analysis and association rule mining technique to discover the semantic concepts from video data. The framework used the apriori algorithm and association rule mining to find the frequent item-sets in the feature data set and generated the classification rules to classify the video shots into different concepts (semantics). Chen and Shyu [<CitationRef CitationID="CR14">14</CitationRef>] proposed a hierarchical temporal association mining approach that integrates the association rule mining and the sequential pattern discovery to systematically determine the temporal patterns for target events. Goyani et al. [<CitationRef CitationID="CR33">33</CitationRef>] proposed an A-priori algorithm to detect the semantic concepts from the cricket video. Initially, a top-down event detection and classification was performed using the hierarchical tree. Then the higher level concept was identified by applying A-Priori algorithm. Maheshkumar [<CitationRef CitationID="CR58">58</CitationRef>] proposed a method that automatically extracts silent events from the video and classifies each event sequence into a concept by sequential association mining. A hierarchical framework was used for soccer (football) video event sequence detection and classification. The association for the events of each excitement clip was computed using an a priori mining algorithm using the sequential association distance to classify the association of the excitement clip into semantic concepts.</Para>
                <Para>Lin and Shyu [<CitationRef CitationID="CR52">52</CitationRef>] proposed weighted association rule mining algorithm able to capturing the different significant degrees of the items (feature-value pairs) and generating the association rules for video semantic concept detection shown in Fig. <InternalRef RefID="Fig6">6</InternalRef>. The framework first applies multiple correspondence analyses to project the features and classes into a new principle component space and discovers the correlation between feature-value pairs and classes. Next, it considered both correlation and percentage information as the measurement to weight the feature-value pairs and generate the association rules. Finally, it performs classification by using these weighted association rules.<Figure Category="Standard" Float="Yes" ID="Fig6">
                    <Caption Language="En">
                      <CaptionNumber>Fig. 6</CaptionNumber>
                      <CaptionContent>
                        <SimplePara>Semantic concept detection</SimplePara>
                      </CaptionContent>
                    </Caption>
                    <MediaObject ID="MO6">
                      <ImageObject Color="BlackWhite" FileRef="MediaObjects/13735_2012_16_Fig6_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                  </Figure>
                </Para>
                <Para>Kea et al. [<CitationRef CitationID="CR47">47</CitationRef>] developed a method based on the frequent pattern tree (FPTree) for mining association rules in video retrieval. The proposed temporal frequent pattern tree growth algorithm mine temporal frequent patterns from TFPTree for finding the rules of the motion events.</Para>
              </Section2>
              <Section2 ID="Sec19">
                <Heading>Video motion mining</Heading>
                <Para>Motion is a key feature that essentially characterizes the contents of the video, representing the temporal information of videos and more objective and consistent compared to other features such as color, texture and so on. There have been some approaches to extract camera motion and motion activity in video sequences. While dealing with the problem of object tracking, algorithms are always proposed on the basis of known object region in the frames and so the most challenging problem in the visual information retrieval is the recognition and detection of the objects in the moving videos. The camera motion having a vital role to play some of the key issues in video motion detections are, the camera placed in static location while the objects are moving (surveillance video, sports video); the camera is moving with moving objects (movie); multiple cameras are recording the same objects. The camera motion itself contains a copious knowledge related to the action of the whole match. The important types of camera motion are Pan (left and right), Zoom (in and out), Tilt (up and down), and Unknown (camera motions those are not Pan, Zoom, or Tilt are grouped to Unknown).</Para>
                <Para>Wu et al. [<CitationRef CitationID="CR104">104</CitationRef>] proposed the extraction scheme of the global motion and object trajectory in a video shot for content-based video retrieval. For instance, while browsing the video obtained by surveillance system or watching sports programs, the user always has the need to find out the object moving in some special direction. Zang and Klette [<CitationRef CitationID="CR112">112</CitationRef>] proposed an approach for extraction of a (new) moving object from the background and tracking of a moving object.</Para>
                <Para>Mining patterns from the movements of moving objects is called motion mining. First, the features are extracted (physical, visual and aural, motion features) using objects detection and tracking algorithms and then the significations of the features, trends of moving object activities and patterns of events are mined by computing association relations and spatial-temporal relations among the features.</Para>
              </Section2>
              <Section2 ID="Sec20">
                <Heading>Video pattern mining</Heading>
                <Para>Video pattern mining detects the special patterns modeled in advance and usually characterized as video events such as dialogue, or presentation events in medical video. The existing work can be divided into two categories such as mining similar motion patterns and mining similar objects [<CitationRef CitationID="CR4">4</CitationRef>].</Para>
                <Para>Sivic et al. [<CitationRef CitationID="CR90">90</CitationRef>] described a method for obtaining the principal objects, characters and scenes in a video by measuring the reoccurrence of the spatial configurations of the viewpoint invariant features has three stages: The first stage extracts the neighborhoods occurring in more than a minimum number of key frames considered for clustering, where as the second stage matches the significant neighborhoods using a greedy progressive clustering algorithm, and in the third stage, the resulting clusters are merged based both on spatial and temporal overlap. Burl et al. [<CitationRef CitationID="CR8">8</CitationRef>] presented an algorithm to extract information from raw, surveillance-style video of an outdoor scene containing a mix of people, bicycles, and motorized vehicles. A feature extraction algorithm based on the background estimation and subtraction followed by spatial clustering and multi-object tracking was used to process sequences of video frames into a track set, which encodes the positions, velocities, and the appearances of the various objects as the function of time are mined to answer the user-generated queries. Lai et al. [<CitationRef CitationID="CR50">50</CitationRef>] proposed a motion model that enables to measure the similarities among different animal movements in high precision. A clustering method can separate the recurring movements from the infrequent random movements.</Para>
                <Para>Fleischman et al. [<CitationRef CitationID="CR28">28</CitationRef>] presented an approach in which the temporal information is captured by representing events using a lexicon of hierarchical patterns of human movement that are mined from a large corpora of un-annotated video data. These patterns are used as features for a discriminative model of event classification that exploits tree kernels in a Support Vector Machine. The second category systems aim at grouping frequently appearing objects in videos. Therefore, it is useful to have commonly occurring objects/characters/scenes for various applications [<CitationRef CitationID="CR90">90</CitationRef>]. There is a number of applications: First, they provide entry points for visual search in video databases. Second, they can be used in forming video summaries—the basic elements of a summary often involve the commonly occurring objects and these are then displayed as a storyboard. The third application area is in detecting product placements in a film where the frequently occurring logos or labels are prominent. Mining repeated short clips from video collections and streams are essential for video syntactic segmentation, television broadcast monitoring, commercial skipping, content summary and personalization, as well as video redundancy detection and many other applications.</Para>
                <Para>Xie and Chang [<CitationRef CitationID="CR106">106</CitationRef>] investigated the pattern mining strategies in video streams. They applied different pattern mining models (deterministic and statistic; static and temporal) and devised pattern combination strategies for generating a rich set of pattern hypothesis. Some of the statistical clustering method such as K-means, HMM, HHMM and Deterministic algorithms were considered for video clustering. Yang et al. [<CitationRef CitationID="CR108">108</CitationRef>] proposed a method to repeat the clip mining and the knowledge discovery from the video data. The mining framework unifies to detect both the unknown video repeats and the known video clips of the arbitrary length by the same feature extraction and matching process. The structure analysis method is effective in discovering and modeling the syntactic structure of the news videos and their main objective is to detect the unknown video repeats from the video stream without prior knowledge. Su et al. [<CitationRef CitationID="CR91">91</CitationRef>] presented a method to achieve an effective content-based video retrieval by mining the temporal patterns in the video contents. It was the construction of a special index on video temporal patterns for an efficient retrieval (Fast-Pattern- Index tree) and a unique search strategy for effective retrieval (Pattern-based Search).</Para>
              </Section2>
            </Section1>
            <Section1 ID="Sec21">
              <Heading>Video data mining applications</Heading>
              <Para>The fact that video data are used in many different areas such as sports, medicine, traffic and education programs, shows how significant it is. The potential applications of video mining include annotation, search, mining of traffic information, event detection / anomaly detection in a surveillance video, pattern or trend analysis and detection. There are four types of videos [<CitationRef CitationID="CR63">63</CitationRef>, <CitationRef CitationID="CR72">72</CitationRef>] in our daily life, namely, (a) produced video, (b) raw video, (c) medical video, and (d) broadcast or prerecorded video.</Para>
              <Section2 ID="Sec22">
                <Heading>Produced video data mining</Heading>
                <Para>A produced video is meticulously produced according to a script or plan that is later edited, compiled and distributed for consumption. News videos, dramas, and movies are examples of the produced video with an extremely strong structure but has tremendous intra-genre variation in production styles that vary from country to country or content-creator to content-creator. The success and significance of the video mining depends on the content genre [<CitationRef CitationID="CR23">23</CitationRef>, <CitationRef CitationID="CR24">24</CitationRef>].</Para>
                <Para>For news videos detection of story boundaries either by closed caption and/or speech transcript analysis or by using speaker segmentation and face information have been proved effective whereas for movie contents, the detection of syntactic structures like two-speaker dialogues and also detection of specific events like explosions have been proved immensely useful and for situation comedies, the detection of physical setting using mosaic representation of a scene and the detection of the major cast using audio-visual cues have also been beneficial.</Para>
                <Para>Shirahama et al. [<CitationRef CitationID="CR88">88</CitationRef>] focused on the rhythm in a movie, consisting of the durations of the target character’s appearance and disappearance. Based on this rhythm, they divided the movie into topics, each topic corresponding to one meaningful episode of the character. By investigating such topics, they discovered immensely useful editing patterns of character’s rhythm supported by their semantic features. These rhythms can also be used to annotate certain topics. Shirahama et al. [<CitationRef CitationID="CR87">87</CitationRef>] proposed a video data-mining approach with temporal constraint for extracting the previously unpredictable semantic patterns from a movie. First, having transformed a movie of an unstructured raw material into a multi-stream of raw level metadata, they extracted the sequential patterns then from the multi-stream of raw level metadata using a parallel data mining.</Para>
                <Para>Rasheed et al. [<CitationRef CitationID="CR81">81</CitationRef>] proposed a method to classify movies into four broad categories such as Comedies, Action, Dramas and Horror Films. The video features such as average shot length, color variance, motion content and lighting key are combined in a framework to provide a mapping to these four high-level semantic classes.</Para>
                <Para>Huangy et al. [<CitationRef CitationID="CR35">35</CitationRef>] presented the film classification method that consists of three steps as shown in Fig. <InternalRef RefID="Fig7">7</InternalRef>. Boundary detection being the first step, they analyzed the color, motion and brightness from every shot, and represented the shot with these low-level features in the second step. The final step is the classification process, as there are many classification methods, like K-mean, classification tree, mean-shift, ada-boost, neural network and so on. Generally, these methods can distinguish between the supervised and un-supervised classes. In the future, the classification result can be improved by combining audio or text cues.<Figure Category="Standard" Float="Yes" ID="Fig7">
                    <Caption Language="En">
                      <CaptionNumber>Fig. 7</CaptionNumber>
                      <CaptionContent>
                        <SimplePara>Movie classification</SimplePara>
                      </CaptionContent>
                    </Caption>
                    <MediaObject ID="MO7">
                      <ImageObject Color="BlackWhite" FileRef="MediaObjects/13735_2012_16_Fig7_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                  </Figure>
                </Para>
                <Para>There is a lot of editing patterns available in video editing which is a process of selecting and joining various shots to create final video sequence. Discovering the editing patterns, the video material is edited to precisely convey the editor’s intention to a viewer by using a universal rule called video grammar.</Para>
                <Para>Matsuo et al. [<CitationRef CitationID="CR61">61</CitationRef>] proposed the methods to automatically extract editing rules specific to each video by using the data-mining technique. Two approaches are used to detect the editing patterns in a video stream. They are (i) The extraction of patterns from a multi-symbol stream, and (ii) the extraction of periodic patterns in time-series data.</Para>
              </Section2>
              <Section2 ID="Sec23">
                <Heading>Raw video data mining</Heading>
                <Para>There are two common types of surveillance video used in the real world applications such as the security video generally used for property or public areas and the monitoring video used to monitor the traffic flow. The surveillance systems with data-mining techniques are investigated to find out suspicious people capable of indulging in abnormal activities. However, the captured video data are commonly stored or previewed by operators to find abnormal moving objects or events. The identification of the patterns existing in surveillance applications, building the supervised models and the abnormal event detection are risky tasks [<CitationRef CitationID="CR22">22</CitationRef>]. The semantic events are the incidents captured by the surveillance video on the road, such as car crash, bumping, U-turn and speeding.</Para>
                <Para>Chen et al. [<CitationRef CitationID="CR17">17</CitationRef>] proposed a video data-mining framework that analyzes the traffic video sequences by using background subtraction, image/video segmentation, object tracking, and modeling with multimedia augmented transition network model and multimedia input strings, in the domain of traffic monitoring over an intersection. Dai et al. [<CitationRef CitationID="CR20">20</CitationRef>, <CitationRef CitationID="CR22">22</CitationRef>] proposed a surveillance video data-mining framework of motion data that discovers the similar video segments from surveillance video through a probabilistic model. A mixture of hidden Markov models using an expectation-maximization scheme is fitted to the motion data to identify the similar segments. These surveillance systems with data mining techniques are being investigated to find out suspicious people capable of carrying out terrorist activities.</Para>
                <Para>The collected raw video data in the traffic databases applications cannot provide organized, unsupervised, conveniently accessible and easy-to-use multimedia information to the traffic planners. The analysis and mining of traffic video sequences to discover information such as vehicle identification, traffic flow and the spatio-temporal relations of the vehicles at intersections provides an economic approach for daily traffic operations. In order to discover and provide some important but previously unknown knowledge from the traffic video sequences for the traffic planners, video data-mining techniques need to be employed.</Para>
                <Para>Choudhary et al. [<CitationRef CitationID="CR18">18</CitationRef>] proposed a framework for automated analysis of the surveillance videos using cluster algebra to mine various combinations of patterns from the component summaries (such as time, size, shape, position of objects, etc.) to learn the usual patterns of events and discover unusual ones. Praveenkumar et al. [<CitationRef CitationID="CR49">49</CitationRef>] proposed a framework to discriminate between normal and abnormal event in a surveillance video as shown in Fig. <InternalRef RefID="Fig8">8</InternalRef>. In the framework, the audio-visual features are extracted from the incoming data stream and the resultant real valued feature data is binarized. A feature selection process based on association rule mining has selected highly discriminant features. A short representative signature of the whole database is generated using a novel reservoir sampling algorithm stored in binary form and used with a Support Vector Classifier to help discriminate events as normal or abnormal event.<Figure Category="Standard" Float="Yes" ID="Fig8">
                    <Caption Language="En">
                      <CaptionNumber>Fig. 8</CaptionNumber>
                      <CaptionContent>
                        <SimplePara>Abnormal detection in a surveillance video</SimplePara>
                      </CaptionContent>
                    </Caption>
                    <MediaObject ID="MO8">
                      <ImageObject Color="BlackWhite" FileRef="MediaObjects/13735_2012_16_Fig8_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                  </Figure>
                </Para>
                <Para>Jiang et al. [<CitationRef CitationID="CR74">74</CitationRef>] proposed a context-aware method to detect anomalies. By tracking all the moving objects in the video, three different levels of spatiotemporal contexts are considered, i.e., point anomaly of a video object, sequential anomaly of an object trajectory, and co-occurrence anomaly of multiple video objects. A hierarchical data-mining approach was proposed. At each level, frequency-based analysis was performed to automatically discover regular rules of normal events and thus events deviating from these rules are identified as anomalies.</Para>
                <Para>Gowsikhaa et al. [<CitationRef CitationID="CR30">30</CitationRef>] proposed a method to detect suspicious activities such as object exchange, entry of new person, peeping into other’s answer sheet and person exchange from the video captured by a surveillance camera during examinations based on the face and hand recognition. Figure <InternalRef RefID="Fig9">9</InternalRef> illustrates the brief design of Human Activity Recognition system.<Figure Category="Standard" Float="Yes" ID="Fig9">
                    <Caption Language="En">
                      <CaptionNumber>Fig. 9</CaptionNumber>
                      <CaptionContent>
                        <SimplePara>Human activity recognition system</SimplePara>
                      </CaptionContent>
                    </Caption>
                    <MediaObject ID="MO9">
                      <ImageObject Color="BlackWhite" FileRef="MediaObjects/13735_2012_16_Fig9_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                  </Figure>
                </Para>
                <Para>Anwar et al. [<CitationRef CitationID="CR1">1</CitationRef>] presented a framework to discover the unknown anomalous events associated with a frequent sequence of events that is to discover events, unlikely to follow a frequent sequence of events in surveillance videos.</Para>
              </Section2>
              <Section2 ID="Sec24">
                <Heading>Medical video mining</Heading>
                <Para>Audio and video processing is integrated to mine the medical event information such as dialog, presentation and clinical operation from the detected scenes in a medical video database.</Para>
                <Para>Zhu et al. [<CitationRef CitationID="CR116">116</CitationRef>] developed a video content structure and event-mining framework to achieve more efficient video indexing and access for medical videos. Visual and audio feature processing techniques are utilized to detect some semantic cues such as slides, face and speaker changes, within the video and these detection results are put together to mine three types of events (presentation, dialogue, clinical operation) from the detected video scenes. Zhang et al. [<CitationRef CitationID="CR109">109</CitationRef>] presented an intra-vital video mining system of leukocytes rolling and adhesion. Video mining of vivo microscopy video sequences is very difficult because of severe noises, background movements, leukocytes deformations, and contrast changes. Aligning the video frames to eliminate the noises caused by camera movements, they located the moving leukocytes by applying and comparing a spatiotemporal probabilistic learning method and a neural network framework for time series data. Finally, they removed the noises by applying the median and location-based filtering. They proposed a new method for the automatic recognition of the non-adherent and adherent leukocytes.</Para>
              </Section2>
              <Section2 ID="Sec25">
                <Heading>Broadcast or prerecorded video mining</Heading>
                <Para>Broadcast video can be regarded as being made up of genre (set of video documents sharing similar style). The genre of a video is the broad class to which it may belong to e.g. sports, news and cartoon and so on. The content of broadcast video can be conceptually divided into two parts. First, the semantic content, the story line told by the video. This is split into genre, events and objects. Second, inherent properties of the digital media video termed as editing effects.</Para>
                <Para>In broadcast video data (unscripted content), such as sports video and meetings video, the events happen spontaneously. Data mining can be used by sports organizations in the form of statistical analysis and pattern discovery as well as outcome prediction. Although sports videos (non-edited) are considered as non-scripted, they usually have a relatively well-defined structure (such as the field scene) or repetitive patterns (such as a certain play type) helping us enhances the scriptedness of sports videos for more versatile and flexible access. A sports game usually occurs in one specific playfield but it is often recorded by a number of cameras with fixed positions also.</Para>
                <Para>In sports video data, Mosaic is generated for each shot as the representative image of the shot content [<CitationRef CitationID="CR62">62</CitationRef>] such a mosaic based approach provides two kinds of mining methods: unsupervised mining of the structure without prior knowledge, and supervised mining of key-events with domain knowledge. Without prior knowledge, the play is mined by unsupervised clustering on mosaics as well as a voting process. With prior knowledge, the key-events are mined using Hidden Markov Models.</Para>
                <Para>Chen et al. [<CitationRef CitationID="CR16">16</CitationRef>] proposed sports video mining framework (Fig. <InternalRef RefID="Fig10">10</InternalRef>) and first analyzed the soccer videos by using joint visual and audio features. Then the data pre-filtering step was performed on raw video features with the aid of domain knowledge and classification rules were used to extract the goal events. It can be used for the high-level indexing and selective browsing of soccer videos.<Figure Category="Standard" Float="Yes" ID="Fig10">
                    <Caption Language="En">
                      <CaptionNumber>Fig. 10</CaptionNumber>
                      <CaptionContent>
                        <SimplePara>Sports video mining</SimplePara>
                      </CaptionContent>
                    </Caption>
                    <MediaObject ID="MO10">
                      <ImageObject Color="BlackWhite" FileRef="MediaObjects/13735_2012_16_Fig10_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                  </Figure>
                </Para>
                <Para>Tien et al. [<CitationRef CitationID="CR94">94</CitationRef>] proposed a mining-based method to achieve event detection for broadcasting tennis videos. Initially extracting some high-level features to describe video segments, they further transformed the audiovisual features into symbolic streams and an efficient Max-Sub pattern Tree mining technique to derive all frequent patterns that characterize tennis events. After mining, they categorized frequent patterns into several kinds of events and thus achieved event detection for tennis videos by checking the correspondence between mined patterns and events.</Para>
                <Para>Ding and Fan [<CitationRef CitationID="CR19">19</CitationRef>] presented a multichannel segmental hidden Markov model for sports video mining that incorporates ideas from Coupled HMM, segmental HMM, and hierarchical HMMs. This model offers more flexibility, functionality and capacity than its precedents in two aspects.</Para>
                <Para>Harikrishna et al. [<CitationRef CitationID="CR36">36</CitationRef>] presented an approach for classification of events in cricket videos and summarized its visual content. Using sequential pattern mining and support vector machine they classified the sequence of shots into four events, namely, RUN, FOUR, SIX and OUT.</Para>
              </Section2>
            </Section1>
            <Section1 ID="Sec26">
              <Heading>Key accomplishments of video mining</Heading>
              <Para>Video mining techniques are used for automatically generating critical decisions in numerous current real world problems. Video data mining provides the critical data needed to verify strategies for product placement, product assortment and cross merchandising. The key problem addressed includes view-independent person detection, multi-person tracking, a method for specifying behaviors, and robust behavior recognition. The approach is to use a variety of computer vision and statistical learning techniques under the constraints of a typical retail environment. The extremely crowded scenes pose unique challenges to video analysis that cannot be addressed with conventional approaches. The key insight is to exploit the dense activity of the crowded scene by modeling the rich motion patterns in local areas, effectively capturing the underlying intrinsic structure they form in the video.</Para>
              <Para>Video semantic event detection has become more and more attractive in recent years such as video surveillance [<CitationRef CitationID="CR113">113</CitationRef>], sports highlights detection [<CitationRef CitationID="CR36">36</CitationRef>], TV/Movie abstraction and home video retrieval [<CitationRef CitationID="CR31">31</CitationRef>] and so on. Mining the television programs for getting narrative style time and effect patterns of advertisements. Guha et al. [<CitationRef CitationID="CR34">34</CitationRef>] proposed the surveillance event characterization at two levels. First, a set of time-varying predicates defined on heterogeneous objects moving in unknown environments. Second, the information characterizable is used to index and retrieve the event characterizations.</Para>
              <Para>The mining and automatic analysis surveillance video extracts a valuable knowledge. The examples of knowledge and patterns that can discover and detect from a surveillance video sequence are object identification, object movement pattern recognition, spatio-temporal relations of objects, modeling and detection of normal and abnormal (interesting) events, and event pattern recognition [<CitationRef CitationID="CR74">74</CitationRef>].</Para>
              <Para>The applications of surveillance mining are detecting crowd patterns for traffic control, recognition of suspicious people in large crowds, detecting traffic patterns, monitoring of surveillance in theft or fire protection and care of bedridden patients and young children.</Para>
              <Para>Anjulan and Canagarajah [<CitationRef CitationID="CR2">2</CitationRef>] proposed a framework for performing object mining where video segmentation, feature extraction, tracking, clustering, object retrieval and mining are combined seamlessly within a framework as shown in Fig. <InternalRef RefID="Fig11">11</InternalRef>. Initially, video is segmented into a number of smaller shots, and then, the features are extracted from the shots to represent the visual information. Next, these features are grouped into clusters to represent relevant objects, each cluster approximately corresponding to an object in a shot. These object clusters may contain a number of similar instances of the same object, and these instances are grouped together in the mining stage.<Figure Category="Standard" Float="Yes" ID="Fig11">
                  <Caption Language="En">
                    <CaptionNumber>Fig. 11</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>Framework for object mining</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <MediaObject ID="MO11">
                    <ImageObject Color="BlackWhite" FileRef="MediaObjects/13735_2012_16_Fig11_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </MediaObject>
                </Figure>
              </Para>
              <Para>Oh et al. [<CitationRef CitationID="CR74">74</CitationRef>] developed a model to capture the location of motions occurring in a segment using the two dimensional matrix. The segmented pieces being clustered using the features, an algorithm is used to find whether a segment has normal or abnormal events by clustering and modeling normal events. Lee et al. [<CitationRef CitationID="CR54">54</CitationRef>] proposed a model-based conceptual clustering of moving objects in video surveillance the basis of on formal concept analysis. The formal concept analysis utilized to generate concepts, handle complicated moving objects and provides conceptual descriptions of moving object databases such as significant features and relationships. Movie data serve as a good test case of mining for knowledge consisting of some structure such as shots and scenes more than one media and structural boundaries. The knowledge can be mined for the movie such as composite structure analysis, identifying interesting events and patterns, clustering the movies, cinematic rules and rhymes of characteristic topics.</Para>
              <Para>Gilbert et al. [<CitationRef CitationID="CR32">32</CitationRef>] presented an approach for recognizing actions within movie video sequences as shown in Fig. <InternalRef RefID="Fig12">12</InternalRef>. Initially, 2D corners are detected in three orthogonal planes of the video sequence. Each corner encoded as a three-digit number denoting the spatiotemporal plane. They are used within an iterative hierarchical grouping process to form descriptive compound features. Each corner is grouped within a cuboid-based neighborhood. A set of grouped corners is called a Transaction and these are collected to form a Transaction database which is then mined for finding the most frequently occurring patterns. These patterns are descriptive, distinctive sets of corners, and are called frequent item sets which then become the basic features for mining.<Figure Category="Standard" Float="Yes" ID="Fig12">
                  <Caption Language="En">
                    <CaptionNumber>Fig. 12</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>Recognizing abnormal actions within video sequences</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <MediaObject ID="MO12">
                    <ImageObject Color="BlackWhite" FileRef="MediaObjects/13735_2012_16_Fig12_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </MediaObject>
                </Figure>
              </Para>
              <Para>Cui et al. [<CitationRef CitationID="CR9">9</CitationRef>] proposed a hierarchical visual event pattern mining framework and its applications on recognition and anomaly (both abnormal events and abnormal contexts) detection, including an approach to unsupervised primitive event categorization, an extended Sequential Monte Carlo method for primitive and compound event recognition, and a method for abnormal events and contexts detection as shown in Fig. <InternalRef RefID="Fig13">13</InternalRef>.<Figure Category="Standard" Float="Yes" ID="Fig13">
                  <Caption Language="En">
                    <CaptionNumber>Fig. 13</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>Framework for hierarchical visual event pattern mining</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <MediaObject ID="MO13">
                    <ImageObject Color="BlackWhite" FileRef="MediaObjects/13735_2012_16_Fig13_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </MediaObject>
                </Figure>
              </Para>
              <Para>In the future, video mining will continue to receive attention, especially for its application in online video sharing, security surveillance monitoring and effective image retrieval. Anwar et al. [<CitationRef CitationID="CR42">42</CitationRef>] presented a framework to discover the unknown anomalous events associated with a frequent sequence of events.</Para>
              <Para>Some research organizations are contributing much in the area of video mining such as, DIMACS Workshop [<CitationRef CitationID="CR37">37</CitationRef>], MERL [<CitationRef CitationID="CR39">39</CitationRef>] and DVMM lab of Columbia University [<CitationRef CitationID="CR38">38</CitationRef>]. However, only a limited work has been done in video data mining. Table <InternalRef RefID="Tab1">1</InternalRef> shows the summary of the video data-mining key activities.<Table Float="Yes" ID="Tab1">
                  <Caption Language="En">
                    <CaptionNumber>Table 1</CaptionNumber>
                    <CaptionContent>
                      <SimplePara>Summary of video data-mining key activities</SimplePara>
                    </CaptionContent>
                  </Caption>
                  <tgroup cols="4">
                    <colspec align="left" colname="c1" colnum="1"/>
                    <colspec align="left" colname="c2" colnum="2"/>
                    <colspec align="left" colname="c3" colnum="3"/>
                    <colspec align="left" colname="c4" colnum="4"/>
                    <thead>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>Authors</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>Year</SimplePara>
                        </entry>
                        <entry align="left" colname="c3">
                          <SimplePara>Concepts</SimplePara>
                        </entry>
                        <entry align="left" colname="c4">
                          <SimplePara>Application domain</SimplePara>
                        </entry>
                      </row>
                    </thead>
                    <tbody>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>Gowsikhaa et al. [<CitationRef CitationID="CR30">30</CitationRef>], Anwar [<CitationRef CitationID="CR1">1</CitationRef>]</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>2012</SimplePara>
                        </entry>
                        <entry align="left" colname="c3">
                          <SimplePara>Temporal association rule mining, suspicious human activity detection</SimplePara>
                        </entry>
                        <entry align="left" colname="c4">
                          <SimplePara>Security and surveillance</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>Cui et al. [<CitationRef CitationID="CR9">9</CitationRef>]</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>2011</SimplePara>
                        </entry>
                        <entry align="left" colname="c3">
                          <SimplePara>Hierarchical visual event pattern mining and anomaly detection</SimplePara>
                        </entry>
                        <entry align="left" colname="c4">
                          <SimplePara>Surveillance</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>Gilbert et al. [<CitationRef CitationID="CR32">32</CitationRef>] Harikrishna et al. [<CitationRef CitationID="CR36">36</CitationRef>], Jiang et al. [<CitationRef CitationID="CR42">42</CitationRef>]</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>2011</SimplePara>
                        </entry>
                        <entry align="left" colname="c3">
                          <SimplePara>Sequential pattern mining and support vector machine, hierarchical data mining,</SimplePara>
                        </entry>
                        <entry align="left" colname="c4">
                          <SimplePara>Movie, sports, surveillance</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>Anjulan and Canagarajah [<CitationRef CitationID="CR2">2</CitationRef>] Gaidon et al. [<CitationRef CitationID="CR31">31</CitationRef>], Ding and Fan [<CitationRef CitationID="CR19">19</CitationRef>]</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>2009</SimplePara>
                        </entry>
                        <entry align="left" colname="c3">
                          <SimplePara>Object mining, support vector machine, multichannel segmental hidden markov model</SimplePara>
                        </entry>
                        <entry align="left" colname="c4">
                          <SimplePara>Movie/TV shows, sports video</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>Tien et al. [<CitationRef CitationID="CR94">94</CitationRef>], Huangy et al. [<CitationRef CitationID="CR35">35</CitationRef>], Choudhary et al. [<CitationRef CitationID="CR18">18</CitationRef>], Praveenkumar et al. [<CitationRef CitationID="CR49">49</CitationRef>]</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>2008</SimplePara>
                        </entry>
                        <entry align="left" colname="c3">
                          <SimplePara>Symbolic streams mining, visual feature mining, association rule mining and support vector machine</SimplePara>
                        </entry>
                        <entry align="left" colname="c4">
                          <SimplePara>Sports, movie, surveillance</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>Lee et al. [<CitationRef CitationID="CR54">54</CitationRef>], Zhang et al. [<CitationRef CitationID="CR109">109</CitationRef>]</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>2007</SimplePara>
                        </entry>
                        <entry align="left" colname="c3">
                          <SimplePara>Model-based conceptual clustering, automatic spatiotemporal mining</SimplePara>
                        </entry>
                        <entry align="left" colname="c4">
                          <SimplePara>Medical</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>Guha et al. [<CitationRef CitationID="CR34">34</CitationRef>], Dai et al. [<CitationRef CitationID="CR20">20</CitationRef>, <CitationRef CitationID="CR22">22</CitationRef>]</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>2006</SimplePara>
                        </entry>
                        <entry align="left" colname="c3">
                          <SimplePara>Multi-agent Tracking, surveillance video data mining</SimplePara>
                        </entry>
                        <entry align="left" colname="c4">
                          <SimplePara>Surveillance</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>Rasheed et al. [<CitationRef CitationID="CR81">81</CitationRef>]</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>2005</SimplePara>
                        </entry>
                        <entry align="left" colname="c3">
                          <SimplePara>Classification</SimplePara>
                        </entry>
                        <entry align="left" colname="c4">
                          <SimplePara>Movie</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>Shirahama et al. [<CitationRef CitationID="CR87">87</CitationRef>, <CitationRef CitationID="CR88">88</CitationRef>]</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>2004</SimplePara>
                        </entry>
                        <entry align="left" colname="c3">
                          <SimplePara>Parallel data mining, movie editing pattern</SimplePara>
                        </entry>
                        <entry align="left" colname="c4">
                          <SimplePara>Movie</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>Oh et al. [<CitationRef CitationID="CR74">74</CitationRef>] Chen et al. [<CitationRef CitationID="CR16">16</CitationRef>]</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>2003</SimplePara>
                        </entry>
                        <entry align="left" colname="c3">
                          <SimplePara>Normal or abnormal event segmentation by clustering, multimedia data mining framework</SimplePara>
                        </entry>
                        <entry align="left" colname="c4">
                          <SimplePara>Sports video</SimplePara>
                        </entry>
                      </row>
                      <row>
                        <entry align="left" colname="c1">
                          <SimplePara>Zhang [<CitationRef CitationID="CR113">113</CitationRef>], Zhu et al. [<CitationRef CitationID="CR116">116</CitationRef>], Matsuo et al. [<CitationRef CitationID="CR61">61</CitationRef>]</SimplePara>
                        </entry>
                        <entry align="left" colname="c2">
                          <SimplePara>2002</SimplePara>
                        </entry>
                        <entry align="left" colname="c3">
                          <SimplePara>Independent motion detection, event detection, video editing patterns</SimplePara>
                        </entry>
                        <entry align="left" colname="c4">
                          <SimplePara>Surveillance, medical, movie</SimplePara>
                        </entry>
                      </row>
                    </tbody>
                  </tgroup>
                </Table>
              </Para>
            </Section1>
            <Section1 ID="Sec27">
              <Heading>Research issues in video data mining</Heading>
              <Para>There are five main research issues for video mining: video semantic event detection and concept mining [<CitationRef CitationID="CR52">52</CitationRef>], video object motion analysis [<CitationRef CitationID="CR2">2</CitationRef>], representation of video data model [<CitationRef CitationID="CR111">111</CitationRef>], extracting the appropriate video features, and selecting the video data-mining algorithms.</Para>
              <Para>The mining of semantic concepts in video data is a core challenge of modern video data management, opening applications like intelligent content filters, surveillance, personal video recommendation, or content-based advertisement. The main challenge is the semantic gap problem which focuses on how to predict semantic features from primitive features. There should be a general framework that is not domain specific like news, sports and so on that can detect semantic features from the general videos that are applicable for any kind of videos. There is a need for a system that can extract, analyze and model the multiple semantics from the videos by using the primitive features.</Para>
              <Para>Semantic event detection is still an ordinary problem owing to the large semantic gap and the difficulty of modelling temporal and multimodality characteristics of video streams. The temporal information plays a vital role in the video data mining particularly, in mining and recognizing patterns in film, medical, sports and traffic videos. For example, in the basketball games the history of situations in terms of time-series of states is more vital than distinct states/processes or actions/events to find the correct zone-defense strategy detection. It is indeed useful to know that not only the current positions of each defender, but also their previous positions and movements are a matter of concern.</Para>
              <Para>The analysis of motion characteristics for moving objects in video is an important part of video data mining. For example, in sports one needs to analyze the behaviors and technical features of athletes by their motion trajectory. In the future it is intended to explore the case of fast moving trajectory tracking and multiple moving objects mining algorithms. 3D motion analysis and object based video annotation also considered to improve the performance mining process.</Para>
              <Para>Video data mining needs a model selection because of the different domains having different descriptive complexities [<CitationRef CitationID="CR111">111</CitationRef>]. So the need to develop the general framework to all video domains and evaluate the performance of the video mining algorithm in environments containing more events is of paramount importance. The purpose of the generative video model to bridge the semantic gap between the high-level concepts and the low-level/mid-level features.</Para>
              <Para>Extracting optimal features is an ongoing major challenge. Optimal features should be tailored to the specific application such as motion tracking or event detection, and also utilize multimodal aspects (audio, visual, and text).</Para>
              <Para>Selecting a proper video mining algorithm is a challenging issue. It needs a little assumption for video data. It handles different types and lengths of video data, but it should also represent the patterns with effective model. It must interpret and use the mined semantic information and patterns and soft computing techniques should also be studied in order to improve the results.</Para>
              <Para>Video data mining may be beneficial to (i) reduce the dimension space for storage saving and computation reduction; (ii) advance learning methods to accurately identify target semantics for bridging the semantics between low-level/mid-level features and high-level semantics; (iii) effectively search media content for dynamical media delivery and enable the extensive applications to be media-type driven; (iv) customizable framework for video indexing that can index the video by using the modalities according to the user preferences.</Para>
            </Section1>
            <Section1 ID="Sec28">
              <Heading>Conclusion</Heading>
              <Para>In this paper, a brief overview of video data mining is presented. With over a decade of extensive research, there has been a tremendous development and application activities in the video data-mining domain. It is impossible to give a complete coverage on this topic with limited space and knowledge. There are many challenging research problems facing video mining such as discovering knowledge from spatial-temporal data, inferring high-level semantic concepts from the low-level features extracted from videos and making use of unlabeled data. The detection of unusual and abnormal video events is indispensable for consumer video applications such as sports highlights extraction and commercial message detection as well as surveillance applications. To improve the results of the video data mining, the new features can be constructed by analyzing the heterogeneous data like video text, audio, and videos. Besides spatial features, there are temporal features, audio features, and features of moving objects in the video data and all these features can be used to mine. There is no meaningful clustering or segmentation method that can be universally applied to all kinds of visual media. However, in-depth research is still required on several critical issues so that there can be developments in leaps and bounds in the data-mining field.</Para>
            </Section1>
          </Body>
          <BodyRef FileRef="BodyRef/PDF/13735_2012_Article_16.pdf" TargetType="OnlinePDF"/>
          <BodyRef FileRef="BodyRef/PDF/13735_2012_16_TEX.zip" TargetType="TEX"/>
          <ArticleBackmatter>
            <Bibliography ID="Bib1">
              <Heading>References</Heading>
              <Citation ID="CR1">
                <CitationNumber>1.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>F</Initials>
                    <FamilyName>Anwar</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>I</Initials>
                    <FamilyName>Petrounias</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>T</Initials>
                    <FamilyName>Morris</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>V</Initials>
                    <FamilyName>Kodogiannis</FamilyName>
                  </BibAuthorName>
                  <Year>2012</Year>
                  <ArticleTitle Language="En">Mining anomalous events against frequent sequences in surveillance videos from commercial environments</ArticleTitle>
                  <JournalTitle>Exp Syst Appl</JournalTitle>
                  <VolumeID>39</VolumeID>
                  <FirstPage>4511</FirstPage>
                  <LastPage>4531</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1016/j.eswa.2011.09.134</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Anwar F, Petrounias I, Morris T, Kodogiannis V (2012) Mining anomalous events against frequent sequences in surveillance videos from commercial environments. Exp Syst Appl 39:4511–4531</BibUnstructured>
              </Citation>
              <Citation ID="CR2">
                <CitationNumber>2.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Anjulan</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>N</Initials>
                    <FamilyName>Canagarajah</FamilyName>
                  </BibAuthorName>
                  <Year>2009</Year>
                  <ArticleTitle Language="En">A unified framework for object retrieval and mining</ArticleTitle>
                  <JournalTitle>IEEE Trans Circ Syst Video Technol</JournalTitle>
                  <VolumeID>19</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>63</FirstPage>
                  <LastPage>76</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TCSVT.2008.2005801</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Anjulan A, Canagarajah N (2009) A unified framework for object retrieval and mining. IEEE Trans Circ Syst Video Technol 19(1):63–76</BibUnstructured>
              </Citation>
              <Citation ID="CR3">
                <CitationNumber>3.</CitationNumber>
                <BibUnstructured>Ahmed A (2009) Video representation and processing for multimedia data mining. Semantic mining technologies for multimedia databases. IGI Press, pp 1–31</BibUnstructured>
              </Citation>
              <Citation ID="CR4">
                <CitationNumber>4.</CitationNumber>
                <BibUnstructured>Anjulan A, Canagarajah N (2007) A novel video mining system. In: Proceedings of 14th IEEE international conference on image processing, San Antonio, Texas, pp 185–189</BibUnstructured>
              </Citation>
              <Citation ID="CR5">
                <CitationNumber>5.</CitationNumber>
                <BibUnstructured>Aradhye H, Toderici G, Yagnik J (2009) Video2Text: learning to annotate video content. In: Proceedings of IEEE international conference on data mining workshops, pp 144–152. doi:<ExternalRef><RefSource>10.1109/ICDMW.2009.79</RefSource><RefTarget Address="10.1109/ICDMW.2009.79" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR6">
                <CitationNumber>6.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>CA</Initials>
                    <FamilyName>Bhatt</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>MS</Initials>
                    <FamilyName>Kankanhalli</FamilyName>
                  </BibAuthorName>
                  <Year>2011</Year>
                  <ArticleTitle Language="En">Multimedia data mining: state of the art and challenges</ArticleTitle>
                  <JournalTitle>Multimedia Tools Appl</JournalTitle>
                  <VolumeID>51</VolumeID>
                  <FirstPage>35</FirstPage>
                  <LastPage>76</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1007/s11042-010-0645-5</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Bhatt CA, Kankanhalli MS (2011) Multimedia data mining: state of the art and challenges. Multimedia Tools Appl 51:35–76</BibUnstructured>
              </Citation>
              <Citation ID="CR7">
                <CitationNumber>7.</CitationNumber>
                <BibUnstructured>Brezeale D, Cook DJ (2008) Automatic video classification: a survey of the literature. IEEE Trans Syst Man Cybern Part C: Appl Rev 38(3):416–430. doi:<ExternalRef><RefSource>10.1109/TSMCC.2008.919173</RefSource><RefTarget Address="10.1109/TSMCC.2008.919173" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR8">
                <CitationNumber>8.</CitationNumber>
                <BibUnstructured>Burl MC (2004) Mining Patterns of activity from video data. In: Proceedings of the SIAM international conference on discrete mathematics, pp 532–536. doi:<ExternalRef><RefSource>10.1137/1.9781611972740.61</RefSource><RefTarget Address="10.1137/1.9781611972740.61" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR9">
                <CitationNumber>9.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>P</Initials>
                    <FamilyName>Cui</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>Z-Q</Initials>
                    <FamilyName>Liu</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>L-F</Initials>
                    <FamilyName>Sun</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>S-Q</Initials>
                    <FamilyName>Yang</FamilyName>
                  </BibAuthorName>
                  <Year>2011</Year>
                  <ArticleTitle Language="En">Hierarchical visual event pattern mining and its applications</ArticleTitle>
                  <JournalTitle>J Data Mining Knowl Disc</JournalTitle>
                  <VolumeID>22</VolumeID>
                  <IssueID>3</IssueID>
                  <FirstPage>467</FirstPage>
                  <LastPage>492</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>2785130</Handle>
                  </Occurrence>
                  <Occurrence Type="ZLBID">
                    <Handle>1235.68142</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1007/s10618-010-0195-5</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Cui P, Liu Z-Q, Sun L-F, Yang S-Q (2011) Hierarchical visual event pattern mining and its applications. J Data Mining Knowl Disc 22(3):467–492</BibUnstructured>
              </Citation>
              <Citation ID="CR10">
                <CitationNumber>10.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>B-W</Initials>
                    <FamilyName>Chen</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>J-C</Initials>
                    <FamilyName>Wang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>F</Initials>
                    <FamilyName>Wang</FamilyName>
                  </BibAuthorName>
                  <Year>2009</Year>
                  <ArticleTitle Language="En">A novel video summarization based on mining the story-structure and semantic relations among concept entities</ArticleTitle>
                  <JournalTitle>IEEE Trans Multimedia</JournalTitle>
                  <VolumeID>11</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>295</FirstPage>
                  <LastPage>313</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TMM.2008.2009703</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Chen B-W, Wang J-C, Wang F (2009) A novel video summarization based on mining the story-structure and semantic relations among concept entities. IEEE Trans Multimedia 11(2):295–313</BibUnstructured>
              </Citation>
              <Citation ID="CR11">
                <CitationNumber>11.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>S</Initials>
                    <FamilyName>Colantonio</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>O</Initials>
                    <FamilyName>Salvetti</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Tampucci</FamilyName>
                  </BibAuthorName>
                  <Year>2008</Year>
                  <ArticleTitle Language="En">An infrastructure for mining medical multimedia data</ArticleTitle>
                  <JournalTitle>Lect Notes Comput Sci</JournalTitle>
                  <VolumeID>5077</VolumeID>
                  <FirstPage>102</FirstPage>
                  <LastPage>113</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1007/978-3-540-70720-2_8</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Colantonio S, Salvetti O, Tampucci M (2008) An infrastructure for mining medical multimedia data. Lect Notes Comput Sci 5077:102–113</BibUnstructured>
              </Citation>
              <Citation ID="CR12">
                <CitationNumber>12.</CitationNumber>
                <BibUnstructured>Chen F, Cooper M, Addock (2007) Video summarization preserving synamic content. In: Proceedings of the international workshop on TRECVID video summarization, pp 40–44</BibUnstructured>
              </Citation>
              <Citation ID="CR13">
                <CitationNumber>13.</CitationNumber>
                <BibUnstructured>Chen J, Li T, Zhu L, Ding P, Xu B (2011) Commercial detection by mining maximal repeated sequence in audio stream. Proceedings of IEEE</BibUnstructured>
              </Citation>
              <Citation ID="CR14">
                <CitationNumber>14.</CitationNumber>
                <BibUnstructured>Chen M, Chen S-C, Shyu M-L (2007) Hierarchical temporal association mining for video event detection in video databases. In: The second IEEE international workshop on multimedia databases and data management (MDDM’07), in conjunction with IEEE international conference on data engineering (ICDE2007), Istanbul, Turkey</BibUnstructured>
              </Citation>
              <Citation ID="CR15">
                <CitationNumber>15.</CitationNumber>
                <BibUnstructured>Chen S-C, Chen M, Zhang C, Shyu M-L (2006) Exciting event detection using multi-level multimodal descriptors and data classification. In: Proceedings of eighth IEEE international symposium on multimedia, pp 193–200. doi:<ExternalRef><RefSource>10.1109/ISM.2006.73</RefSource><RefTarget Address="10.1109/ISM.2006.73" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR16">
                <CitationNumber>16.</CitationNumber>
                <BibUnstructured>Chen S-C, Shyu M-L, Zhang C, Luo L, Chen M (2003) Detection of soccer goal shots using joint multimedia features and classification rules. In: Proceedings of international workshop on multimedia data mining (MDM/KDD’2003), USA, pp 36–44</BibUnstructured>
              </Citation>
              <Citation ID="CR17">
                <CitationNumber>17.</CitationNumber>
                <BibUnstructured>Chen S-C, Shyu M-L, Zhang C, Strickrott J (2001) Multimedia data mining for traffic video sequences. In: Proceedings second international workshop on multimedia data mining MDM/KDD’2001 in conjunction with ACM SIGKDD seventh international conference on knowledge discovery and data mining, pp 78–86</BibUnstructured>
              </Citation>
              <Citation ID="CR18">
                <CitationNumber>18.</CitationNumber>
                <BibUnstructured>Choudhary A, Chaudhury S, Basnerjee S (2008) A framework for analysis of surveillance videos. In: Proceedings of sixth Indian conference on computer vision, graphics&amp; image processing, pp 344–350. doi:<ExternalRef><RefSource>10.1109/ICVGIP.2008.76</RefSource><RefTarget Address="10.1109/ICVGIP.2008.76" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR19">
                <CitationNumber>19.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>Y</Initials>
                    <FamilyName>Ding</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>G</Initials>
                    <FamilyName>Fan</FamilyName>
                  </BibAuthorName>
                  <Year>2009</Year>
                  <ArticleTitle Language="En">Sports video mining via multi-channel segmental hidden Markov models</ArticleTitle>
                  <JournalTitle>IEEE Trans Multimedia</JournalTitle>
                  <VolumeID>11</VolumeID>
                  <IssueID>7</IssueID>
                  <FirstPage>1301</FirstPage>
                  <LastPage>1309</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TMM.2009.2030828</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Ding Y, Fan G (2009) Sports video mining via multi-channel segmental hidden Markov models. IEEE Trans Multimedia 11(7):1301–1309</BibUnstructured>
              </Citation>
              <Citation ID="CR20">
                <CitationNumber>20.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>K</Initials>
                    <FamilyName>Dai</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Zhang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>G</Initials>
                    <FamilyName>Li</FamilyName>
                  </BibAuthorName>
                  <Year>2006</Year>
                  <ArticleTitle Language="En">Video mining: concepts, approaches and applications</ArticleTitle>
                  <JournalTitle>Proc IEEE</JournalTitle>
                  <VolumeID>2006</VolumeID>
                  <FirstPage>477</FirstPage>
                  <LastPage>481</LastPage>
                </BibArticle>
                <BibUnstructured>Dai K, Zhang J, Li G (2006) Video mining: concepts, approaches and applications. Proc IEEE 2006:477–481</BibUnstructured>
              </Citation>
              <Citation ID="CR21">
                <CitationNumber>21.</CitationNumber>
                <BibBook>
                  <BibAuthorName>
                    <Initials>C</Initials>
                    <FamilyName>Djeraba</FamilyName>
                  </BibAuthorName>
                  <Year>2003</Year>
                  <BookTitle>Multimedia mining: a highway to intelligent multimedia documents</BookTitle>
                  <PublisherName>Springer</PublisherName>
                  <PublisherLocation>Berlin</PublisherLocation>
                </BibBook>
                <BibUnstructured>Djeraba C (2003) Multimedia mining: a highway to intelligent multimedia documents. Springer, Berlin</BibUnstructured>
              </Citation>
              <Citation ID="CR22">
                <CitationNumber>22.</CitationNumber>
                <BibUnstructured>Dai KX, Li GH, Gan YL (2006) A probabilistic model for surveillance video mining. In: Proceedings of the fifth international conference on machine learning and, cybernetics, pp 1144– 1148. doi:<ExternalRef><RefSource>10.1109/ICMLC.2006.258594</RefSource><RefTarget Address="10.1109/ICMLC.2006.258594" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR23">
                <CitationNumber>23.</CitationNumber>
                <BibUnstructured>Divakaan A, Peker K, Chang S, Radhakrishnan R, Xie L (2004) VideoMining: pattern discovery versus pattern recognition. In: Proceedings IEEE international conference on image processing (ICIP’2004). Mitsubishi Electric Research Laboratories</BibUnstructured>
              </Citation>
              <Citation ID="CR24">
                <CitationNumber>24.</CitationNumber>
                <BibUnstructured>Divakaran A, Miyahara K, Peker KA, Radhakrishnan R, Xiong Z (2004) Video mining using combinations of unsupervised and supervised learning techniques. In: Proceedings of SPIE conference on storage and retrieval for multimedia databases, vol 5307, pp 235–243</BibUnstructured>
              </Citation>
              <Citation ID="CR25">
                <CitationNumber>25.</CitationNumber>
                <BibUnstructured>Doudpota SM, Guha S (2011) Mining movies to extract song sequences. In: Proceedings of MDMKDD’11. doi:<ExternalRef><RefSource>10.1145/2237827.2237829</RefSource><RefTarget Address="10.1145/2237827.2237829" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR26">
                <CitationNumber>26.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Fan</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>H</Initials>
                    <FamilyName>Luo</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>AK</Initials>
                    <FamilyName>Elmagarmid</FamilyName>
                  </BibAuthorName>
                  <Year>2004</Year>
                  <ArticleTitle Language="En">Concept-oriented indexing of video databases: towards semantic sensitive retrieval and browsing</ArticleTitle>
                  <JournalTitle>IEEE Trans Image Process</JournalTitle>
                  <VolumeID>13</VolumeID>
                  <IssueID>7</IssueID>
                  <FirstPage>974</FirstPage>
                  <LastPage>992</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TIP.2004.827232</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Fan J, Luo H, Elmagarmid AK (2004) Concept-oriented indexing of video databases: towards semantic sensitive retrieval and browsing. IEEE Trans Image Process 13(7):974–992</BibUnstructured>
              </Citation>
              <Citation ID="CR27">
                <CitationNumber>27.</CitationNumber>
                <BibUnstructured>Fan J, Zhu X, Hacid M-S, Elmagarmid AK (2002) Multimedia tools and applications. Model-based video classification toward hierarchical representation indexing and access. Kluwer, Dordrecht, pp 97–120</BibUnstructured>
              </Citation>
              <Citation ID="CR28">
                <CitationNumber>28.</CitationNumber>
                <BibUnstructured>Fleischman M, Decamp P, Roy D (2006) Mining temporal patterns of movement for video content classification. In: Proceedings of the 8th ACM international workshop on multimedia, information retrieval, pp 183–192. doi:<ExternalRef><RefSource>10.1145/1178677.1178704</RefSource><RefTarget Address="10.1145/1178677.1178704" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR29">
                <CitationNumber>29.</CitationNumber>
                <BibUnstructured>Fu C-J, Li G-H, Dai K-X (2005) A framework for video structure mining. In: Proceedings of the fourth international conference on machine learning and cybernetics, vol 3, pp 1524–1528</BibUnstructured>
              </Citation>
              <Citation ID="CR30">
                <CitationNumber>30.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>D</Initials>
                    <FamilyName>Gowsikhaa</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>AS</Initials>
                    <FamilyName>Manjunath</FamilyName>
                  </BibAuthorName>
                  <Year>2012</Year>
                  <ArticleTitle Language="En">Suspicious human activity detection from surveillance videos</ArticleTitle>
                  <JournalTitle>Int J Int Distrib Comput Syst</JournalTitle>
                  <VolumeID>2</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>141</FirstPage>
                  <LastPage>149</LastPage>
                </BibArticle>
                <BibUnstructured>Gowsikhaa D, Manjunath AS (2012) Suspicious human activity detection from surveillance videos. Int J Int Distrib Comput Syst 2(2):141–149</BibUnstructured>
              </Citation>
              <Citation ID="CR31">
                <CitationNumber>31.</CitationNumber>
                <BibUnstructured>Gaidon A, Marszalek M, Schmid C (2009) Mining visual actions from movies. In: Proceedings of the British machine conference. BMVA Press, pp 125.1–125.11</BibUnstructured>
              </Citation>
              <Citation ID="CR32">
                <CitationNumber>32.</CitationNumber>
                <BibUnstructured>Gilbert A, Illingworth J, Bowden R (2011) Action recognition using mined hierarchical compound features. IEEE Trans Pattern Anal Mach Intell 33(5):883–897. doi:<ExternalRef><RefSource>10.1109/TPAMI.2010.144</RefSource><RefTarget Address="10.1109/TPAMI.2010.144" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR33">
                <CitationNumber>33.</CitationNumber>
                <BibUnstructured>Goyanil M, Dutta S, Gohil G, Naik S (2011) Wicket fall concept mining from cricket video using a-priori algorithm. Proc Int J Multimedia Appl (IJMA) 3:1</BibUnstructured>
              </Citation>
              <Citation ID="CR34">
                <CitationNumber>34.</CitationNumber>
                <BibUnstructured>Guha P, Biswas A, Mukerjee A, Sateesh P, Venkatesh KS (2006) Surveillance video mining. In: Proceedings of the third international conference on visual information engineering, Bangalore (India), September 26–28, 2006</BibUnstructured>
              </Citation>
              <Citation ID="CR35">
                <CitationNumber>35.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>H-Y</Initials>
                    <FamilyName>Huangy</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>W-S</Initials>
                    <FamilyName>Shih</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>W-H</Initials>
                    <FamilyName>Hsu</FamilyName>
                  </BibAuthorName>
                  <Year>2008</Year>
                  <ArticleTitle Language="En">A film classifier based on low-level visual features</ArticleTitle>
                  <JournalTitle>J Multimedia</JournalTitle>
                  <VolumeID>3</VolumeID>
                  <IssueID>3</IssueID>
                  <FirstPage>26</FirstPage>
                  <LastPage>33</LastPage>
<Occurrence Type="DOI">
<Handle>10.4304/jmm.3.3.26-33</Handle>
</Occurrence>
                </BibArticle>
                <BibUnstructured>Huangy H-Y, Shih W-S, Hsu W-H (2008) A film classifier based on low-level visual features. J Multimedia 3(3):26–33</BibUnstructured>
              </Citation>
              <Citation ID="CR36">
                <CitationNumber>36.</CitationNumber>
                <BibUnstructured>Harikrishna N, Satheesh S, Dinesh Sriram S, Easwarakumar KS (2011) Temporal classification of events in cricket videos. In: Proceedings of seventeenth national conference on communications NCC 2011. Indian Institute of Science, Bangalore</BibUnstructured>
              </Citation>
              <Citation ID="CR37">
                <CitationNumber>37.</CitationNumber>
                <BibUnstructured>
                  <ExternalRef>
                    <RefSource>http://dimacs.rutgers.edu/Workshops/Video/abstracts.html</RefSource>
                    <RefTarget Address="http://dimacs.rutgers.edu/Workshops/Video/abstracts.html" TargetType="URL"/>
                  </ExternalRef>
                </BibUnstructured>
              </Citation>
              <Citation ID="CR38">
                <CitationNumber>38.</CitationNumber>
                <BibUnstructured>
                  <ExternalRef>
                    <RefSource>http://www.ee.columbia.edu/ln/dvmm/newHome.htm</RefSource>
                    <RefTarget Address="http://www.ee.columbia.edu/ln/dvmm/newHome.htm" TargetType="URL"/>
                  </ExternalRef>
                </BibUnstructured>
              </Citation>
              <Citation ID="CR39">
                <CitationNumber>39.</CitationNumber>
                <BibUnstructured>
                  <ExternalRef>
                    <RefSource>http://www.merl.com/areas/VideoMining/</RefSource>
                    <RefTarget Address="http://www.merl.com/areas/VideoMining/" TargetType="URL"/>
                  </ExternalRef>
                </BibUnstructured>
              </Citation>
              <Citation ID="CR40">
                <CitationNumber>40.</CitationNumber>
                <BibUnstructured>Hu W, Xie N, Li L, Zeng X, Maybank S (2011) A survey on visual content-based video indexing and retrieval. IEEE Trans Syst Man Cybern C: Appl Rev 1–23</BibUnstructured>
              </Citation>
              <Citation ID="CR41">
                <CitationNumber>41.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>F</Initials>
                    <FamilyName>Jiang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Yuan</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>SA</Initials>
                    <FamilyName>Tsaftaris</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>AK</Initials>
                    <FamilyName>Katsaggelos</FamilyName>
                  </BibAuthorName>
                  <Year>2011</Year>
                  <ArticleTitle Language="En">Anomalous video event detection using spatiotemporal context</ArticleTitle>
                  <JournalTitle>Int J Comput Vis Image Underst</JournalTitle>
                  <VolumeID>115</VolumeID>
                  <FirstPage>323</FirstPage>
                  <LastPage>333</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1016/j.cviu.2010.10.008</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Jiang F, Yuan J, Tsaftaris SA, Katsaggelos AK (2011) Anomalous video event detection using spatiotemporal context. Int J Comput Vis Image Underst 115:323–333</BibUnstructured>
              </Citation>
              <Citation ID="CR42">
                <CitationNumber>42.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>F</Initials>
                    <FamilyName>Jiang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Yuan</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>SA</Initials>
                    <FamilyName>Tsaftaris</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>AK</Initials>
                    <FamilyName>Katsaggelos</FamilyName>
                  </BibAuthorName>
                  <Year>2011</Year>
                  <ArticleTitle Language="En">Anomalous video event detection using spatiotemporal context</ArticleTitle>
                  <JournalTitle>Comput Vis Image Underst</JournalTitle>
                  <VolumeID>115</VolumeID>
                  <FirstPage>323</FirstPage>
                  <LastPage>333</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1016/j.cviu.2010.10.008</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Jiang F, Yuan J, Tsaftaris SA, Katsaggelos AK (2011) Anomalous video event detection using spatiotemporal context. Comput Vis Image Underst 115:323–333</BibUnstructured>
              </Citation>
              <Citation ID="CR43">
                <CitationNumber>43.</CitationNumber>
                <BibUnstructured>Jiang S, Tian Y, Huang Q, Huang T, Gao W (2009) Content-based video semantic analysis. Semantic mining technologies for multimedia databases. IGI Press</BibUnstructured>
              </Citation>
              <Citation ID="CR44">
                <CitationNumber>44.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>F</Initials>
                    <FamilyName>Kokkoras</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>H</Initials>
                    <FamilyName>Jiang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>I</Initials>
                    <FamilyName>Vlahavas</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>AK</Initials>
                    <FamilyName>Elmagarmid</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>EN</Initials>
                    <FamilyName>Houstis</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>WG</Initials>
                    <FamilyName>Aref</FamilyName>
                  </BibAuthorName>
                  <Year>2002</Year>
                  <ArticleTitle Language="En">Smart VideoText: a video data model based on conceptual graphs</ArticleTitle>
                  <JournalTitle>ACM Multimedia Syst J</JournalTitle>
                  <VolumeID>8</VolumeID>
                  <IssueID>4</IssueID>
                  <FirstPage>328</FirstPage>
                  <LastPage>338</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1007/s005300200054</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Kokkoras F, Jiang H, Vlahavas I, Elmagarmid AK, Houstis EN, Aref WG (2002) Smart VideoText: a video data model based on conceptual graphs. ACM Multimedia Syst J 8(4):328–338</BibUnstructured>
              </Citation>
              <Citation ID="CR45">
                <CitationNumber>45.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>S</Initials>
                    <FamilyName>Kotsiantis</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>D</Initials>
                    <FamilyName>Kanellopoulos</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>P</Initials>
                    <FamilyName>Pintelas</FamilyName>
                  </BibAuthorName>
                  <Year>2004</Year>
                  <ArticleTitle Language="En">Multimedia mining</ArticleTitle>
                  <JournalTitle>WSEAS Trans Syst</JournalTitle>
                  <VolumeID>10</VolumeID>
                  <IssueID>3</IssueID>
                  <FirstPage>3263</FirstPage>
                  <LastPage>3268</LastPage>
                </BibArticle>
                <BibUnstructured>Kotsiantis S, Kanellopoulos D, Pintelas P (2004) Multimedia mining. WSEAS Trans Syst 10(3):3263–3268</BibUnstructured>
              </Citation>
              <Citation ID="CR46">
                <CitationNumber>46.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>D</Initials>
                    <FamilyName>Kucuk</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Yazici</FamilyName>
                  </BibAuthorName>
                  <Year>2011</Year>
                  <ArticleTitle Language="En">Exploiting information extraction techniques for automatic semantic video indexing with an application to Turkish news videos</ArticleTitle>
                  <JournalTitle>Int J Knowl-Based Syst</JournalTitle>
                  <VolumeID>24</VolumeID>
                  <FirstPage>844</FirstPage>
                  <LastPage>857</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1016/j.knosys.2011.03.006</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Kucuk D, Yazici A (2011) Exploiting information extraction techniques for automatic semantic video indexing with an application to Turkish news videos. Int J Knowl-Based Syst 24:844–857</BibUnstructured>
              </Citation>
              <Citation ID="CR47">
                <CitationNumber>47.</CitationNumber>
                <BibUnstructured>Kea J, Zhana Y, Chenc X, Wanga M (2012) The retrieval of motion event by associations of temporal frequent pattern growth. Future Generation Comput Syst (in press)</BibUnstructured>
              </Citation>
              <Citation ID="CR48">
                <CitationNumber>48.</CitationNumber>
                <BibUnstructured>Kiran Sree P (2008) Video data mining framework for information retrieval. In: Proceedings of NCKM-2008, Annamalai University, Tamilnadu, India</BibUnstructured>
              </Citation>
              <Citation ID="CR49">
                <CitationNumber>49.</CitationNumber>
                <BibUnstructured>Kumar P, Roy S, Mittal A, Kumar P (2008) On-line data management framework for multimedia surveillance system. In: Proceedings of national conference on communications, 01–03 Feb 2008. IIT, Bombay</BibUnstructured>
              </Citation>
              <Citation ID="CR50">
                <CitationNumber>50.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>C</Initials>
                    <FamilyName>Lai</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>T</Initials>
                    <FamilyName>Rafa</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>DE</Initials>
                    <FamilyName>Nelson</FamilyName>
                  </BibAuthorName>
                  <Year>2006</Year>
                  <ArticleTitle Language="En">Mining motion patterns using color motion map clustering</ArticleTitle>
                  <JournalTitle>SIGKDD Explor</JournalTitle>
                  <VolumeID>8</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>3</FirstPage>
                  <LastPage>10</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1145/1233321.1233322</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Lai C, Rafa T, Nelson DE (2006) Mining motion patterns using color motion map clustering. SIGKDD Explor 8(2):3–10</BibUnstructured>
              </Citation>
              <Citation ID="CR51">
                <CitationNumber>51.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>S</Initials>
                    <FamilyName>Lu</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>MR</Initials>
                    <FamilyName>Lyu</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>I</Initials>
                    <FamilyName>King</FamilyName>
                  </BibAuthorName>
                  <Year>2004</Year>
                  <ArticleTitle Language="En">Video summarization by spatial-temporal graph optimization</ArticleTitle>
                  <JournalTitle>Proc IEEE Int Symp Circuits Syst</JournalTitle>
                  <VolumeID>2</VolumeID>
                  <FirstPage>197</FirstPage>
                  <LastPage>201</LastPage>
                </BibArticle>
                <BibUnstructured>Lu S, Lyu MR, King I (2004) Video summarization by spatial-temporal graph optimization. Proc IEEE Int Symp Circuits Syst 2:197–201</BibUnstructured>
              </Citation>
              <Citation ID="CR52">
                <CitationNumber>52.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>L</Initials>
                    <FamilyName>Lin</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>M-L</Initials>
                    <FamilyName>Shyu</FamilyName>
                  </BibAuthorName>
                  <Year>2010</Year>
                  <ArticleTitle Language="En">Weighted association rule mining for video semantic detection</ArticleTitle>
                  <JournalTitle>Int J Multimedia Data Eng Manag</JournalTitle>
                  <VolumeID>1</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>37</FirstPage>
                  <LastPage>54</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.4018/jmdem.2010111203</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Lin L, Shyu M-L (2010) Weighted association rule mining for video semantic detection. Int J Multimedia Data Eng Manag 1(1):37–54</BibUnstructured>
              </Citation>
              <Citation ID="CR53">
                <CitationNumber>53.</CitationNumber>
                <BibUnstructured>Leavitt N (2002) Let’s hear it for audio mining. ComputerMagazine</BibUnstructured>
              </Citation>
              <Citation ID="CR54">
                <CitationNumber>54.</CitationNumber>
                <BibUnstructured>Lee J, Rajauria P, Shah SK (2007) A model-based conceptual clustering of moving objects in video surveillance. In: Proceedings of SPIE IS&amp;T electronic imaging. SPIE, vol 6506, Jan 28–Feb 1, 2007, San Jose</BibUnstructured>
              </Citation>
              <Citation ID="CR55">
                <CitationNumber>55.</CitationNumber>
                <BibUnstructured>Lin L, Ravitz G, Shyu M-L, Chen S-C (2007) Video semantic concept discovery using multimodal-based association classification. ICME07. doi:<ExternalRef><RefSource>10.1109/ICME.2007.4284786</RefSource><RefTarget Address="10.1109/ICME.2007.4284786" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR56">
                <CitationNumber>56.</CitationNumber>
                <BibUnstructured>Liu H-Y, He T (2009) Semantic event mining in soccer video based on multiple feature fusion. In: Proceedings of international conference on information technology and computer science, pp 297–301</BibUnstructured>
              </Citation>
              <Citation ID="CR57">
                <CitationNumber>57.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>E</Initials>
                    <FamilyName>Moxley</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>T</Initials>
                    <FamilyName>Mei</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>BS</Initials>
                    <FamilyName>Manjunath</FamilyName>
                  </BibAuthorName>
                  <Year>2010</Year>
                  <ArticleTitle Language="En">Video annotation through search and graph reinforcement mining</ArticleTitle>
                  <JournalTitle>IEEE Trans Multimedia</JournalTitle>
                  <VolumeID>12</VolumeID>
                  <IssueID>3</IssueID>
                  <FirstPage>184</FirstPage>
                  <LastPage>194</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TMM.2010.2041101</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Moxley E, Mei T, Manjunath BS (2010) Video annotation through search and graph reinforcement mining. IEEE Trans Multimedia 12(3):184–194</BibUnstructured>
              </Citation>
              <Citation ID="CR58">
                <CitationNumber>58.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>HK</Initials>
                    <FamilyName>Maheshkumar</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>K</Initials>
                    <FamilyName>Palaniappan</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>S</Initials>
                    <FamilyName>Sengupta</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>G</Initials>
                    <FamilyName>Seetharaman</FamilyName>
                  </BibAuthorName>
                  <Year>2009</Year>
                  <ArticleTitle Language="En">Semantic concept mining based on hierarchical event detection for soccer video indexing</ArticleTitle>
                  <JournalTitle>J Multimedia</JournalTitle>
                  <VolumeID>4</VolumeID>
                  <IssueID>5</IssueID>
                  <FirstPage>298</FirstPage>
                  <LastPage>307</LastPage>
                </BibArticle>
                <BibUnstructured>Maheshkumar HK, Palaniappan K, Sengupta S, Seetharaman G (2009) Semantic concept mining based on hierarchical event detection for soccer video indexing. J Multimedia 4(5):298–307</BibUnstructured>
              </Citation>
              <Citation ID="CR59">
                <CitationNumber>59.</CitationNumber>
                <BibUnstructured>Ma YF, Lu L, Zhang, HJ, Li M (2002) A user attention model for video summarization. In: Proceedings of the tenth ACM international conference on multimedia, pp 533–542. doi:<ExternalRef><RefSource>10.1145/641007.641116</RefSource><RefTarget Address="10.1145/641007.641116" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR60">
                <CitationNumber>60.</CitationNumber>
                <BibUnstructured>Marsala C, Detyniecki M (2003) Fuzzy data mining for video. In: Proceedings of the international conference of the European society for fuzzy logic and technology—EUSFLAT’2003, pp 73–80</BibUnstructured>
              </Citation>
              <Citation ID="CR61">
                <CitationNumber>61.</CitationNumber>
                <BibUnstructured>Matsuo Y, Amano M, Uehara K (2002) Mining video editing rules in video streams. In: Proceedings of the tenth ACM international conference on multimedia, pp 255–258. doi:<ExternalRef><RefSource>10.1145/641007.641058</RefSource><RefTarget Address="10.1145/641007.641058" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR62">
                <CitationNumber>62.</CitationNumber>
                <BibUnstructured>Mei T, Ma Y-F, Zhou H-Q, Ma W-Y, Zhang H-J (2005) Sports video mining with mosaic. In: Proceedings of 11th international multimedia modelling conference (MMM’05), Melbourne, Australia, pp 107–114</BibUnstructured>
              </Citation>
              <Citation ID="CR63">
                <CitationNumber>63.</CitationNumber>
                <BibUnstructured>Mihajlovic V, Petkovic M (2001) Automatic annotation of Formula 1 races for content-based video retrieval. CTIT Tech Rep Series, TR-CTIT 01-41</BibUnstructured>
              </Citation>
              <Citation ID="CR64">
                <CitationNumber>64.</CitationNumber>
                <BibUnstructured>Missaoui R, Palenichka RM (2005) Effective image and video mining: an overview of model-based approaches. In: Proceedings of 6th international workshop on multimedia data mining: mining integrated media and complex data, pp 43–52</BibUnstructured>
              </Citation>
              <Citation ID="CR65">
                <CitationNumber>65.</CitationNumber>
                <BibUnstructured>Mitra S, Tinkuacharya (2003) Data mining multimedia, soft computing, and bioinformatics. Wiley, Hoboken</BibUnstructured>
              </Citation>
              <Citation ID="CR66">
                <CitationNumber>66.</CitationNumber>
                <BibUnstructured>Mongy S, Bouali F, Djeraba C (2005) Analyzing user’s behavior on a video database. In: Proceedings of the 6th ACM international workshop on multimedia data mining: mining integrated media and complex data (MDM/KDD 2005), pp 95–100</BibUnstructured>
              </Citation>
              <Citation ID="CR67">
                <CitationNumber>67.</CitationNumber>
                <BibUnstructured>Moxley E, Mei T, Hua XS, Ma W-Y, Manjunath BS (2008) Automatic video annotation through search and mining. In: Proceedings of IEEE international conference on multimedia and expo (ICME), pp 685–688. doi:<ExternalRef><RefSource>10.1109/ICME.2008.4607527</RefSource><RefTarget Address="10.1109/ICME.2008.4607527" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR68">
                <CitationNumber>68.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>MR</Initials>
                    <FamilyName>Naphade</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>TS</Initials>
                    <FamilyName>Huang</FamilyName>
                  </BibAuthorName>
                  <Year>2001</Year>
                  <ArticleTitle Language="En">A probabilistic framework for semantic video indexing, filtering, and retrieval</ArticleTitle>
                  <JournalTitle>IEEE Trans Multimedia</JournalTitle>
                  <VolumeID>3</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>141</FirstPage>
                  <LastPage>152</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/6046.909601</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Naphade MR, Huang TS (2001) A probabilistic framework for semantic video indexing, filtering, and retrieval. IEEE Trans Multimedia 3(1):141–152</BibUnstructured>
              </Citation>
              <Citation ID="CR69">
                <CitationNumber>69.</CitationNumber>
                <BibUnstructured>Nemrava J, Svátek V, Buitelaar P, Declerck T (2008) Text mining as support for semantic video indexing and analysis. In: Proceedings of the 2nd K-space PhD Jamboree workshop, Paris, France, July 25, 2008</BibUnstructured>
              </Citation>
              <Citation ID="CR70">
                <CitationNumber>70.</CitationNumber>
                <BibUnstructured>Ngo C-W, Ma Y-F, Zhang H-J (2003) Automatic video summarization by graph modeling. In: Proceedings of the ninth IEEE international conference on computer vision (ICCV 2003), vol 2, pp 104–109</BibUnstructured>
              </Citation>
              <Citation ID="CR71">
                <CitationNumber>71.</CitationNumber>
                <BibUnstructured>Nowozin S, Bakir GH, Tsuda K (2007) Discriminative subsequence mining for action classification. In: Proceedings of eleventh IEEE international conference on computer vision (ICCV2007), pp 1–8. doi:<ExternalRef><RefSource>10.1109/ICCV.2007.4409049</RefSource><RefTarget Address="10.1109/ICCV.2007.4409049" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR72">
                <CitationNumber>72.</CitationNumber>
                <BibUnstructured>Oh J, Bandi B (2002) Multimedia data mining framework for raw video sequences. In: Proceedings of the third international workshop on multimedia data mining (MDM/KDD’2002) in conjunction with the eight ACM SIGKDD international conference on knowledge discovery &amp; data mining, pp 1–10</BibUnstructured>
              </Citation>
              <Citation ID="CR73">
                <CitationNumber>73.</CitationNumber>
                <BibUnstructured>Oh J, Lee J, Hwang S (2005) Video data mining. Idea Group Inc</BibUnstructured>
              </Citation>
              <Citation ID="CR74">
                <CitationNumber>74.</CitationNumber>
                <BibUnstructured>Oh J, Lee J, Kote S (2003) Real time video data mining for surveillance video streams. In: Proceedings of the seventh Pacific-Asia conference on knowledge discovery and data mining, pp 222–233. doi:<ExternalRef><RefSource>10.1007/3-540-36175-8_22</RefSource><RefTarget Address="10.1007/3-540-36175-8_22" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR75">
                <CitationNumber>75.</CitationNumber>
                <BibUnstructured>Oh JH, Wen Q, Hwang S, Lee J (2005) Video abstraction. Video data management and information retrieval, chap XIV. Idea Group Inc, IRM Press</BibUnstructured>
              </Citation>
              <Citation ID="CR76">
                <CitationNumber>76.</CitationNumber>
                <BibUnstructured>Okada Y, Tada T, Fukuta K, Nagashima T (2010) Audio classification based on a closed itemset mining algorithm. Proc IEEE 2010:60–65. doi:<ExternalRef><RefSource>10.1109/CISIM.2010.5643689</RefSource><RefTarget Address="10.1109/CISIM.2010.5643689" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR77">
                <CitationNumber>77.</CitationNumber>
                <BibUnstructured>Pan J-Y, Faloutsos C (2001) VideoGraph: a new tool for video mining and classification. In: Proceedings of joint conference on digital libraries (JCDL’01), pp 116–117. doi:<ExternalRef><RefSource>10.1145/379437.379462</RefSource><RefTarget Address="10.1145/379437.379462" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR78">
                <CitationNumber>78.</CitationNumber>
                <BibUnstructured>Pan J-Y, Faloutsos C (2002) VideoCube: a novel tool for video mining and classification. In: Proceedings of the fifth international conference on Asian digital libraries (ICADL 2002), pp 194–205. doi:<ExternalRef><RefSource>10.1007/3-540-36227-4_20</RefSource><RefTarget Address="10.1007/3-540-36227-4_20" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR79">
                <CitationNumber>79.</CitationNumber>
                <BibUnstructured>Petkovic M, Jonker W (2001) Content-based retrieval of spatio-temporal video events. In: Proceedings of multimedia computing and information management track of IRMA international conference</BibUnstructured>
              </Citation>
              <Citation ID="CR80">
                <CitationNumber>80.</CitationNumber>
                <BibUnstructured>Quack T, Ferrari V, Gool LV (2006) Video mining with frequent itemset configurations. In: Proceedings of international conference on image and video retrieval (CIVR (2006), Tempe, AZ, USA</BibUnstructured>
              </Citation>
              <Citation ID="CR81">
                <CitationNumber>81.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>Z</Initials>
                    <FamilyName>Rasheed</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>Y</Initials>
                    <FamilyName>Sheikh</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Shah</FamilyName>
                  </BibAuthorName>
                  <Year>2005</Year>
                  <ArticleTitle Language="En">On the use of computable features for film classification</ArticleTitle>
                  <JournalTitle>IEEE Trans Circuit Sys Video Technol</JournalTitle>
                  <VolumeID>15</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>52</FirstPage>
                  <LastPage>64</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TCSVT.2004.839993</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Rasheed Z, Sheikh Y, Shah M (2005) On the use of computable features for film classification. IEEE Trans Circuit Sys Video Technol 15(1):52–64</BibUnstructured>
              </Citation>
              <Citation ID="CR82">
                <CitationNumber>82.</CitationNumber>
                <BibUnstructured>Rasheed Z, Shah M (2003) Video categorization using semantics and semiotics, chap 7. Video mining. Kluwer, Dordrecht</BibUnstructured>
              </Citation>
              <Citation ID="CR83">
                <CitationNumber>83.</CitationNumber>
                <BibUnstructured>Rui Y, Huang TD (2000) A unified framework for video summarization. Browsing and retrieval, image and video processing handbook, pp 705–715</BibUnstructured>
              </Citation>
              <Citation ID="CR84">
                <CitationNumber>84.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>CGM</Initials>
                    <FamilyName>Snoek</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Worring</FamilyName>
                  </BibAuthorName>
                  <Year>2005</Year>
                  <ArticleTitle Language="En">Multimodal video indexing: a review of the state-of-the-art</ArticleTitle>
                  <JournalTitle>Multimedia Tools Appl</JournalTitle>
                  <VolumeID>25</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>5</FirstPage>
                  <LastPage>35</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1023/B:MTAP.0000046380.27575.a5</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Snoek CGM, Worring M (2005) Multimodal video indexing: a review of the state-of-the-art. Multimedia Tools Appl 25(1):5–35</BibUnstructured>
              </Citation>
              <Citation ID="CR85">
                <CitationNumber>85.</CitationNumber>
                <BibUnstructured>Salway A (1999) Video annotation: the role of specialist text. PhD dissertation, Department of Computing, University of Surrey</BibUnstructured>
              </Citation>
              <Citation ID="CR86">
                <CitationNumber>86.</CitationNumber>
                <BibUnstructured>Sanjeevkumar RJ, Praveenkumar K (2007) Multimedia data mining in digital libraries: standards and features. In: Proceedings of conference on recent advances in Information Science&amp; Technology (READIT-2007), pp 54–60</BibUnstructured>
              </Citation>
              <Citation ID="CR87">
                <CitationNumber>87.</CitationNumber>
                <BibUnstructured>Shirahama K, Ideno K, Uehara K (2005) Video data mining: mining semantic patterns with temporal constraints from movies. In: Proceeding of seventh IEEE symposium on multimedia, pp 598–604</BibUnstructured>
              </Citation>
              <Citation ID="CR88">
                <CitationNumber>88.</CitationNumber>
                <BibUnstructured>Shirahama K, Iwamoto K, Uehera K (2004) Video data mining: rhythms in a movie. In: Proceedings of IEEE international conference on multimedia and expo, vol 2, pp 1463–1466. doi:<ExternalRef><RefSource>10.1109/ICME.2004.1394511</RefSource><RefTarget Address="10.1109/ICME.2004.1394511" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR89">
                <CitationNumber>89.</CitationNumber>
                <BibUnstructured>SivaSelvan B, Gopalan NP (2007) Efficient algorithms for video association mining. In: Proceedings of the 20th conference of the Canadian Society for computational studies of intelligence on advances in artificial intelligence, vol 4509. Springer, Berlin, pp 250–260</BibUnstructured>
              </Citation>
              <Citation ID="CR90">
                <CitationNumber>90.</CitationNumber>
                <BibUnstructured>Sivic J, Zisserman A (2004) Video data mining using configurations of viewpoint invariant regions. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 488–495</BibUnstructured>
              </Citation>
              <Citation ID="CR91">
                <CitationNumber>91.</CitationNumber>
                <BibUnstructured>Su J-H, Huang Y-T, Tseng VS (2008) Efficient content-based video retrieval by mining temporal patterns. In: Proceedings of 9th international workshop on multimedia data mining associated with the ACM SIGKDD 2008, pp 36–42</BibUnstructured>
              </Citation>
              <Citation ID="CR92">
                <CitationNumber>92.</CitationNumber>
                <BibUnstructured>Suresh V, Krishna Mohan C, Kumaraswamy R, Yegnanarayana B (2005) Combining multiple evidence for video classification. In: Proceedings of IEEE international conference on intelligent sensing and information processing (ICISIP-05), pp 187–192</BibUnstructured>
              </Citation>
              <Citation ID="CR93">
                <CitationNumber>93.</CitationNumber>
                <BibBook>
                  <BibAuthorName>
                    <Initials>BM</Initials>
                    <FamilyName>Thuraisingham</FamilyName>
                  </BibAuthorName>
                  <Year>2001</Year>
                  <BookTitle>Managing and mining multimedia databases</BookTitle>
                  <PublisherName>CRC Press</PublisherName>
                  <PublisherLocation>Boca Raton</PublisherLocation>
                  <Occurrence Type="DOI">
                    <Handle>10.1201/9781420042559</Handle>
                  </Occurrence>
                </BibBook>
                <BibUnstructured>Thuraisingham BM (2001) Managing and mining multimedia databases. CRC Press, Boca Raton</BibUnstructured>
              </Citation>
              <Citation ID="CR94">
                <CitationNumber>94.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>M-C</Initials>
                    <FamilyName>Tien</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>Y-T</Initials>
                    <FamilyName>Wang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>C-W</Initials>
                    <FamilyName>Chou</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>K-Y</Initials>
                    <FamilyName>Hsieh</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>W-T</Initials>
                    <FamilyName>Chu</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>J-L</Initials>
                    <FamilyName>Wu</FamilyName>
                  </BibAuthorName>
                  <Year>2008</Year>
                  <ArticleTitle Language="En">Event detection in tennis matches based on video data mining</ArticleTitle>
                  <JournalTitle>Proc ICME</JournalTitle>
                  <VolumeID>2008</VolumeID>
                  <FirstPage>1477</FirstPage>
                  <LastPage>1480</LastPage>
                </BibArticle>
                <BibUnstructured>Tien M-C, Wang Y-T, Chou C-W, Hsieh K-Y, Chu W-T, Wu J-L (2008) Event detection in tennis matches based on video data mining. Proc ICME 2008:1477–1480</BibUnstructured>
              </Citation>
              <Citation ID="CR95">
                <CitationNumber>95.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>VS</Initials>
                    <FamilyName>Tseng</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>J-H</Initials>
                    <FamilyName>Su</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>J-H</Initials>
                    <FamilyName>Huang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>C-J</Initials>
                    <FamilyName>Chen</FamilyName>
                  </BibAuthorName>
                  <Year>2008</Year>
                  <ArticleTitle Language="En">Integrated mining of visual features, speech features, and frequent patterns for semantic video annotation</ArticleTitle>
                  <JournalTitle>IEEE Trans Multimedia</JournalTitle>
                  <VolumeID>10</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>260</FirstPage>
                  <LastPage>268</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TMM.2007.911832</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Tseng VS, Su J-H, Huang J-H, Chen C-J (2008) Integrated mining of visual features, speech features, and frequent patterns for semantic video annotation. IEEE Trans Multimedia 10(2):260–268</BibUnstructured>
              </Citation>
              <Citation ID="CR96">
                <CitationNumber>96.</CitationNumber>
                <BibUnstructured>Tien M-C, Wang Y-T, Chou C-W, Hsieh K-Y, Chu W-T, Wu J-L (2008) Event detection in tennis matches based on video data mining. In: Proceedings of ICME, pp 1477–1480</BibUnstructured>
              </Citation>
              <Citation ID="CR97">
                <CitationNumber>97.</CitationNumber>
                <BibUnstructured>Turaga PK, Veeraraghavan A, Chellappa R (2007) From videos to verbs: mining videos for activities using a cascade of dynamical systems. In: Proceedings of IEEE conference on computer vision and pattern recognition, pp 1–8. doi:<ExternalRef><RefSource>10.1109/CVPR.2007.383170</RefSource><RefTarget Address="10.1109/CVPR.2007.383170" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR98">
                <CitationNumber>98.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>L</Initials>
                    <FamilyName>Vibha</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>P</Initials>
                    <FamilyName>Chetana Hegde</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>D</Initials>
                    <FamilyName>Shenoy</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>KR</Initials>
                    <FamilyName>Venugopal</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>LM</Initials>
                    <FamilyName>Patnaik</FamilyName>
                  </BibAuthorName>
                  <Year>2008</Year>
                  <ArticleTitle Language="En">Dynamic object detection, tracking and counting in video streams for multimedia mining</ArticleTitle>
                  <JournalTitle>IAENG Int J Comput Sci</JournalTitle>
                  <VolumeID>35</VolumeID>
                  <FirstPage>3</FirstPage>
                </BibArticle>
                <BibUnstructured>Vibha L, Chetana Hegde P, Shenoy D, Venugopal KR, Patnaik LM (2008) Dynamic object detection, tracking and counting in video streams for multimedia mining. IAENG Int J Comput Sci 35:3</BibUnstructured>
              </Citation>
              <Citation ID="CR99">
                <CitationNumber>99.</CitationNumber>
                <BibUnstructured>Vailaya A, Jain A, Zhang H (1996) Video clustering. Tech Rep. No. MSU-CPS-96-64, Michigan State University, USA</BibUnstructured>
              </Citation>
              <Citation ID="CR100">
                <CitationNumber>100.</CitationNumber>
                <BibUnstructured>Vassiliadis B, Stefani A, Drossos L, Ioannou K (2005) Knowledge discovery in multimedia repositories: the role of metadata. In: Proceedings of 7th WSEAS international conference on mathematical methods and computational techniques in electrical engineering, pp 330–335</BibUnstructured>
              </Citation>
              <Citation ID="CR101">
                <CitationNumber>101.</CitationNumber>
                <BibUnstructured>Wang F, Lu W, Liu J, Shah M, Xu D (2008) Automatic video annotation with adaptive number of key words. In: Proceeding of 19th international conference on pattern recognition (ICPR 2008), pp 1–4</BibUnstructured>
              </Citation>
              <Citation ID="CR102">
                <CitationNumber>102.</CitationNumber>
                <BibUnstructured>Wang Y, Xing C, Zhou L (2006) Video semantic models: survey and evaluation. Int J Comput Sci Network Sec 6(2A):10–21</BibUnstructured>
              </Citation>
              <Citation ID="CR103">
                <CitationNumber>103.</CitationNumber>
                <BibUnstructured>Weber J, Lefever S, Gancarski P (2010) Video object mining: issues and perspectives. In: Proceedings of IEEE fourth international conference on semantic computing, pp 85–91. doi:<ExternalRef><RefSource>10.1109/ICSC.2010.71</RefSource><RefTarget Address="10.1109/ICSC.2010.71" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR104">
                <CitationNumber>104.</CitationNumber>
                <BibUnstructured>Wu C, He Y, Zhao L, Zhong Y (2002) Motion feature extraction scheme for content-based video retrieval, storage and retrieval for media databases. Proc SPIE 4676:296–305</BibUnstructured>
              </Citation>
              <Citation ID="CR105">
                <CitationNumber>105.</CitationNumber>
                <BibUnstructured>Xiong Z, Zhou XS, Tian Q, Rui Y, Huang TS (2006) Semantic retrieval of video. IEEE Signal Process Mag 23(2):18–27</BibUnstructured>
              </Citation>
              <Citation ID="CR106">
                <CitationNumber>106.</CitationNumber>
                <BibUnstructured>Xie L, Chang S-F (2006) Pattern mining in visual concept streams. In: Proceedings of IEEE international conference on multimedia and expo (ICME06), Toronto, Canada </BibUnstructured>
              </Citation>
              <Citation ID="CR107">
                <CitationNumber>107.</CitationNumber>
                <BibBook>
                  <BibAuthorName>
                    <Initials>I</Initials>
                    <FamilyName>Yahiaoui</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>B</Initials>
                    <FamilyName>Merialdo</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>B</Initials>
                    <FamilyName>Huet</FamilyName>
                  </BibAuthorName>
                  <Year>2006</Year>
                  <BookTitle>Automatic video summarization. Interactive video algorithms and technologies</BookTitle>
                  <PublisherName>Springer</PublisherName>
                  <PublisherLocation>Berlin</PublisherLocation>
                </BibBook>
                <BibUnstructured>Yahiaoui I, Merialdo B, Huet B (2006) Automatic video summarization. Interactive video algorithms and technologies. Springer, Berlin</BibUnstructured>
              </Citation>
              <Citation ID="CR108">
                <CitationNumber>108.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>X-F</Initials>
                    <FamilyName>Yang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>Q</Initials>
                    <FamilyName>Tian</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>P</Initials>
                    <FamilyName>Xue</FamilyName>
                  </BibAuthorName>
                  <Year>2007</Year>
                  <ArticleTitle Language="En">Efficient short video repeat identification with application to news video structure analysis</ArticleTitle>
                  <JournalTitle>IEEE Trans Multimedia</JournalTitle>
                  <VolumeID>9</VolumeID>
                  <IssueID>3</IssueID>
                  <FirstPage>600</FirstPage>
                  <LastPage>610</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TMM.2006.889352</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Yang X-F, Tian Q, Xue P (2007) Efficient short video repeat identification with application to news video structure analysis. IEEE Trans Multimedia 9(3):600–610</BibUnstructured>
              </Citation>
              <Citation ID="CR109">
                <CitationNumber>109.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>C</Initials>
                    <FamilyName>Zhang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>W-B</Initials>
                    <FamilyName>Chen</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>L</Initials>
                    <FamilyName>Yang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>X</Initials>
                    <FamilyName>Chen</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>JK</Initials>
                    <FamilyName>Johnstone</FamilyName>
                  </BibAuthorName>
                  <Year>2007</Year>
                  <ArticleTitle Language="En">Automatic in vivo microscopy video mining for leukocytes</ArticleTitle>
                  <JournalTitle>SIGKDD Explor</JournalTitle>
                  <VolumeID>9</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>30</FirstPage>
                  <LastPage>37</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1145/1294301.1294309</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Zhang C, Chen W-B, Yang L, Chen X, Johnstone JK (2007) Automatic in vivo microscopy video mining for leukocytes. SIGKDD Explor 9(1):30–37</BibUnstructured>
              </Citation>
              <Citation ID="CR110">
                <CitationNumber>110.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>L</Initials>
                    <FamilyName>Zhou</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>Y</Initials>
                    <FamilyName>Shi</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Feng</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Sears</FamilyName>
                  </BibAuthorName>
                  <Year>2005</Year>
                  <ArticleTitle Language="En">Data mining for detecting errors in dictation speech recognition</ArticleTitle>
                  <JournalTitle>Trans Speech Audio Process</JournalTitle>
                  <VolumeID>13</VolumeID>
                  <IssueID>5</IssueID>
                  <FirstPage>681</FirstPage>
                  <LastPage>689</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TSA.2005.851874</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Zhou L, Shi Y, Feng J, Sears A (2005) Data mining for detecting errors in dictation speech recognition. Trans Speech Audio Process 13(5):681–689</BibUnstructured>
              </Citation>
              <Citation ID="CR111">
                <CitationNumber>111.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>X</Initials>
                    <FamilyName>Zhu</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>X</Initials>
                    <FamilyName>Wu</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Elmagarmid</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>Z</Initials>
                    <FamilyName>Feng</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>L</Initials>
                    <FamilyName>Wu</FamilyName>
                  </BibAuthorName>
                  <Year>2005</Year>
                  <ArticleTitle Language="En">Video data mining: semantic indexing and event detection from the association perspective</ArticleTitle>
                  <JournalTitle>IEEE Trans Knowl Data Eng</JournalTitle>
                  <VolumeID>17</VolumeID>
                  <IssueID>5</IssueID>
                  <FirstPage>1</FirstPage>
                  <LastPage>14</LastPage>
                  <Occurrence Type="ZLBID">
                    <Handle>1087.65609</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TKDE.2005.71</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Zhu X, Wu X, Elmagarmid A, Feng Z, Wu L (2005) Video data mining: semantic indexing and event detection from the association perspective. IEEE Trans Knowl Data Eng 17(5):1–14</BibUnstructured>
              </Citation>
              <Citation ID="CR112">
                <CitationNumber>112.</CitationNumber>
                <BibUnstructured>Zang Q, Klette R (2003) Object classification and tracking in video surveillance. Communication and Information Technology Research Technical Report 128. doi:<ExternalRef><RefSource>10.1007/978-3-540-45179-2_25</RefSource><RefTarget Address="10.1007/978-3-540-45179-2_25" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR113">
                <CitationNumber>113.</CitationNumber>
                <BibUnstructured>Zhang Z (2002) Mining surveillance video for independent motion detection. In: Proceedings of IEEE international conference on data mining (ICDM), Maebashi City, Japan, Dec 2002</BibUnstructured>
              </Citation>
              <Citation ID="CR114">
                <CitationNumber>114.</CitationNumber>
                <BibUnstructured>Zhao N, Chen SC, Shyu M-L (2006) Video database modeling and temporal pattern retrieval using hierarchical Markov model mediator. In: Proceedings of first IEEE international workshop on multimedia databases&amp; data Management, pp 10–19</BibUnstructured>
              </Citation>
              <Citation ID="CR115">
                <CitationNumber>115.</CitationNumber>
                <BibUnstructured>Zhu X, Aref W, Fan J, Catlina A, Elmagarmid A (2003) Medical video mining for efficient database indexing, management and access. In: Proceedings 19th international conference on data engineering, issue 5–8, pp 569–580</BibUnstructured>
              </Citation>
              <Citation ID="CR116">
                <CitationNumber>116.</CitationNumber>
                <BibUnstructured>Zhu X, Fan J, Aref W, Elmagarmid A (2002) ClassMiner: mining medical video content structure and events towards efficient access and scalable skimming. In: Proceedings of ACM SIGMOD workshop on research issues in data mining and knowledge discovery, Madison, Wisconsin, USA, pp 9–16</BibUnstructured>
              </Citation>
              <Citation ID="CR117">
                <CitationNumber>117.</CitationNumber>
                <BibUnstructured>Zhu X, Fan J, Hacid M-S, Elmagarmid A (2002) ClassMiner: mining medical video for scalable skimming and summarization. In: Proceedings of multimedia’02, Juan-les-Pins, France</BibUnstructured>
              </Citation>
            </Bibliography>
          </ArticleBackmatter>
        </Article>
      </Issue>
    </Volume>
  </Journal>
</Publisher>
