<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE Publisher PUBLIC "-//Springer-Verlag//DTD A++ V2.4//EN" "http://devel.springer.de/A++/V2.4/DTD/A++V2.4.dtd">
<Publisher>
  <PublisherInfo>
    <PublisherName>Springer London</PublisherName>
    <PublisherLocation>London</PublisherLocation>
    <PublisherImprintName>Springer</PublisherImprintName>
  </PublisherInfo>
  <Journal OutputMedium="All">
    <JournalInfo JournalProductType="ArchiveJournal" NumberingStyle="ContentOnly">
      <JournalID>13735</JournalID>
      <JournalDOI>10.1007/13735.2192-662X</JournalDOI>
      <JournalPrintISSN>2192-6611</JournalPrintISSN>
      <JournalElectronicISSN>2192-662X</JournalElectronicISSN>
      <JournalTitle>International Journal of Multimedia Information Retrieval</JournalTitle>
      <JournalAbbreviatedTitle>Int J Multimed Info Retr</JournalAbbreviatedTitle>
      <JournalSubjectGroup>
        <JournalSubject Code="SCI" Type="Primary">Computer Science</JournalSubject>
        <JournalSubject Code="SCI18059" Priority="1" Type="Secondary">Multimedia Information Systems</JournalSubject>
        <JournalSubject Code="SCI18032" Priority="2" Type="Secondary">Information Storage and Retrieval</JournalSubject>
        <JournalSubject Code="SCI18040" Priority="3" Type="Secondary">Information Systems Applications (incl. Internet)</JournalSubject>
        <JournalSubject Code="SCI18030" Priority="4" Type="Secondary">Data Mining and Knowledge Discovery</JournalSubject>
        <JournalSubject Code="SCI22021" Priority="5" Type="Secondary">Image Processing and Computer Vision</JournalSubject>
        <JournalSubject Code="SCI00001" Priority="6" Type="Secondary">Computer Science, general</JournalSubject>
        <SubjectCollection Code="SC6">Computer Science</SubjectCollection>
      </JournalSubjectGroup>
    </JournalInfo>
    <Volume OutputMedium="All">
      <VolumeInfo TocLevels="0" VolumeType="Regular">
        <VolumeIDStart>4</VolumeIDStart>
        <VolumeIDEnd>4</VolumeIDEnd>
        <VolumeIssueCount>4</VolumeIssueCount>
      </VolumeInfo>
      <Issue IssueType="Regular" OutputMedium="All">
        <IssueInfo IssueType="Regular" TocLevels="0">
          <IssueIDStart>4</IssueIDStart>
          <IssueIDEnd>4</IssueIDEnd>
          <IssueArticleCount>6</IssueArticleCount>
          <IssueHistory>
            <OnlineDate>
              <Year>2015</Year>
              <Month>10</Month>
              <Day>7</Day>
            </OnlineDate>
            <PrintDate>
              <Year>2015</Year>
              <Month>10</Month>
              <Day>7</Day>
            </PrintDate>
            <CoverDate>
              <Year>2015</Year>
              <Month>12</Month>
            </CoverDate>
            <PricelistYear>2015</PricelistYear>
          </IssueHistory>
          <IssueCopyright>
            <CopyrightHolderName>Springer-Verlag London</CopyrightHolderName>
            <CopyrightYear>2015</CopyrightYear>
          </IssueCopyright>
        </IssueInfo>
        <Article ID="s13735-013-0043-7" OutputMedium="All">
          <ArticleInfo ArticleType="OriginalPaper" ContainsESM="No" Language="En" NumberingStyle="ContentOnly" TocLevels="0">
            <ArticleID>43</ArticleID>
            <ArticleDOI>10.1007/s13735-013-0043-7</ArticleDOI>
            <ArticleSequenceNumber>5</ArticleSequenceNumber>
            <ArticleTitle Language="En" OutputMedium="All">Studying the impact of sequence clustering on near-duplicate video retrieval: an experimental comparison</ArticleTitle>
            <ArticleCategory>Regular Paper</ArticleCategory>
            <ArticleFirstPage>279</ArticleFirstPage>
            <ArticleLastPage>288</ArticleLastPage>
            <ArticleHistory>
              <RegistrationDate>
                <Year>2013</Year>
                <Month>8</Month>
                <Day>24</Day>
              </RegistrationDate>
              <Received>
                <Year>2013</Year>
                <Month>6</Month>
                <Day>21</Day>
              </Received>
              <Accepted>
                <Year>2013</Year>
                <Month>8</Month>
                <Day>6</Day>
              </Accepted>
              <OnlineDate>
                <Year>2015</Year>
                <Month>7</Month>
                <Day>23</Day>
              </OnlineDate>
            </ArticleHistory>
            <ArticleCopyright>
              <CopyrightHolderName>Springer-Verlag London</CopyrightHolderName>
              <CopyrightYear>2015</CopyrightYear>
            </ArticleCopyright>
            <ArticleGrants Type="Regular">
              <MetadataGrant Grant="OpenAccess"/>
              <AbstractGrant Grant="OpenAccess"/>
              <BodyPDFGrant Grant="Restricted"/>
              <BodyHTMLGrant Grant="Restricted"/>
              <BibliographyGrant Grant="Restricted"/>
              <ESMGrant Grant="Restricted"/>
            </ArticleGrants>
          </ArticleInfo>
          <ArticleHeader>
            <AuthorGroup>
              <Author AffiliationIDS="Aff1" ID="Au1">
                <AuthorName DisplayOrder="Western">
                  <GivenName>Yandan</GivenName>
                  <FamilyName>Wang</FamilyName>
                </AuthorName>
                <Contact>
                  <Email>yandan.wang@wzu.edu.cn</Email>
                </Contact>
              </Author>
              <Author AffiliationIDS="Aff2" CorrespondingAffiliationID="Aff2" ID="Au2">
                <AuthorName DisplayOrder="Western">
                  <GivenName>Mohammed</GivenName>
                  <FamilyName>Belkhatir</FamilyName>
                </AuthorName>
                <Contact>
                  <Email>mohammed.belkhatir@univ-lyon1.fr</Email>
                </Contact>
              </Author>
              <Affiliation ID="Aff1">
                <OrgDivision>School of Physics and Electronic Information Engineering</OrgDivision>
                <OrgName>Wenzhou University</OrgName>
                <OrgAddress>
                  <City>Wenzhou</City>
                  <State>Zhejiang</State>
                  <Country Code="CN">China</Country>
                </OrgAddress>
              </Affiliation>
              <Affiliation ID="Aff2">
                <OrgDivision>Faculty of Computer Science</OrgDivision>
                <OrgName>University of Lyon</OrgName>
                <OrgAddress>
                  <City>Villeurbanne</City>
                  <Country Code="FR">France</Country>
                </OrgAddress>
              </Affiliation>
            </AuthorGroup>
            <Abstract ID="Abs1" Language="En" OutputMedium="All">
              <Heading>Abstract</Heading>
              <Para ID="Par1">In this paper, we propose studying the impact of clustering on near-duplicate video (NDV) retrieval. The aim is to reduce the search space at retrieval time through a pre-processing clustering step performed on the dataset off-line and retrieving NDVs based on the formed clusters. Our contribution is a novel clustering framework inspired by a bioinformatics technique, namely DNA multiple sequence alignment (MSA). A series of video keyframes in chronological order is represented as an alphabetical genome, analogous to a DNA sequence and MSA is employed to automatically partition the NDVs in a video collection into clusters. After discussing the advantages and shortcomings of the main state-of-the-art clustering approaches for video clustering in the theoretical part of the paper, we empirically evaluate the performance of the proposed MSA-based framework against five clustering algorithms representative of these mainstream approaches: Birch, Cure, Dbscan, Expectation-Maximization and Proclus. Also, we show that our clustering-based approach, while being significantly faster than non-clustering-based <Emphasis Type="Italic">n</Emphasis>-gram and edit distance NDV retrieval techniques, yields better mean average precision retrieval accuracy.</Para>
            </Abstract>
            <KeywordGroup Language="En" OutputMedium="All">
              <Heading>Keywords</Heading>
              <Keyword>Near-duplicate video retrieval</Keyword>
              <Keyword>Clustering</Keyword>
              <Keyword>Multiple sequence alignment</Keyword>
            </KeywordGroup>
          </ArticleHeader>
          <Body>
            <Section1 ID="Sec1">
              <Heading>Introduction</Heading>
              <Para ID="Par2">Broadband internet access is nowadays increasingly common and facilitates rapid advancement of multimedia technology, in particular uploading, editing, indexing and searching for large quantities of video data. Online video collections are indeed rapidly becoming larger, in no small part due to plenty of duplicated or near duplicate videos (NDVs) being collected. With millions or even billions of videos, these collections are very difficult to control, manage, maintain and search. Therefore, the investigation of effective content management approaches plays an important role in improving the management of large-scale video collections and also enhancing the performance of content-based low-level video retrieval. Automatic video content-based clustering is one way to address this topical issue, facilitating organization to improve the effectiveness of video document indexing and access. Based on a sample of 24 popular queries from YouTube, Google Video and Yahoo! Video, on average there are 27 % redundant videos that are duplicates or near duplicates of the most popular version of a video in the search results [<CitationRef CitationID="CR6">6</CitationRef>]. Banking on this observation, the need to develop tools to identify NDVs becomes obvious. In this field, a wide range of index structures has been studied for the sake of improving the accuracy of search results, e.g., [<CitationRef CitationID="CR1">1</CitationRef>, <CitationRef CitationID="CR6">6</CitationRef>, <CitationRef CitationID="CR7">7</CitationRef>]. However, video collections are often large and their processing is computationally intensive. As a consequence, off-line pre-processing techniques to assist in reducing the computational load at retrieval are needed. Accordingly, we propose an NDV retrieval framework that incorporates a clustering step, used to pre-process the video collection. The clustering technique is based on the concept of multiple sequence alignment (MSA) consists in aligning multiple proteins or DNA sequences to discover whether they have a common ancestor. Approaches such as progressive alignment and iterative methods [<CitationRef CitationID="CR3">3</CitationRef>] have been proposed to tackle this issue, along with the popular dynamic programming solution which, however, is infeasible for aligning large numbers of sequences due to its exponential complexity. Feng and Dolittle [<CitationRef CitationID="CR8">8</CitationRef>] were the first to propose progressive alignment for MSA to address the problem of high computational complexity and real-time processing requirements. More recently, programs such as Clustal series [<CitationRef CitationID="CR5">5</CitationRef>] and T-Coffee [<CitationRef CitationID="CR2">2</CitationRef>] have been developed that use the progressive alignment principle proposed in [<CitationRef CitationID="CR2">2</CitationRef>] to find effective and efficient techniques to align multiple DNA sequences.</Para>
              <Para ID="Par3">Experimentally, we evaluate the performance of the proposed MSA-based framework against five algorithms representative of the state-of-the-art mainstream clustering approaches: Birch, Cure, Dbscan, Expectation-Maximization (EM) and Proclus. Also, we show that MSA-based clustering makes NDV retrieval faster while yielding better mean average precision (MAP) results in terms of retrieval accuracy than a non-clustering-based approach and state-of-the-art techniques.</Para>
              <Para ID="Par4">The goals of our paper are:<OrderedList>
                  <ListItem>
                    <ItemNumber>(a)</ItemNumber>
                    <ItemContent>
                      <Para ID="Par5">To introduce a novel framework for NDV retrieval based on a pre-processing clustering step for the sake of scalability while enforcing retrieval accuracy.</Para>
                    </ItemContent>
                  </ListItem>
                  <ListItem>
                    <ItemNumber>(b)</ItemNumber>
                    <ItemContent>
                      <Para ID="Par6">To propose a novel clustering technique based on MSA, which is particularly suitable for signature-based video indexing. To the best of our knowledge, it is the first clustering framework based on an alignment scheme rather than the state-of-the-art metric-based techniques.</Para>
                    </ItemContent>
                  </ListItem>
                  <ListItem>
                    <ItemNumber>(c)</ItemNumber>
                    <ItemContent>
                      <Para ID="Par7">To empirically demonstrate the effectiveness of our clustering-based NDV retrieval framework, in terms of retrieval accuracy and speed, with respect to a non-clustering-based framework on the one hand and both state-of-the-art and clustering-based NDV retrieval systems on the other hand.</Para>
                    </ItemContent>
                  </ListItem>
                </OrderedList>The rest of the paper is organized as follows. In Sect. <InternalRef RefID="Sec2">2</InternalRef>, we review related works in NDV retrieval and present state-of-the-art clustering techniques. Section <InternalRef RefID="Sec7">3</InternalRef> presents the anatomy of our clustering-based NDV retrieval framework and details the proposed MSA-based clustering technique. The experimental evaluation is presented in Sect. <InternalRef RefID="Sec15">4</InternalRef> and the conclusions and perspectives given in Sect. <InternalRef RefID="Sec19">5</InternalRef>.</Para>
            </Section1>
            <Section1 ID="Sec2">
              <Heading>Related work</Heading>
              <Para ID="Par8">We propose in this section to review related works in NDV retrieval and then present the main state-of-the-art approaches for data clustering, which is the core component of our proposed NDV retrieval framework.</Para>
              <Section2 ID="Sec3">
                <Heading>Near-duplicate video retrieval</Heading>
                <Para ID="Par9">Two main aspects are traditionally studied to improve NDV retrieval performance in terms of accuracy and speed: (i) Indexing, which deals with the representation of video documents. (ii) Matching, which encompasses the measurement of similarity between video representations. To cope with large-scale collections, several frameworks (e.g., [<CitationRef CitationID="CR9">9</CitationRef>, <CitationRef CitationID="CR10">10</CitationRef>, <CitationRef CitationID="CR17">17</CitationRef>–<CitationRef CitationID="CR19">19</CitationRef>]) propose addressing issues related to both indexing and matching for effective NDV retrieval.</Para>
                <Section3 ID="Sec4">
                  <Heading>Video indexing structures</Heading>
                  <Para ID="Par10">Among the different types of video indexing structures, signature-based representations are the most common in the literature due to their often straightforward computation. Considering a sequence of frames (or keyframes) ordered temporally, low-level features such as colors, textures and shapes... are extracted and used to represent the content. Due to the trivial nature of such descriptors to characterize the video, retrieval performance is often sub-par. More elaborate features such as the ordinal feature [<CitationRef CitationID="CR25">25</CitationRef>] have been suggested to further improve the retrieval accuracy. The ordinal feature is based on splitting keyframes into <Emphasis Type="Italic">nxn</Emphasis> blocks, which are then processed for low-level feature extraction prior to matching and ranking. Ordinal features take therefore into account spatial information. The local interest point-based representations (such as SIFT, SURF...) are the most complex and expensive to compute, but have been shown to achieve the best retrieval performance. To reduce the computationally intensive keyframe representation process with hundreds or thousands of local interest point descriptors, index structures such as PCA-SIFT [<CitationRef CitationID="CR27">27</CitationRef>] have been proposed to characterize the frame content in a more compact manner. However, it is still difficult to use local feature-based index structures for large-scale video retrieval. The most successful extension of local interest point descriptors for video retrieval in large-scale collections is the high-level Bag-of-Words (BoW) structure, which either consists of SIFT features [<CitationRef CitationID="CR12">12</CitationRef>] or color features for the sake of reducing the computational load [<CitationRef CitationID="CR11">11</CitationRef>]. To enforce scalability, histogram-based index representations (such as the bounded coordinate system (BCS) [<CitationRef CitationID="CR17">17</CitationRef>]) propose representing the video content through global or local keyframe-based features. The authors in [<CitationRef CitationID="CR7">7</CitationRef>] characterize the video content with a 720-dimensional color-based histogram. The latter is generated by dividing each keyframe into six blocks and then ranking them, thus resulting in 720 possible rankings which constitute the bins of the histogram. Although it is advantageous in terms of compactness and scalability, the drawback of this type of representation is that it ignores video temporal information. Balancing retrieval accuracy and computational effectiveness through signature-based indexing is not trivial. On the one hand, compact signatures entail faster retrieval computations at the expense of richly characterizing the video content. On the other hand, non-trivial signatures preserving both spatial and temporal information are responsible for more computationally intensive retrieval computations. The need for more elaborate indexing structures to assist in effectively narrowing the search space at retrieval time is therefore prevalent.</Para>
                  <Para ID="Par11">A second category of structures, i.e., the tree-based structures, have been designed to cope with this issue, therefore allowing to perform scalable real-time retrieval. The mrkd-tree [<CitationRef CitationID="CR20">20</CitationRef>], kd-tree [<CitationRef CitationID="CR21">21</CitationRef>] and GeM-tree [<CitationRef CitationID="CR22">22</CitationRef>] are examples of such structures making it possible to handle retrieval in high-dimensional spaces of low-level features. A detailed survey of multimedia index structures for fast query processing [<CitationRef CitationID="CR24">24</CitationRef>] also mentions tree-based descriptors such as the R-tree, B-tree, Multidimensional B<InlineEquation ID="IEq1">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2013_43_Article_IEq1.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$^{+}$$]]></EquationSource>
                      <EquationSource Format="MATHML">
                        <math xmlns:xlink="http://www.w3.org/1999/xlink">
                          <msup>
                            <mrow/>
                            <mo>+</mo>
                          </msup>
                        </math>
                      </EquationSource>
                    </InlineEquation>-tree (MB<InlineEquation ID="IEq2">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2013_43_Article_IEq2.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$^{+}$$]]></EquationSource>
                      <EquationSource Format="MATHML">
                        <math xmlns:xlink="http://www.w3.org/1999/xlink">
                          <msup>
                            <mrow/>
                            <mo>+</mo>
                          </msup>
                        </math>
                      </EquationSource>
                    </InlineEquation>-tree) and Telescopic Vector-tree (TV-tree).</Para>
                  <Para ID="Par12">The most recent approaches use more elaborate indexing schemes for the sake of scalability. In the work presented in [<CitationRef CitationID="CR11">11</CitationRef>], the authors propose processing a large dataset of 1 million videos. Scalability is achieved by using the Bag-of-Words (BoW) index descriptor. The latter is obtained after quantizing the high-dimensional global frame features into visual words (i.e., a specific grouping of low-level features for the high-level characterization of the video content) which are then assembled together to form the BoW. An inverted file is then used to index the visual words for quickly processing the query video. Another indexing approach coined coarse-to-fine filtering strategy [<CitationRef CitationID="CR25">25</CitationRef>, <CitationRef CitationID="CR26">26</CitationRef>] is based on two filtering stages: the first (i.e., coarse filtering) consists of processing the dataset based on cheap-to-compute global features, while the second aims at locating the search target(s) in the dimensionality reduced search space for focused matching.</Para>
                </Section3>
                <Section3 ID="Sec5">
                  <Heading>Matching techniques</Heading>
                  <Para ID="Par13">Matching techniques aim at measuring the similarity between video index structures and a given query by assigning a matching score. The latter involves a computation that must be both effective in terms of assessing the degree of similarity between video query and index structures on the one hand and computationally effective on the other hand. Metric-based similarity computation (e.g., Euclidean distance, cosine similarity, etc.) is the most common means employed to measure the similarity between histogram-based video representations. We previously discussed that videos can be represented as a sequence of keyframe feature descriptors ordered temporally. Therefore, a variety of sequence alignment techniques have been explored for measuring pairwise similarity of index and query representations. The most common matching paradigm for sequence-based representations is dynamic programming [<CitationRef CitationID="CR17">17</CitationRef>, <CitationRef CitationID="CR25">25</CitationRef>, <CitationRef CitationID="CR28">28</CitationRef>], for which the distance between two videos is measured by the penalty of alignment, i.e., the addition of the penalties associated with matches, mismatches and gaps. Several techniques such as edit distance, dynamic time warping and longest common substring have previously been studied. However, dynamic programming is rather slow to compute and also very sensitive to the length difference between two video sequences. Accordingly, an <Emphasis Type="Italic">n</Emphasis>-gram-based technique developed to overcome this issue was shown to outperform dynamic programming-based state-of-the-art techniques in terms of both retrieval accuracy and speed [<CitationRef CitationID="CR1">1</CitationRef>]. Other similarity measurement techniques can be found in the state-of-the-art in accordance with the video representation scheme. For example, two similarity measurement techniques are proposed in [<CitationRef CitationID="CR30">30</CitationRef>], the one-to-one matching scheme based on Near Duplicate Keyframe Cliques that connect all near duplicate keyframes within a same group and the Earth Mover’s Distance similarity computation. Two alternative similarity measures in addition to the latter are introduced in [<CitationRef CitationID="CR31">31</CitationRef>] adapted for the introduced video cuboid signature: the <InlineEquation ID="IEq3">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2013_43_Article_IEq3.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$L_\mathrm{p}$$]]></EquationSource>
                      <EquationSource Format="MATHML">
                        <math xmlns:xlink="http://www.w3.org/1999/xlink">
                          <msub>
                            <mi>L</mi>
                            <mi mathvariant="normal">p</mi>
                          </msub>
                        </math>
                      </EquationSource>
                    </InlineEquation>-norm distance and the Jaccard similarity measure. In [<CitationRef CitationID="CR32">32</CitationRef>], the results of video search are re-ranked by a Bayesian framework based on hinge distance and preference strength distance. In [<CitationRef CitationID="CR10">10</CitationRef>], the authors adopt a non-expensive computational histogram intersection method to measure similarity by considering compact spatio-temporal binary video representations.<Figure Category="Standard" Float="Yes" ID="Fig1">
                      <Caption Language="En">
                        <CaptionNumber>Fig. 1</CaptionNumber>
                        <CaptionContent>
                          <SimplePara>Clustering-based NDV retrieval framework</SimplePara>
                        </CaptionContent>
                      </Caption>
                      <MediaObject ID="MO1">
                        <ImageObject Color="Color" FileRef="MediaObjects/13735_2013_43_Fig1_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                      </MediaObject>
                    </Figure>
                           </Para>
                </Section3>
              </Section2>
              <Section2 ID="Sec6">
                <Heading>Clustering techniques</Heading>
                <Para ID="Par14">In this section, we briefly review the main clustering techniques in the literature and provide example algorithms illustrating these techniques:<UnorderedList Mark="Bullet">
                    <ItemContent>
                      <Para ID="Par15">
                                    <Emphasis Type="Italic">Partitioning clustering</Emphasis> constructs various partitions of the data and then evaluates them by some criterion. The commonly used K-means [<CitationRef CitationID="CR4">4</CitationRef>] clustering algorithm illustrates this technique, it has linear complexity and is trivial to implement.</Para>
                    </ItemContent>
                    <ItemContent>
                      <Para ID="Par16">
                                    <Emphasis Type="Italic">Hierarchical clustering</Emphasis> creates a hierarchical decomposition of the data, either through agglomerative or divisive approaches. The Birch [<CitationRef CitationID="CR12">12</CitationRef>] and Cure [<CitationRef CitationID="CR13">13</CitationRef>] algorithms are representative of this approach.</Para>
                    </ItemContent>
                    <ItemContent>
                      <Para ID="Par17">
                                    <Emphasis Type="Italic">Density-based clustering</Emphasis> introduces connectivity and density functions to form clusters. For example, the Dbscan [<CitationRef CitationID="CR16">16</CitationRef>] clustering algorithm is based on forming clusters by meeting the criteria of specified density within a geometrical circular shape.</Para>
                    </ItemContent>
                    <ItemContent>
                      <Para ID="Par18">
                                    <Emphasis Type="Italic">Grid-based clustering</Emphasis>, implemented by the clique [<CitationRef CitationID="CR29">29</CitationRef>] algorithm, is based on a multiple-level granularity structure.</Para>
                    </ItemContent>
                    <ItemContent>
                      <Para ID="Par19">
                                    <Emphasis Type="Italic">Model-based clustering</Emphasis> techniques, such as EM [<CitationRef CitationID="CR14">14</CitationRef>], rely on hypothesizing models for the formed clusters and finding the best fit for each other.</Para>
                    </ItemContent>
                    <ItemContent>
                      <Para ID="Par20">
                                    <Emphasis Type="Italic">Subspace-based clustering</Emphasis> projects high-dimensional data into lower dimensions for clustering. The Orclus [<CitationRef CitationID="CR23">23</CitationRef>] and Proclus [<CitationRef CitationID="CR15">15</CitationRef>] algorithms illustrate this approach.</Para>
                    </ItemContent>
                  </UnorderedList>All the above-listed clustering techniques are suitable to cluster data in the form of numerical vectors. An <InlineEquation ID="IEq4">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2013_43_Article_IEq4.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$n \times m$$]]></EquationSource>
                    <EquationSource Format="MATHML">
                      <math xmlns:xlink="http://www.w3.org/1999/xlink">
                        <mrow>
                          <mi>n</mi>
                          <mo>×</mo>
                          <mi>m</mi>
                        </mrow>
                      </math>
                    </EquationSource>
                  </InlineEquation> matrix is derived, where <Emphasis Type="Italic">n</Emphasis> is the number of data items and <Emphasis Type="Italic">m</Emphasis> is the length of the vector. However, to take into account temporal information, a video signature consists of a chronological concatenation of keyframe features such as colors, edge histograms, etc. In such a representation scheme, the characterization of the video content takes the form of a sequence of numerical vectors or a string of symbols. Accordingly, we propose developing a clustering technique based on sequence alignment to cater to video representations consisting of a sequence of temporally ordered keyframe features.</Para>
              </Section2>
            </Section1>
            <Section1 ID="Sec7">
              <Heading>NDV retrieval framework</Heading>
              <Para ID="Par21">Our aim is to study the impact of clustering on NDV retrieval effectiveness and computational efficiency. In tradition, video collections are not pre-processed before the indexing and retrieval operations. In our work, we explore the impact of pre-processing the video collection through a clustering step such that most similar videos are grouped into the same clusters to facilitate the subsequent retrieval of videos of interest.</Para>
              <Section2 ID="Sec8">
                <Heading>Overall organization</Heading>
                <Para ID="Par22">Figure <InternalRef RefID="Fig1">1</InternalRef> shows the overall clustering-based NDV retrieval framework. First, all videos in the collection are preprocessed through segmentation and keyframe extraction techniques, resulting in a video being characterized by a series of keyframes in chronological order. Representative features for each keyframe are automatically extracted. The entire video is described as a sequence of low-level features represented by symbols. We then process the video dataset offline through clustering. By this, the computational load at retrieval time is obviously reduced in the sense that a query video is compared to index representations of cluster centroids rather than those of all videos in the collection.</Para>
                <Para ID="Par23">As discussed in the related work section, the state-of-the-art clustering techniques are only suitable for video representations in the form of numerical vectors with equal lengths. However, videos are widely represented as variable length sequences of low-level features. Therefore state-of-the-art clustering techniques are only applicable for a limited subset of video index representations. However, the proposed MSA-based clustering technique caters to video index sequences of variable lengths and is therefore chosen as a pre-processing step of the NDV retrieval framework. The technique is further detailed in the next section.<Figure Category="Standard" Float="Yes" ID="Fig2">
                    <Caption Language="En">
                      <CaptionNumber>Fig. 2</CaptionNumber>
                      <CaptionContent>
                        <SimplePara>MSA clustering framework</SimplePara>
                      </CaptionContent>
                    </Caption>
                    <MediaObject ID="MO16">
                      <ImageObject Color="Color" FileRef="MediaObjects/13735_2013_43_Fig2_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                    </MediaObject>
                  </Figure>
                        </Para>
              </Section2>
              <Section2 ID="Sec9">
                <Heading>MSA-based video clustering</Heading>
                <Para ID="Par24">The overview of the MSA-based video clustering process is proposed in Fig. <InternalRef RefID="Fig2">2</InternalRef> and detailed in Algorithm 1. First, an important aspect to highlight is that a video (examples from the experimental dataset are shown in Fig. <InternalRef RefID="Fig3">3</InternalRef>) is described as a sequence of low-level features represented by symbols analogous to a DNA sequence. Second, a proximity measure (distance or similarity) is computed for pairs of video sequences and a proximity matrix constructed. Third, based on the proximity matrix, a guide tree is built using a neighbor joining algorithm that: (i) creates a profile (i.e., a group of aligned video DNA sequences) by joining the two most related videos to generate the first branching of the guide tree, then (ii) progressively joins the less proximate videos into the guide tree until all videos are included in hierarchical groups. The guide tree heuristic only roughly classifies the videos into clusters. Fourth, the refinement of the clustering is done by progressive multiple sequence alignment according to the guide tree. Alignment may be between one video and another video (1-to-1 alignment), one video and one video profile (1-to-<Emphasis Type="Italic">n</Emphasis> alignment), or one profile to another profile (<Emphasis Type="Italic">m</Emphasis>-to-<Emphasis Type="Italic">n</Emphasis> alignment). Fifth, the video sequence set will be aligned with similar fragments in blocks as shown in Fig. <InternalRef RefID="Fig2">2</InternalRef>, Stage 5. A score for each profile alignment result is computed and videos in this alignment will be considered members of one cluster if this score is above a given threshold.</Para>
                <Figure Category="Standard" Float="No" ID="Figa">
                  <MediaObject ID="MO3">
                    <ImageObject Color="BlackWhite" FileRef="MediaObjects/13735_2013_43_Figa_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                  </MediaObject>
                </Figure>
                <Para ID="Par25">The details of the algorithms applied in each phase are described below. In Sect. <InternalRef RefID="Sec10">3.2.1</InternalRef>, we introduce the video signature representation process. The construction of the proximity matrix is introduced in Sect. <InternalRef RefID="Sec11">3.2.2</InternalRef>. The guide tree construction process is detailed in Sect. <InternalRef RefID="Sec12">3.2.3</InternalRef>. In Sect. <InternalRef RefID="Sec13">3.2.4</InternalRef>, we describe how to select videos and profiles and process progressive multiple sequence alignment of the selected elements. Then we discuss how to measure the alignment result and assign a score to decide on the formation of a cluster in Sect. <InternalRef RefID="Sec14">3.2.5</InternalRef>.</Para>
                <Section3 ID="Sec10">
                  <Heading>Video signature representation</Heading>
                  <Para ID="Par26">
                    <Figure Category="Standard" Float="Yes" ID="Fig3">
                      <Caption Language="En">
                        <CaptionNumber>Fig. 3</CaptionNumber>
                        <CaptionContent>
                          <SimplePara>NDV examples</SimplePara>
                        </CaptionContent>
                      </Caption>
                      <MediaObject ID="MO17">
                        <ImageObject Color="Color" FileRef="MediaObjects/13735_2013_43_Fig3_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                      </MediaObject>
                    </Figure>
                    <Figure Category="Standard" Float="Yes" ID="Fig4">
                      <Caption Language="En">
                        <CaptionNumber>Fig. 4</CaptionNumber>
                        <CaptionContent>
                          <SimplePara>Keyframe pattern extraction processing</SimplePara>
                        </CaptionContent>
                      </Caption>
                      <MediaObject ID="MO18">
                        <ImageObject Color="BlackWhite" FileRef="MediaObjects/13735_2013_43_Fig4_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </MediaObject>
                    </Figure>
                  </Para>
                  <Para ID="Par27">To represent videos as DNA sequences, we borrow the idea of the spatial ordinal based representation described in [<CitationRef CitationID="CR7">7</CitationRef>] with slight changes as shown in Fig. <InternalRef RefID="Fig4">4</InternalRef>, where we illustrate how to represent the frame with a single alphabet symbol. As far as ranking is concerned, instead of dividing keyframe into <InlineEquation ID="IEq5">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2013_43_Article_IEq5.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$3 \times 2$$]]></EquationSource>
                      <EquationSource Format="MATHML">
                        <math xmlns:xlink="http://www.w3.org/1999/xlink">
                          <mrow>
                            <mn>3</mn>
                            <mo>×</mo>
                            <mn>2</mn>
                          </mrow>
                        </math>
                      </EquationSource>
                    </InlineEquation> blocks and using the Y, Cb, Cr color data to compute the block average value, we divide each keyframe into <InlineEquation ID="IEq6">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2013_43_Article_IEq6.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$2 \times 2$$]]></EquationSource>
                      <EquationSource Format="MATHML">
                        <math xmlns:xlink="http://www.w3.org/1999/xlink">
                          <mrow>
                            <mn>2</mn>
                            <mo>×</mo>
                            <mn>2</mn>
                          </mrow>
                        </math>
                      </EquationSource>
                    </InlineEquation> blocks and use the gray-scale measurements. For <InlineEquation ID="IEq7">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2013_43_Article_IEq7.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$2 \times 2$$]]></EquationSource>
                      <EquationSource Format="MATHML">
                        <math xmlns:xlink="http://www.w3.org/1999/xlink">
                          <mrow>
                            <mn>2</mn>
                            <mo>×</mo>
                            <mn>2</mn>
                          </mrow>
                        </math>
                      </EquationSource>
                    </InlineEquation> blocks, we have <InlineEquation ID="IEq8">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2013_43_Article_IEq8.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$4 \times 3 \times 2 \times 1=24$$]]></EquationSource>
                      <EquationSource Format="MATHML">
                        <math xmlns:xlink="http://www.w3.org/1999/xlink">
                          <mrow>
                            <mn>4</mn>
                            <mo>×</mo>
                            <mn>3</mn>
                            <mo>×</mo>
                            <mn>2</mn>
                            <mo>×</mo>
                            <mn>1</mn>
                            <mo>=</mo>
                            <mn>24</mn>
                          </mrow>
                        </math>
                      </EquationSource>
                    </InlineEquation> possible spatial ordinal patterns. For each pattern, a single alphabet symbol is assigned. In total, 24 letters are introduced: A to X. In such a process, each video is represented by a sequence of alphabetical symbols ranging from A to X named video DNA.</Para>
                </Section3>
                <Section3 ID="Sec11">
                  <Heading>Generation of the video proximity matrix</Heading>
                  <Para ID="Par28">As shown in Fig. <InternalRef RefID="Fig6">6</InternalRef>, the proximity matrix is built up by storing pairwise proximity (similarity) measures. The similarity is computed by a sliding window-based <Emphasis Type="Italic">n</Emphasis>-gram technique; however, any similarity/distance metric could be applied. The comparison between two video DNA representations is performed only within the window configuration (Fig. <InternalRef RefID="Fig5">5</InternalRef>). The window size and step size (which is the size to slide the window forward after comparison is done) are given as parameters. At each point, the similarity between two videos within the current window is calculated and compared to the similarity calculation from the previous window location. The best similarity score will be preserved until the sliding window has moved along the entire video DNA representation. The details of the algorithm are shown in Algorithm 2 [<CitationRef CitationID="CR1">1</CitationRef>].</Para>
                  <Figure Category="Standard" Float="No" ID="Figb">
                    <MediaObject ID="MO6">
                      <ImageObject Color="BlackWhite" FileRef="MediaObjects/13735_2013_43_Figb_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                  </Figure>
                  <Para ID="Par29">
                    <Figure Category="Standard" Float="Yes" ID="Fig5">
                      <Caption Language="En">
                        <CaptionNumber>Fig. 5</CaptionNumber>
                        <CaptionContent>
                          <SimplePara>Overview of the sliding window-based similarity technique</SimplePara>
                        </CaptionContent>
                      </Caption>
                      <MediaObject ID="MO19">
                        <ImageObject Color="Color" FileRef="MediaObjects/13735_2013_43_Fig5_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                      </MediaObject>
                    </Figure>
                    <Figure Category="Standard" Float="Yes" ID="Fig6">
                      <Caption Language="En">
                        <CaptionNumber>Fig. 6</CaptionNumber>
                        <CaptionContent>
                          <SimplePara>Proximity matrix</SimplePara>
                        </CaptionContent>
                      </Caption>
                      <MediaObject ID="MO20">
                        <ImageObject Color="BlackWhite" FileRef="MediaObjects/13735_2013_43_Fig6_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </MediaObject>
                    </Figure>
                    <Figure Category="Standard" Float="Yes" ID="Fig7">
                      <Caption Language="En">
                        <CaptionNumber>Fig. 7</CaptionNumber>
                        <CaptionContent>
                          <SimplePara>Joining of V3 and V4</SimplePara>
                        </CaptionContent>
                      </Caption>
                      <MediaObject ID="MO21">
                        <ImageObject Color="BlackWhite" FileRef="MediaObjects/13735_2013_43_Fig7_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </MediaObject>
                    </Figure>
                  </Para>
                </Section3>
                <Section3 ID="Sec12">
                  <Heading>Construction of the guide tree</Heading>
                  <Para ID="Par30">The guide tree is built incrementally following a greedy heuristic neighbor joining algorithm. It is based on the selection of two video DNA descriptors that have the highest similarity according to the proximity matrix, and joining them in the same tree node as a profile (Fig. <InternalRef RefID="Fig7">7</InternalRef>). In this model, each video DNA is a leaf node of the guide tree. After every joining of one video to one video (1:1), one video to a video profile (1:<Emphasis Type="Italic">n</Emphasis>) and one video profile to another video profile (<Emphasis Type="Italic">m</Emphasis>:<Emphasis Type="Italic">n</Emphasis>), the proximity matrix is updated by computing the average proximity of the new profile to all of the other videos and video profiles.</Para>
                </Section3>
                <Section3 ID="Sec13">
                  <Heading>Progressive multiple video alignment</Heading>
                  <Para ID="Par31">In this phase, we select videos according to the guide tree and align them progressively using a dynamic programming technique. Normalized sum-of-pairs scoring is applied to measure profile alignment at each phase of the process. The scoring result will be used in the cluster formation phase to make a decision on whether videos in the aligned profile shall continue to be considered for the alignment process or be gathered within one cluster.</Para>
                  <Para ID="Par32">
                              <Emphasis Type="Italic">3.2.4.1 Video sequence selection</Emphasis> We select videos that are related according to the guide tree constructed in the previous phase. The order of selection follows that of the insertion within the guide tree, which is heuristic. After relevant videos are selected, they are next considered for the progressive alignment process.</Para>
                  <Para ID="Par33">
                              <Emphasis Type="Italic">3.2.4.2 Progressive alignment</Emphasis> Three cases are considered for progressive alignment (i) One-to-one: a single video DNA is aligned to a single video DNA. (ii) One-to-profile: a single video DNA is aligned to many video DNAs (i.e. a profile). (iii) Profile-to-profile: many video DNAs are aligned to many videos DNAs. In the simplest one-to-one case, two video DNAs are aligned in the same fashion as two alphabet sequences by using dynamic programming, e.g., edit distance.</Para>
                  <Para ID="Par34">In the most complex profile-to-profile alignment case, the video profile is treated as a sequence of data points where the number of points is equal to the number of profile columns and the number of dimensions is equal to the number of rows. The data points in the profile are the same as the number of dimensions, since the profile is a row <InlineEquation ID="IEq9">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2013_43_Article_IEq9.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$\times $$]]></EquationSource>
                      <EquationSource Format="MATHML">
                        <math xmlns:xlink="http://www.w3.org/1999/xlink">
                          <mo>×</mo>
                        </math>
                      </EquationSource>
                    </InlineEquation> column matrix. In other words, profile-to-profile alignment involves exactly aligning two sequences of data points (time series), which in the case of the video DNA representation consists of a succession of letters. Accordingly, dynamic time warping is applied to align a profile to another profile. Once a gap is introduced in a profile, it is kept in each sequence at the same column, which follows the policy of “once a gap, always a gap” [<CitationRef CitationID="CR3">3</CitationRef>]. Algorithm 3 details profile-to-profile alignment. The one-to-profile case is itself a simplification of the profile-to-profile process.</Para>
                  <Figure Category="Standard" Float="No" ID="Figc">
                    <MediaObject ID="MO10">
                      <ImageObject Color="BlackWhite" FileRef="MediaObjects/13735_2013_43_Figc_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </MediaObject>
                  </Figure>
                </Section3>
                <Section3 ID="Sec14">
                  <Heading>Formation of video clusters</Heading>
                  <Para ID="Par35">Normalized sum-of-pairs scoring is used to evaluate the similarity between the profiles resulting from the progressive alignment step. The final score is computed as the average of the normalized scores for each column. As an illustration, we consider <Emphasis Type="Italic">n</Emphasis> video DNA representations in the alignment for which we perform the scoring operation. With <Emphasis Type="Italic">k</Emphasis> columns in the aligned profile, SP(C), calculated as in Eq. <InternalRef RefID="Equ1">1</InternalRef>, is the normalized sum of each column score:<Equation ID="Equ1">
                      <EquationNumber>1</EquationNumber>
                      <MediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2013_43_Article_Equ1.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </MediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$\begin{aligned} \mathrm{SP(C)}=\frac{1}{k}\sum \limits _{i=1}^k {\mathrm{SP}(c_i)} \end{aligned}$$]]></EquationSource>
                      <EquationSource Format="MATHML">
                        <math xmlns:xlink="http://www.w3.org/1999/xlink" display="block">
                          <mrow>
                            <mtable columnspacing="0.5ex">
                              <mtr>
                                <mtd columnalign="right">
                                  <mrow>
                                    <mrow>
                                      <mi mathvariant="normal">SP</mi>
                                      <mo stretchy="false">(</mo>
                                      <mi mathvariant="normal">C</mi>
                                      <mo stretchy="false">)</mo>
                                    </mrow>
                                    <mo>=</mo>
                                    <mfrac>
                                      <mn>1</mn>
                                      <mi>k</mi>
                                    </mfrac>
                                    <munderover>
                                      <mo movablelimits="false">∑</mo>
                                      <mrow>
                                        <mi>i</mi>
                                        <mo>=</mo>
                                        <mn>1</mn>
                                      </mrow>
                                      <mi>k</mi>
                                    </munderover>
                                    <mrow>
                                      <mi mathvariant="normal">SP</mi>
                                      <mo stretchy="false">(</mo>
                                      <msub>
                                        <mi>c</mi>
                                        <mi>i</mi>
                                      </msub>
                                      <mo stretchy="false">)</mo>
                                    </mrow>
                                  </mrow>
                                </mtd>
                              </mtr>
                            </mtable>
                          </mrow>
                        </math>
                      </EquationSource>
                    </Equation>where SP<InlineEquation ID="IEq10">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2013_43_Article_IEq10.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$(c_{i})$$]]></EquationSource>
                      <EquationSource Format="MATHML">
                        <math xmlns:xlink="http://www.w3.org/1999/xlink">
                          <mrow>
                            <mo stretchy="false">(</mo>
                            <msub>
                              <mi>c</mi>
                              <mi>i</mi>
                            </msub>
                            <mo stretchy="false">)</mo>
                          </mrow>
                        </math>
                      </EquationSource>
                    </InlineEquation> is the matching score corresponding to the column position <InlineEquation ID="IEq11">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2013_43_Article_IEq11.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$c_{i}$$]]></EquationSource>
                      <EquationSource Format="MATHML">
                        <math xmlns:xlink="http://www.w3.org/1999/xlink">
                          <msub>
                            <mi>c</mi>
                            <mi>i</mi>
                          </msub>
                        </math>
                      </EquationSource>
                    </InlineEquation> for a given video pair. After that, for normalization purposes, the score of each column is divided by the number of comparisons, which is <InlineEquation ID="IEq12">
                      <InlineMediaObject>
                        <ImageObject Color="BlackWhite" FileRef="13735_2013_43_Article_IEq12.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                      </InlineMediaObject>
                      <EquationSource Format="TEX"><![CDATA[$$\frac{nx(n-1)}{2}$$]]></EquationSource>
                      <EquationSource Format="MATHML">
                        <math xmlns:xlink="http://www.w3.org/1999/xlink">
                          <mfrac>
                            <mrow>
                              <mi>n</mi>
                              <mi>x</mi>
                              <mo stretchy="false">(</mo>
                              <mi>n</mi>
                              <mo>-</mo>
                              <mn>1</mn>
                              <mo stretchy="false">)</mo>
                            </mrow>
                            <mn>2</mn>
                          </mfrac>
                        </math>
                      </EquationSource>
                    </InlineEquation>.</Para>
                  <Para ID="Par36">The scoring result is then compared to a given threshold to assess whether all videos in the corresponding profile will be to constitute a cluster. This threshold is especially significant since it determines the similarity of video DNA representations in the same cluster and the number of clusters generated. A high threshold implies high intra-cluster similarity (i.e., high purity) and a high number of clusters generated, while a low threshold implies low intra-cluster similarity (i.e., low purity) and a small number of clusters formed. Its value is determined experimentally (c.f. Sect. <InternalRef RefID="Sec16">4.1</InternalRef>).</Para>
                </Section3>
              </Section2>
            </Section1>
            <Section1 ID="Sec15">
              <Heading>Experiments</Heading>
              <Para ID="Par37">In the experiments, we use the CC_WEB_VIDEO dataset, which was collected from real-world websites such as YouTube, Yahoo!, etc. The experimental dataset consists of 5,000 videos (3,789 near duplicate videos and 1,211 noise videos) and 1,000 query videos. Example videos and corresponding near-duplicates are shown in Fig. <InternalRef RefID="Fig3">3</InternalRef>. We use the MSA-based preprocessing technique described to cluster the video collection. We then evaluate the performance of NDV retrieval with and without the clustering preprocessing step. We also evaluate our techniques against the state-of-the-art <Emphasis Type="Italic">n</Emphasis>-gram and dynamic programming-based edit distance techniques. Additionally, we evaluate the proposed MSA based clustering against state-of-the-art clustering techniques in a retrieval experiment. The experiments are conducted on a standard PC with an Intel(R) Core(TM) 2.67 GHz processor and 2 GB of RAM. In Sect. <InternalRef RefID="Sec16">4.1</InternalRef>, we discuss the empirical clustering framework and the influence of the clustering threshold introduced in Sect. <InternalRef RefID="Sec14">3.2.5</InternalRef>. In Sect. <InternalRef RefID="Sec17">4.2</InternalRef>, we present the evaluation of NDV retrieval based on our MSA technique in terms of retrieval effectiveness and accuracy through the standard recall-precision indicators. In Sect. <InternalRef RefID="Sec18">4.3</InternalRef>, we compare the proposed MSA-based clustering technique with state-of-the-art clustering techniques in the framework in an NDV retrieval experimental framework.</Para>
              <Section2 ID="Sec16">
                <Heading>Empirical validation of MSA-based NDV clustering</Heading>
                <Para ID="Par38">Two key criteria impact the NDV retrieval performance using a preprocessing step of clustering: (i) a low number of clusters is correlated to computational effectiveness as it involves a reduced number of comparisons between the query video and representative cluster videos; (ii) higher intra-cluster purity is linked to improved retrieval accuracy, since videos that are the most similar will be clustered together. Hence, the goal is to minimize the number of clusters while maximizing intra-cluster purity. In our experiment, we set the clustering threshold to 0.3 based on an extensive comparison with other thresholds. With a threshold value of 0.3, 807 clusters are generated and the average intra-cluster purity is 0.6759. Table <InternalRef RefID="Tab1">1</InternalRef> shows the purity and number of clusters formed for thresholds varying from 0.1 to 0.5, while Table <InternalRef RefID="Tab2">2</InternalRef> shows the increment in terms of purity and number of clusters formed corresponding to different threshold intervals. We clearly notice that the increment of purity sharply decreases with increasing threshold values, as well as the increment of the number of formed clusters. The selection of the threshold value and corresponding number of clusters formed is based on optimizing both retrieval accuracy and speed for varying threshold values on a subset composed of 240 videos. The benefit in terms of both retrieval accuracy and speed is apparent when the number of formed clusters is six to ten times smaller than the size of the entire dataset.<Table Float="Yes" ID="Tab1">
                    <Caption Language="En">
                      <CaptionNumber>Table 1</CaptionNumber>
                      <CaptionContent>
                        <SimplePara>Purity and number of formed clusters corresponding to different thresholds</SimplePara>
                      </CaptionContent>
                    </Caption>
                    <tgroup cols="6">
                      <colspec align="left" colname="c1" colnum="1"/>
                      <colspec align="left" colname="c2" colnum="2"/>
                      <colspec align="left" colname="c3" colnum="3"/>
                      <colspec align="left" colname="c4" colnum="4"/>
                      <colspec align="left" colname="c5" colnum="5"/>
                      <colspec align="left" colname="c6" colnum="6"/>
                      <thead>
                        <row>
                          <entry align="left" colname="c1">
                            <SimplePara>Threshold</SimplePara>
                          </entry>
                          <entry align="left" colname="c2">
                            <SimplePara>0.1</SimplePara>
                          </entry>
                          <entry align="left" colname="c3">
                            <SimplePara>0.2</SimplePara>
                          </entry>
                          <entry align="left" colname="c4">
                            <SimplePara>0.3</SimplePara>
                          </entry>
                          <entry align="left" colname="c5">
                            <SimplePara>0.4</SimplePara>
                          </entry>
                          <entry align="left" colname="c6">
                            <SimplePara>0.5</SimplePara>
                          </entry>
                        </row>
                      </thead>
                      <tbody>
                        <row>
                          <entry align="left" colname="c1">
                            <SimplePara>Purity</SimplePara>
                          </entry>
                          <entry align="left" colname="c2">
                            <SimplePara>0.5755</SimplePara>
                          </entry>
                          <entry align="left" colname="c3">
                            <SimplePara>0.6388</SimplePara>
                          </entry>
                          <entry align="left" colname="c4">
                            <SimplePara>0.6759</SimplePara>
                          </entry>
                          <entry align="left" colname="c5">
                            <SimplePara>0.7011</SimplePara>
                          </entry>
                          <entry align="left" colname="c6">
                            <SimplePara>0.7104</SimplePara>
                          </entry>
                        </row>
                        <row>
                          <entry align="left" colname="c1">
                            <SimplePara>No. of clusters</SimplePara>
                          </entry>
                          <entry align="left" colname="c2">
                            <SimplePara>560</SimplePara>
                          </entry>
                          <entry align="left" colname="c3">
                            <SimplePara>711</SimplePara>
                          </entry>
                          <entry align="left" colname="c4">
                            <SimplePara>807</SimplePara>
                          </entry>
                          <entry align="left" colname="c5">
                            <SimplePara>880</SimplePara>
                          </entry>
                          <entry align="left" colname="c6">
                            <SimplePara>928</SimplePara>
                          </entry>
                        </row>
                      </tbody>
                    </tgroup>
                  </Table>
                           <Table Float="Yes" ID="Tab2">
                    <Caption Language="En">
                      <CaptionNumber>Table 2</CaptionNumber>
                      <CaptionContent>
                        <SimplePara>Increment of purity and number of formed clusters for different threshold intervals</SimplePara>
                      </CaptionContent>
                    </Caption>
                    <tgroup cols="5">
                      <colspec align="left" colname="c1" colnum="1"/>
                      <colspec align="left" colname="c2" colnum="2"/>
                      <colspec align="left" colname="c3" colnum="3"/>
                      <colspec align="left" colname="c4" colnum="4"/>
                      <colspec align="left" colname="c5" colnum="5"/>
                      <thead>
                        <row>
                          <entry align="left" colname="c1">
                            <SimplePara>Threshold intervals</SimplePara>
                          </entry>
                          <entry align="left" colname="c2">
                            <SimplePara>0.1–0.2</SimplePara>
                          </entry>
                          <entry align="left" colname="c3">
                            <SimplePara>0.2–0.3</SimplePara>
                          </entry>
                          <entry align="left" colname="c4">
                            <SimplePara>0.3–0.4</SimplePara>
                          </entry>
                          <entry align="left" colname="c5">
                            <SimplePara>0.4–0.5</SimplePara>
                          </entry>
                        </row>
                      </thead>
                      <tbody>
                        <row>
                          <entry align="left" colname="c1">
                            <SimplePara>Increment of purity</SimplePara>
                          </entry>
                          <entry align="left" colname="c2">
                            <SimplePara>0.0633</SimplePara>
                          </entry>
                          <entry align="left" colname="c3">
                            <SimplePara>0.0371</SimplePara>
                          </entry>
                          <entry align="left" colname="c4">
                            <SimplePara>0.0252</SimplePara>
                          </entry>
                          <entry align="left" colname="c5">
                            <SimplePara>0.0093</SimplePara>
                          </entry>
                        </row>
                        <row>
                          <entry align="left" colname="c1">
                            <SimplePara>Increment of number of formed clusters</SimplePara>
                          </entry>
                          <entry align="left" colname="c2">
                            <SimplePara>151</SimplePara>
                          </entry>
                          <entry align="left" colname="c3">
                            <SimplePara>96</SimplePara>
                          </entry>
                          <entry align="left" colname="c4">
                            <SimplePara>73</SimplePara>
                          </entry>
                          <entry align="left" colname="c5">
                            <SimplePara>48</SimplePara>
                          </entry>
                        </row>
                      </tbody>
                    </tgroup>
                  </Table>
                        </Para>
              </Section2>
              <Section2 ID="Sec17">
                <Heading>Evaluation of clustering-based NDV retrieval against state-of-the-art techniques</Heading>
                <Para ID="Par39">To speed up the processing of the video collection at retrieval time, a query video is compared to a representative of each cluster which could be a randomly chosen index representation or that of the cluster centroid. In our experiment, we consider the index representation of the cluster centroids to be compared to a query video representation. We submit 1,000 queries to evaluate the impact of clustering on NDV retrieval. If the similarity between the query video and the centroid is greater than the given retrieval threshold, all videos in the cluster are retrieved as near duplicates of the query video.</Para>
                <Para ID="Par40">Figure <InternalRef RefID="Fig8">8</InternalRef> shows total retrieval time(s) with respect to the number of processed queries for the near-duplicate retrieval techniques with and without MSA-based clustering, a state-of-the-art near-duplicate retrieval system based on <Emphasis Type="Italic">n</Emphasis>-gram matching to compute video similarity [<CitationRef CitationID="CR1">1</CitationRef>] and a state-of-the-art system implementing edit distance (through dynamic programming-based computation of the Levenshtein distance between two video sequence representations). We can see that with the number of queries growing, the difference between average retrieval times increases to the advantage of NDV retrieval based on the clustering preprocessing technique. When all 1,000 queries are processed, the NDV retrieval with MSA clustering is at least five times faster than the other techniques.<Figure Category="Standard" Float="Yes" ID="Fig8">
                    <Caption Language="En">
                      <CaptionNumber>Fig. 8</CaptionNumber>
                      <CaptionContent>
                        <SimplePara>Total retrieval time(s) with respect to the number of processed queries</SimplePara>
                      </CaptionContent>
                    </Caption>
                    <MediaObject ID="MO22">
                      <ImageObject Color="Color" FileRef="MediaObjects/13735_2013_43_Fig8_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                    </MediaObject>
                  </Figure>
                           <Figure Category="Standard" Float="Yes" ID="Fig9">
                    <Caption Language="En">
                      <CaptionNumber>Fig. 9</CaptionNumber>
                      <CaptionContent>
                        <SimplePara>Comparison of retrieval precision-recall curves between MSA clustering based and other non-clustering based techniques</SimplePara>
                      </CaptionContent>
                    </Caption>
                    <MediaObject ID="MO23">
                      <ImageObject Color="Color" FileRef="MediaObjects/13735_2013_43_Fig9_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                    </MediaObject>
                  </Figure>
                        </Para>
                <Para ID="Par41">In this experiment, we aim to show that even though NDV retrieval with clustering-based preprocessing is faster, it yields equivalent results in terms of retrieval accuracy than the baseline NDV retrieval without preprocessing. For this, in Fig. <InternalRef RefID="Fig9">9</InternalRef> we provide the recall/precision curves for both systems over the 1,000 queries. All curves are roughly superimposed for recall values lower than 0.1 except for the curve representing the performance of the technique based on edit distance which clearly underperforms in comparison to the others. The precision of NDV retrieval with clustering-based preprocessing is slightly higher than that of NDV retrieval without clustering and ED for recall values greater than 0.1 (the <Emphasis Type="Italic">n</Emphasis>-gram matching-based technique here clearly underperforms). We tested for the statistical significance of the difference between the average precisions and found no significant difference using the <Emphasis Type="Italic">t</Emphasis>-test. This clearly demonstrates that the three techniques are equivalent in terms of retrieval accuracy in a precision/recall-based evaluation setting. However, we can clearly see that the curve of clustering-based always lies above that of ED as well as other techniques when the recall value is roughly greater than 0.1. This clearly demonstrates that the accuracy of NDV retrieval based on pre-clustering technique is also slightly enhanced, though the improvement is not so significant.</Para>
              </Section2>
              <Section2 ID="Sec18">
                <Heading>Experimental comparison of the impact of clustering techniques on NDV retrieval</Heading>
                <Para ID="Par42">In subsection B, we evaluated our NDV retrieval framework with MSA based clustering against state-of-the-art NDV retrieval systems. In this section, we evaluate the effectiveness of our framework against other frameworks incorporating a preprocessing step of clustering based on the five main algorithms of the state-of-the-art: Birch [<CitationRef CitationID="CR12">12</CitationRef>], Cure [<CitationRef CitationID="CR13">13</CitationRef>], Dbscan [<CitationRef CitationID="CR16">16</CitationRef>], EM [<CitationRef CitationID="CR14">14</CitationRef>] and Proclus [<CitationRef CitationID="CR15">15</CitationRef>] in an NDV retrieval experimentation. Similarly to the previous experiment, 1,000 queries are considered. For all the compared frameworks, we adjust the clustering parameters to form a number of clusters sensibly close to that used for MSA-based clustering (i.e. 807 clusters).</Para>
                <Para ID="Par43">As a consequence, retrieval speed is comparable for all the considered frameworks. However, the video index representations used for the five state-of-the-art clustering algorithms consist of a 24-bin histogram (c.f. Fig. <InternalRef RefID="Fig4">4</InternalRef> where each possible ranking corresponds to one histogram bin), rather than the video DNA sequence used for MSA based clustering. The corresponding precision–recall curves are shown in Fig. <InternalRef RefID="Fig10">10</InternalRef>. We can see that the curve of the framework incorporating MSA-based clustering is almost superimposed with that of the frameworks implementing clustering based on the Birch and Proclus algorithms. We tested for the statistical significance of the difference between the average precisions of the proposed framework incorporating MSA-based clustering and the frameworks implementing clustering based on the Birch and Proclus algorithms. We found no significant difference using the <Emphasis Type="Italic">t</Emphasis>-test: (<InlineEquation ID="IEq13">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2013_43_Article_IEq13.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$t=0.1967$$]]></EquationSource>
                    <EquationSource Format="MATHML">
                      <math xmlns:xlink="http://www.w3.org/1999/xlink">
                        <mrow>
                          <mi>t</mi>
                          <mo>=</mo>
                          <mn>0.1967</mn>
                        </mrow>
                      </math>
                    </EquationSource>
                  </InlineEquation>, <InlineEquation ID="IEq14">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2013_43_Article_IEq14.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$p>0.05$$]]></EquationSource>
                    <EquationSource Format="MATHML">
                      <math xmlns:xlink="http://www.w3.org/1999/xlink">
                        <mrow>
                          <mi>p</mi>
                          <mo>&gt;</mo>
                          <mn>0.05</mn>
                        </mrow>
                      </math>
                    </EquationSource>
                  </InlineEquation>, <InlineEquation ID="IEq15">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2013_43_Article_IEq15.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$df=19$$]]></EquationSource>
                    <EquationSource Format="MATHML">
                      <math xmlns:xlink="http://www.w3.org/1999/xlink">
                        <mrow>
                          <mi>d</mi>
                          <mi>f</mi>
                          <mo>=</mo>
                          <mn>19</mn>
                        </mrow>
                      </math>
                    </EquationSource>
                  </InlineEquation>) and (<InlineEquation ID="IEq16">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2013_43_Article_IEq16.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$t=-0.0949$$]]></EquationSource>
                    <EquationSource Format="MATHML">
                      <math xmlns:xlink="http://www.w3.org/1999/xlink">
                        <mrow>
                          <mi>t</mi>
                          <mo>=</mo>
                          <mo>-</mo>
                          <mn>0.0949</mn>
                        </mrow>
                      </math>
                    </EquationSource>
                  </InlineEquation>, <InlineEquation ID="IEq17">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2013_43_Article_IEq17.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$p>0.05$$]]></EquationSource>
                    <EquationSource Format="MATHML">
                      <math xmlns:xlink="http://www.w3.org/1999/xlink">
                        <mrow>
                          <mi>p</mi>
                          <mo>&gt;</mo>
                          <mn>0.05</mn>
                        </mrow>
                      </math>
                    </EquationSource>
                  </InlineEquation>, <InlineEquation ID="IEq18">
                    <InlineMediaObject>
                      <ImageObject Color="BlackWhite" FileRef="13735_2013_43_Article_IEq18.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/>
                    </InlineMediaObject>
                    <EquationSource Format="TEX"><![CDATA[$$df=19$$]]></EquationSource>
                    <EquationSource Format="MATHML">
                      <math xmlns:xlink="http://www.w3.org/1999/xlink">
                        <mrow>
                          <mi>d</mi>
                          <mi>f</mi>
                          <mo>=</mo>
                          <mn>19</mn>
                        </mrow>
                      </math>
                    </EquationSource>
                  </InlineEquation>), respectively. All three clustering-based NDV retrieval frameworks therefore yield equivalent average precision results. Additionally, the framework implementing clustering based on the EM algorithm obviously underperforms in comparison to the three best performing previous frameworks in terms of precision–recall. In particular, for recall values greater than 0.6, the corresponding precision clearly degenerates. For recall values lower than 0.6 and 0.8, respectively, the curves of the NDV retrieval frameworks incorporating a clustering step based on the Cure and Dbscan algorithms are also clearly below those of the three best performing frameworks. Additionally, the mean average precisions of all frameworks are listed in Table <InternalRef RefID="Tab3">3</InternalRef>. Although we did not find any significant statistical difference between the mean average precision of the proposed framework and that of the frameworks implementing the Birch and Proclus algorithms, the advantage of our MSA-based clustering relies on the fact that it is based on alignment, which is more flexible for handling video index signatures whilst other techniques are metric based. To the best of our knowledge, our clustering technique is the only one that makes use of sequence alignment, which is clearly advantageous for signature-based multimedia representations.<Figure Category="Standard" Float="Yes" ID="Fig10">
                    <Caption Language="En">
                      <CaptionNumber>Fig. 10</CaptionNumber>
                      <CaptionContent>
                        <SimplePara>Comparison of clustering based frameworks for NDV retrieval</SimplePara>
                      </CaptionContent>
                    </Caption>
                    <MediaObject ID="MO24">
                      <ImageObject Color="Color" FileRef="MediaObjects/13735_2013_43_Fig10_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/>
                    </MediaObject>
                  </Figure>
                           <Table Float="Yes" ID="Tab3">
                    <Caption Language="En">
                      <CaptionNumber>Table 3</CaptionNumber>
                      <CaptionContent>
                        <SimplePara>Mean average precision of NDV retrieval based on different clustering algorithms</SimplePara>
                      </CaptionContent>
                    </Caption>
                    <tgroup cols="6">
                      <colspec align="left" colname="c1" colnum="1"/>
                      <colspec align="left" colname="c2" colnum="2"/>
                      <colspec align="left" colname="c3" colnum="3"/>
                      <colspec align="left" colname="c4" colnum="4"/>
                      <colspec align="left" colname="c5" colnum="5"/>
                      <colspec align="left" colname="c6" colnum="6"/>
                      <thead>
                        <row>
                          <entry align="left" colname="c1">
                            <SimplePara>MSA</SimplePara>
                          </entry>
                          <entry align="left" colname="c2">
                            <SimplePara>Birch</SimplePara>
                          </entry>
                          <entry align="left" colname="c3">
                            <SimplePara>Cure</SimplePara>
                          </entry>
                          <entry align="left" colname="c4">
                            <SimplePara>Dbscan</SimplePara>
                          </entry>
                          <entry align="left" colname="c5">
                            <SimplePara>EM</SimplePara>
                          </entry>
                          <entry align="left" colname="c6">
                            <SimplePara>Proclus</SimplePara>
                          </entry>
                        </row>
                      </thead>
                      <tbody>
                        <row>
                          <entry align="left" colname="c1">
                            <SimplePara>0.5493</SimplePara>
                          </entry>
                          <entry align="left" colname="c2">
                            <SimplePara>0.5713</SimplePara>
                          </entry>
                          <entry align="left" colname="c3">
                            <SimplePara>0.5187</SimplePara>
                          </entry>
                          <entry align="left" colname="c4">
                            <SimplePara>0.4523</SimplePara>
                          </entry>
                          <entry align="left" colname="c5">
                            <SimplePara>0.5055</SimplePara>
                          </entry>
                          <entry align="left" colname="c6">
                            <SimplePara>0.5510</SimplePara>
                          </entry>
                        </row>
                      </tbody>
                    </tgroup>
                  </Table>
                        </Para>
              </Section2>
            </Section1>
            <Section1 ID="Sec19">
              <Heading>Conclusion and future works</Heading>
              <Para ID="Par44">In this paper, we presented an automatic clustering framework based on progressive multiple video DNA sequence alignment for effectively clustering NDV collections. The approach is heuristic and optimum results are produced. In an NDV retrieval empirical setting, the proposed framework based on MSA clustering outperforms a non-clustering-based framework on the one hand and state-of-the-art <Emphasis Type="Italic">n</Emphasis>-gram and dynamic programming-based systems on the other. State-of-the-art frameworks incorporating a clustering pre-processing step are also compared to our MSA-based framework. We showed that the best performing state-of-the-art frameworks are comparable in terms of performance to the proposed framework. However, these are limited to very specific metric-based representations of the video content, while our framework is able to take into account variable length numerical or non-numerical sequences used to index the video content.</Para>
              <Para ID="Par45">Due to the promising nature of these results, we propose in future works to deal with techniques to represent clusters more efficiently as well as introduce other indexing techniques into our framework to further enhance NDV retrieval performance.</Para>
            </Section1>
          </Body>
          <BodyRef FileRef="BodyRef/PDF/13735_2013_Article_43.pdf" PDFType="Typeset" TargetType="OnlinePDF"/>
          <BodyRef FileRef="BodyRef/PDF/13735_2013_43_TEX.zip" TargetType="TEX"/>
          <ArticleBackmatter>
            <Bibliography ID="Bib1">
              <Heading>References</Heading>
              <Citation ID="CR1">
                <CitationNumber>1.</CitationNumber>
                <BibUnstructured>Belkhatir M, Tahayna B (2012) Near-duplicate video detection featuring coupled temporal and perceptual visual structures and logical inference based matching. Inf Process Manage 48(3):489–501. doi:<ExternalRef><RefSource>10.1016/j.ipm.2011.03.003</RefSource><RefTarget Address="10.1016/j.ipm.2011.03.003" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR2">
                <CitationNumber>2.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>C</Initials>
                    <FamilyName>Notredame</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>DG</Initials>
                    <FamilyName>Higgins</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Heringa</FamilyName>
                  </BibAuthorName>
                  <Year>2000</Year>
                  <ArticleTitle Language="En">T-coffee: a novel method for fast and accurate multiple sequence alignment</ArticleTitle>
                  <JournalTitle>J Mol Biol</JournalTitle>
                  <VolumeID>302</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>205</FirstPage>
                  <LastPage>217</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1006/jmbi.2000.4042</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Notredame C, Higgins DG, Heringa J (2000) T-coffee: a novel method for fast and accurate multiple sequence alignment. J Mol Biol 302(1):205–217</BibUnstructured>
              </Citation>
              <Citation ID="CR3">
                <CitationNumber>3.</CitationNumber>
                <BibUnstructured>Edgar RC (2004) MUSCLE: multiple sequence alignment with high accuracy and high throughput. Nucleic Acids Res 32(5):1792–1797. doi:<ExternalRef><RefSource>10.1093/nar/gkh340</RefSource><RefTarget Address="10.1093/nar/gkh340" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR4">
                <CitationNumber>4.</CitationNumber>
                <BibUnstructured>MacQueen J et al (1967) Some methods for classification and analysis of multivariate observations. In: Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, vol 1, pp 281–297</BibUnstructured>
              </Citation>
              <Citation ID="CR5">
                <CitationNumber>5.</CitationNumber>
                <BibUnstructured>Thompson JD, Higgins DG, Gibson TJ (1994) CLUSTAL W: improving the sensitivity of progressive multiple sequence alignment through sequence weighting, position-specific gap penalties and weight matrix choice. Nucleic Acids Res 22(22):4673–4680. doi:<ExternalRef><RefSource>10.1093/nar/22.22.4673</RefSource><RefTarget Address="10.1093/nar/22.22.4673" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR6">
                <CitationNumber>6.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>X</Initials>
                    <FamilyName>Wu</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>CW</Initials>
                    <FamilyName>Ngo</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>AG</Initials>
                    <FamilyName>Hauptmann</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>HK</Initials>
                    <FamilyName>Tan</FamilyName>
                  </BibAuthorName>
                  <Year>2009</Year>
                  <ArticleTitle Language="En">Real-time near-duplicate elimination for web video search with content and context</ArticleTitle>
                  <JournalTitle>IEEE Trans Multimedia</JournalTitle>
                  <VolumeID>11</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>196</FirstPage>
                  <LastPage>207</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TMM.2008.2009673</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Wu X, Ngo CW, Hauptmann AG, Tan HK (2009) Real-time near-duplicate elimination for web video search with content and context. IEEE Trans Multimedia 11(2):196–207</BibUnstructured>
              </Citation>
              <Citation ID="CR7">
                <CitationNumber>7.</CitationNumber>
                <BibUnstructured>Yuan J, Tian Q, Ranganath S (2004) Fast and robust search method for short video clips from large video collection. In: Proceedings of the 17th international conference on pattern recognition, ICPR, vol 4, pp 866–869</BibUnstructured>
              </Citation>
              <Citation ID="CR8">
                <CitationNumber>8.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>DF</Initials>
                    <FamilyName>Feng</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>RF</Initials>
                    <FamilyName>Doolittle</FamilyName>
                  </BibAuthorName>
                  <Year>1987</Year>
                  <ArticleTitle Language="En">Progressive sequence alignment as a prerequisitet to correct phylogenetic trees</ArticleTitle>
                  <JournalTitle>J Mol Evol</JournalTitle>
                  <VolumeID>25</VolumeID>
                  <IssueID>4</IssueID>
                  <FirstPage>351</FirstPage>
                  <LastPage>360</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1007/BF02603120</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Feng DF, Doolittle RF (1987) Progressive sequence alignment as a prerequisitet to correct phylogenetic trees. J Mol Evol 25(4):351–360</BibUnstructured>
              </Citation>
              <Citation ID="CR9">
                <CitationNumber>9.</CitationNumber>
                <BibUnstructured>Song J, Yang Y, Huang Z, Shen HT, Hong R (2011) Multiple feature hashing for real-time large scale near-duplicate video retrieval. In: Proceedings of the 19th ACM international conference on Multimedia, pp 423–432. doi:<ExternalRef><RefSource>10.1145/2072298.2072354</RefSource><RefTarget Address="10.1145/2072298.2072354" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR10">
                <CitationNumber>10.</CitationNumber>
                <BibUnstructured>Shang L, Yang L, Wang F, Chan KP, Hua XS (2010) Real-time large scale near-duplicate web video retrieval. In: Proceedings of 18th ACM international conference on Multimedia, pp 531–540. doi:<ExternalRef><RefSource>10.1145/1873951.1874021</RefSource><RefTarget Address="10.1145/1873951.1874021" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR11">
                <CitationNumber>11.</CitationNumber>
                <BibUnstructured>Cai Y, Yang L, Ping W, Wang F, Mei T, Hua XS, Li S (2011) Million-scale near-duplicate video retrieval system. In: Proceedings of the 19th ACM international conference on multimedia, pp 837–838. doi:<ExternalRef><RefSource>10.1145/2072298.2072484</RefSource><RefTarget Address="10.1145/2072298.2072484" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR12">
                <CitationNumber>12.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>T</Initials>
                    <FamilyName>Zhang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>R</Initials>
                    <FamilyName>Ramakrishnan</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Livny</FamilyName>
                  </BibAuthorName>
                  <Year>1996</Year>
                  <ArticleTitle Language="En">BIRCH: an efficient data clustering method for very large databases</ArticleTitle>
                  <JournalTitle>ACM SIGMOD Record</JournalTitle>
                  <VolumeID>25</VolumeID>
                  <FirstPage>103</FirstPage>
                  <LastPage>114</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1145/235968.233324</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Zhang T, Ramakrishnan R, Livny M (1996) BIRCH: an efficient data clustering method for very large databases. ACM SIGMOD Record 25:103–114</BibUnstructured>
              </Citation>
              <Citation ID="CR13">
                <CitationNumber>13.</CitationNumber>
                <BibUnstructured>Guha S, Rastogi R, Shim K (1998) CURE: an efficient clustering algorithm for large databases. ACM SIGMOD Record 27:73–84. doi:<ExternalRef><RefSource>10.1145/276304.276312</RefSource><RefTarget Address="10.1145/276304.276312" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR14">
                <CitationNumber>14.</CitationNumber>
                <BibUnstructured>McLachlan GJ, Krishnan T (1997) The EM algorithm and extensions, vol 274. Wiley, New York</BibUnstructured>
              </Citation>
              <Citation ID="CR15">
                <CitationNumber>15.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>CC</Initials>
                    <FamilyName>Aggarwal</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>JL</Initials>
                    <FamilyName>Wolf</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>PS</Initials>
                    <FamilyName>Yu</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>C</Initials>
                    <FamilyName>Procopiuc</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>JS</Initials>
                    <FamilyName>Park</FamilyName>
                  </BibAuthorName>
                  <Year>1999</Year>
                  <ArticleTitle Language="En">Fast algorithms for projected clustering</ArticleTitle>
                  <JournalTitle>ACM SIGMOD Record</JournalTitle>
                  <VolumeID>28</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>61</FirstPage>
                  <LastPage>72</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1145/304181.304188</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Aggarwal CC, Wolf JL, Yu PS, Procopiuc C, Park JS (1999) Fast algorithms for projected clustering. ACM SIGMOD Record 28(2):61–72</BibUnstructured>
              </Citation>
              <Citation ID="CR16">
                <CitationNumber>16.</CitationNumber>
                <BibUnstructured>Ester M, Kriegel HP, Sander J, Xu X (1996) A density-based algorithm for discovering clusters in large spatial databases with noise. In: Proceedings of the 2nd international conference on knowledge discovery and data mining, vol 1996, pp 226–231</BibUnstructured>
              </Citation>
              <Citation ID="CR17">
                <CitationNumber>17.</CitationNumber>
                <BibUnstructured>Shen HT, Zhou X, Huang Z, Shao J, Zhou X (2007) UQLIPS: a real-time near-duplicate video clip detection system. In: Proceedings of the 33rd international conference on very large data bases, pp 1374–1377</BibUnstructured>
              </Citation>
              <Citation ID="CR18">
                <CitationNumber>18.</CitationNumber>
                <BibUnstructured>Tan HK, Ngo CW, Hong R, Chua TS (2009) Scalable detection of partial near-duplicate videos by visual-temporal consistency. In: Proceedings of the 17th ACM international conference on multimedia, pp 145–154. doi:<ExternalRef><RefSource>10.1145/1631272.1631295</RefSource><RefTarget Address="10.1145/1631272.1631295" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR19">
                <CitationNumber>19.</CitationNumber>
                <BibUnstructured>Paisitkriangkrai S, Mei T, Zhang J, Hua XS (2010) Scalable clip-based near-duplicate video detection with ordinal measure. In: Proceedings of the ACM international conference on image and video retrieval, pp 121–128. doi:<ExternalRef><RefSource>10.1145/1816041.1816062</RefSource><RefTarget Address="10.1145/1816041.1816062" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR20">
                <CitationNumber>20.</CitationNumber>
                <BibUnstructured>Duan LY, Yuan JS, Tian Q, Xu CS (2004) Fast and robust video clip search using index structure. In: Proceedings of the 12th annual ACM international conference on multimedia, pp 756–757. doi:<ExternalRef><RefSource>10.1145/1027527.1027701</RefSource><RefTarget Address="10.1145/1027527.1027701" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR21">
                <CitationNumber>21.</CitationNumber>
                <BibUnstructured>Gao L, Li Z, Katsaggelos AK (2008) A kd-tree based dynamic indexing scheme for video retrieval and geometry matching. In: Proceedings of 17th international conference on computer communications and networks, ICCCN’08, pp 1–5</BibUnstructured>
              </Citation>
              <Citation ID="CR22">
                <CitationNumber>22.</CitationNumber>
                <BibUnstructured>Chatterjee K, Chen SC (2008) GeM-tree: towards a generalized multidimensional index structure supporting image and video retrieval. In: Proceedings of the 10th IEEE international symposium on multimedia, ISM 2008, pp 631–636</BibUnstructured>
              </Citation>
              <Citation ID="CR23">
                <CitationNumber>23.</CitationNumber>
                <BibUnstructured>Aggarwal CC, Yu PS (2000) Finding generalized projected clusters in high dimensional spaces. In: Proceedings of the ACM SIGMOD international conference on management of data, pp 70–81. doi:<ExternalRef><RefSource>10.1145/342009.335383</RefSource><RefTarget Address="10.1145/342009.335383" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR24">
                <CitationNumber>24.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>G</Initials>
                    <FamilyName>Lu</FamilyName>
                  </BibAuthorName>
                  <Year>2002</Year>
                  <ArticleTitle Language="En">Techniques and data structures for efficient multimedia retrieval based on similarity</ArticleTitle>
                  <JournalTitle>IEEE Trans Multimedia</JournalTitle>
                  <VolumeID>4</VolumeID>
                  <IssueID>3</IssueID>
                  <FirstPage>372</FirstPage>
                  <LastPage>384</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TMM.2002.802831</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Lu G (2002) Techniques and data structures for efficient multimedia retrieval based on similarity. IEEE Trans Multimedia 4(3):372–384</BibUnstructured>
              </Citation>
              <Citation ID="CR25">
                <CitationNumber>25.</CitationNumber>
                <BibUnstructured>Hua XS, Chen X, Zhang HJ (2004) Robust video signature based on ordinal measure. In: Proceedings of the international conference on image processing, ICIP’04, vol 1, pp 685–688</BibUnstructured>
              </Citation>
              <Citation ID="CR26">
                <CitationNumber>26.</CitationNumber>
                <BibUnstructured>Zhu J, Hoi SC, Lyu MR, Yan S (2008) Near-duplicate keyframe retrieval by nonrigid image matching. In: Proceedings of the 16th ACM international conference on multimedia, pp 41–50. doi:<ExternalRef><RefSource>10.1145/1459359.1459366</RefSource><RefTarget Address="10.1145/1459359.1459366" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR27">
                <CitationNumber>27.</CitationNumber>
                <BibUnstructured>Ke Y, Sukthankar R, Huston L (2004) Efficient near-duplicate detection and sub-image retrieval. In: Proceedings of the 12th annual ACM international conference on multimedia, pp 869–876</BibUnstructured>
              </Citation>
              <Citation ID="CR28">
                <CitationNumber>28.</CitationNumber>
                <BibUnstructured>Zhou J, Zhang XP (2005) Automatic identification of digital video based on shot-level sequence matching. In: Proceedings of the 13th annual ACM international conference on multimedia, pp 515–518. doi:<ExternalRef><RefSource>10.1145/1101149.1101265</RefSource><RefTarget Address="10.1145/1101149.1101265" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR29">
                <CitationNumber>29.</CitationNumber>
                <BibUnstructured>Agrawal R, Gehrke J, Gunopulos D, Raghavan P (1998) Automatic subspace clustering of high dimensional data for data mining applications. In: Proceedings of the ACM SIGMOD international conference on management of data, pp 94–105. doi:<ExternalRef><RefSource>10.1145/276304.276314</RefSource><RefTarget Address="10.1145/276304.276314" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR30">
                <CitationNumber>30.</CitationNumber>
                <BibUnstructured>Cheng X, Chia LT (2010) Stratification-based keyframe cliques for removal of near-duplicates in video search results. In: Proceedings of the international conference on multimedia information retrieval, pp 313–322. doi:<ExternalRef><RefSource>10.1145/1743384.1743437</RefSource><RefTarget Address="10.1145/1743384.1743437" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR31">
                <CitationNumber>31.</CitationNumber>
                <BibUnstructured>Zhou X, Chen L (2010) Monitoring near duplicates over video streams. In: Proceedings of the 18th ACM international conference on multimedia, pp 521–530. doi:<ExternalRef><RefSource>10.1145/1873951.1874020</RefSource><RefTarget Address="10.1145/1873951.1874020" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR32">
                <CitationNumber>32.</CitationNumber>
                <BibUnstructured>Tian X, Yang L, Wang J, Yang Y, Wu X, Hua XS (2008) Bayesian video search reranking. In: Proceeding of the 16th ACM international conference on multimedia, pp 131–140. doi:<ExternalRef><RefSource>10.1145/1459359.1459378</RefSource><RefTarget Address="10.1145/1459359.1459378" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
            </Bibliography>
          </ArticleBackmatter>
        </Article>
      </Issue>
    </Volume>
  </Journal>
</Publisher>
