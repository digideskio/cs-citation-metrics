<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE Publisher PUBLIC "-//Springer-Verlag//DTD A++ V2.4//EN" "http://devel.springer.de/A++/V2.4/DTD/A++V2.4.dtd">
<Publisher>
  <PublisherInfo>
    <PublisherName>Springer-Verlag</PublisherName>
    <PublisherLocation>London</PublisherLocation>
  </PublisherInfo>
  <Journal OutputMedium="All">
    <JournalInfo JournalProductType="ArchiveJournal" NumberingStyle="ContentOnly">
      <JournalID>13735</JournalID>
      <JournalPrintISSN>2192-6611</JournalPrintISSN>
      <JournalElectronicISSN>2192-662X</JournalElectronicISSN>
      <JournalTitle>International Journal of Multimedia Information Retrieval</JournalTitle>
      <JournalAbbreviatedTitle>Int J Multimed Info Retr</JournalAbbreviatedTitle>
      <JournalSubjectGroup>
        <JournalSubject Type="Primary">Computer Science</JournalSubject>
        <JournalSubject Type="Secondary">Data Mining and Knowledge Discovery</JournalSubject>
        <JournalSubject Type="Secondary">Image Processing and Computer Vision</JournalSubject>
        <JournalSubject Type="Secondary">Information Systems Applications (incl. Internet)</JournalSubject>
        <JournalSubject Type="Secondary">Information Storage and Retrieval</JournalSubject>
        <JournalSubject Type="Secondary">Multimedia Information Systems</JournalSubject>
        <JournalSubject Type="Secondary">Computer Science, general</JournalSubject>
      </JournalSubjectGroup>
    </JournalInfo>
    <Volume OutputMedium="All">
      <VolumeInfo TocLevels="0" VolumeType="Regular">
        <VolumeIDStart>1</VolumeIDStart>
        <VolumeIDEnd>1</VolumeIDEnd>
        <VolumeIssueCount>4</VolumeIssueCount>
      </VolumeInfo>
      <Issue IssueType="Regular" OutputMedium="All">
        <IssueInfo IssueType="Regular" TocLevels="0">
          <IssueIDStart>2</IssueIDStart>
          <IssueIDEnd>2</IssueIDEnd>
          <IssueArticleCount>5</IssueArticleCount>
          <IssueHistory>
            <OnlineDate>
              <Year>2012</Year>
              <Month>6</Month>
              <Day>19</Day>
            </OnlineDate>
            <PrintDate>
              <Year>2012</Year>
              <Month>6</Month>
              <Day>18</Day>
            </PrintDate>
            <CoverDate>
              <Year>2012</Year>
              <Month>7</Month>
            </CoverDate>
            <PricelistYear>2012</PricelistYear>
          </IssueHistory>
          <IssueCopyright>
            <CopyrightHolderName>Springer-Verlag London Limited</CopyrightHolderName>
            <CopyrightYear>2012</CopyrightYear>
          </IssueCopyright>
        </IssueInfo>
        <Article ID="s13735-012-0014-4" OutputMedium="All">
          <ArticleInfo ArticleType="OriginalPaper" ContainsESM="No" Language="En" NumberingStyle="ContentOnly" TocLevels="0">
            <ArticleID>14</ArticleID>
            <ArticleDOI>10.1007/s13735-012-0014-4</ArticleDOI>
            <ArticleSequenceNumber>1</ArticleSequenceNumber>
            <ArticleTitle Language="En" OutputMedium="All">Interactive search in image retrieval: a survey</ArticleTitle>
            <ArticleCategory>Trends and Surveys</ArticleCategory>
            <ArticleFirstPage>71</ArticleFirstPage>
            <ArticleLastPage>86</ArticleLastPage>
            <ArticleHistory>
              <RegistrationDate>
                <Year>2012</Year>
                <Month>5</Month>
                <Day>3</Day>
              </RegistrationDate>
              <Received>
                <Year>2012</Year>
                <Month>4</Month>
                <Day>7</Day>
              </Received>
              <Revised>
                <Year>2012</Year>
                <Month>4</Month>
                <Day>27</Day>
              </Revised>
              <Accepted>
                <Year>2012</Year>
                <Month>4</Month>
                <Day>28</Day>
              </Accepted>
              <OnlineDate>
                <Year>2012</Year>
                <Month>6</Month>
                <Day>8</Day>
              </OnlineDate>
            </ArticleHistory>
            <ArticleCopyright>
              <CopyrightHolderName>The Author(s)</CopyrightHolderName>
              <CopyrightYear>2012</CopyrightYear>
            </ArticleCopyright>
            <ArticleGrants Type="OpenChoice">
              <MetadataGrant Grant="OpenAccess"/>
              <AbstractGrant Grant="OpenAccess"/>
              <BodyPDFGrant Grant="OpenAccess"/>
              <BodyHTMLGrant Grant="OpenAccess"/>
              <BibliographyGrant Grant="OpenAccess"/>
              <ESMGrant Grant="OpenAccess"/>
            </ArticleGrants>
          </ArticleInfo>
          <ArticleHeader>
            <AuthorGroup>
              <Author AffiliationIDS="Aff1 Aff2">
                <AuthorName DisplayOrder="Western">
                  <GivenName>Bart</GivenName>
                  <FamilyName>Thomee</FamilyName>
                </AuthorName>
                <Contact>
                  <Email>bthomee@yahoo-inc.com</Email>
                </Contact>
              </Author>
              <Author AffiliationIDS="Aff2" CorrespondingAffiliationID="Aff2">
                <AuthorName DisplayOrder="Western">
                  <GivenName>Michael</GivenName>
                  <GivenName>S.</GivenName>
                  <FamilyName>Lew</FamilyName>
                </AuthorName>
                <Contact>
                  <Email>mlew@liacs.nl</Email>
                </Contact>
              </Author>
              <Affiliation ID="Aff1">
                <OrgName>Yahoo! Research</OrgName>
                <OrgAddress>
                  <Street>Avinguda Diagonal 177</Street>
                  <Postcode>08018 </Postcode>
                  <City>Barcelona</City>
                  <Country Code="ES">Spain</Country>
                </OrgAddress>
              </Affiliation>
              <Affiliation ID="Aff2">
                <OrgName>Leiden University</OrgName>
                <OrgAddress>
                  <Street>Niels Bohrweg 1</Street>
                  <Postcode>2333 CA </Postcode>
                  <City>Leiden</City>
                  <Country Code="NL">The Netherlands</Country>
                </OrgAddress>
              </Affiliation>
            </AuthorGroup>
            <Abstract ID="Abs1" Language="En" OutputMedium="All">
              <Heading>Abstract</Heading>
              <Para>We are living in an Age of Information where the amount of accessible data from science and culture is almost limitless. However, this also means that finding an item of interest is increasingly difficult, a digital needle in the proverbial haystack. In this article, we focus on the topic of content-based image retrieval using interactive search techniques, i.e., how does one interactively find any kind of imagery from any source, regardless of whether it is photographic, MRI or X-ray? We highlight trends and ideas from over 170 recent research papers aiming to capture the wide spectrum of paradigms and methods in interactive search, including its subarea relevance feedback. Furthermore, we identify promising research directions and several grand challenges for the future.</Para>
            </Abstract>
            <KeywordGroup Language="En" OutputMedium="All">
              <Heading>Keywords</Heading>
              <Keyword>Multimedia information retrieval</Keyword>
              <Keyword>Content-based image retrieval</Keyword>
              <Keyword>Image search</Keyword>
              <Keyword>Interactive search</Keyword>
              <Keyword>Relevance feedback</Keyword>
              <Keyword>Human–computer interaction</Keyword>
            </KeywordGroup>
          </ArticleHeader>
          <Body>
            <Section1 ID="Sec1">
              <Heading>Introduction</Heading>
              <Para>Terabytes of imagery are being accumulated daily from a wide variety of sources such as the Internet, medical centers (MRI, X-ray, CT scans) or digital libraries. It is not uncommon for one’s personal computer to contain thousands of photos stored in digital photo albums. At present, billions of images can even be found on the World Wide Web. But with that many images within our reach, how do we go about finding the ones we want to see at a particular moment in time? Interactive search methods are meant to address the problem of finding the right imagery based on an interactive dialog with the search system. Some recent examples of the interfaces to these interactive image search systems are shown in Fig. <InternalRef RefID="Fig1">1</InternalRef>.</Para>
              <Para>Furthermore, interactive search allows the user to find imagery, even when there is not a word known to the user for the concept he has in mind. Interactive retrieval systems can, for example, assist a virologist in identifying potentially life-threatening bacteria within a databases containing characteristics of tens of thousands of bacteria and viruses, or assist a radiologist in making his diagnosis of the patient by providing the most relevant examples from credible sources.</Para>
              <Para>The areas of interactive search with the greatest societal impact have been in WWW image search engines and recommendation systems. Google, Yahoo! and Microsoft have added interactive visual content-based search methods into their worldwide search engines, which allows search by similar shape and/or color (see Fig. <InternalRef RefID="Fig2">2</InternalRef>) and are used by millions of people each day. The recommendation systems have been implemented by companies such as Amazon, NetFlix and Napster in wide and diverse contexts, from books to clothing, from movies to music. They give recommendations of what the user would be interested in next based on feedback from prior ratings. Furthermore, Internet advertisements are usually driven by relevance feedback strategies where clicked upon products and links are used to show the next set of advertisements to the user in real time. If a user clicks upon some shoes at a major retailer website, he will probably be shown advertisements for shoes at the next websites that he visits. In image retrieval, another good example is Getty Images where the audience is assumed to be knowledgeable and their image search engine reflects this by having multimodal interactive image search capabilities by both content, context, style, composition and user feedback. Moreover, interactive image search has become important in medical facilities both in hospitals and in research labs [<CitationRef CitationID="CR3">3</CitationRef>]. These systems allow interactive searching on both 2D and 3D imagery from X-ray, MRI, ultrasound and electron microscopy.<Figure Category="Standard" Float="Yes" ID="Fig1"><Caption Language="En"><CaptionNumber>Fig. 1</CaptionNumber><CaptionContent><SimplePara>Examples of user interfaces. The ‘tendril’ interface [<CitationRef CitationID="CR1">1</CitationRef>] (<Emphasis Type="Italic">left</Emphasis>) is specifically designed to support the user in exploring the visual space, where changes to the query result in branching off the initial path. The ‘FreeEye’ interface [<CitationRef CitationID="CR2">2</CitationRef>] (<Emphasis Type="Italic">right</Emphasis>) assists the user in browsing the database, where the selected image is surrounded by similar ones</SimplePara></CaptionContent></Caption><MediaObject ID="MO1"><ImageObject Color="Color" FileRef="MediaObjects/13735_2012_14_Fig1_HTML.jpg" Format="JPEG" Rendition="HTML" Type="Halftone"/></MediaObject></Figure>
                <Figure Category="Standard" Float="Yes" ID="Fig2"><Caption Language="En"><CaptionNumber>Fig. 2</CaptionNumber><CaptionContent><SimplePara>An example from Google Product Search (<Emphasis Type="Italic">top</Emphasis>) showing items that are visually similar by shape and color, and from Microsoft Bing image search (<Emphasis Type="Italic">bottom</Emphasis>) showing the interface and resulting visually similar images by color (<Emphasis Type="Italic">purple</Emphasis>) (color figure online)</SimplePara></CaptionContent></Caption><MediaObject ID="MO2"><ImageObject Color="Color" FileRef="MediaObjects/13735_2012_14_Fig2_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/></MediaObject></Figure>
              </Para>
              <Para>Text search relies on annotations that are frequently missing in both personal and public image collections. When annotations are either missing or incomplete, the only alternative is to use methods that analyze the pictorial content of the imagery in order to find the images of interest. This field of research is also known as content-based image retrieval. Since the early 1990s the field has evolved and has made significant breakthroughs. “The early years” of image retrieval were summarized by Smeulders et al. [<CitationRef CitationID="CR4">4</CitationRef>], painting a detailed picture of a field in the process of learning how to successfully harness the enormous potential of computer vision and pattern recognition. The number of publications increased dramatically over the past decade. The comprehensive reviews of Datta et al. [<CitationRef CitationID="CR5">5</CitationRef>, <CitationRef CitationID="CR6">6</CitationRef>], Lew et al. [<CitationRef CitationID="CR7">7</CitationRef>] and Huang et al. [<CitationRef CitationID="CR8">8</CitationRef>] provide a good insight into the more recent advances in the entire field of multimedia information retrieval and, in particular, content-based image retrieval.</Para>
              <Para>A particularly well explored subarea of interactive search is called relevance feedback where the search system solicits user feedback on the relevance of results over the course of several rounds of interaction, where after each round the system ideally returns images that better correspond to what the user has in mind. A strength of relevance feedback systems is that the user feedback is simplified to an extreme, typically just a binary “relevant” or “not relevant”. This strength is also a weakness in that the user can often provide richer feedback than relevance. The last review dedicated to relevance feedback in image retrieval was published in 2003 [<CitationRef CitationID="CR9">9</CitationRef>], but with the rapid progress of technology, many novel and interesting techniques have been introduced since then. As is covered in this paper, researchers have gone far beyond simple relevance feedback and frequently integrate more diverse information and techniques into the interactive search process.</Para>
              <Para>In this survey, we reviewed all papers in the ACM, IEEE and Springer digital libraries related to interactive search in content-based image retrieval over the period of 2002–2011 and selected a representative set for inclusion in this overview. This survey is aimed at content-based image retrieval researchers and intends to provide insight into the trends and diversity of interactive search techniques in image retrieval from the perspectives of the users and the systems. This paper will not be discussing the simplest uses (i.e. keyword search) of interactive search. We will be covering more sophisticated types of interactive search which delve into deeper levels of interaction such as wider, multimodal queries and answers, and the next generation approaches of using user feedback such as active learning. We try to present the trends, the larger clusters of research, some of the frontier research, and the major challenges.</Para>
              <Para>We have organized our discussion according to the view of interactive image retrieval as a dialog between user and system, looking at both sides of the story. In Sect. <InternalRef RefID="Sec2">2</InternalRef> we therefore first capture the state of the art by considering how the user interacts with the system and in Sect. <InternalRef RefID="Sec8">3</InternalRef> we then reverse their roles by considering how the system interacts with the user. Because the majority of research focuses on improving interactive image retrieval from the system’s perspective, we have consequently directed more attention to that side of the discussion. In Sect. <InternalRef RefID="Sec15">4</InternalRef> we continue by looking at the ways that retrieval systems are presently evaluated and benchmarked. Finally, in Sect. <InternalRef RefID="Sec19">5</InternalRef> we summarize the promising frontiers and present several grand challenges.</Para>
            </Section1>
            <Section1 ID="Sec2">
              <Heading>Interactive search from the user’s point of view</Heading>
              <Para>A rough overview of the interactive search process is shown in Fig. <InternalRef RefID="Fig3">3</InternalRef>. Note that real systems typically have significantly greater complexity. In the first step, the user issues a query using the interface of the retrieval system and shortly thereafter is presented with the initial results. The user can then interact with the system in order to obtain improved results. Conceivably, the ideal interaction would be through questions and answers (Q&amp;A), similar to the interaction at a library help desk. Through a series of questions and answers the librarian helps the user find what he is interested in, often with the question “Is this what you are looking for?”. This type of interaction would eventually uncover the images that are relevant to the user and which ones are not. In principle, feedback can be given as many times as the user wants, although generally he will stop giving feedback after a few iterations, either because he is satisfied with the retrieval results or because the results no longer improve.
<Figure Category="Standard" Float="Yes" ID="Fig3"><Caption Language="En"><CaptionNumber>Fig. 3</CaptionNumber><CaptionContent><SimplePara>The interactive search process from the user’s point of view</SimplePara></CaptionContent></Caption><MediaObject ID="MO3"><ImageObject Color="Color" FileRef="MediaObjects/13735_2012_14_Fig3_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/></MediaObject></Figure>
              </Para>
              <Section2 ID="Sec3">
                <Heading>Query specification</Heading>
                <Para>The most common way for a retrieval session to start is similar to the Q&amp;A interaction one would have with a librarian. One might provide some descriptive text (i.e. keywords) [<CitationRef CitationID="CR10">10</CitationRef>], provide an example image [<CitationRef CitationID="CR11">11</CitationRef>] or in some situations use the favorites based on the history of the user [<CitationRef CitationID="CR2">2</CitationRef>]. The query step can also be skipped directly when the system shows a random selection of images from the database for the user to give feedback on [<CitationRef CitationID="CR12">12</CitationRef>]. When image segmentation is involved there are a variety of ways to query the retrieval system, such as selecting one or more pre-segmented regions of interest [<CitationRef CitationID="CR13">13</CitationRef>, <CitationRef CitationID="CR14">14</CitationRef>] or drawing outlines of objects of interest [<CitationRef CitationID="CR15">15</CitationRef>, <CitationRef CitationID="CR16">16</CitationRef>]. A novel way to compose the initial query is to let the user first choose keywords from a thesaurus, after which per keyword one of its associated visual regions is selected [<CitationRef CitationID="CR17">17</CitationRef>].</Para>
              </Section2>
              <Section2 ID="Sec4">
                <Heading>Retrieval results</Heading>
                <Para>The standard way in which the results are displayed is a ranked list with the images most similar to the query shown at the top of the list. Because giving feedback on the best matching images does not provide the retrieval system with much additional information other than what it already knows about the user’s interest, a second list is also often shown, which contains the images most informative to the system [<CitationRef CitationID="CR18">18</CitationRef>]. These are usually the images that the system is most uncertain about, for instance those that are on or near a hyperplane when using SVM-based retrieval. This principle, called <Emphasis Type="Italic">active learning</Emphasis>, is discussed in more detail in Sect. <InternalRef RefID="Sec11">3.3</InternalRef>. Innovative ways of displaying the retrieval results are discussed in Sect. <InternalRef RefID="Sec6">2.4</InternalRef>.</Para>
              </Section2>
              <Section2 ID="Sec5">
                <Heading>User interaction</Heading>
                <Para>Many of the systems have interaction which is designed to be used by a machine learning algorithm which gives rise naturally to labeling results as either positive and/or negative examples. These examples are given as feedback to the systems to improve the next iteration of results. Researchers have explored using positive feedback only [<CitationRef CitationID="CR19">19</CitationRef>], positive and negative feedback [<CitationRef CitationID="CR20">20</CitationRef>], positive, neutral and negative feedback [<CitationRef CitationID="CR21">21</CitationRef>], and multiple relevance levels: four relevance levels [<CitationRef CitationID="CR22">22</CitationRef>, <CitationRef CitationID="CR23">23</CitationRef>], five levels [<CitationRef CitationID="CR17">17</CitationRef>] or even seven levels [<CitationRef CitationID="CR24">24</CitationRef>]. An alternative approach is to let the user indicate by what percentage a sample image meets what he has in mind [<CitationRef CitationID="CR25">25</CitationRef>].</Para>
                <Para>While positive/negative examples are important to learning, in many cases it can be advantageous to allow the user to give other kinds of input which may be in other modalities (text, audio, images, etc.), other categories, or personal preferences. Thus, some systems allow the user to input multiple kinds of information in addition to labeled examples [<CitationRef CitationID="CR1">1</CitationRef>, <CitationRef CitationID="CR2">2</CitationRef>, <CitationRef CitationID="CR26">26</CitationRef>, <CitationRef CitationID="CR27">27</CitationRef>, <CitationRef CitationID="CR28">28</CitationRef>, <CitationRef CitationID="CR29">29</CitationRef>, <CitationRef CitationID="CR30">30</CitationRef>, <CitationRef CitationID="CR31">31</CitationRef>]. In addition, sketch interfaces allow the user to give a fundamentally different kind of input to the system [<CitationRef CitationID="CR32">32</CitationRef>, <CitationRef CitationID="CR33">33</CitationRef>], which can potentially give a finer degree of control over the results. In the Q&amp;A paradigm [<CitationRef CitationID="CR34">34</CitationRef>, <CitationRef CitationID="CR35">35</CitationRef>], results may be dynamically selected to best fit the question, based on deeper analysis of the user query. For example, by detecting verbs in the user query or results, the system can determine that a video showing the actions will provide a better answer than an image or only text.</Para>
                <Para>When the system uses segmented images it is possible to implement more elaborate feedback schemes, for instance allowing the splitting or merging of image regions [<CitationRef CitationID="CR36">36</CitationRef>], or supporting drawing a rectangle inside a positive example to select a region of interest [<CitationRef CitationID="CR37">37</CitationRef>]. An interesting discussion on the role and impact of negative images and how to interpret their meaning can be found in [<CitationRef CitationID="CR38">38</CitationRef>]. Besides giving explicit feedback, it is also possible to consider the user’s actions as a form of implicit feedback [<CitationRef CitationID="CR39">39</CitationRef>], which may be used to refine the results that are shown to the user in the next result screen. An example of implicit feedback is a click-through action, where the user clicks on an image with the intention to see it in more detail [<CitationRef CitationID="CR40">40</CitationRef>]. In contrast with the traditional query-based retrieval model, the ostensive relevance feedback model [<CitationRef CitationID="CR41">41</CitationRef>, <CitationRef CitationID="CR42">42</CitationRef>] accommodates for changes in the user’s information needs as they evolve over time through exposure to new information over the course of a single search session.</Para>
              </Section2>
              <Section2 ID="Sec6">
                <Heading>The interface</Heading>
                <Para>The role of the interface in the search process is often limited to displaying a small set of search results that are arranged in a grid, where the user can refine the query by indicating the relevance of each individual image. In recent literature, several interfaces break with this convention, aiming to offer an improved search experience (see Figs. <InternalRef RefID="Fig1">1</InternalRef>, <InternalRef RefID="Fig4">4</InternalRef>). These interfaces mainly focus on one, or a combination, of the following aspects:</Para>
                <Para><Emphasis Type="Italic">Support for easy browsing of the image collection</Emphasis>, for instance through an ontological representation of the image collection where the user can zoom in on different concepts of interest [<CitationRef CitationID="CR43">43</CitationRef>], by easily shifting the focus of attention from image to image allowing the user to visually explore the local relevant neighborhood surrounding an image [<CitationRef CitationID="CR2">2</CitationRef>, <CitationRef CitationID="CR44">44</CitationRef>] or by letting users easily navigate to other promising areas in feature space, which is particularly useful when the search no longer improves with the current set of relevant images [<CitationRef CitationID="CR12">12</CitationRef>].</Para>
                <Para><Emphasis Type="Italic">Better presentation of the search results</Emphasis>, with for instance giving more screen space to images that are likely to be more relevant to the query than to less relevant images [<CitationRef CitationID="CR45">45</CitationRef>], dynamically reorganizing the displayed pages into visual islands [<CitationRef CitationID="CR46">46</CitationRef>] that enable the user to explore deeper into a particular dimension he is interested in, or visualizing the results where similar images are placed closer together [<CitationRef CitationID="CR47">47</CitationRef>, <CitationRef CitationID="CR48">48</CitationRef>].</Para>
                <Para><Emphasis Type="Italic">Multiple query modalities, result modalities and ways of giving feedback</Emphasis>, for instance by allowing the user to query by grouping and/or moving images [<CitationRef CitationID="CR49">49</CitationRef>, <CitationRef CitationID="CR50">50</CitationRef>], ‘scribbling’ on images to make it clear to the retrieval system which parts of an image should be considered foreground and which parts background [<CitationRef CitationID="CR51">51</CitationRef>], or providing the user with the best mixture of media for expressing a query or understanding the results.</Para>
              </Section2>
              <Section2 ID="Sec7">
                <Heading>Trends and advances</Heading>
                <Para>The increasing popularity of higher level image descriptors has expressed itself in approaches that are tailored to support those ways of searching. In particular, we have noticed an increase in research on how to best leverage region-based image retrieval, offering new ways to initiate the search, give feedback and visualize the retrieval results. During the last decade we have seen the interface transition from having only a supportive role to playing a more substantial role in finding images. The interfaces have evolved from simple grids to a wide variety of approaches, which include but are not limited to image clusters, ontologies, image linked representations (e.g. the tendril interface), and 3D visualizations.</Para>
                <Para>Recent advances have expanded the frontiers in both the user interface and the kinds of interaction the user can have with the system. In particular, these systems allow the user to ask multi-modal queries/questions and also give multi-modal input on the set of results. Furthermore, it is also a growing trend to integrate browsing and search as well as provide varying levels of explanations for why the results were chosen.
<Figure Category="Standard" Float="Yes" ID="Fig4"><Caption Language="En"><CaptionNumber>Fig. 4</CaptionNumber><CaptionContent><SimplePara>Examples of user interfaces. The ‘similarity visualization’ interface [<CitationRef CitationID="CR47">47</CitationRef>] (<Emphasis Type="Italic">left</Emphasis>) displays a representative set of images from the entire collection, where similar images are projected close to each other and dissimilar ones far away. The ‘visual islands’ interface [<CitationRef CitationID="CR46">46</CitationRef>] (<Emphasis Type="Italic">right</Emphasis>) reorganizes search results into colored clusters of related images</SimplePara></CaptionContent></Caption><MediaObject ID="MO4"><ImageObject Color="Color" FileRef="MediaObjects/13735_2012_14_Fig4_HTML.jpg" Format="JPEG" Rendition="HTML" Type="Halftone"/></MediaObject></Figure>
                  <Figure Category="Standard" Float="Yes" ID="Fig5"><Caption Language="En"><CaptionNumber>Fig. 5</CaptionNumber><CaptionContent><SimplePara>The interactive search process from the system’s point of view</SimplePara></CaptionContent></Caption><MediaObject ID="MO5"><ImageObject Color="Color" FileRef="MediaObjects/13735_2012_14_Fig5_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/></MediaObject></Figure>
                </Para>
              </Section2>
            </Section1>
            <Section1 ID="Sec8">
              <Heading>Interactive search from the system’s point of view</Heading>
              <Para>A global overview of a retrieval system is shown in Fig. <InternalRef RefID="Fig5">5</InternalRef>. The images in the database are converted into a particular image representation, which can optionally be stored in an indexing structure to speed up the search. Once a query is received, the system applies an algorithm to learn what kind of images the user is interested in, after which the database images are ranked and shown to the user with the best matches first. Any feedback the user gives can optionally be stored in a log for the purpose of discovering search patterns, so learning will improve in the long run. In this section, we cover the recent advances on each of these parts of a retrieval system.
<Figure Category="Standard" Float="Yes" ID="Fig6"><Caption Language="En"><CaptionNumber>Fig. 6</CaptionNumber><CaptionContent><SimplePara>Images overlaid with detected visual words. Identically colored squares indicate identical visual words, while differently colored squares indicate different visual words (color figure online)</SimplePara></CaptionContent></Caption><MediaObject ID="MO6"><ImageObject Color="Color" FileRef="MediaObjects/13735_2012_14_Fig6_HTML.jpg" Format="JPEG" Rendition="HTML" Type="Halftone"/></MediaObject></Figure>
              </Para>
              <Section2 ID="Sec9">
                <Heading>Image representation</Heading>
                <Para>By itself an image is simply a rectangular grid of colored pixels. In the brain of a human observer these pixels form meanings based on the person’s memories and experiences, expressing itself in a near-instantaneous recognition of objects, events and locations. However, to a computer an image does not mean anything, unless it is told how to interpret it. Often images are converted into low-level features, which ideally capture the image characteristics in such a way that it is easy for the retrieval system to determine how similar two images are as perceived by the user. In current research, the attention is shifting to mid-level and high-level image representations.</Para>
                <Para><Emphasis Type="Italic">Mid-level representations</Emphasis> focus on particular parts of the image that are important, such as sub-images [<CitationRef CitationID="CR52">52</CitationRef>], regions [<CitationRef CitationID="CR53">53</CitationRef>, <CitationRef CitationID="CR54">54</CitationRef>] and salient details [<CitationRef CitationID="CR36">36</CitationRef>, <CitationRef CitationID="CR55">55</CitationRef>]. After these image elements have been determined, they are often seen as standalone entities during the search. However, some approaches represent them in a hierarchical [<CitationRef CitationID="CR43">43</CitationRef>, <CitationRef CitationID="CR56">56</CitationRef>, <CitationRef CitationID="CR57">57</CitationRef>] or graph-based structure and exploit this structure when searching for improved retrieval results. The multiple instance learning and bagging approach [<CitationRef CitationID="CR37">37</CitationRef>, <CitationRef CitationID="CR58">58</CitationRef>, <CitationRef CitationID="CR59">59</CitationRef>, <CitationRef CitationID="CR60">60</CitationRef>, <CitationRef CitationID="CR61">61</CitationRef>] lends itself very well to image retrieval, because an image can be seen as a bag of visual words where these visual words can, for instance, be interest points, regions, patches or objects (see Fig. <InternalRef RefID="Fig6">6</InternalRef>). By incorporating feedback, the idea is that the user can only give feedback on the entire bag (i.e. the image), although he might only be interested in one or more specific instances (i.e. visual words) in that bag. The goal is then for the system to obtain a hypothesis from the feedback images that predicts which visual words the user is looking for. An unconventional way of using bags is presented in [<CitationRef CitationID="CR62">62</CitationRef>], where the multiple instance learning technique does not assume that a bag is positive when one or more of its instances are positive.
<Figure Category="Standard" Float="Yes" ID="Fig7"><Caption Language="En"><CaptionNumber>Fig. 7</CaptionNumber><CaptionContent><SimplePara>A thesaurus is used to link keywords to images [<CitationRef CitationID="CR74">74</CitationRef>]</SimplePara></CaptionContent></Caption><MediaObject ID="MO7"><ImageObject Color="BlackWhite" FileRef="MediaObjects/13735_2012_14_Fig7_HTML.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></MediaObject></Figure></Para>
                <Para><Emphasis Type="Italic">High-level representations</Emphasis> are designed with semantics in mind. The way semantics are expressed is usually in the form of concepts, which are commonly seen as a coherent collection of image patches (‘visual concepts’) or sometimes as the equivalent of keywords (‘textual concepts’). The number of visual concepts present in an image collection can be fixed beforehand [<CitationRef CitationID="CR63">63</CitationRef>, <CitationRef CitationID="CR64">64</CitationRef>], estimated beforehand [<CitationRef CitationID="CR57">57</CitationRef>, <CitationRef CitationID="CR65">65</CitationRef>], or alternatively automatically determined while the system is running using adaptive approaches [<CitationRef CitationID="CR66">66</CitationRef>, <CitationRef CitationID="CR67">67</CitationRef>]. A thesaurus, such as WordNet [<CitationRef CitationID="CR68">68</CitationRef>], is often used to link annotations to image concepts [<CitationRef CitationID="CR69">69</CitationRef>, <CitationRef CitationID="CR70">70</CitationRef>], for instance by linking them through synonymy, hypernymy, hyponymy, etc. [<CitationRef CitationID="CR71">71</CitationRef>] (see Fig. <InternalRef RefID="Fig7">7</InternalRef>). Since manually annotating large collections of images is a tedious task, much research is directed at automatic annotation, mostly offline [<CitationRef CitationID="CR72">72</CitationRef>, <CitationRef CitationID="CR73">73</CitationRef>], but also driven by relevance feedback [<CitationRef CitationID="CR74">74</CitationRef>]. Finding the best balance between using keywords for searching and using visual features for searching is one of the newer topics in image retrieval [<CitationRef CitationID="CR75">75</CitationRef>, <CitationRef CitationID="CR76">76</CitationRef>]. For instance, in [<CitationRef CitationID="CR40">40</CitationRef>] the image ranking presented to the user is composed first using a textual query vector to rank all database images and then using a visual query vector to re-rank them.</Para>
              </Section2>
              <Section2 ID="Sec10">
                <Heading>Indexing and filtering</Heading>
                <Para>Finding images that have high similarity with a query image often requires the entire database to be traversed for one-on-one comparisons. When dealing with large image collections this becomes prohibitive due to the amount of time the traversal takes. In the last few decades various indexing and filtering schemes have been proposed to reduce the number of database images to look at, thus improving the responsiveness of the system as perceived by the user. A good theoretical overview of indexing structures that can be used to index high-dimensional spaces is given in [<CitationRef CitationID="CR77">77</CitationRef>].</Para>
                <Para>The majority of recent research in this direction focuses on the clustering of images, so that a reduction of the number of images to consider is then a matter of finding out which cluster(s) the query image belongs to [<CitationRef CitationID="CR14">14</CitationRef>, <CitationRef CitationID="CR78">78</CitationRef>, <CitationRef CitationID="CR79">79</CitationRef>]. Often the image clusters are stored in a hierarchical indexing structure to allow for a step-wise refinement of the number of images to consider [<CitationRef CitationID="CR80">80</CitationRef>, <CitationRef CitationID="CR81">81</CitationRef>]. Alternatively, the set of images that are likely relevant to the query can be quickly established by approximating their feature vectors [<CitationRef CitationID="CR52">52</CitationRef>, <CitationRef CitationID="CR82">82</CitationRef>]. A third way to reduce the number of images to inspect is by partitioning the feature space and only looking at that area of space which the query image belongs to [<CitationRef CitationID="CR83">83</CitationRef>, <CitationRef CitationID="CR84">84</CitationRef>]. Hashing is a form of space partitioning and is considered to be an efficient approach for indexing [<CitationRef CitationID="CR85">85</CitationRef>, <CitationRef CitationID="CR86">86</CitationRef>, <CitationRef CitationID="CR87">87</CitationRef>].</Para>
              </Section2>
              <Section2 ID="Sec11">
                <Heading>Active learning and classification</Heading>
                <Para>The core of the retrieval system is the algorithm that learns which images in the database the user is interested in by analyzing the query image and any implicit or explicit feedback. Typical interactive systems have two categories of images to show the user: (1) <Emphasis Type="Italic">clarification images</Emphasis>, which are images that may not be wanted by the user but that will help the learning algorithm improve its accuracy, and (2) <Emphasis Type="Italic">relevant images</Emphasis>, which are the images wanted by the user. How to decide which imagery to select for the first category is addressed by an area called “active learning”, which we first describe in more detail below.</Para>
                <Para><Emphasis Type="Italic">Active learning</Emphasis> Arguably, the most important challenge in interactive search systems is how to reduce the interaction effort from the user while maximizing the accuracy of the results. From a theoretical perspective, how can we measure the information associated with an unlabeled example, so a learner can select the optimal set of unlabeled examples to show to the user that maximizes its information gain and thus minimizes the expected future classification error [<CitationRef CitationID="CR88">88</CitationRef>, <CitationRef CitationID="CR89">89</CitationRef>, <CitationRef CitationID="CR90">90</CitationRef>, <CitationRef CitationID="CR91">91</CitationRef>]?</Para>
                <Para>This category as pertaining to image search is usually called <Emphasis Type="Italic">active learning</Emphasis> in the research community and is closely related to <Emphasis Type="Italic">relevance feedback</Emphasis>, which many consider to be a special case of active learning. Especially during the last few years researchers are going beyond just selecting the unlabeled examples closest to the decision boundary by also aiming to maximize diversity amongst the chosen images [<CitationRef CitationID="CR71">71</CitationRef>, <CitationRef CitationID="CR92">92</CitationRef>, <CitationRef CitationID="CR93">93</CitationRef>, <CitationRef CitationID="CR94">94</CitationRef>]. For instance, by trying to avoid selecting examples with certain visual properties that are already overly present in the list of top-ranked images [<CitationRef CitationID="CR18">18</CitationRef>] or by clustering the unlabeled candidate images by their similarity, so only a few examples per cluster need to be picked [<CitationRef CitationID="CR95">95</CitationRef>, <CitationRef CitationID="CR96">96</CitationRef>, <CitationRef CitationID="CR97">97</CitationRef>].</Para>
                <Para>When multiple learners are used, a typical strategy is to select unlabeled examples for which the learners disagree the most in terms of their labeling [<CitationRef CitationID="CR98">98</CitationRef>, <CitationRef CitationID="CR99">99</CitationRef>, <CitationRef CitationID="CR100">100</CitationRef>, <CitationRef CitationID="CR101">101</CitationRef>]. With large image databases being commonplace, another focus in recent years has been placed on strategies to reduce the computational complexity [<CitationRef CitationID="CR102">102</CitationRef>], in particular, by filtering out unlabeled examples that are unlikely to contribute much to the decision boundary, so less examples need to be considered by the active learning algorithm [<CitationRef CitationID="CR103">103</CitationRef>, <CitationRef CitationID="CR104">104</CitationRef>]. Integrating large external knowledge databases [<CitationRef CitationID="CR24">24</CitationRef>, <CitationRef CitationID="CR105">105</CitationRef>, <CitationRef CitationID="CR106">106</CitationRef>] into the search algorithm has seen increasing attention. These systems frequently use the external databases such as the WWW, Wikipedia, or social media networks to provide clarification of the user intent [<CitationRef CitationID="CR107">107</CitationRef>] or to form additional links/connections between imagery and multimodal information towards minimizing the number of queries to the user [<CitationRef CitationID="CR71">71</CitationRef>].</Para>
                <Para>In the literature we can find diverse and interesting approaches for improving the feature space. Feature selection and manifold learning can reduce the complexity of the feature space and improve the shape of the clusters to make the relevance problem easier to learn by the classifier. The inclusion of synthetic imagery in the feedback process can be especially beneficial towards assisting in active learning. Recent work in each these directions is described below.
<Figure Category="Standard" Float="Yes" ID="Fig8"><Caption Language="En"><CaptionNumber>Fig. 8</CaptionNumber><CaptionContent><SimplePara>A manifold is learned by projecting the relevant images close together and the irrelevant ones far away [<CitationRef CitationID="CR118">118</CitationRef>]</SimplePara></CaptionContent></Caption><MediaObject ID="MO8"><ImageObject Color="BlackWhite" FileRef="MediaObjects/13735_2012_14_Fig8_HTML.gif" Format="GIF" Rendition="HTML" Type="LinedrawHalftone"/></MediaObject></Figure>
                  <Figure Category="Standard" Float="Yes" ID="Fig9"><Caption Language="En"><CaptionNumber>Fig. 9</CaptionNumber><CaptionContent><SimplePara>Example of synthetic imagery such as used in [<CitationRef CitationID="CR11">11</CitationRef>], where several images are synthesized containing an object in different arrangements</SimplePara></CaptionContent></Caption><MediaObject ID="MO9"><ImageObject Color="Color" FileRef="MediaObjects/13735_2012_14_Fig9_HTML.jpg" Format="JPEG" Rendition="HTML" Type="Halftone"/></MediaObject></Figure>
                </Para>
                <Para><Emphasis Type="Italic">Feature selection and weighting</Emphasis> One of the ways to discover the hidden information from the user’s feedback is let the search mainly focus on those features that feedback images have in common [<CitationRef CitationID="CR108">108</CitationRef>, <CitationRef CitationID="CR109">109</CitationRef>, <CitationRef CitationID="CR110">110</CitationRef>]. The feature space can also be transformed to discover hidden properties amongst relevant images, which is often done using principal component analysis [<CitationRef CitationID="CR111">111</CitationRef>], discriminant component analysis [<CitationRef CitationID="CR112">112</CitationRef>] or linear discriminant analysis [<CitationRef CitationID="CR113">113</CitationRef>]. One of the drawbacks of linear discriminant analysis is that negative feedback is treated as belonging to a single class, which is why researchers currently focus on multi-class [<CitationRef CitationID="CR114">114</CitationRef>] or biased [<CitationRef CitationID="CR115">115</CitationRef>] extensions to improve retrieval performance.</Para>
                <Para><Emphasis Type="Italic">Manifold learning</Emphasis> Manifold learning aims to learn the local structure formed by the query and feedback images, by creating a subspace where the relevant images are projected close together while the irrelevant images are projected far away (see Fig. <InternalRef RefID="Fig8">8</InternalRef>). The most promising and popular approaches are currently based on linear extensions of graph embedding [<CitationRef CitationID="CR116">116</CitationRef>, <CitationRef CitationID="CR117">117</CitationRef>, <CitationRef CitationID="CR118">118</CitationRef>, <CitationRef CitationID="CR119">119</CitationRef>, <CitationRef CitationID="CR120">120</CitationRef>], which mostly differ in their choices of the affinity graph and the constraint graph.</Para>
                <Para><Emphasis Type="Italic">Synthetic and pseudo-imagery</Emphasis> An interesting development is the use of synthetic or pseudo-imagery during relevance feedback to improve the search results [<CitationRef CitationID="CR11">11</CitationRef>, <CitationRef CitationID="CR121">121</CitationRef>, <CitationRef CitationID="CR122">122</CitationRef>, <CitationRef CitationID="CR123">123</CitationRef>, <CitationRef CitationID="CR124">124</CitationRef>]. When the system wants to ask the user about a particular region of feature space to clarify the decision boundary, there may not be an suitable image in the database due to the sparsity of images compared to the dimensionality of the feature space. By giving the system the ability to synthesize imagery corresponding to a point in feature space, the system can then clarify the uncertain area, as subsequent feedback on these synthetic images would allow the system to better narrow down what the user is looking for (see Fig. <InternalRef RefID="Fig9">9</InternalRef>).</Para>
                <Para>As the user interacts with the system and gives it positive and/or negative feedback, this feedback can be given to learning algorithms to address the classification of images as relevant images, which can then be cast as a classic machine learning problem:<UnorderedList Mark="Dash"><ItemContent><Para>Cluster approaches: methods which represent the clusters of the images in feature space, such as query point or nearest neighbor-based learning.</Para></ItemContent><ItemContent><Para>Decision plane approaches: methods which represent the decision planes between clusters of images, such as artificial neural networks, support vector machines and kernel approaches.</Para></ItemContent><ItemContent><Para>Combining learners: methods that combine multiple classifiers to improve the overall accuracy.</Para></ItemContent></UnorderedList>There is extensive literature describing the theory and motivation for the methods above, which is beyond the scope of this survey. We restrict ourselves to concise descriptions of recent developments in this area.</Para>
                <Para><Emphasis Type="Italic">Artificial neural networks</Emphasis> One of the popular approaches is the RBF network [<CitationRef CitationID="CR125">125</CitationRef>, <CitationRef CitationID="CR126">126</CitationRef>], which uses radial basis functions as activation functions. These functions have the advantage over sigmoids that generally only one layer of hidden radial units is sufficient to model any function. Another popular approach is the self-organizing map [<CitationRef CitationID="CR127">127</CitationRef>, <CitationRef CitationID="CR128">128</CitationRef>], which in contrast with other kinds of neural networks does not need supervision during training. It projects the high-dimensional feature vectors down to only a few dimensions, typically two. Feedback causes the relevance information to spread to the neighboring units, based on the assumption that similar images are located near each other on the map surface. The spreading of the relevance values happens by convolving the surface with window or kernel functions (see Fig. <InternalRef RefID="Fig10">10</InternalRef>).
<Figure Category="Standard" Float="Yes" ID="Fig10"><Caption Language="En"><CaptionNumber>Fig. 10</CaptionNumber><CaptionContent><SimplePara>The positive (<Emphasis Type="Italic">white</Emphasis>) and negative (<Emphasis Type="Italic">black</Emphasis>) map units in a self-organizing map (<Emphasis Type="Italic">left</Emphasis>) are convolved with a low-pass filter mask, leading to the relevance values being spread across the map surface (<Emphasis Type="Italic">right</Emphasis>) [<CitationRef CitationID="CR128">128</CitationRef>]</SimplePara></CaptionContent></Caption><MediaObject ID="MO10"><ImageObject Color="BlackWhite" FileRef="MediaObjects/13735_2012_14_Fig10_HTML.jpg" Format="JPEG" Rendition="HTML" Type="Halftone"/></MediaObject></Figure></Para>
                <Para><Emphasis Type="Italic">Support vector machine</Emphasis> The current trend is the development of techniques that aim to overcome the inherent limitations of standard SVMs, such as targeting the imbalanced training set [<CitationRef CitationID="CR127">127</CitationRef>, <CitationRef CitationID="CR129">129</CitationRef>, <CitationRef CitationID="CR130">130</CitationRef>], filtering out noisy feedback [<CitationRef CitationID="CR131">131</CitationRef>], reducing the amount of computation necessary between rounds of feedback [<CitationRef CitationID="CR132">132</CitationRef>] or offering more flexibility in the labeling of examples [<CitationRef CitationID="CR133">133</CitationRef>]. For instance, a fuzzy SVM [<CitationRef CitationID="CR134">134</CitationRef>] uses the fuzzy class membership values to reduce the effect of less important examples, so that the examples with higher confidence have a larger effect on the decision boundary.</Para>
                <Para><Emphasis Type="Italic">Kernels</Emphasis> Many approaches, such as support vector machines, use kernels to convert the feature space to a higher- or lower-dimensional space, where ideally the images of interest can be linearly separated from all other images. We show the popularity of common kernel variations in Table <InternalRef RefID="Tab1">1</InternalRef>. The kernel that is used is generally fixed, i.e. the type of kernel and its parameters are determined beforehand, although particularly in recent work positive and negative feedback is used to guide the design and/or selection of optimal kernels [<CitationRef CitationID="CR135">135</CitationRef>, <CitationRef CitationID="CR136">136</CitationRef>, <CitationRef CitationID="CR137">137</CitationRef>].<Table Float="Yes" ID="Tab1"><Caption Language="En"><CaptionNumber>Table 1</CaptionNumber><CaptionContent><SimplePara>Popularity of kernel variations</SimplePara></CaptionContent></Caption><tgroup cols="5"><colspec align="left" colname="c1" colnum="1"/><colspec align="left" colname="c2" colnum="2"/><colspec align="left" colname="c3" colnum="3"/><colspec align="left" colname="c4" colnum="4"/><colspec align="left" colname="c5" colnum="5"/><thead><row><entry align="left" colname="c1"><SimplePara>Linear</SimplePara></entry><entry align="left" colname="c2"><SimplePara>Polynomial</SimplePara></entry><entry align="left" colname="c3"><SimplePara>Triangular</SimplePara></entry><entry align="left" colname="c4"><SimplePara>Gaussian</SimplePara></entry><entry align="left" colname="c5"><SimplePara>Laplacian</SimplePara></entry></row></thead><tbody><row><entry align="left" colname="c1"><SimplePara>1 %</SimplePara></entry><entry align="left" colname="c2"><SimplePara>12 %</SimplePara></entry><entry align="left" colname="c3"><SimplePara>6 %</SimplePara></entry><entry align="left" colname="c4"><SimplePara>73 %</SimplePara></entry><entry align="left" colname="c5"><SimplePara>8 %</SimplePara></entry></row></tbody></tgroup></Table></Para>
                <Para>
                  <IndexTerm> </IndexTerm>
                </Para>
                <Para><Emphasis Type="Italic">Combining learners</Emphasis> Instead of using a single learner to classify an unlabeled image, multiple independent learners can be combined to obtain a better classification, e.g. by combining their individual decision functions into an overall decision function [<CitationRef CitationID="CR138">138</CitationRef>, <CitationRef CitationID="CR139">139</CitationRef>], by majority voting [<CitationRef CitationID="CR110">110</CitationRef>, <CitationRef CitationID="CR130">130</CitationRef>, <CitationRef CitationID="CR134">134</CitationRef>] or by selecting the most appropriate learner(s) for a particular query [<CitationRef CitationID="CR140">140</CitationRef>].</Para>
                <Para><Emphasis Type="Italic">Probabilistic classifiers</Emphasis> Mixture models [<CitationRef CitationID="CR141">141</CitationRef>, <CitationRef CitationID="CR142">142</CitationRef>] are used to overcome the limitations of using only a single density function to model the relevant class. Mixture models are a combination of multiple probabilistic distributions, where the number of distributions (components) it comprises is ideally identical to the number of classes present in the data. Other approaches in this category aim to learn the probabilistic model and unconditional density of the positive and/or negative classes [<CitationRef CitationID="CR143">143</CitationRef>, <CitationRef CitationID="CR144">144</CitationRef>].
<Table Float="Yes" ID="Tab2"><Caption Language="En"><CaptionNumber>Table 2</CaptionNumber><CaptionContent><SimplePara>Popularity of classification approaches</SimplePara></CaptionContent></Caption><tgroup cols="6"><colspec align="left" colname="c1" colnum="1"/><colspec align="left" colname="c2" colnum="2"/><colspec align="left" colname="c3" colnum="3"/><colspec align="left" colname="c4" colnum="4"/><colspec align="left" colname="c5" colnum="5"/><colspec align="left" colname="c6" colnum="6"/><thead><row><entry align="left" colname="c1"><SimplePara>One-class</SimplePara></entry><entry align="left" colname="c2"><SimplePara>Two-class</SimplePara></entry><entry align="left" colname="c3"><SimplePara><InlineEquation ID="IEq1"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_14_Article_IEq1.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$1+x$$]]></EquationSource></InlineEquation></SimplePara></entry><entry align="left" colname="c4"><SimplePara><InlineEquation ID="IEq2"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_14_Article_IEq2.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x+1$$]]></EquationSource></InlineEquation></SimplePara></entry><entry align="left" colname="c5"><SimplePara><InlineEquation ID="IEq3"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_14_Article_IEq3.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x+y$$]]></EquationSource></InlineEquation></SimplePara></entry><entry align="left" colname="c6"><SimplePara>Soft label</SimplePara></entry></row></thead><tbody><row><entry align="left" colname="c1"><SimplePara>32 %</SimplePara></entry><entry align="left" colname="c2"><SimplePara>38 %</SimplePara></entry><entry align="left" colname="c3"><SimplePara>18 %</SimplePara></entry><entry align="left" colname="c4"><SimplePara>2 %</SimplePara></entry><entry align="left" colname="c5"><SimplePara>2 %</SimplePara></entry><entry align="left" colname="c6"><SimplePara>8 %</SimplePara></entry></row></tbody></tgroup></Table><Table Float="Yes" ID="Tab3"><Caption Language="En"><CaptionNumber>Table 3</CaptionNumber><CaptionContent><SimplePara>Popularity of common similarity measures</SimplePara></CaptionContent></Caption><tgroup cols="8"><colspec align="left" colname="c1" colnum="1"/><colspec align="left" colname="c2" colnum="2"/><colspec align="left" colname="c3" colnum="3"/><colspec align="left" colname="c4" colnum="4"/><colspec align="left" colname="c5" colnum="5"/><colspec align="left" colname="c6" colnum="6"/><colspec align="left" colname="c7" colnum="7"/><colspec align="left" colname="c8" colnum="8"/><thead><row><entry align="left" colname="c1"><SimplePara>Manhattan</SimplePara></entry><entry align="left" colname="c2"><SimplePara>Euclidean</SimplePara></entry><entry align="left" colname="c3"><SimplePara>Minkowski</SimplePara></entry><entry align="left" colname="c4"><SimplePara>Earth Mover’s distance</SimplePara></entry><entry align="left" colname="c5"><SimplePara>Bhattacharyya</SimplePara></entry><entry align="left" colname="c6"><SimplePara>Mahalanobis</SimplePara></entry><entry align="left" colname="c7"><SimplePara>Hausdorff</SimplePara></entry><entry align="left" colname="c8"><SimplePara>Kullback-Leibler</SimplePara></entry></row></thead><tbody><row><entry align="left" colname="c1"><SimplePara>11 %</SimplePara></entry><entry align="left" colname="c2"><SimplePara>49 %</SimplePara></entry><entry align="left" colname="c3"><SimplePara>1 %</SimplePara></entry><entry align="left" colname="c4"><SimplePara>4 %</SimplePara></entry><entry align="left" colname="c5"><SimplePara>1 %</SimplePara></entry><entry align="left" colname="c6"><SimplePara>5 %</SimplePara></entry><entry align="left" colname="c7"><SimplePara>2 %</SimplePara></entry><entry align="left" colname="c8"><SimplePara>4 %</SimplePara></entry></row></tbody></tgroup><tgroup cols="8"><colspec align="left" colname="c1" colnum="1"/><colspec align="left" colname="c2" colnum="2"/><colspec align="left" colname="c3" colnum="3"/><colspec align="left" colname="c4" colnum="4"/><colspec align="left" colname="c5" colnum="5"/><colspec align="left" colname="c6" colnum="6"/><colspec align="left" colname="c7" colnum="7"/><colspec align="left" colname="c8" colnum="8"/><thead><row><entry align="left" colname="c1"><SimplePara> Chi-square</SimplePara></entry><entry align="left" colname="c2"><SimplePara>Probability</SimplePara></entry><entry align="left" colname="c3"><SimplePara>Cosine</SimplePara></entry><entry align="left" colname="c4"><SimplePara>Graph</SimplePara></entry><entry align="left" colname="c5"><SimplePara>Unified feature matching</SimplePara></entry><entry align="left" colname="c6"><SimplePara>Integrated region matching</SimplePara></entry><entry align="left" nameend="c8" namest="c7"><SimplePara>d<Emphasis Type="Italic">N</Emphasis>/d<Emphasis Type="Italic">R</Emphasis>
                            </SimplePara></entry></row></thead><tbody><row><entry align="left" colname="c1"><SimplePara>3 %</SimplePara></entry><entry align="left" colname="c2"><SimplePara>10 %</SimplePara></entry><entry align="left" colname="c3"><SimplePara>2 %</SimplePara></entry><entry align="left" colname="c4"><SimplePara>3 %</SimplePara></entry><entry align="left" colname="c5"><SimplePara>1 %</SimplePara></entry><entry align="left" colname="c6"><SimplePara>1 %</SimplePara></entry><entry align="left" nameend="c8" namest="c7"><SimplePara>3 %</SimplePara></entry></row></tbody></tgroup><tfooter><SimplePara>Note that (a) Minkowski refers to all similarity measures in its family other than Manhattan and Euclidean, (b) Probability refers to similarity measures that calculate the likelihood of an image belonging to the target category, (c) Graph refers to similarity measures that determine the shortest path between two nodes in a graph, (d) d<Emphasis Type="Italic">R</Emphasis>/d<Emphasis Type="Italic">N</Emphasis> refers to similarity measures that compare the relative distance of an image to its nearest relevant and nearest irrelevant neighbors</SimplePara></tfooter></Table></Para>
                <Para><Emphasis Type="Italic">Classification approaches</Emphasis> Some methods directly assign relevance scores to each image in the database, whereas other methods attempt to classify the images using a <Emphasis Type="Italic">one-class</Emphasis> approach, where a model is built for only the relevant class [<CitationRef CitationID="CR58">58</CitationRef>], or a <Emphasis Type="Italic">two-class</Emphasis> approach, where a model is built that either classifies an image as positive or as negative [<CitationRef CitationID="CR145">145</CitationRef>]. Other variations exist that allow for more flexibility, for instance <InlineEquation ID="IEq4"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_14_Article_IEq4.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$1+x$$]]></EquationSource></InlineEquation> [<CitationRef CitationID="CR92">92</CitationRef>], <InlineEquation ID="IEq5"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_14_Article_IEq5.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x+1$$]]></EquationSource></InlineEquation> [<CitationRef CitationID="CR138">138</CitationRef>], <InlineEquation ID="IEq6"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_14_Article_IEq6.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$x+y$$]]></EquationSource></InlineEquation> [<CitationRef CitationID="CR49">49</CitationRef>] and <Emphasis Type="Italic">soft label</Emphasis> [<CitationRef CitationID="CR146">146</CitationRef>]. The popularity of the classification approaches as used in the recent literature is shown in Table <InternalRef RefID="Tab2">2</InternalRef>.</Para>
              </Section2>
              <Section2 ID="Sec12">
                <Heading>Similarity measures, distance and ranking</Heading>
                <Para>What matters the most in image retrieval is the list of results that is shown to the user, with the most relevant images shown first. In general, to obtain this ranking a similarity measure is used that assigns a score to each database image indicating how relevant the system thinks it is to the user’s interests. The advantages and disadvantages of using a metric to measure perceptual similarity are discussed in [<CitationRef CitationID="CR147">147</CitationRef>], in which the authors argue for incorporating the notion of <Emphasis Type="Italic">betweenness</Emphasis> when ranking images to allow for a better relative ordering between them. Ways of calculating scores include using the relative distance of an image to its nearest relevant and nearest irrelevant neighbors [<CitationRef CitationID="CR148">148</CitationRef>, <CitationRef CitationID="CR149">149</CitationRef>] or combining multiple similarity measures to give a single relevance score [<CitationRef CitationID="CR59">59</CitationRef>, <CitationRef CitationID="CR150">150</CitationRef>]. Relevance feedback can also be considered to be an ordinal regression problem [<CitationRef CitationID="CR23">23</CitationRef>, <CitationRef CitationID="CR151">151</CitationRef>], where users do not give an absolute but rather a relative judgment between images.</Para>
                <Para>We show the popularity of common similarity measures in Table <InternalRef RefID="Tab3">3</InternalRef>. As can be seen the Euclidean (<InlineEquation ID="IEq7"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_14_Article_IEq7.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$$\text{ L}_{2}$$]]></EquationSource></InlineEquation>) distance measure is used most frequently, although in a substantial number of papers it was only used in the initial iteration and a more advanced similarity measure was applied once feedback was received. Many similarity measures are tailored to the problem to solve and thus quite specialized, which are therefore not included in the table.</Para>
              </Section2>
              <Section2 ID="Sec13">
                <Heading>Long-term learning</Heading>
                <Para>In contrast with short-term learning, where the state of the retrieval system is reset after every user session, long-term learning is designed to use the information gathered during previous retrieval sessions to improve the retrieval results in future sessions. Long-term learning is also frequently referred to as collaborative filtering. The most popular approach for long-term learning is to infer relationships between images by analyzing the <Emphasis Type="Italic">feedback log </Emphasis>[<CitationRef CitationID="CR52">52</CitationRef>, <CitationRef CitationID="CR79">79</CitationRef>, <CitationRef CitationID="CR152">152</CitationRef>], which contains all feedback given by users over time. From the accumulated feedback logs a semantic space can be learned containing the relationships between the images and one or more classes, typically obtained by applying matrix factorization [<CitationRef CitationID="CR153">153</CitationRef>, <CitationRef CitationID="CR154">154</CitationRef>, <CitationRef CitationID="CR155">155</CitationRef>] or clustering [<CitationRef CitationID="CR156">156</CitationRef>] techniques. Whereas the early long-term learning methods mostly built static relevance models, the recent trend is to continuously update the model after receiving new feedback [<CitationRef CitationID="CR157">157</CitationRef>, <CitationRef CitationID="CR158">158</CitationRef>, <CitationRef CitationID="CR159">159</CitationRef>, <CitationRef CitationID="CR160">160</CitationRef>].</Para>
              </Section2>
              <Section2 ID="Sec14">
                <Heading>Trends and advances</Heading>
                <Para>It is generally agreed upon that minimizing the number of questions that need to be asked (small training set problem) is one of the grand challenges. Over the past decade we have seen several different trends that include, but are not limited to, (1) query point movement, (2) query set movement, (3) input near decision borders, and (4) input reflecting additional information sources. By query point movement, we refer to the Rocchio [<CitationRef CitationID="CR9">9</CitationRef>] inspired methods where a single query point is shifted towards the positive examples and away from the negative examples. This paradigm has worked surprisingly well when there is little feedback; however, it has a notable problem that it cannot adjust to multiple clusters of relevant results. This led to query set movement approaches, which move multiple query points that ideally end up in each relevant cluster in the database; yet, this method has distinct weaknesses when there are many clusters or when the class separation between positive and negative clusters is small. In reaction, the research community investigated decision border approaches where the user was asked to clarify the ambiguous regions near the borders. In a large image database, however, the number of decision borders can be very large, so that even in the simplest case where the system needs to get feedback for every decision border this can result in an overload of questions to the user. This, in turn, has led to methods which attempt to gain clarification by exploiting additional or external sources, such as personal history, the Internet, or Wikipedia. Another challenge has been shown to be the problem of sparsity in the image database which has recently been addressed by using both external sources and synthetic imagery.</Para>
                <Para>From the articles published during the last decade we can see the perception of image retrieval slowly shifting from pixel-based to concept-based, especially because it generally has led to an increase in retrieval performance. This new concept-based view has inspired the development of many new high-level descriptors. The bag-of-words and manifold learning approaches remain popular, and especially the latter has become a particularly active research area, providing a stimulating and competitive research environment. Long-term learning and approaches that combine multiple information sources have also demonstrated steady and significant improvements in retrieval performance over the previous years. Rocchio [<CitationRef CitationID="CR9">9</CitationRef>] approaches are currently only used for comparative benchmarks relative to a novel algorithm.</Para>
                <Para>
                  <IndexTerm> </IndexTerm>
                </Para>
              </Section2>
            </Section1>
            <Section1 ID="Sec15">
              <Heading>Evaluation and benchmarking</Heading>
              <Para>Assessing user satisfaction and general evaluation of interactive retrieval systems [<CitationRef CitationID="CR7">7</CitationRef>, <CitationRef CitationID="CR161">161</CitationRef>, <CitationRef CitationID="CR162">162</CitationRef>] is well known to be both difficult and challenging. Experiments that are well executed from a statistical point of view require a relatively large number of diverse and independent participants. In our field such studies are rarely performed, although this is understandable due to the difficulty in obtaining cooperation from a large number of users and in the rapidly advancing technological nature of our research. More often than not our experiments limit themselves to a group of (frequently computer science) students [<CitationRef CitationID="CR81">81</CitationRef>] or use a computer simulation of user behavior [<CitationRef CitationID="CR163">163</CitationRef>]. Simulated users are easy to create, allow for the experiments to be performed quickly and give a rough indication of the performance of the retrieval system. However, these simulated users are, in general, too perfect in their relevance judgments and do not exhibit the inconsistencies (e.g. mistakenly labeling an image as relevant), individuality (e.g. two users have a different perception of the same image) and laziness (e.g. not wanting to label many images) of real users. By involving simulated users, we can very well end up with skewed results. In Table <InternalRef RefID="Tab4">4</InternalRef>, we show how the experiments are evaluated in current research. As can be seen, the majority of experiments is conducted with simulated users, with only a small number of experiments involving real users. Some works provide no evaluation, because they present a novel idea and only show a proof of concept.<Table Float="Yes" ID="Tab4"><Caption Language="En"><CaptionNumber>Table 4</CaptionNumber><CaptionContent><SimplePara>User-based evaluation of experiments</SimplePara></CaptionContent></Caption><tgroup cols="3"><colspec align="left" colname="c1" colnum="1"/><colspec align="left" colname="c2" colnum="2"/><colspec align="left" colname="c3" colnum="3"/><thead><row><entry align="left" colname="c1"><SimplePara>Real users</SimplePara></entry><entry align="left" colname="c2"><SimplePara>Simulated users</SimplePara></entry><entry align="left" colname="c3"><SimplePara>No evaluation</SimplePara></entry></row></thead><tbody><row><entry align="left" colname="c1"><SimplePara>13 %</SimplePara></entry><entry align="left" colname="c2"><SimplePara>84 %</SimplePara></entry><entry align="left" colname="c3"><SimplePara>3 %</SimplePara></entry></row></tbody></tgroup></Table>
                <Table Float="Yes" ID="Tab5"><Caption Language="En"><CaptionNumber>Table 5</CaptionNumber><CaptionContent><SimplePara>Most popular databases used in image retrieval using interactive search</SimplePara></CaptionContent></Caption><tgroup cols="5"><colspec align="left" colname="c1" colnum="1"/><colspec align="left" colname="c2" colnum="2"/><colspec align="left" colname="c3" colnum="3"/><colspec align="left" colname="c4" colnum="4"/><colspec align="left" colname="c5" colnum="5"/><thead><row><entry align="left" colname="c1"><SimplePara>Rank</SimplePara></entry><entry align="left" colname="c2"><SimplePara>Name</SimplePara></entry><entry align="left" colname="c3"><SimplePara>Institute</SimplePara></entry><entry align="left" colname="c4"><SimplePara>Type</SimplePara></entry><entry align="left" colname="c5"><SimplePara>No. of images</SimplePara></entry></row></thead><tbody><row><entry align="left" colname="c1"><SimplePara>1</SimplePara></entry><entry align="left" colname="c2"><SimplePara>Corel [<CitationRef CitationID="CR167">167</CitationRef>]</SimplePara></entry><entry align="left" colname="c3"><SimplePara>Corel</SimplePara></entry><entry align="left" colname="c4"><SimplePara>Stock photo</SimplePara></entry><entry align="left" colname="c5"><SimplePara>80K</SimplePara></entry></row><row><entry align="left" colname="c1"><SimplePara>2</SimplePara></entry><entry align="left" colname="c2"><SimplePara>MIRFLICKR [<CitationRef CitationID="CR168">168</CitationRef>, <CitationRef CitationID="CR169">169</CitationRef>]</SimplePara></entry><entry align="left" colname="c3"><SimplePara>Leiden University</SimplePara></entry><entry align="left" colname="c4"><SimplePara>General photo</SimplePara></entry><entry align="left" colname="c5"><SimplePara>1,000K</SimplePara></entry></row><row><entry align="left" colname="c1"><SimplePara>3</SimplePara></entry><entry align="left" colname="c2"><SimplePara>Brodatz [<CitationRef CitationID="CR170">170</CitationRef>]</SimplePara></entry><entry align="left" colname="c3"><SimplePara>Brodatz</SimplePara></entry><entry align="left" colname="c4"><SimplePara>Texture</SimplePara></entry><entry align="left" colname="c5"><SimplePara>1K</SimplePara></entry></row><row><entry align="left" colname="c1"><SimplePara>4</SimplePara></entry><entry align="left" colname="c2"><SimplePara>Ponce [<CitationRef CitationID="CR171">171</CitationRef>]</SimplePara></entry><entry align="left" colname="c3"><SimplePara>University of Illinois at Urbana-Champaign</SimplePara></entry><entry align="left" colname="c4"><SimplePara>Texture</SimplePara></entry><entry align="left" colname="c5"><SimplePara>1K</SimplePara></entry></row><row><entry align="left" colname="c1"><SimplePara>5</SimplePara></entry><entry align="left" colname="c2"><SimplePara>VisTex [<CitationRef CitationID="CR172">172</CitationRef>]</SimplePara></entry><entry align="left" colname="c3"><SimplePara>Massachusetts Institute of Technology</SimplePara></entry><entry align="left" colname="c4"><SimplePara>Texture</SimplePara></entry><entry align="left" colname="c5"><SimplePara><InlineEquation ID="IEq8"><InlineMediaObject><ImageObject Color="BlackWhite" FileRef="13735_2012_14_Article_IEq8.gif" Format="GIF" Rendition="HTML" Type="Linedraw"/></InlineMediaObject><EquationSource Format="TEX"><![CDATA[$${<}1$$]]></EquationSource></InlineEquation>K</SimplePara></entry></row><row><entry align="left" colname="c1"><SimplePara>6</SimplePara></entry><entry align="left" colname="c2"><SimplePara>Caltech-256 [<CitationRef CitationID="CR173">173</CitationRef>]</SimplePara></entry><entry align="left" colname="c3"><SimplePara>California Institute of Technology</SimplePara></entry><entry align="left" colname="c4"><SimplePara>Object</SimplePara></entry><entry align="left" colname="c5"><SimplePara>30K</SimplePara></entry></row><row><entry align="left" colname="c1"><SimplePara>7</SimplePara></entry><entry align="left" colname="c2"><SimplePara>PASCAL VOC [<CitationRef CitationID="CR174">174</CitationRef>]</SimplePara></entry><entry align="left" colname="c3"><SimplePara>VOC Challenge</SimplePara></entry><entry align="left" colname="c4"><SimplePara>Object</SimplePara></entry><entry align="left" colname="c5"><SimplePara>12K</SimplePara></entry></row><row><entry align="left" colname="c1"><SimplePara>8</SimplePara></entry><entry align="left" colname="c2"><SimplePara>ImageCLEF Medical [<CitationRef CitationID="CR175">175</CitationRef>]</SimplePara></entry><entry align="left" colname="c3"><SimplePara>ImageCLEF</SimplePara></entry><entry align="left" colname="c4"><SimplePara>Medical</SimplePara></entry><entry align="left" colname="c5"><SimplePara>231K</SimplePara></entry></row><row><entry align="left" colname="c1"><SimplePara>9</SimplePara></entry><entry align="left" colname="c2"><SimplePara>Columbia-COIL [<CitationRef CitationID="CR176">176</CitationRef>]</SimplePara></entry><entry align="left" colname="c3"><SimplePara>Columbia University</SimplePara></entry><entry align="left" colname="c4"><SimplePara>Object</SimplePara></entry><entry align="left" colname="c5"><SimplePara>2K</SimplePara></entry></row></tbody></tgroup></Table>
                <Table Float="Yes" ID="Tab6"><Caption Language="En"><CaptionNumber>Table 6</CaptionNumber><CaptionContent><SimplePara>Performance-based evaluation of experiments</SimplePara></CaptionContent></Caption><tgroup cols="6"><colspec align="left" colname="c1" colnum="1"/><colspec align="left" colname="c2" colnum="2"/><colspec align="left" colname="c3" colnum="3"/><colspec align="left" colname="c4" colnum="4"/><colspec align="left" colname="c5" colnum="5"/><colspec align="left" colname="c6" colnum="6"/><thead><row><entry align="left" colname="c1"><SimplePara>Precision-recall</SimplePara></entry><entry align="left" colname="c2"><SimplePara>Precision</SimplePara></entry><entry align="left" colname="c3"><SimplePara>Recall</SimplePara></entry><entry align="left" colname="c4"><SimplePara>Mean average precision</SimplePara></entry><entry align="left" colname="c5"><SimplePara>Retrieval time</SimplePara></entry><entry align="left" colname="c6"><SimplePara>Other</SimplePara></entry></row></thead><tbody><row><entry align="left" colname="c1"><SimplePara>15 %</SimplePara></entry><entry align="left" colname="c2"><SimplePara>44 %</SimplePara></entry><entry align="left" colname="c3"><SimplePara>24 %</SimplePara></entry><entry align="left" colname="c4"><SimplePara>4 %</SimplePara></entry><entry align="left" colname="c5"><SimplePara>7 %</SimplePara></entry><entry align="left" colname="c6"><SimplePara>6 %</SimplePara></entry></row></tbody></tgroup></Table>
              </Para>
              <Para>A brief look at current ways of evaluating interactive search systems is covered in [<CitationRef CitationID="CR164">164</CitationRef>] and an in-depth review can be found in [<CitationRef CitationID="CR165">165</CitationRef>], where guidelines are additionally suggested on how to raise the standard of evaluation. An evaluation benchmarking framework is proposed in [<CitationRef CitationID="CR166">166</CitationRef>], so relevance feedback algorithms can be fairly compared with each other.</Para>
              <Section2 ID="Sec16">
                <Heading>Image databases</Heading>
                <Para>There is a large variation in the image databases used by the research community that focuses on interactive search. Photographic imagery is the most popular kind of imagery. From our study, the Corel stock photography image set (e.g. [<CitationRef CitationID="CR167">167</CitationRef>]) has been used most frequently because it was the first large image set which could be considered representative for real world usage. However, it is also known to have significant and diverse problems [<CitationRef CitationID="CR167">167</CitationRef>] and that it is both illegal to distribute and is no longer sold. The copyright situation of the Corel image set motivated the research community to create large representative image sets which were both legal to redistribute and easily downloadable, such as the MIRFLICKR [<CitationRef CitationID="CR168">168</CitationRef>, <CitationRef CitationID="CR169">169</CitationRef>] sets that contain images collected from thousands of users from the photo sharing website Flickr. The list of most popular databases used in image retrieval from our literature search is shown in Table <InternalRef RefID="Tab5">5</InternalRef> from most frequently to least frequently used. Please note that many of the databases grow over time so the most current version will often be larger than the number listed.</Para>
              </Section2>
              <Section2 ID="Sec17">
                <Heading>Performance measures</Heading>
                <Para>Recently, several new performance measures have been proposed [<CitationRef CitationID="CR177">177</CitationRef>]. A notable measure is <Emphasis Type="Italic">generalized efficiency</Emphasis> [<CitationRef CitationID="CR165">165</CitationRef>], which normalizes the performance of a feedback method using the optimal classifier performance. This measure is particularly useful for benchmarking several methods with respect to a baseline method. Table <InternalRef RefID="Tab6">6</InternalRef> shows the popularity of current methods to evaluate retrieval performance. As can be seen <Emphasis Type="Italic">precision</Emphasis> is the most popular evaluation method, with <Emphasis Type="Italic">recall</Emphasis> second most popular and the combined <Emphasis Type="Italic">precision-recall</Emphasis> as third.</Para>
              </Section2>
              <Section2 ID="Sec18">
                <Heading>Trends and advances</Heading>
                <Para>Standardization has received significantly greater attention during the past years. We have witnessed several efforts to fulfill this need, ranging from benchmarking frameworks to standard image databases, such as the recent test sets that aim to provide researchers with a large number of images that are well-annotated and free of copyright. Considering that the volume of digital media in the world is rapidly expanding, having access to large image collections for training and testing new algorithms is important because it is not clear which algorithms scale well to millions. In the recent years, researchers have been moving away from the Corel image database and started creating open access databases for specific areas in image retrieval.</Para>
              </Section2>
            </Section1>
            <Section1 ID="Sec19">
              <Heading>Discussion and conclusions</Heading>
              <Para>Over the years, we have seen the performance of interactive search systems steadily improve. Nonetheless, much research remains to be done. In this section, we will discuss the most promising research directions and identify several open issues and challenges.</Para>
              <Section2 ID="Sec20">
                <Heading>Promising research directions</Heading>
                <Para>Below we outline top research directions that, based on our literature review, are on the frontier of interactive search.<UnorderedList Mark="Dash"><ItemContent><Para><Emphasis Type="Italic">Interaction in the question and answer paradigm</Emphasis> The Q&amp;A paradigm has the strength that it is probably the most natural and intuitive for the user. Recent Q&amp;A research has focused significantly more on multimodal (as opposed to monomodal) approaches for both posing the questions and displaying the answers. These systems can also dynamically select the best types of media for clarifying the answer to a specific question.</Para></ItemContent><ItemContent><Para><Emphasis Type="Italic">Interaction on the learned models</Emphasis> Beyond giving direct feedback on the results, preliminary work was started involving mid-level and high-level representations (see Sect. <InternalRef RefID="Sec8">3</InternalRef>). Multi-scale approaches using segmented image components are certainly novel and promising.</Para></ItemContent><ItemContent><Para><Emphasis Type="Italic">Interaction by explanation: providing reasons along with results</Emphasis> In the classic relevance feedback model, results are typically given but it is not clear to the user why the results were selected. In future interactive search systems, we expect to see systems which explain to the user why the results were chosen and allow the user to give feedback on the criteria used in the explanations, as opposed to only simply giving feedback on the image results.</Para></ItemContent><ItemContent><Para><Emphasis Type="Italic">Interaction with external or synthesized knowledge sources</Emphasis> In the prior work in this area, most of the systems limited themselves only to the imagery in the local collection. However, it has been found that utilizing additional image collections and knowledge sources can significantly improve the quality of results. Currently, using very large multimedia databases such as Wikipedia as external knowledge sources is an active and fertile direction.</Para></ItemContent><ItemContent><Para><Emphasis Type="Italic">Social interaction: recommendation systems and collaborative filtering</Emphasis> The small training set problem is of particular concern because humans do not want to label thousands of images. An interesting approach is to examine potential benefits from using algorithms from the area of collaborative filtering and recommendation systems. These systems have remarkably high performance in deciding which media items (often video) will be of interest to the user based on a social database of ranked items.</Para></ItemContent></UnorderedList>
                </Para>
              </Section2>
              <Section2 ID="Sec21">
                <Heading>Grand challenges</Heading>
                <Para>The past decade has brought many scientific advances in interactive image search theory and techniques. Moreover, there has been significant societal impact through the adoption of interactive image search in the largest WWW image search engines (Google, Bing, and Yahoo!), as well as in numerous systems in application areas such as medical image retrieval, professional stock photography databases, and cultural heritage preservation. Arguably, interactive search is the most important paradigm, because in a human sense it is the most effective method for us, while in a theoretical sense it allows the system to minimize the information required for answering a query by making careful choices about the questions to pose to the user. In conclusion, the grand challenges can be summarized as follows:<OrderedList><ListItem><ItemNumber>1.</ItemNumber><ItemContent><Para><Emphasis Type="Italic">What is the optimal user interface and information transfer for queries and results</Emphasis>? Our current systems usually seek to minimize the number of user labeled examples or the search time on the assumption that it will improve the user satisfaction or experience. A fundamentally different perspective is to focus on the user experience. This means that other aspects than accuracy may be considered important, such as the user’s satisfaction/enjoyment or the user’s feeling of understanding why the results were given. A longer search time might be preferable if the overall user experience is better. Recent developments in the industry have led to new interfaces that may be more intuitive. For example, touch-based technology has become intuitive and user-friendly through the popularity of smart phones and tablets. These developments open up new interaction possibilities between the search engine and the user. Novel interfaces can be potentially created that deliver a better search experience to such devices, while at the same time reaching a large number of users. Now that the Web 2.0, the social internet, is also becoming more and more prevalent, techniques that analyze the content produced by users all over the world show great promise to further the state of the art. The millions of photos that are commented on and tagged on a daily basis can provide invaluable knowledge to better understand the relations between images and their content.</Para></ItemContent></ListItem><ListItem><ItemNumber>2.</ItemNumber><ItemContent><Para><Emphasis Type="Italic">How can we achieve good accuracy with the least number of training examples?</Emphasis> The most commonly cited challenge in the research literature is the small training set problem, which means that, in general, the user does not want to manually label a large number of images. Developing new learning algorithms and/or integrating knowledge databases that can give good accuracy using only a small set of user-labeled images is perhaps the most important grand challenge of our field. Other promising techniques include manifold learning, multimodal fusion and utilizing implicit feedback. Novel learning algorithms are being regularly developed in the machine learning and the neuroscience fields. A particularly interesting direction comes from spiking networks and BCM theory [<CitationRef CitationID="CR178">178</CitationRef>], which conceivably is the most accurate model of learning in the visual cortex. Another recent novel direction is that of synthetic imagery.</Para></ItemContent></ListItem><ListItem><ItemNumber>3.</ItemNumber><ItemContent><Para><Emphasis Type="Italic">How should we evaluate and improve our interactive systems</Emphasis>? Evaluation projects in interactive search systems are in their infancy. There are several major issues to address in how to create or obtain high-quality ground truth for real image search contexts. One major issue is the way in which evaluation benchmarks are constructed. The current ones typically focus on the overall performance/accuracy of a search engine. However, it would be of significantly greater value if they could focus on benchmarks which give insight into each system’s weaknesses and strengths. Another issue is to determine what kinds of results are satisfactory to a user. For assessing the performance of a system, precision- and recall-based performance measures are the most popular choices at the moment. However, the research literature has shown that these measures are unable to provide a complete assessment of the system under study and argues that the notion of <Emphasis Type="Italic">generality</Emphasis>, i.e. the fraction of relevant items in the database, should be an important criterion when evaluating and comparing the performance of systems. A third issue is that currently researchers are largely guessing what kinds of imagery users are interested in, the kinds of queries and also the amount of effort (and other behavioral aspects) the user is willing to expend on a search. Currently, most researchers attempt to use simulated users to test their algorithms, while knowing that the simulated behavior may not mirror human user behavior. While simulations are very useful to get an initial impression on the performance of a new algorithm, they cannot replace actual user experiments since retrieval systems are specifically designed for users. One valuable direction for further study would thus be to properly model the behavior of simulated users after their real counterparts. It is noteworthy that the user behavior information largely exists in the logs of the WWW search engines. Thus, on the one hand, as a research community, we would like to have the user history from large search engines such as Yahoo! and Google. On the other hand, we realize that there are many legal concerns (e.g. user privacy) that prevent this information from being distributed. Finding a solution to this impasse could result in major improvements in interactive image search engines.</Para></ItemContent></ListItem></OrderedList>
                </Para>
              </Section2>
            </Section1>
          </Body>
          <BodyRef FileRef="BodyRef/PDF/13735_2012_Article_14.pdf" TargetType="OnlinePDF"/>
          <BodyRef FileRef="BodyRef/PDF/13735_2012_14_TEX.zip" TargetType="TEX"/>
          <ArticleBackmatter>
            <Acknowledgments>
              <Heading>Acknowledgments</Heading>
              <SimplePara>Leiden University and NWO BSIK/BRICKS supported this research under Grant #642.066.603.</SimplePara>
              <FormalPara RenderingStyle="Style1">
                <Heading>Open Access</Heading>
                <Para>This article is distributed under the terms of the Creative Commons Attribution License which permits any use, distribution, and reproduction in any medium, provided the original author(s) and the source are credited.</Para>
              </FormalPara>
            </Acknowledgments>
            <Bibliography ID="Bib1">
              <Heading>References</Heading>
              <Citation ID="CR1">
                <CitationNumber>1.</CitationNumber>
                <BibUnstructured>Andre P, Cutrell E, Tan D, Smith G (2009) Designing novel image search interfaces by understanding unique characteristics and usage. In: Proceedings of international conference on human–computer interaction, vol 2, pp 340–353</BibUnstructured>
              </Citation>
              <Citation ID="CR2">
                <CitationNumber>2.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>K</Initials>
                    <FamilyName>Ren</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>R</Initials>
                    <FamilyName>Sarvas</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Calic</FamilyName>
                  </BibAuthorName>
                  <Year>2010</Year>
                  <ArticleTitle Language="En">Interactive search and browsing interface for large-scale visual repositories</ArticleTitle>
                  <JournalTitle>Multimedia Tools Appl</JournalTitle>
                  <VolumeID>49</VolumeID>
                  <FirstPage>513</FirstPage>
                  <LastPage>528</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1007/s11042-009-0445-y</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Ren K, Sarvas R, Calic J (2010) Interactive search and browsing interface for large-scale visual repositories. Multimedia Tools Appl 49:513–528</BibUnstructured>
              </Citation>
              <Citation ID="CR3">
                <CitationNumber>3.</CitationNumber>
                <BibUnstructured>Zhou X, Zillner S, Moeller M, Sintek M, Zhan Y, Krishnan A, Gupta A (2008) Semantics and CBIR: a medical imaging perspective. In: Proceedings of ACM international conference on image and video retrieval, pp 571–580. doi:<ExternalRef><RefSource>10.1145/1386352.1386436</RefSource><RefTarget Address="10.1145/1386352.1386436" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR4">
                <CitationNumber>4.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>AWM</Initials>
                    <FamilyName>Smeulders</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Worring</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>S</Initials>
                    <FamilyName>Santini</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Gupta</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>R</Initials>
                    <FamilyName>Jain</FamilyName>
                  </BibAuthorName>
                  <Year>2000</Year>
                  <ArticleTitle Language="En">Content-based image retrieval at the end of the early years</ArticleTitle>
                  <JournalTitle>IEEE Trans Pattern Anal Machine Intell</JournalTitle>
                  <VolumeID>22</VolumeID>
                  <IssueID>12</IssueID>
                  <FirstPage>1349</FirstPage>
                  <LastPage>1380</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/34.895972</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Smeulders AWM, Worring M, Santini S, Gupta A, Jain R (2000) Content-based image retrieval at the end of the early years. IEEE Trans Pattern Anal Machine Intell 22(12):1349–1380</BibUnstructured>
              </Citation>
              <Citation ID="CR5">
                <CitationNumber>5.</CitationNumber>
                <BibUnstructured>Datta R, Li J, Wang JZ (2005) Content-based image retrieval: approaches and trends of the new age. In: Proceedings of ACM international workshop on multimedia, information retrieval, pp 253–262. doi:<ExternalRef><RefSource>10.1145/1101826.1101866</RefSource><RefTarget Address="10.1145/1101826.1101866" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR6">
                <CitationNumber>6.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>R</Initials>
                    <FamilyName>Datta</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>D</Initials>
                    <FamilyName>Joshi</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Li</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>JZ</Initials>
                    <FamilyName>Wang</FamilyName>
                  </BibAuthorName>
                  <Year>2008</Year>
                  <ArticleTitle Language="En">Image retrieval: ideas, influences, and trends of the new age</ArticleTitle>
                  <JournalTitle>ACM Comput Surv</JournalTitle>
                  <VolumeID>40</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage> 1</FirstPage>
                  <LastPage>60</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1145/1348246.1348248</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Datta R, Joshi D, Li J, Wang JZ (2008) Image retrieval: ideas, influences, and trends of the new age. ACM Comput Surv 40(2): 1–60</BibUnstructured>
              </Citation>
              <Citation ID="CR7">
                <CitationNumber>7.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>MS</Initials>
                    <FamilyName>Lew</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>N</Initials>
                    <FamilyName>Sebe</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>C</Initials>
                    <FamilyName>Djeraba</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>R</Initials>
                    <FamilyName>Jain</FamilyName>
                  </BibAuthorName>
                  <Year>2006</Year>
                  <ArticleTitle Language="En">Content-based multimedia information retrieval: state of the art and challenges</ArticleTitle>
                  <JournalTitle>ACM Trans Multimedia Comput Commun Appl</JournalTitle>
                  <VolumeID>2</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>1</FirstPage>
                  <LastPage>19</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1145/1126004.1126005</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Lew MS, Sebe N, Djeraba C, Jain R (2006) Content-based multimedia information retrieval: state of the art and challenges. ACM Trans Multimedia Comput Commun Appl 2(1):1–19</BibUnstructured>
              </Citation>
              <Citation ID="CR8">
                <CitationNumber>8.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>TS</Initials>
                    <FamilyName>Huang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>CK</Initials>
                    <FamilyName>Dagli</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>S</Initials>
                    <FamilyName>Rajaram</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>EY</Initials>
                    <FamilyName>Chang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>MI</Initials>
                    <FamilyName>Mandel</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>GE</Initials>
                    <FamilyName>Poliner</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>DPW</Initials>
                    <FamilyName>Ellis</FamilyName>
                  </BibAuthorName>
                  <Year>2008</Year>
                  <ArticleTitle Language="En">Active learning for interactive multimedia retrieval</ArticleTitle>
                  <JournalTitle>Proc IEEE</JournalTitle>
                  <VolumeID>96</VolumeID>
                  <IssueID>4</IssueID>
                  <FirstPage>648</FirstPage>
                  <LastPage>667</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/JPROC.2008.916364</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Huang TS, Dagli CK, Rajaram S, Chang EY, Mandel MI, Poliner GE, Ellis DPW (2008) Active learning for interactive multimedia retrieval. Proc IEEE 96(4):648–667</BibUnstructured>
              </Citation>
              <Citation ID="CR9">
                <CitationNumber>9.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>XS</Initials>
                    <FamilyName>Zhou</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>TS</Initials>
                    <FamilyName>Huang</FamilyName>
                  </BibAuthorName>
                  <Year>2003</Year>
                  <ArticleTitle Language="En">Relevance feedback in image retrieval: a comprehensive review</ArticleTitle>
                  <JournalTitle>ACM Multimedia Syst</JournalTitle>
                  <VolumeID>8</VolumeID>
                  <IssueID>6</IssueID>
                  <FirstPage>536</FirstPage>
                  <LastPage>544</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1007/s00530-002-0070-3</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Zhou XS, Huang TS (2003) Relevance feedback in image retrieval: a comprehensive review. ACM Multimedia Syst 8(6):536–544</BibUnstructured>
              </Citation>
              <Citation ID="CR10">
                <CitationNumber>10.</CitationNumber>
                <BibUnstructured>Kherfi ML, Brahmi D, Ziou D (2004) Combining visual features with semantics for a more effective image retrieval. In: Proceedings of IEEE international conference on pattern recognition, vol 2, pp 961–964. doi:<ExternalRef><RefSource>10.1109/ICPR.2004.1334418</RefSource><RefTarget Address="10.1109/ICPR.2004.1334418" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR11">
                <CitationNumber>11.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>G</Initials>
                    <FamilyName>Aggarwal</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>TV</Initials>
                    <FamilyName>Ashwin</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>S</Initials>
                    <FamilyName>Ghosal</FamilyName>
                  </BibAuthorName>
                  <Year>2002</Year>
                  <ArticleTitle Language="En">An image retrieval system with automatic query modification</ArticleTitle>
                  <JournalTitle>IEEE Trans. on Multimedia</JournalTitle>
                  <VolumeID>4</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>201</FirstPage>
                  <LastPage>214</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TMM.2002.1017734</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Aggarwal G, Ashwin TV, Ghosal S (2002) An image retrieval system with automatic query modification. IEEE Trans. on Multimedia 4(2):201–214</BibUnstructured>
              </Citation>
              <Citation ID="CR12">
                <CitationNumber>12.</CitationNumber>
                <BibUnstructured>Thomee B, Huiskes MJ, Bakker EM, Lew MS (2009) Deep exploration for experiential image retrieval. In: Proceedings of ACM International Conference on Multimedia, 673–676. doi:<ExternalRef><RefSource>10.1145/1631272.1631385</RefSource><RefTarget Address="10.1145/1631272.1631385" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR13">
                <CitationNumber>13.</CitationNumber>
                <BibUnstructured>Kutics A, Nakagawa A, Tanaka K, Yamada M, Sanbe Y, Ohtsuka S (2003) Linking images and keywords for semantics-based image retrieval. In: Proceedings of IEEE international conference on multimedia and expo, vol 1, pp 777–780. doi:<ExternalRef><RefSource>10.1109/ICME.2003.1221033</RefSource><RefTarget Address="10.1109/ICME.2003.1221033" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR14">
                <CitationNumber>14.</CitationNumber>
                <BibUnstructured>Chiang C-C, Hsieh M-H, Hung Y-P, Lee GC (2005) Region filtering using color and texture features for image retrieval. In: Proceedings of ACM conference on image and video retrieval, pp 487–496. doi:<ExternalRef><RefSource>10.1007/11526346_52</RefSource><RefTarget Address="10.1007/11526346_52" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR15">
                <CitationNumber>15.</CitationNumber>
                <BibUnstructured>Amores J, Sebe N, Redeva P, Gevers T, Smeulders A (2004) Boosting contextual information in content-based image retrieval. In: Proceedings of ACM international workshop on multimedia, information retrieval, pp 31–38. doi:<ExternalRef><RefSource>10.1145/1026711.1026717</RefSource><RefTarget Address="10.1145/1026711.1026717" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR16">
                <CitationNumber>16.</CitationNumber>
                <BibUnstructured>Ko BC, Byun H (2002) Integrated region-based image retrieval using region’s spatial relationships. In: Proceedings of IEEE international conference on pattern recognition, vol 1, pp 196–199</BibUnstructured>
              </Citation>
              <Citation ID="CR17">
                <CitationNumber>17.</CitationNumber>
                <BibUnstructured>Torres JM, Hutchison D, Reis LP (2007) Semantic image retrieval using region-based relevance feedback. In: Proceedings of international workshop on adaptive multimedia retrieval: user, context, and, feedback, pp 192–206. doi:<ExternalRef><RefSource>10.1007/978-3-540-71545-0_15</RefSource><RefTarget Address="10.1007/978-3-540-71545-0_15" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR18">
                <CitationNumber>18.</CitationNumber>
                <BibUnstructured>Huiskes MJ (2006) Image searching and browsing by active aspect-based relevance learning. In: Proceedings of international conference on image and video retrieval, pp 211–220. doi:<ExternalRef><RefSource>10.1007/11788034_22</RefSource><RefTarget Address="10.1007/11788034_22" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR19">
                <CitationNumber>19.</CitationNumber>
                <BibUnstructured>Jin X, French JC (2003) Improving image retrieval effectiveness via multiple queries. In: Proceedings of ACM international workshop on multimedia databases, pp 86–94. doi:<ExternalRef><RefSource>10.1145/951676.951692</RefSource><RefTarget Address="10.1145/951676.951692" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR20">
                <CitationNumber>20.</CitationNumber>
                <BibUnstructured>Zhang C, Chen X (2005) Region-based image clustering and retrieval using multiple instance learning. In: Proceedings of international conference on image and video retrieval, pp 194–204. doi:<ExternalRef><RefSource>10.1007/11526346_23</RefSource><RefTarget Address="10.1007/11526346_23" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR21">
                <CitationNumber>21.</CitationNumber>
                <BibUnstructured>Yang J, Li Q, Zhuang Y (2002) Image retrieval and relevance feedback using peer indexing. In: Proceedings of IEEE international conference on multimedia and expo, vol 2, pp 409–412</BibUnstructured>
              </Citation>
              <Citation ID="CR22">
                <CitationNumber>22.</CitationNumber>
                <BibUnstructured>Ko BC, Byun H (2002) Probabilistic neural networks supporting multi-class relevance feedback in region-based image retrieval. In: Proceedings of IEEE international conference on pattern recognition, vol 4, pp 138–141</BibUnstructured>
              </Citation>
              <Citation ID="CR23">
                <CitationNumber>23.</CitationNumber>
                <BibUnstructured>Wu H, Lu H, Ma S (2004) WillHunter: interactive image retrieval with multilevel relevance measurement. In: Proceedings of IEEE international conference on pattern recognition, vol 2, pp 1009–1012</BibUnstructured>
              </Citation>
              <Citation ID="CR24">
                <CitationNumber>24.</CitationNumber>
                <BibUnstructured>Haas M, Oerlemans A, Lew MS (2005) Relevance feedback methods in content based retrieval and video summarization. In: Proceedings of IEEE international conference on multimedia and expo, pp 1038–1041</BibUnstructured>
              </Citation>
              <Citation ID="CR25">
                <CitationNumber>25.</CitationNumber>
                <BibUnstructured>Huang X, Chen S-C, Shyu M-L (2003) Incorporating real-valued multiple instance learning into relevance feedback for image retrieval. In: Proceedings of IEEE international conference on multimedia and expo, vol 2, pp 321–324</BibUnstructured>
              </Citation>
              <Citation ID="CR26">
                <CitationNumber>26.</CitationNumber>
                <BibUnstructured>Li G, Ming Z, Li H, Chua T (2009) Video reference: question answering on YouTube. In: Proceedings of ACM international conference on multimedia, pp 773–776. doi:<ExternalRef><RefSource>10.1145/1631272.1631411</RefSource><RefTarget Address="10.1145/1631272.1631411" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR27">
                <CitationNumber>27.</CitationNumber>
                <BibUnstructured>von Ahn L, Dabbish L (2004) Labeling images with a computer game. In: Proceedings of ACM conference on human factors in computing systems, pp 319–326. doi:<ExternalRef><RefSource>10.1145/985692.985733</RefSource><RefTarget Address="10.1145/985692.985733" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR28">
                <CitationNumber>28.</CitationNumber>
                <BibUnstructured>Andrenucci A, Sneiders E (2005) Automated question answering: review of the main approaches. In: Proceedings of IEEE international conference on information technology and applications, vol 1, pp 514–519. doi:<ExternalRef><RefSource>10.1109/ICITA.2005.78</RefSource><RefTarget Address="10.1109/ICITA.2005.78" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR29">
                <CitationNumber>29.</CitationNumber>
                <BibUnstructured>Sahbi H, Etyngier P, Audibert J, Keriven R (2008) Manifold learning using robust graph Laplacian for interactive image search. In: Proceedings of IEEE conference on computer vision and pattern recognition, pp 1–8. doi:<ExternalRef><RefSource>10.1109/CVPR.2008.4587625</RefSource><RefTarget Address="10.1109/CVPR.2008.4587625" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR30">
                <CitationNumber>30.</CitationNumber>
                <BibUnstructured>Xu H, Wang J, Hua X (2010) Interactive image search by 2D semantic map. In: Proceedings of ACM international conference on World Wide Web, pp 1321–1324. doi:<ExternalRef><RefSource>10.1145/1772690.1772912</RefSource><RefTarget Address="10.1145/1772690.1772912" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR31">
                <CitationNumber>31.</CitationNumber>
                <BibUnstructured>Meng J, Yuan J, Jiang Y, Narashimhan N, Vasudevan V, Wu Y (2010) Interactive visual object search through mutual information maximization, In: Proceedings of ACM international conference on multimedia, pp 1147–1150</BibUnstructured>
              </Citation>
              <Citation ID="CR32">
                <CitationNumber>32.</CitationNumber>
                <BibUnstructured>Wang C, Li Z, Zhang L (2010) MindFinder: image search by interactive sketching and tagging. Proc. ACM International Conference on, World Wide Web, pp 1309–1312. doi:<ExternalRef><RefSource>10.1145/1772690.1772909</RefSource><RefTarget Address="10.1145/1772690.1772909" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR33">
                <CitationNumber>33.</CitationNumber>
                <BibUnstructured>Cao Y, Wang C, Zhang L (2011) Edgel index for large-scale sketch-based image search. In: Proceedings of IEEE conference on computer vision and pattern recognition, pp 761–768. doi:<ExternalRef><RefSource>10.1109/CVPR.2011.5995460</RefSource><RefTarget Address="10.1109/CVPR.2011.5995460" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR34">
                <CitationNumber>34.</CitationNumber>
                <BibUnstructured>Nie L, Wang M, Zha Z, Li G, Chua T (2011) Multimedia answering: enriching text QA with media information. In: Proceedings of ACM conference on research and development in information retrieval, pp 695–704. doi:<ExternalRef><RefSource>10.1145/2009916.2010010</RefSource><RefTarget Address="10.1145/2009916.2010010" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR35">
                <CitationNumber>35.</CitationNumber>
                <BibUnstructured>Yeh T, Lee J, Darrell T (2008) Photo-based question answering. In: Proceedings of ACM international conference on multimedia, pp 389–398. doi:<ExternalRef><RefSource>10.1145/1459359.1459412</RefSource><RefTarget Address="10.1145/1459359.1459412" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR36">
                <CitationNumber>36.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>GP</Initials>
                    <FamilyName>Nguyen</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Worring</FamilyName>
                  </BibAuthorName>
                  <Year>2005</Year>
                  <ArticleTitle Language="En">Relevance feedback based saliency adaptation in CBIR</ArticleTitle>
                  <JournalTitle>ACM Multimedia Syst</JournalTitle>
                  <VolumeID>10</VolumeID>
                  <IssueID>6</IssueID>
                  <FirstPage>499</FirstPage>
                  <LastPage>512</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1007/s00530-005-0178-3</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Nguyen GP, Worring M (2005) Relevance feedback based saliency adaptation in CBIR. ACM Multimedia Syst 10(6):499–512</BibUnstructured>
              </Citation>
              <Citation ID="CR37">
                <CitationNumber>37.</CitationNumber>
                <BibUnstructured>Tran DA, Pamidimukkala SR, Nguyen P (2008) Relevance-feedback image retrieval based on multiple-instance learning. In: Proceedings of IEEE international conference on computer and information science, pp 597–602. doi:<ExternalRef><RefSource>10.1109/ICIS.2008.83</RefSource><RefTarget Address="10.1109/ICIS.2008.83" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR38">
                <CitationNumber>38.</CitationNumber>
                <BibUnstructured>Kherfi ML, Ziou D, Bernardi A (2002) Learning from negative example in relevance feedback for content-based image retrieval. In: Proceedings of IEEE international conference on pattern recognition, vol 2, pp 933–936. doi:<ExternalRef><RefSource>10.1109/ICPR.2002.1048458</RefSource><RefTarget Address="10.1109/ICPR.2002.1048458" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR39">
                <CitationNumber>39.</CitationNumber>
                <BibUnstructured>Liu J, Li Z, Li M, Lu H, Ma S (2007) Human behaviour consistent relevance feedback model for image retrieval. In: Proceedings of ACM international conference on multimedia, pp 269–272. doi:<ExternalRef><RefSource>10.1145/1291233.1291289</RefSource><RefTarget Address="10.1145/1291233.1291289" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR40">
                <CitationNumber>40.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>E</Initials>
                    <FamilyName>Cheng</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>F</Initials>
                    <FamilyName>Jing</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>L</Initials>
                    <FamilyName>Zhang</FamilyName>
                  </BibAuthorName>
                  <Year>2009</Year>
                  <ArticleTitle Language="En">A unified relevance feedback framework for web image retrieval</ArticleTitle>
                  <JournalTitle>IEEE Trans Image Process</JournalTitle>
                  <VolumeID>18</VolumeID>
                  <IssueID>6</IssueID>
                  <FirstPage>1350</FirstPage>
                  <LastPage>1357</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>2742163</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TIP.2009.2017128</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Cheng E, Jing F, Zhang L (2009) A unified relevance feedback framework for web image retrieval. IEEE Trans Image Process 18(6):1350–1357</BibUnstructured>
              </Citation>
              <Citation ID="CR41">
                <CitationNumber>41.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>I</Initials>
                    <FamilyName>Campbell</FamilyName>
                  </BibAuthorName>
                  <Year>2000</Year>
                  <ArticleTitle Language="En">Interactive evaluation of the ostensive model using a new test collection of images with multiple relevance assessments</ArticleTitle>
                  <JournalTitle>J Inf Retrieval</JournalTitle>
                  <VolumeID>2</VolumeID>
                  <FirstPage>87</FirstPage>
                  <LastPage>114</LastPage>
                </BibArticle>
                <BibUnstructured>Campbell I (2000) Interactive evaluation of the ostensive model using a new test collection of images with multiple relevance assessments. J Inf Retrieval 2:87–114</BibUnstructured>
              </Citation>
              <Citation ID="CR42">
                <CitationNumber>42.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Urban</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>JM</Initials>
                    <FamilyName>Jose</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>CJ</Initials>
                    <FamilyName>Rijsbergen</FamilyName>
                  </BibAuthorName>
                  <Year>2006</Year>
                  <ArticleTitle Language="En">An adaptive technique for content-based image retrieval</ArticleTitle>
                  <JournalTitle>Multimedia Tools Appl</JournalTitle>
                  <VolumeID>31</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>1</FirstPage>
                  <LastPage> 28</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1007/s11042-006-0035-1</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Urban J, Jose JM, Rijsbergen CJ (2006) An adaptive technique for content-based image retrieval. Multimedia Tools Appl 31(1):1– 28</BibUnstructured>
              </Citation>
              <Citation ID="CR43">
                <CitationNumber>43.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Fan</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>Y</Initials>
                    <FamilyName>Gao</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>H</Initials>
                    <FamilyName>Luo</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>R</Initials>
                    <FamilyName>Jain</FamilyName>
                  </BibAuthorName>
                  <Year>2008</Year>
                  <ArticleTitle Language="En">Mining multilevel image semantics via hierarchical classification</ArticleTitle>
                  <JournalTitle>IEEE Trans Multimedia</JournalTitle>
                  <VolumeID>10</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>167</FirstPage>
                  <LastPage>181</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TMM.2007.911775</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Fan J, Gao Y, Luo H, Jain R (2008) Mining multilevel image semantics via hierarchical classification. IEEE Trans Multimedia 10(2):167–181</BibUnstructured>
              </Citation>
              <Citation ID="CR44">
                <CitationNumber>44.</CitationNumber>
                <BibUnstructured>Thomee B, Huiskes MJ, Bakker EM, Lew MS (2009) An exploration-based interface for interactive image retrieval. In: Proceedings of IEEE international symposium on image and signal processing, pp 192–197. doi:<ExternalRef><RefSource>10.1109/ISPA.2009.5297746</RefSource><RefTarget Address="10.1109/ISPA.2009.5297746" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR45">
                <CitationNumber>45.</CitationNumber>
                <BibUnstructured>Mavandadi S, Aarabi P, Khaleghi A, Appel R (2006) Predictive dynamic user interfaces for interactive visual search. In: Proceedings of IEEE international conference on multimedia and expo, pp 381–384. doi:<ExternalRef><RefSource>10.1109/ICME.2006.262516</RefSource><RefTarget Address="10.1109/ICME.2006.262516" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR46">
                <CitationNumber>46.</CitationNumber>
                <BibUnstructured>Zavesky E, Chang S-F, Yang C-C (2008) Visual islands: intuitive browsing of visual search results. In: Proceedings of ACM international conference on image and video retrieval, pp 617– 626. doi:<ExternalRef><RefSource>10.1145/1386352.1386442</RefSource><RefTarget Address="10.1145/1386352.1386442" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR47">
                <CitationNumber>47.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>GP</Initials>
                    <FamilyName>Nguyen</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Worring</FamilyName>
                  </BibAuthorName>
                  <Year>2008</Year>
                  <ArticleTitle Language="En">Optimization of interactive visual-similarity-based search</ArticleTitle>
                  <JournalTitle>ACM Trans Multimedia Comput Commun Appl</JournalTitle>
                  <VolumeID>4</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>499</FirstPage>
                  <LastPage>512</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1145/1324287.1324294</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Nguyen GP, Worring M (2008) Optimization of interactive visual-similarity-based search. ACM Trans Multimedia Comput Commun Appl 4(1):499–512</BibUnstructured>
              </Citation>
              <Citation ID="CR48">
                <CitationNumber>48.</CitationNumber>
                <BibUnstructured>Wang X, McKenna SJ, Han J (2009) High-entropy layouts for content-based browsing and retrieval. In: Proceedings of ACM international conference on image and video retrieval, article 16. doi:<ExternalRef><RefSource>10.1145/1646396.1646418</RefSource><RefTarget Address="10.1145/1646396.1646418" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR49">
                <CitationNumber>49.</CitationNumber>
                <BibUnstructured>Nakazato M, Huang TS (2002) Extending image retrieval with group-oriented interface. In: Proceedings of IEEE international conference on multimedia and expo, vol 1, pp 201–204. doi:<ExternalRef><RefSource>10.1109/ICME.2002.1035753</RefSource><RefTarget Address="10.1109/ICME.2002.1035753" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR50">
                <CitationNumber>50.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Urban</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>JM</Initials>
                    <FamilyName>Jose</FamilyName>
                  </BibAuthorName>
                  <Year>2007</Year>
                  <ArticleTitle Language="En">Evaluating a workspace’s usefulness for image retrieval</ArticleTitle>
                  <JournalTitle>ACM Multimedia Syst</JournalTitle>
                  <VolumeID>12</VolumeID>
                  <IssueID>4–5</IssueID>
                  <FirstPage>355</FirstPage>
                  <LastPage>373</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1007/s00530-006-0051-z</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Urban J, Jose JM (2007) Evaluating a workspace’s usefulness for image retrieval. ACM Multimedia Syst 12(4–5):355–373</BibUnstructured>
              </Citation>
              <Citation ID="CR51">
                <CitationNumber>51.</CitationNumber>
                <BibUnstructured>Guan J, Qiu G (2007) Learning user intention in relevance feedback using optimization. In: Proceedings of ACM international workshop on multimedia, information retrieval, pp 41–50. doi:<ExternalRef><RefSource>10.1145/1290082.1290091</RefSource><RefTarget Address="10.1145/1290082.1290091" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR52">
                <CitationNumber>52.</CitationNumber>
                <BibUnstructured>Shyu M, Chen S-C, Chen M, Zhang C, Sarinnapakorn K (2003) Image database retrieval utilizing affinity relationships. In: Proceedings of ACM international workshop on multimedia databases, pp 78–85. doi:<ExternalRef><RefSource>10.1145/951676.951691</RefSource><RefTarget Address="10.1145/951676.951691" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR53">
                <CitationNumber>53.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>Y</Initials>
                    <FamilyName>Sun</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>S</Initials>
                    <FamilyName>Ozawa</FamilyName>
                  </BibAuthorName>
                  <Year>2005</Year>
                  <ArticleTitle Language="En">HIRBIR: a hierarchical approach to region-based image retrieval</ArticleTitle>
                  <JournalTitle>ACM Multimedia Syst</JournalTitle>
                  <VolumeID>10</VolumeID>
                  <IssueID>6</IssueID>
                  <FirstPage> 559</FirstPage>
                  <LastPage>569</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1007/s00530-005-0182-7</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Sun Y, Ozawa S (2005) HIRBIR: a hierarchical approach to region-based image retrieval. ACM Multimedia Syst 10(6): 559–569</BibUnstructured>
              </Citation>
              <Citation ID="CR54">
                <CitationNumber>54.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>Y</Initials>
                    <FamilyName>Chen</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>JZ</Initials>
                    <FamilyName>Wang</FamilyName>
                  </BibAuthorName>
                  <Year>2002</Year>
                  <ArticleTitle Language="En">A region-based fuzzy feature matching approach to content-based image retrieval</ArticleTitle>
                  <JournalTitle>IEEE Trans Pattern Anal Mach Intell</JournalTitle>
                  <VolumeID>24</VolumeID>
                  <IssueID>9</IssueID>
                  <FirstPage>1252</FirstPage>
                  <LastPage>1267</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TPAMI.2002.1033216</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Chen Y, Wang JZ (2002) A region-based fuzzy feature matching approach to content-based image retrieval. IEEE Trans Pattern Anal Mach Intell 24(9):1252–1267</BibUnstructured>
              </Citation>
              <Citation ID="CR55">
                <CitationNumber>55.</CitationNumber>
                <BibUnstructured>Ko BC, Kwak SY, Byun H (2004) SVM-based salient region(s) extraction method for image retrieval. In: Proceedings of IEEE international conference on pattern recognition, vol 2, pp 977–980</BibUnstructured>
              </Citation>
              <Citation ID="CR56">
                <CitationNumber>56.</CitationNumber>
                <BibUnstructured>Luo J, Nascimento MA (2004) Content-based sub-image retrieval using relevance feedback. In: Proceedings of ACM international workshop on multimedia databases, pp 2–9. doi:<ExternalRef><RefSource>10.1145/1032604.1032607</RefSource><RefTarget Address="10.1145/1032604.1032607" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR57">
                <CitationNumber>57.</CitationNumber>
                <BibUnstructured>Zhang R, Zhang Z (2004) Hidden semantic concept discovery in region based image retrieval. Proc. IEEE Conference on Computer Vision and Pattern Recognition 2:996–1001</BibUnstructured>
              </Citation>
              <Citation ID="CR58">
                <CitationNumber>58.</CitationNumber>
                <BibUnstructured>Chen X, Zhang C, Chen S-C, Chen M (2005) A latent semantic indexing based method for solving multiple instance learning problem in region-based image retrieval. In: Proceedings of IEEE international symposium on multimedia, pp 37–45</BibUnstructured>
              </Citation>
              <Citation ID="CR59">
                <CitationNumber>59.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>R</Initials>
                    <FamilyName>Rahmani</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>SA</Initials>
                    <FamilyName>Goldman</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>H</Initials>
                    <FamilyName>Zhang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>SR</Initials>
                    <FamilyName>Cholleti</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>JE</Initials>
                    <FamilyName>Fritts</FamilyName>
                  </BibAuthorName>
                  <Year>2008</Year>
                  <ArticleTitle Language="En">Localized content-based image retrieval</ArticleTitle>
                  <JournalTitle>IEEE Trans Pattern Anal Mach Intell</JournalTitle>
                  <VolumeID>30</VolumeID>
                  <IssueID>11</IssueID>
                  <FirstPage>1902</FirstPage>
                  <LastPage>1912</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TPAMI.2008.112</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Rahmani R, Goldman SA, Zhang H, Cholleti SR, Fritts JE (2008) Localized content-based image retrieval. IEEE Trans Pattern Anal Mach Intell 30(11):1902–1912</BibUnstructured>
              </Citation>
              <Citation ID="CR60">
                <CitationNumber>60.</CitationNumber>
                <BibUnstructured>Fu Z, Robles-Kelly A (2009) An instance selection approach to multiple instance learning. In: Proceedings of IEEE conference on computer vision and pattern recognition, pp 911–918</BibUnstructured>
              </Citation>
              <Citation ID="CR61">
                <CitationNumber>61.</CitationNumber>
                <BibUnstructured>Sivic J, Russell BC, Efros AA, Zisserman A, Freeman WT (2005) Discovering objects and their location in images. In: Proceedings of IEEE international conference on computer vision, vol 1, pp 370–377. doi:<ExternalRef><RefSource>10.1109/ICCV.2005.77</RefSource><RefTarget Address="10.1109/ICCV.2005.77" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR62">
                <CitationNumber>62.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>Y</Initials>
                    <FamilyName>Chen</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Bi</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>JZ</Initials>
                    <FamilyName>Wang</FamilyName>
                  </BibAuthorName>
                  <Year>2006</Year>
                  <ArticleTitle Language="En">Miles: multiple-instance learning via embedded instance selection</ArticleTitle>
                  <JournalTitle>IEEE Trans Pattern Anal Mach Intell</JournalTitle>
                  <VolumeID>28</VolumeID>
                  <IssueID>12</IssueID>
                  <FirstPage>1</FirstPage>
                  <LastPage>17</LastPage>
                  <Occurrence Type="ZLBID">
                    <Handle>1100.94023</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TPAMI.2006.239</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Chen Y, Bi J, Wang JZ (2006) Miles: multiple-instance learning via embedded instance selection. IEEE Trans Pattern Anal Mach Intell 28(12):1–17</BibUnstructured>
              </Citation>
              <Citation ID="CR63">
                <CitationNumber>63.</CitationNumber>
                <BibUnstructured>Chatzis S, Doulamis A, Varvarigou T (2007) A content-based image retrieval scheme allowing for robust automatic personalization. In: Proceedings of ACM international conference on image and video retrieval, pp 1–8. doi:<ExternalRef><RefSource>10.1145/1282280.1282281</RefSource><RefTarget Address="10.1145/1282280.1282281" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR64">
                <CitationNumber>64.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>J-H</Initials>
                    <FamilyName>Lim</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>JS</Initials>
                    <FamilyName>Jin</FamilyName>
                  </BibAuthorName>
                  <Year>2005</Year>
                  <ArticleTitle Language="En">A structured learning framework for content-based image indexing and visual query</ArticleTitle>
                  <JournalTitle>ACM Multimedia Syst</JournalTitle>
                  <VolumeID>10</VolumeID>
                  <IssueID>4</IssueID>
                  <FirstPage>317</FirstPage>
                  <LastPage>331</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1007/s00530-004-0158-z</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Lim J-H, Jin JS (2005) A structured learning framework for content-based image indexing and visual query. ACM Multimedia Syst 10(4):317–331</BibUnstructured>
              </Citation>
              <Citation ID="CR65">
                <CitationNumber>65.</CitationNumber>
                <BibUnstructured>Dong A, Bhanu B (2003) A new semi-supervised EM algorithm for image retrieval. In: Proceedings of IEEE conference on computer vision and pattern recognition, vol 2, pp 662–667</BibUnstructured>
              </Citation>
              <Citation ID="CR66">
                <CitationNumber>66.</CitationNumber>
                <BibUnstructured>Dong A, Bhanu B (2003) Active concept learning for image retrieval in dynamic databases. In: Proceedings of IEEE international conference on computer vision, pp 90–95. doi:<ExternalRef><RefSource>10.1109/ICCV.2003.1238318</RefSource><RefTarget Address="10.1109/ICCV.2003.1238318" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR67">
                <CitationNumber>67.</CitationNumber>
                <BibUnstructured>Fung CC, Chung K-P (2007) Establishing semantic relationship in inter-query learning for content-based image retrieval systems. In: Proceedings of Pacific-Asia conference on knowledge discovery and data mining, pp 498–506. doi:<ExternalRef><RefSource>10.1007/978-3-540-71701-0_51</RefSource><RefTarget Address="10.1007/978-3-540-71701-0_51" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR68">
                <CitationNumber>68.</CitationNumber>
                <BibUnstructured>Fellbaum C (1998) WordNet: an electronic lexical database. MIT Press, Cambridge</BibUnstructured>
              </Citation>
              <Citation ID="CR69">
                <CitationNumber>69.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>Y</Initials>
                    <FamilyName>Lu</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>H-J</Initials>
                    <FamilyName>Zhang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>L</Initials>
                    <FamilyName>Wenyin</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>C</Initials>
                    <FamilyName>Hu</FamilyName>
                  </BibAuthorName>
                  <Year>2003</Year>
                  <ArticleTitle Language="En">Joint semantics and feature based image retrieval using relevance feedback</ArticleTitle>
                  <JournalTitle>IEEE Trans Multimedia</JournalTitle>
                  <VolumeID>5</VolumeID>
                  <IssueID>3</IssueID>
                  <FirstPage>339</FirstPage>
                  <LastPage>347</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TMM.2003.813280</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Lu Y, Zhang H-J, Wenyin L, Hu C (2003) Joint semantics and feature based image retrieval using relevance feedback. IEEE Trans Multimedia 5(3):339–347</BibUnstructured>
              </Citation>
              <Citation ID="CR70">
                <CitationNumber>70.</CitationNumber>
                <BibUnstructured>Yang C, Dong M, Fotouhi F (2005) Semantic feedback for interactive image retrieval. In: Proceedings of ACM international conference on multimedia, pp 415–418. doi:<ExternalRef><RefSource>10.1145/1101149.1101240</RefSource><RefTarget Address="10.1145/1101149.1101240" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR71">
                <CitationNumber>71.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Ferecatu</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>N</Initials>
                    <FamilyName>Boujemaa</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Crucianu</FamilyName>
                  </BibAuthorName>
                  <Year>2008</Year>
                  <ArticleTitle Language="En">Semantic interactive image retrieval combining visual and conceptual content description</ArticleTitle>
                  <JournalTitle>ACM Multimedia Syst</JournalTitle>
                  <VolumeID>13</VolumeID>
                  <IssueID>5–6</IssueID>
                  <FirstPage>309</FirstPage>
                  <LastPage>322</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1007/s00530-007-0094-9</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Ferecatu M, Boujemaa N, Crucianu M (2008) Semantic interactive image retrieval combining visual and conceptual content description. ACM Multimedia Syst 13(5–6):309–322</BibUnstructured>
              </Citation>
              <Citation ID="CR72">
                <CitationNumber>72.</CitationNumber>
                <BibUnstructured>Liu X, Cheng B, Yan S, Tang J, Chua TS, Jin H (2009) Label to region by bi-layer sparsity priors. In: Proceedings of ACM international conference on multimedia, pp 115–124. doi:<ExternalRef><RefSource>10.1145/1631272.1631291</RefSource><RefTarget Address="10.1145/1631272.1631291" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR73">
                <CitationNumber>73.</CitationNumber>
                <BibUnstructured>Lu Z, Ip HHS, He Q (2009) Context-based multi-label image annotation. In: Proceedings of ACM international conference on image and video retrieval, article 30. doi:<ExternalRef><RefSource>10.1145/1646396.1646434</RefSource><RefTarget Address="10.1145/1646396.1646434" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR74">
                <CitationNumber>74.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>H-J</Initials>
                    <FamilyName>Zhang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>Z</Initials>
                    <FamilyName>Chen</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Li</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>Z</Initials>
                    <FamilyName>Su</FamilyName>
                  </BibAuthorName>
                  <Year>2003</Year>
                  <ArticleTitle Language="En">Relevance feedback and learning in content-based image search</ArticleTitle>
                  <JournalTitle>J World Wide Web</JournalTitle>
                  <VolumeID>6</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>131</FirstPage>
                  <LastPage>155</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1023/A:1023618504691</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Zhang H-J, Chen Z, Li M, Su Z (2003) Relevance feedback and learning in content-based image search. J World Wide Web 6(2):131–155</BibUnstructured>
              </Citation>
              <Citation ID="CR75">
                <CitationNumber>75.</CitationNumber>
                <BibUnstructured>Urban J, Jose JM (2006) Adaptive image retrieval using a graph model for semantic feature integration. In: Proceedings of ACM international workshop on multimedia, information retrieval, pp 117–126</BibUnstructured>
              </Citation>
              <Citation ID="CR76">
                <CitationNumber>76.</CitationNumber>
                <BibUnstructured>Wang X-J, Ma W-Y, Zhang L, Li X (2005) Multi-graph enabled active learning for multimodal web image retrieval. In: Proceedings of ACM international workshop on multimedia, information retrieval, pp 65–72. doi:<ExternalRef><RefSource>10.1145/1101826.1101839</RefSource><RefTarget Address="10.1145/1101826.1101839" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR77">
                <CitationNumber>77.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>C</Initials>
                    <FamilyName>Böhm</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>S</Initials>
                    <FamilyName>Berchtold</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>DA</Initials>
                    <FamilyName>Keim</FamilyName>
                  </BibAuthorName>
                  <Year>2001</Year>
                  <ArticleTitle Language="En">Searching in high-dimensional spaces: index structures for improving the performance of multimedia databases</ArticleTitle>
                  <JournalTitle>ACM Comput Surv</JournalTitle>
                  <VolumeID>33</VolumeID>
                  <IssueID>3</IssueID>
                  <FirstPage> 322</FirstPage>
                  <LastPage>373</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1145/502807.502809</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Böhm C, Berchtold S, Keim DA (2001) Searching in high-dimensional spaces: index structures for improving the performance of multimedia databases. ACM Comput Surv 33(3): 322–373</BibUnstructured>
              </Citation>
              <Citation ID="CR78">
                <CitationNumber>78.</CitationNumber>
                <BibUnstructured>Goh K-S, Li B, Chang EY (2002) DynDex: a dynamic and non-metric space indexer. In: Proceedings of ACM international conference on multimedia, pp 466–475. doi:<ExternalRef><RefSource>10.1145/641007.641107</RefSource><RefTarget Address="10.1145/641007.641107" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR79">
                <CitationNumber>79.</CitationNumber>
                <BibUnstructured>Zhou X, Zhang Q, Lin L, Deng A, Wu G (2003) Image retrieval by fuzzy clustering of relevance feedback records. In: Proceedings of IEEE international conference on multimedia and expo, vol 2, pp 305–308</BibUnstructured>
              </Citation>
              <Citation ID="CR80">
                <CitationNumber>80.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>T</Initials>
                    <FamilyName>Wang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>Y</Initials>
                    <FamilyName>Rui</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>S-M</Initials>
                    <FamilyName>Hu</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>J-G</Initials>
                    <FamilyName>Sun</FamilyName>
                  </BibAuthorName>
                  <Year>2003</Year>
                  <ArticleTitle Language="En">Adaptive tree similarity learning for image retrieval</ArticleTitle>
                  <JournalTitle>ACM Multimedia Syst</JournalTitle>
                  <VolumeID>9</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>131</FirstPage>
                  <LastPage> 143</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1007/s00530-003-0084-5</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Wang T, Rui Y, Hu S-M, Sun J-G (2003) Adaptive tree similarity learning for image retrieval. ACM Multimedia Syst 9(2):131– 143</BibUnstructured>
              </Citation>
              <Citation ID="CR81">
                <CitationNumber>81.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>R</Initials>
                    <FamilyName>Zhang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>Z</Initials>
                    <FamilyName>Zhang</FamilyName>
                  </BibAuthorName>
                  <Year>2005</Year>
                  <ArticleTitle Language="En">FAST: toward more effective and efficient image retrieval</ArticleTitle>
                  <JournalTitle>ACM Multimedia Syst</JournalTitle>
                  <VolumeID>10</VolumeID>
                  <IssueID>6</IssueID>
                  <FirstPage>529</FirstPage>
                  <LastPage> 543</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1007/s00530-005-0180-9</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Zhang R, Zhang Z (2005) FAST: toward more effective and efficient image retrieval. ACM Multimedia Syst 10(6):529– 543</BibUnstructured>
              </Citation>
              <Citation ID="CR82">
                <CitationNumber>82.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>DR</Initials>
                    <FamilyName>Heisterkamp</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Peng</FamilyName>
                  </BibAuthorName>
                  <Year>2005</Year>
                  <ArticleTitle Language="En">Kernel vector approximation files for relevance feedback retrieval in large image databases</ArticleTitle>
                  <JournalTitle>Multimedia Tools Appl</JournalTitle>
                  <VolumeID>26</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>175</FirstPage>
                  <LastPage>189</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1007/s11042-005-0454-4</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Heisterkamp DR, Peng J (2005) Kernel vector approximation files for relevance feedback retrieval in large image databases. Multimedia Tools Appl 26(2):175–189</BibUnstructured>
              </Citation>
              <Citation ID="CR83">
                <CitationNumber>83.</CitationNumber>
                <BibUnstructured>Tandon P, Nigam P, Pudi V, Jawahar CV (2008) FISH: a practical system for fast interactive image search in huge databases. In: Proceedings of ACM international conference on image and video retrieval, pp 369–378. doi:<ExternalRef><RefSource>10.1145/1386352.1386400</RefSource><RefTarget Address="10.1145/1386352.1386400" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR84">
                <CitationNumber>84.</CitationNumber>
                <BibUnstructured>Yu N, Vu K, Hua KA (2007) An in-memory relevance feedback technique for high-performance image retrieval systems. In: Proceedings of ACM international conference on image and video retrieval, pp 9–16. doi:<ExternalRef><RefSource>10.1145/1282280.1282282</RefSource><RefTarget Address="10.1145/1282280.1282282" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR85">
                <CitationNumber>85.</CitationNumber>
                <BibUnstructured>Indyk P, Motwani R (1998) Approximate nearest neighbors: towards removing the curse of dimensionality. In: Proceedings of ACM symposium on theory of computing, pp 604–613. doi:<ExternalRef><RefSource>10.1145/276698.276876</RefSource><RefTarget Address="10.1145/276698.276876" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR86">
                <CitationNumber>86.</CitationNumber>
                <BibUnstructured>Kuo Y-H, Chen K-T, Chiang C-H, Hsu WH (2009) Query expansion for hash-based image object retrieval. In: Proceedings of ACM international conference on multimedia, pp 65–74. doi:<ExternalRef><RefSource>10.1145/1631272.1631284</RefSource><RefTarget Address="10.1145/1631272.1631284" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR87">
                <CitationNumber>87.</CitationNumber>
                <BibUnstructured>Yang H, Wang Q, He Z (2008) Randomized sub-vectors hashing for high-dimensional image feature matching. In: Proceedings of ACM international conference on multimedia, pp 705–708. doi:<ExternalRef><RefSource>10.1145/1459359.1459465</RefSource><RefTarget Address="10.1145/1459359.1459465" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR88">
                <CitationNumber>88.</CitationNumber>
                <BibUnstructured>Jing F, Li M, Zhang H-J, Zhang B (2004) Entropy-based active learning with support vector machines for content-based image retrieval. In: Proceedings of IEEE international conference on multimedia and expo, vol 1, pp 85–88</BibUnstructured>
              </Citation>
              <Citation ID="CR89">
                <CitationNumber>89.</CitationNumber>
                <BibUnstructured>Hoi SCH, Jin R, Zu J, Lyu MR (2009) Semisupervised SVM batch mode active learning with applications to image retrieval. IEEE Trans Inf Syst 27(3) (article 16). doi:<ExternalRef><RefSource>10.1145/1508850.1508854</RefSource><RefTarget Address="10.1145/1508850.1508854" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR90">
                <CitationNumber>90.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>X</Initials>
                    <FamilyName>He</FamilyName>
                  </BibAuthorName>
                  <Year>2010</Year>
                  <ArticleTitle Language="En">Laplacian regularized D-Optimal Design for active learning and its application to image retrieval</ArticleTitle>
                  <JournalTitle>IEEE Trans Image Process</JournalTitle>
                  <VolumeID>19</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>254</FirstPage>
                  <LastPage>263</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>2744469</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TIP.2009.2032342</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>He X (2010) Laplacian regularized D-Optimal Design for active learning and its application to image retrieval. IEEE Trans Image Process 19(1):254–263</BibUnstructured>
              </Citation>
              <Citation ID="CR91">
                <CitationNumber>91.</CitationNumber>
                <BibUnstructured>Peng X, King I (2006) Biased minimax probability machine active learning for relevance feedback in content-based image retrieval. In: Proceedings of intelligent data engineering and automated, learning, pp 953–960. doi:<ExternalRef><RefSource>10.1007/11875581_114</RefSource><RefTarget Address="10.1007/11875581_114" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR92">
                <CitationNumber>92.</CitationNumber>
                <BibUnstructured>Dagli CK, Rajaram S, Huang TS (2006) Leveraging active learning for relevance feedback using an information theoretic diversity measure. In: Proceedings of international conference on image and video retrieval, pp 123–132. doi:<ExternalRef><RefSource>10.1007/11788034_13</RefSource><RefTarget Address="10.1007/11788034_13" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR93">
                <CitationNumber>93.</CitationNumber>
                <BibUnstructured>Chang EY, Lai W-C (2004) Active learning and its scalability for image retrieval. In: Proceedings of IEEE international conference on multimedia and expo, vol 1, pp 73–76. doi:<ExternalRef><RefSource>10.1109/ICME.2004.1394128</RefSource><RefTarget Address="10.1109/ICME.2004.1394128" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR94">
                <CitationNumber>94.</CitationNumber>
                <BibUnstructured>Goh K-S, Chang EY, Lai W-C (2004) Multimodal concept-dependent active learning for image retrieval. In: Proceedings of ACM international conference on multimedia, pp 564–571. doi:<ExternalRef><RefSource>10.1145/1027527.1027664</RefSource><RefTarget Address="10.1145/1027527.1027664" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR95">
                <CitationNumber>95.</CitationNumber>
                <BibUnstructured>Yang J, Li Y, Tian Y, Duan L, Gao W (2009) Multiple kernel active learning for image classification. In: Proceedings of IEEE international conference on multimedia and expo, pp 550–553. doi:<ExternalRef><RefSource>10.1109/ICME.2009.5202555</RefSource><RefTarget Address="10.1109/ICME.2009.5202555" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR96">
                <CitationNumber>96.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>R</Initials>
                    <FamilyName>Liu</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>Y</Initials>
                    <FamilyName>Wang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>T</Initials>
                    <FamilyName>Baba</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>D</Initials>
                    <FamilyName>Masumoto</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>S</Initials>
                    <FamilyName>Nagata</FamilyName>
                  </BibAuthorName>
                  <Year>2008</Year>
                  <ArticleTitle Language="En">SVM-based active feedback in image retrieval using clustering and unlabeled data</ArticleTitle>
                  <JournalTitle>Pattern Recogn</JournalTitle>
                  <VolumeID>41</VolumeID>
                  <IssueID>8</IssueID>
                  <FirstPage>2645</FirstPage>
                  <LastPage>2655</LastPage>
                  <Occurrence Type="ZLBID">
                    <Handle>1151.68397</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1016/j.patcog.2008.01.023</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Liu R, Wang Y, Baba T, Masumoto D, Nagata S (2008) SVM-based active feedback in image retrieval using clustering and unlabeled data. Pattern Recogn 41(8):2645–2655</BibUnstructured>
              </Citation>
              <Citation ID="CR97">
                <CitationNumber>97.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Cord</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>PH</Initials>
                    <FamilyName>Gosselin</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>S</Initials>
                    <FamilyName>Philipp-Foliguet</FamilyName>
                  </BibAuthorName>
                  <Year>2007</Year>
                  <ArticleTitle Language="En">Stochastic exploration and active learning for image retrieval</ArticleTitle>
                  <JournalTitle>Image Vis Comput</JournalTitle>
                  <VolumeID>25</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>14</FirstPage>
                  <LastPage>23</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1016/j.imavis.2006.01.004</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Cord M, Gosselin PH, Philipp-Foliguet S (2007) Stochastic exploration and active learning for image retrieval. Image Vis Comput 25(1):14–23</BibUnstructured>
              </Citation>
              <Citation ID="CR98">
                <CitationNumber>98.</CitationNumber>
                <BibUnstructured>Singh R, Kothari R (2003) Relevance feedback algorithm based on learning from labeled and unlabeled data. In: Proceedings of IEEE international conference on multimedia and expo, pp 433–436. doi:<ExternalRef><RefSource>10.1109/ICME.2003.1220947</RefSource><RefTarget Address="10.1109/ICME.2003.1220947" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR99">
                <CitationNumber>99.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>Z-H</Initials>
                    <FamilyName>Zhou</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>K-J</Initials>
                    <FamilyName>Chen</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>H-B</Initials>
                    <FamilyName>Dai</FamilyName>
                  </BibAuthorName>
                  <Year>2006</Year>
                  <ArticleTitle Language="En">Enhancing relevance feedback in image retrieval using unlabeled data</ArticleTitle>
                  <JournalTitle>ACM Trans Inf Syst</JournalTitle>
                  <VolumeID>24</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>219</FirstPage>
                  <LastPage>244</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1145/1148020.1148023</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Zhou Z-H, Chen K-J, Dai H-B (2006) Enhancing relevance feedback in image retrieval using unlabeled data. ACM Trans Inf Syst 24(2):219–244</BibUnstructured>
              </Citation>
              <Citation ID="CR100">
                <CitationNumber>100.</CitationNumber>
                <BibUnstructured>Cheng J, Wang K Multi-view sampling for relevance feedback in image retrieval. In: Proceedings of IEEE international conference on pattern recognition, pp 881–884</BibUnstructured>
              </Citation>
              <Citation ID="CR101">
                <CitationNumber>101.</CitationNumber>
                <BibUnstructured>Zhang X, Cheng J, Xu C, Lu H, Ma S (2009) Multi-view multi-label active learning for image classification. In: Proceedings of IEEE international conference on multimedia and expo, pp 258–261</BibUnstructured>
              </Citation>
              <Citation ID="CR102">
                <CitationNumber>102.</CitationNumber>
                <BibUnstructured>Zhang X, Cheng J, Lu H, Ma S (2008) Selective sampling based on dynamic certainty propagation for image retrieval. In: Proceedings of international multimedia modeling conference, pp 425–435. doi:<ExternalRef><RefSource>10.1007/978-3-540-77409-9_40</RefSource><RefTarget Address="10.1007/978-3-540-77409-9_40" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR103">
                <CitationNumber>103.</CitationNumber>
                <BibUnstructured>He X, Min W, Cai D, Zhou K (2007) Laplacian optimal design for image retrieval. In: Proceedings of ACM conference on research and development in information retrieval, pp 119–126. doi:<ExternalRef><RefSource>10.1145/1277741.1277764</RefSource><RefTarget Address="10.1145/1277741.1277764" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR104">
                <CitationNumber>104.</CitationNumber>
                <BibUnstructured>Hörster E, Lienhart R, Slaney M (2007) Image retrieval on large-scale image databases. In: Proceedings of ACM conference on image and video retrieval, pp 17–24</BibUnstructured>
              </Citation>
              <Citation ID="CR105">
                <CitationNumber>105.</CitationNumber>
                <BibUnstructured>Popescu A, Grefenstette G (2011) Social media driven image retrieval. In: Proceedings of ACM international conference on multimedia retrieval (article 33). doi:<ExternalRef><RefSource>10.1145/1991996.1992029</RefSource><RefTarget Address="10.1145/1991996.1992029" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR106">
                <CitationNumber>106.</CitationNumber>
                <BibUnstructured>Rawashdeh M, Kim H, El Saddik A (2011) Folksonomy-boosted social media search and ranking. In: Proceedings of ACM international conference on multimedia retrieval (article 27). doi:<ExternalRef><RefSource>10.1145/1991996.1992023</RefSource><RefTarget Address="10.1145/1991996.1992023" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR107">
                <CitationNumber>107.</CitationNumber>
                <BibUnstructured>Hu J, Wang G, Lochovsky F, Sun J, Chen Z (2009) Understanding user’s query intent with wikipedia. In: Proceedings of international conference on WWW, pp 471–480</BibUnstructured>
              </Citation>
              <Citation ID="CR108">
                <CitationNumber>108.</CitationNumber>
                <BibUnstructured>Das G, Ray S, Wilson C (2006) Feature re-weighting in content-based image retrieval. In: Proceedings of international conference on image and video retrieval, pp 193–200. doi:<ExternalRef><RefSource>10.1007/11788034_20</RefSource><RefTarget Address="10.1007/11788034_20" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR109">
                <CitationNumber>109.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Grigorova</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>FGB</Initials>
                    <FamilyName>Natale</FamilyName>
                    <Particle>De</Particle>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>CK</Initials>
                    <FamilyName>Dagli</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>TS</Initials>
                    <FamilyName>Huang</FamilyName>
                  </BibAuthorName>
                  <Year>2007</Year>
                  <ArticleTitle Language="En">Content-based image retrieval by feature adaptation and relevance feedback</ArticleTitle>
                  <JournalTitle>IEEE Trans Multimedia</JournalTitle>
                  <VolumeID>9</VolumeID>
                  <IssueID>6</IssueID>
                  <FirstPage>1183</FirstPage>
                  <LastPage>1192</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TMM.2007.902828</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Grigorova A, De Natale FGB, Dagli CK, Huang TS (2007) Content-based image retrieval by feature adaptation and relevance feedback. IEEE Trans Multimedia 9(6):1183–1192</BibUnstructured>
              </Citation>
              <Citation ID="CR110">
                <CitationNumber>110.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>Y</Initials>
                    <FamilyName>Wu</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Zhang</FamilyName>
                  </BibAuthorName>
                  <Year>2004</Year>
                  <ArticleTitle Language="En">Interactive pattern analysis for relevance feedback in multimedia information retrieval</ArticleTitle>
                  <JournalTitle>ACM Multimedia Syst</JournalTitle>
                  <VolumeID>10</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>41</FirstPage>
                  <LastPage>55</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1007/s00530-004-0136-5</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Wu Y, Zhang A (2004) Interactive pattern analysis for relevance feedback in multimedia information retrieval. ACM Multimedia Syst 10(1):41–55</BibUnstructured>
              </Citation>
              <Citation ID="CR111">
                <CitationNumber>111.</CitationNumber>
                <BibUnstructured>Franco A, Lumini A, Maio D (2004) A new approach for relevance feedback through positive and negative samples. In: Proceedings of IEEE international conference on pattern recognition, vol 4, pp 905–908. doi:<ExternalRef><RefSource>10.1109/ICPR.2004.1333919</RefSource><RefTarget Address="10.1109/ICPR.2004.1333919" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR112">
                <CitationNumber>112.</CitationNumber>
                <BibUnstructured>Hoi SCH, Liu W, Lyu MR, Ma W-Y (2006) Learning distance metrics with contextual constraints for image retrieval. In: Proceedings of IEEE conference on computer vision and pattern recognition, vol 2, pp 2072–2078. doi:<ExternalRef><RefSource>10.1109/CVPR.2006.167</RefSource><RefTarget Address="10.1109/CVPR.2006.167" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR113">
                <CitationNumber>113.</CitationNumber>
                <BibUnstructured>Huang R, Liu Q, Lu H, Ma S (2002) Solving the small sample size problem of LDA. In: Proceedings of IEEE international conference on pattern recognition, vol 3, pp 29–32</BibUnstructured>
              </Citation>
              <Citation ID="CR114">
                <CitationNumber>114.</CitationNumber>
                <BibUnstructured>Yoshizawa T, Schweitzer H (2004) Long-term learning of semantic grouping from relevance-feedback. In: Proceedings of ACM international workshop on multimedia, information retrieval, pp 165–172. doi:<ExternalRef><RefSource>10.1145/1026711.1026739</RefSource><RefTarget Address="10.1145/1026711.1026739" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR115">
                <CitationNumber>115.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>D</Initials>
                    <FamilyName>Tao</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>X</Initials>
                    <FamilyName>Tang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>X</Initials>
                    <FamilyName>Li</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>Y</Initials>
                    <FamilyName>Rui</FamilyName>
                  </BibAuthorName>
                  <Year>2006</Year>
                  <ArticleTitle Language="En">Direct kernel biased discriminant analysis: a new content-based image retrieval relevance feedback algorithm</ArticleTitle>
                  <JournalTitle>IEEE Trans Multimedia</JournalTitle>
                  <VolumeID>8</VolumeID>
                  <IssueID>4</IssueID>
                  <FirstPage>716</FirstPage>
                  <LastPage>727</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TMM.2005.861375</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Tao D, Tang X, Li X, Rui Y (2006) Direct kernel biased discriminant analysis: a new content-based image retrieval relevance feedback algorithm. IEEE Trans Multimedia 8(4):716–727</BibUnstructured>
              </Citation>
              <Citation ID="CR116">
                <CitationNumber>116.</CitationNumber>
                <BibUnstructured>Lin Y-Y, Liu T-L, Chen H-T (2005) Semantic manifold learning for image retrieval. In: Proceedings of ACM international conference on multimedia, pp 249–258. doi:<ExternalRef><RefSource>10.1145/1101149.1101193</RefSource><RefTarget Address="10.1145/1101149.1101193" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR117">
                <CitationNumber>117.</CitationNumber>
                <BibUnstructured>He X, Niyogi P (2003) Locality preserving projections. Advances in neural information processing systems, vol 16. MIT Press, Cambridge</BibUnstructured>
              </Citation>
              <Citation ID="CR118">
                <CitationNumber>118.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>X</Initials>
                    <FamilyName>He</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>D</Initials>
                    <FamilyName>Cai</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Han</FamilyName>
                  </BibAuthorName>
                  <Year>2008</Year>
                  <ArticleTitle Language="En">Learning a maximum margin subspace for image retrieval</ArticleTitle>
                  <JournalTitle>IEEE Trans Knowl Data Eng</JournalTitle>
                  <VolumeID>20</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>189</FirstPage>
                  <LastPage> 201</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TKDE.2007.190692</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>He X, Cai D, Han J (2008) Learning a maximum margin subspace for image retrieval. IEEE Trans Knowl Data Eng 20(2):189– 201</BibUnstructured>
              </Citation>
              <Citation ID="CR119">
                <CitationNumber>119.</CitationNumber>
                <BibUnstructured>Yu J, Tian Q (2006) Learning image manifolds by semantic subspace projection. In: Proceedings of ACM international conference on multimedia, pp 297–306. doi:<ExternalRef><RefSource>10.1145/1180639.1180710</RefSource><RefTarget Address="10.1145/1180639.1180710" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR120">
                <CitationNumber>120.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>W</Initials>
                    <FamilyName>Bian</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>D</Initials>
                    <FamilyName>Tao</FamilyName>
                  </BibAuthorName>
                  <Year>2010</Year>
                  <ArticleTitle Language="En">Biased discriminant Euclidean embedding for content-based image retrieval</ArticleTitle>
                  <JournalTitle>IEEE Trans Image Process</JournalTitle>
                  <VolumeID>19</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>545</FirstPage>
                  <LastPage>554</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>2729935</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TIP.2009.2035223</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Bian W, Tao D (2010) Biased discriminant Euclidean embedding for content-based image retrieval. IEEE Trans Image Process 19(2):545–554</BibUnstructured>
              </Citation>
              <Citation ID="CR121">
                <CitationNumber>121.</CitationNumber>
                <BibUnstructured>Hoiem D, Sukthankar R, Schneiderman H, Huston L (2004) Object-based image retrieval using the statistical structure of images. In: Proceedings of IEEE conference on computer vision and pattern recognition, vol 2, pp 490–497. doi:<ExternalRef><RefSource>10.1109/CVPR.2004.1315204</RefSource><RefTarget Address="10.1109/CVPR.2004.1315204" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR122">
                <CitationNumber>122.</CitationNumber>
                <BibUnstructured>Thomee B, Huiskes MJ, Bakker EM, Lew MS (2008) Using an artificial imagination for texture retrieval. In: Proceedings of IEEE international conference on pattern recognition, pp 1–4. doi:<ExternalRef><RefSource>10.1109/ICPR.2008.4761476</RefSource><RefTarget Address="10.1109/ICPR.2008.4761476" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR123">
                <CitationNumber>123.</CitationNumber>
                <BibUnstructured>Jing F, Li M, Zhang L, Zhang H-J, Zhang B (2003) Learning in region-based image retrieval. In: Proceedings of ACM conference on image and video retrieval, pp 199–204. doi:<ExternalRef><RefSource>10.1007/3-540-45113-7_21</RefSource><RefTarget Address="10.1007/3-540-45113-7_21" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR124">
                <CitationNumber>124.</CitationNumber>
                <BibUnstructured>Karthik S, Jawahar CV (2006) Efficient region based indexing and retrieval for images with elastic bucket tries. In: Proceedings of IEEE international conference on pattern recognition, vol 4, pp 169–172. doi:<ExternalRef><RefSource>10.1109/ICPR.2006.481</RefSource><RefTarget Address="10.1109/ICPR.2006.481" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR125">
                <CitationNumber>125.</CitationNumber>
                <BibUnstructured>Wu K, Yap K-H, Chau L-P (2006) Region-based image retrieval using radial basis function network. In: Proceedings of IEEE international conference on multimedia and expo, pp 1777–1780. doi:<ExternalRef><RefSource>10.1109/ICME.2006.262896</RefSource><RefTarget Address="10.1109/ICME.2006.262896" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR126">
                <CitationNumber>126.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>P</Initials>
                    <FamilyName>Muneesawang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>L</Initials>
                    <FamilyName>Guan</FamilyName>
                  </BibAuthorName>
                  <Year>2004</Year>
                  <ArticleTitle Language="En">An interactive approach for CBIR using a network of radial basis functions</ArticleTitle>
                  <JournalTitle>IEEE Trans Multimedia</JournalTitle>
                  <VolumeID>6</VolumeID>
                  <IssueID>5</IssueID>
                  <FirstPage>703</FirstPage>
                  <LastPage>716</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TMM.2004.834866</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Muneesawang P, Guan L (2004) An interactive approach for CBIR using a network of radial basis functions. IEEE Trans Multimedia 6(5):703–716</BibUnstructured>
              </Citation>
              <Citation ID="CR127">
                <CitationNumber>127.</CitationNumber>
                <BibUnstructured>Chan C-H, King I (2004) Using biased support vector machine to improve retrieval result in image retrieval with self-organizing map. In: Proceedings of international conference on neural information processing, pp 714–719. doi:<ExternalRef><RefSource>10.1007/978-3-540-30499-9_109</RefSource><RefTarget Address="10.1007/978-3-540-30499-9_109" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR128">
                <CitationNumber>128.</CitationNumber>
                <BibUnstructured>Koskela M, Laaksonen J, Oja E (2002) Implementing relevance feedback as convolutions of local neighborhoods on self-organizing maps. in: Proceedings of international conference on artificial, neural networks, pp 137–142. doi:<ExternalRef><RefSource>10.1007/3-540-46084-5_159</RefSource><RefTarget Address="10.1007/3-540-46084-5_159" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR129">
                <CitationNumber>129.</CitationNumber>
                <BibUnstructured>Hoi C-H, Chan C, Huang K, Lyu MR, King I (2004) Biased support vector machine for relevance feedback in image retrieval. In: Proceedings of IEEE international joint conference on neural networks, vol 4, pp 3189–3194</BibUnstructured>
              </Citation>
              <Citation ID="CR130">
                <CitationNumber>130.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>D</Initials>
                    <FamilyName>Tao</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>X</Initials>
                    <FamilyName>Tang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>X</Initials>
                    <FamilyName>Li</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>X</Initials>
                    <FamilyName>Wu</FamilyName>
                  </BibAuthorName>
                  <Year>2006</Year>
                  <ArticleTitle Language="En">Asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrieval</ArticleTitle>
                  <JournalTitle>IEEE Trans Pattern Anal Mach Intell</JournalTitle>
                  <VolumeID>28</VolumeID>
                  <IssueID>7</IssueID>
                  <FirstPage>1088</FirstPage>
                  <LastPage>1099</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TPAMI.2006.134</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Tao D, Tang X, Li X, Wu X (2006) Asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrieval. IEEE Trans Pattern Anal Mach Intell 28(7):1088–1099</BibUnstructured>
              </Citation>
              <Citation ID="CR131">
                <CitationNumber>131.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Zhang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>L</Initials>
                    <FamilyName>Ye</FamilyName>
                  </BibAuthorName>
                  <Year>2009</Year>
                  <ArticleTitle Language="En">Content based image retrieval using unclean positive examples</ArticleTitle>
                  <JournalTitle>IEEE Trans Image Process</JournalTitle>
                  <VolumeID>18</VolumeID>
                  <IssueID>10</IssueID>
                  <FirstPage>2370</FirstPage>
                  <LastPage> 2375</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>2814621</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TIP.2009.2026669</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Zhang J, Ye L (2009) Content based image retrieval using unclean positive examples. IEEE Trans Image Process 18(10):2370– 2375</BibUnstructured>
              </Citation>
              <Citation ID="CR132">
                <CitationNumber>132.</CitationNumber>
                <BibUnstructured>Wang L, Li X, Xue P, Chan KL (2005) A novel framework for SVM-based image retrieval on large databases. In: Proceedings of ACM international conference on multimedia, pp 487–490. doi:<ExternalRef><RefSource>10.1145/1101149.1101258</RefSource><RefTarget Address="10.1145/1101149.1101258" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR133">
                <CitationNumber>133.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>SCH</Initials>
                    <FamilyName>Hoi</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>MR</Initials>
                    <FamilyName>Lyu</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>R</Initials>
                    <FamilyName>Jin</FamilyName>
                  </BibAuthorName>
                  <Year>2006</Year>
                  <ArticleTitle Language="En">A unified log-based relevance feedback scheme for image retrieval</ArticleTitle>
                  <JournalTitle>IEEE Trans Knowl Data Eng</JournalTitle>
                  <VolumeID>18</VolumeID>
                  <IssueID>4</IssueID>
                  <FirstPage>509</FirstPage>
                  <LastPage>524</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TKDE.2006.1599389</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Hoi SCH, Lyu MR, Jin R (2006) A unified log-based relevance feedback scheme for image retrieval. IEEE Trans Knowl Data Eng 18(4):509–524</BibUnstructured>
              </Citation>
              <Citation ID="CR134">
                <CitationNumber>134.</CitationNumber>
                <BibUnstructured>Rao Y, Mundur P, Yesha Y (2006) Fuzzy SVM ensembles for relevance feedback in image retrieval. In: Proceedings of international conference on image and video retrieval, pp 350–359. doi:<ExternalRef><RefSource>10.1007/11788034_36</RefSource><RefTarget Address="10.1007/11788034_36" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR135">
                <CitationNumber>135.</CitationNumber>
                <BibUnstructured>Zhou XS, Garg A, Huang TS (2004) A discussion of nonlinear variants of biased discriminants for interactive image retrieval. In: Proceedings of international conference on image and video retrieval, pp 1948–1959. doi:<ExternalRef><RefSource>10.1007/978-3-540-27814-6_43</RefSource><RefTarget Address="10.1007/978-3-540-27814-6_43" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR136">
                <CitationNumber>136.</CitationNumber>
                <BibUnstructured>Wang L, Gao Y, Chan KL, Xue P, Yau W-Y (2005) Retrieval with knowledge-driven kernel design: an approach to improving SVM-based CBIR with relevance feedback. In: Proceedings of IEEE international conference on computer vision, vol 2, pp 1355–1362</BibUnstructured>
              </Citation>
              <Citation ID="CR137">
                <CitationNumber>137.</CitationNumber>
                <BibUnstructured>Xie H, Andreu V, Ortega A (2006) Quantization-based probabilistic feature modeling for kernel design in content-based image retrieval. In: Proceedings of ACM international workshop on multimedia, information retrieval, pp 23–32. doi:<ExternalRef><RefSource>10.1145/1178677.1178684</RefSource><RefTarget Address="10.1145/1178677.1178684" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR138">
                <CitationNumber>138.</CitationNumber>
                <BibUnstructured>Hoi C-H, Lyu MR (2004) Group-based relevance feedback with support vector machine ensembles. In: Proceedings of IEEE international conference on pattern recognition, vol 3, pp 874–877</BibUnstructured>
              </Citation>
              <Citation ID="CR139">
                <CitationNumber>139.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>K</Initials>
                    <FamilyName>Tieu</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>P</Initials>
                    <FamilyName>Viola</FamilyName>
                  </BibAuthorName>
                  <Year>2004</Year>
                  <ArticleTitle Language="En">Boosting image retrieval</ArticleTitle>
                  <JournalTitle>Int J Comput Vis</JournalTitle>
                  <VolumeID>56</VolumeID>
                  <IssueID>1–2</IssueID>
                  <FirstPage>17</FirstPage>
                  <LastPage>36</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1023/B:VISI.0000004830.93820.78</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Tieu K, Viola P (2004) Boosting image retrieval. Int J Comput Vis 56(1–2):17–36</BibUnstructured>
              </Citation>
              <Citation ID="CR140">
                <CitationNumber>140.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>P-Y</Initials>
                    <FamilyName>Yin</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>B</Initials>
                    <FamilyName>Bhanu</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>K-C</Initials>
                    <FamilyName>Chang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Dong</FamilyName>
                  </BibAuthorName>
                  <Year>2005</Year>
                  <ArticleTitle Language="En">Integrating relevance feedback techniques for image retrieval using reinforcement learning</ArticleTitle>
                  <JournalTitle>IEEE Trans Pattern Anal Mach Intell</JournalTitle>
                  <VolumeID>27</VolumeID>
                  <IssueID>10</IssueID>
                  <FirstPage>1536</FirstPage>
                  <LastPage>1551</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TPAMI.2005.201</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Yin P-Y, Bhanu B, Chang K-C, Dong A (2005) Integrating relevance feedback techniques for image retrieval using reinforcement learning. IEEE Trans Pattern Anal Mach Intell 27(10):1536–1551</BibUnstructured>
              </Citation>
              <Citation ID="CR141">
                <CitationNumber>141.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>T</Initials>
                    <FamilyName>Amin</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Zeytinoglu</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>L</Initials>
                    <FamilyName>Guan</FamilyName>
                  </BibAuthorName>
                  <Year>2007</Year>
                  <ArticleTitle Language="En">Application of Laplacian mixture model to image and video retrieval</ArticleTitle>
                  <JournalTitle>IEEE Trans Multimedia</JournalTitle>
                  <VolumeID>9</VolumeID>
                  <IssueID>7</IssueID>
                  <FirstPage>1416</FirstPage>
                  <LastPage>1429</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TMM.2007.906587</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Amin T, Zeytinoglu M, Guan L (2007) Application of Laplacian mixture model to image and video retrieval. IEEE Trans Multimedia 9(7):1416–1429</BibUnstructured>
              </Citation>
              <Citation ID="CR142">
                <CitationNumber>142.</CitationNumber>
                <BibUnstructured>Qian F, Li M, Zhang L, Zhang H-J, Zhang B (2002) Gaussian mixture model for relevance feedback in image retrieval. In: Proceedings of IEEE international conference on multimedia and expo, vol 1, pp 229–232</BibUnstructured>
              </Citation>
              <Citation ID="CR143">
                <CitationNumber>143.</CitationNumber>
                <BibUnstructured>Zhang R, Zhang Z (2004) Stretching Bayesian learning in the relevance feedback of image retrieval. In: Proceedings of European conference on computer vision, vol 3, pp 996–1001</BibUnstructured>
              </Citation>
              <Citation ID="CR144">
                <CitationNumber>144.</CitationNumber>
                <BibUnstructured>Wu H, Lu H, Ma S (2002) The role of sample distribution in relevance feedback for content based image retrieval. In: Proceedings of IEEE international conference on multimedia and expo, pp 225–228</BibUnstructured>
              </Citation>
              <Citation ID="CR145">
                <CitationNumber>145.</CitationNumber>
                <BibUnstructured>Gondra I, Heisterkamp DR (2004) Learning in region-based image retrieval with generalized support vector machines. In: Proceedings of IEEE conference on computer vision and pattern recognition, workshop, pp 149–156</BibUnstructured>
              </Citation>
              <Citation ID="CR146">
                <CitationNumber>146.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>Y-S</Initials>
                    <FamilyName>Chen</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>C</Initials>
                    <FamilyName>Shahabi</FamilyName>
                  </BibAuthorName>
                  <Year>2003</Year>
                  <ArticleTitle Language="En">Yoda, an adaptive soft classification model: content-based similarity queries and beyond</ArticleTitle>
                  <JournalTitle>ACM Multimedia Syst</JournalTitle>
                  <VolumeID>8</VolumeID>
                  <IssueID>6</IssueID>
                  <FirstPage>523</FirstPage>
                  <LastPage>535</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1007/s00530-002-0074-z</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Chen Y-S, Shahabi C (2003) Yoda, an adaptive soft classification model: content-based similarity queries and beyond. ACM Multimedia Syst 8(6):523–535</BibUnstructured>
              </Citation>
              <Citation ID="CR147">
                <CitationNumber>147.</CitationNumber>
                <BibUnstructured>ten Brinke W, Squire DMcG, Bigelow J (2004) Similarity: measurement, ordering and betweenness. In: Proceedings of international conference on knowledge-based intelligent information and engineering systems, pp 169–184. doi:<ExternalRef><RefSource>10.1007/978-3-540-30133-2_132</RefSource><RefTarget Address="10.1007/978-3-540-30133-2_132" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR148">
                <CitationNumber>148.</CitationNumber>
                <BibUnstructured>Giacinto G, Roli F (2004) Nearest-prototype relevance feedback for content based image retrieval. In: Proceedings of IEEE international conference on pattern recognition, vol 2, pp 989–992. doi:<ExternalRef><RefSource>10.1109/ICPR.2004.1334425</RefSource><RefTarget Address="10.1109/ICPR.2004.1334425" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR149">
                <CitationNumber>149.</CitationNumber>
                <BibUnstructured>Royal M, Chang R, Qi X (2007) Learning from relevance feedback sessions using a k-nearest-neighbor-based semantic repository. In: Proceedings of IEEE international conference on multimedia and expo, pp 1994–1997. doi:<ExternalRef><RefSource>10.1109/icme.2007.4285070</RefSource><RefTarget Address="10.1109/icme.2007.4285070" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR150">
                <CitationNumber>150.</CitationNumber>
                <BibUnstructured>Zhang J, Ye L (2007) An unified framework based on p-norm for feature aggregation in content-based image retrieval. In: Proceedings of IEEE international symposium on multimedia, pp 195–201. doi:<ExternalRef><RefSource>10.1109/ISM.2007.4412374</RefSource><RefTarget Address="10.1109/ISM.2007.4412374" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR151">
                <CitationNumber>151.</CitationNumber>
                <BibUnstructured>Wu H, Lu H, Ma S (2003) A practical SVM-based algorithm for ordinal regression in image retrieval. In: Proceedings of ACM international conference on multimedia, pp 612–621. doi:<ExternalRef><RefSource>10.1145/957013.957144</RefSource><RefTarget Address="10.1145/957013.957144" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR152">
                <CitationNumber>152.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>H</Initials>
                    <FamilyName>Müller</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>T</Initials>
                    <FamilyName>Pun</FamilyName>
                  </BibAuthorName>
                  <Year>2004</Year>
                  <ArticleTitle Language="En">Learning from user behavior in image retrieval: application of market basket analysis</ArticleTitle>
                  <JournalTitle>Int J Comput Vis</JournalTitle>
                  <VolumeID>56</VolumeID>
                  <IssueID>1</IssueID>
                  <FirstPage>65</FirstPage>
                  <LastPage>77</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1023/B:VISI.0000004832.02269.45</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Müller H, Pun T (2004) Learning from user behavior in image retrieval: application of market basket analysis. Int J Comput Vis 56(1):65–77</BibUnstructured>
              </Citation>
              <Citation ID="CR153">
                <CitationNumber>153.</CitationNumber>
                <BibUnstructured>He X, Ma W-Y, King O, Li M, Zhang H-J (2002) Learning and inferring a semantic space from user’s relevance feedback for image retrieval. In: Proceedings of ACM international conference on multimedia, pp 343–346</BibUnstructured>
              </Citation>
              <Citation ID="CR154">
                <CitationNumber>154.</CitationNumber>
                <BibUnstructured>Shah-hosseini A, Knapp GM (2006) Semantic image retrieval based on probabilistic latent semantic analysis. In: Proceedings of ACM international conference on multimedia, pp 703–706. doi:<ExternalRef><RefSource>10.1145/1180639.1180788</RefSource><RefTarget Address="10.1145/1180639.1180788" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR155">
                <CitationNumber>155.</CitationNumber>
                <BibUnstructured>Chen Y, Rege M, Dong M, Fotouhi F (2007) Deriving semantics for image clustering from accumulated user feedbacks. In: Proceedings of ACM international conference on multimedia, pp 313–316. doi:<ExternalRef><RefSource>10.1145/1291233.1291300</RefSource><RefTarget Address="10.1145/1291233.1291300" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR156">
                <CitationNumber>156.</CitationNumber>
                <BibUnstructured>Cheng H, Hua KA, Vu K (2008) Leveraging user query log: toward improving image data clustering. In: Proceedings of ACM conference on image and video retrieval, pp 27–36. doi:<ExternalRef><RefSource>10.1145/1386352.1386360</RefSource><RefTarget Address="10.1145/1386352.1386360" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR157">
                <CitationNumber>157.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>P-Y</Initials>
                    <FamilyName>Yin</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>B</Initials>
                    <FamilyName>Bhanu</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>K-C</Initials>
                    <FamilyName>Chang</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>A</Initials>
                    <FamilyName>Dong</FamilyName>
                  </BibAuthorName>
                  <Year>2008</Year>
                  <ArticleTitle Language="En">Long-term cross-session relevance feedback using virtual features</ArticleTitle>
                  <JournalTitle>IEEE Trans Knowl Data Eng</JournalTitle>
                  <VolumeID>20</VolumeID>
                  <IssueID>3</IssueID>
                  <FirstPage>352</FirstPage>
                  <LastPage>368</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TKDE.2007.190697</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Yin P-Y, Bhanu B, Chang K-C, Dong A (2008) Long-term cross-session relevance feedback using virtual features. IEEE Trans Knowl Data Eng 20(3):352–368</BibUnstructured>
              </Citation>
              <Citation ID="CR158">
                <CitationNumber>158.</CitationNumber>
                <BibUnstructured>Barrett S, Chang R, Qi X (2009) A fuzzy combined learning approach to content-based image retrieval. In: Proceedings of IEEE international conference on multimedia and expo, pp 838–841. doi:<ExternalRef><RefSource>10.1109/ICME.2009.5202625</RefSource><RefTarget Address="10.1109/ICME.2009.5202625" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR159">
                <CitationNumber>159.</CitationNumber>
                <BibUnstructured>Oh S, Chung MG, Sull S (2004) Relevance feedback reinforced with semantics accumulation. In: Proceedings of conference on image and video retrieval, pp 448–454. doi:<ExternalRef><RefSource>10.1007/978-3-540-27814-6_53</RefSource><RefTarget Address="10.1007/978-3-540-27814-6_53" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR160">
                <CitationNumber>160.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Rege</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>M</Initials>
                    <FamilyName>Dong</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>F</Initials>
                    <FamilyName>Fotouhi</FamilyName>
                  </BibAuthorName>
                  <Year>2007</Year>
                  <ArticleTitle Language="En">Building a user-centered semantic hierarchy in image databases</ArticleTitle>
                  <JournalTitle>ACM Multimedia Syst</JournalTitle>
                  <VolumeID>12</VolumeID>
                  <IssueID>4</IssueID>
                  <FirstPage>325</FirstPage>
                  <LastPage>338</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1007/s00530-006-0049-6</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Rege M, Dong M, Fotouhi F (2007) Building a user-centered semantic hierarchy in image databases. ACM Multimedia Syst 12(4):325–338</BibUnstructured>
              </Citation>
              <Citation ID="CR161">
                <CitationNumber>161.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>DP</Initials>
                    <FamilyName>Huijsmans</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>N</Initials>
                    <FamilyName>Sebe</FamilyName>
                  </BibAuthorName>
                  <Year>2005</Year>
                  <ArticleTitle Language="En">How to complete performance graphs in content-based image retrieval: add generality and normalize scope</ArticleTitle>
                  <JournalTitle>IEEE Trans Pattern Anal Machine Intell</JournalTitle>
                  <VolumeID>27</VolumeID>
                  <IssueID>2</IssueID>
                  <FirstPage>245</FirstPage>
                  <LastPage>251</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TPAMI.2005.30</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Huijsmans DP, Sebe N (2005) How to complete performance graphs in content-based image retrieval: add generality and normalize scope. IEEE Trans Pattern Anal Machine Intell 27(2):245–251</BibUnstructured>
              </Citation>
              <Citation ID="CR162">
                <CitationNumber>162.</CitationNumber>
                <BibUnstructured>Tronci R, Falqui L, Piras L, Giacinto G (2011) A study on the evaluation of relevance feedback in multi-tagged image datasets. In: Proceedings of IEEE symposium on multimedia, pp 452–457. doi:<ExternalRef><RefSource>10.1109/ISM.2011.80</RefSource><RefTarget Address="10.1109/ISM.2011.80" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR163">
                <CitationNumber>163.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>C-J</Initials>
                    <FamilyName>Li</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>C-T</Initials>
                    <FamilyName>Hsu</FamilyName>
                  </BibAuthorName>
                  <Year>2008</Year>
                  <ArticleTitle Language="En">Image retrieval with relevance feedback based on graph-theoretic region correspondence estimation</ArticleTitle>
                  <JournalTitle>IEEE Trans Multimedia</JournalTitle>
                  <VolumeID>10</VolumeID>
                  <IssueID>3</IssueID>
                  <FirstPage>447</FirstPage>
                  <LastPage>456</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TMM.2008.917421</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Li C-J, Hsu C-T (2008) Image retrieval with relevance feedback based on graph-theoretic region correspondence estimation. IEEE Trans Multimedia 10(3):447–456</BibUnstructured>
              </Citation>
              <Citation ID="CR164">
                <CitationNumber>164.</CitationNumber>
                <BibUnstructured>Marchand-Maillet S, Worring M (2006) Benchmarking image and video retrieval: an overview. In: Proceedings of ACM international workshop on multimedia, information retrieval, pp 297–300. doi:<ExternalRef><RefSource>10.1145/1178677.1178718</RefSource><RefTarget Address="10.1145/1178677.1178718" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR165">
                <CitationNumber>165.</CitationNumber>
                <BibUnstructured>Huiskes MJ, Lew MS (2008) Performance evaluation of relevance feedback methods. In: Proceedings of ACM international conference on image and video retrieval, pp 239–248. doi:<ExternalRef><RefSource>10.1145/1386352.1386387</RefSource><RefTarget Address="10.1145/1386352.1386387" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR166">
                <CitationNumber>166.</CitationNumber>
                <BibUnstructured>Jin X, French JC, Michel J (2006) Toward consistent evaluation of relevance feedback approaches in multimedia retrieval. In: Proceedings of international workshop on adaptive multimedia retrieval: user, context, and feedback, pp 191–206. doi:<ExternalRef><RefSource>10.1007/11670834_16</RefSource><RefTarget Address="10.1007/11670834_16" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR167">
                <CitationNumber>167.</CitationNumber>
                <BibUnstructured>Müller H, Marchand-Maillet S, Pun T (2002) The truth about Corel—evaluation in image retrieval. In: Proceedings of international conference on image and video retrieval, pp 38–49</BibUnstructured>
              </Citation>
              <Citation ID="CR168">
                <CitationNumber>168.</CitationNumber>
                <BibUnstructured>Huiskes MJ, Lew MS (2008) The MIR Flickr retrieval evaluation. In: Proceedings of ACM international conference on multimedia, information retrieval, pp 39–43. doi:<ExternalRef><RefSource>10.1145/1460096.1460104</RefSource><RefTarget Address="10.1145/1460096.1460104" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR169">
                <CitationNumber>169.</CitationNumber>
                <BibUnstructured>Huiskes MJ, Thomee B, Lew MS (2010) New trends and ideas in visual concept detection. In: Proceedings of ACM international conference on multimedia, information retrieval, pp 527–536. doi:<ExternalRef><RefSource>10.1145/1743384.1743475</RefSource><RefTarget Address="10.1145/1743384.1743475" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR170">
                <CitationNumber>170.</CitationNumber>
                <BibUnstructured>Brodatz P (1966) Textures: a photographic album for artists and designers. Dover, NY</BibUnstructured>
              </Citation>
              <Citation ID="CR171">
                <CitationNumber>171.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>S</Initials>
                    <FamilyName>Lazebnik</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>C</Initials>
                    <FamilyName>Schmid</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>J</Initials>
                    <FamilyName>Ponce</FamilyName>
                  </BibAuthorName>
                  <Year>2005</Year>
                  <ArticleTitle Language="En">A sparse texture representation using local affine regions</ArticleTitle>
                  <JournalTitle>IEEE Trans Pattern Anal Mach Intell</JournalTitle>
                  <VolumeID>27</VolumeID>
                  <IssueID>8</IssueID>
                  <FirstPage>1265</FirstPage>
                  <LastPage>1278</LastPage>
                  <Occurrence Type="DOI">
                    <Handle>10.1109/TPAMI.2005.151</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Lazebnik S, Schmid C, Ponce J (2005) A sparse texture representation using local affine regions. IEEE Trans Pattern Anal Mach Intell 27(8):1265–1278</BibUnstructured>
              </Citation>
              <Citation ID="CR172">
                <CitationNumber>172.</CitationNumber>
                <BibUnstructured>Pickard R, Graszyk C, Mann S, Wachman J, Pickard L, Campbell L (1995) VisTex databases. Technical report, MIT Media Laboratory</BibUnstructured>
              </Citation>
              <Citation ID="CR173">
                <CitationNumber>173.</CitationNumber>
                <BibUnstructured>Griffin G, Holub A, Perona P (2007) Caltech-256 object category dataset. Technical report, California Institute of Technology</BibUnstructured>
              </Citation>
              <Citation ID="CR174">
                <CitationNumber>174.</CitationNumber>
                <BibUnstructured>Everingham H, Winn J (2007) The Pascal VOC challenge 2007 development kit. University of Leeds, Technical report</BibUnstructured>
              </Citation>
              <Citation ID="CR175">
                <CitationNumber>175.</CitationNumber>
                <BibUnstructured>Kalpathy-Cramer J, Müller H, Bedrick S, Eggel I, Garcia Seco de Herrera A, Tsikrika T (2011) Overview of the CLEF 2011 medical image classification and retrieval tasks. In: Proceedings of Cross-Language Evaluation Forum</BibUnstructured>
              </Citation>
              <Citation ID="CR176">
                <CitationNumber>176.</CitationNumber>
                <BibUnstructured>Nene SA, Nayar SK, Murase H (1996) Columbia Object Image Library (COIL-100), Technical Report CUCS-006-96. Columbia University</BibUnstructured>
              </Citation>
              <Citation ID="CR177">
                <CitationNumber>177.</CitationNumber>
                <BibUnstructured>Chang H, Yeung D-Y (2007) Locally smooth metric learning with application to image retrieval. In: Proceedings of IEEE international conference on computer vision, pp 1–7. doi:<ExternalRef><RefSource>10.1109/ICCV.2007.4408862</RefSource><RefTarget Address="10.1109/ICCV.2007.4408862" TargetType="DOI"/></ExternalRef>.</BibUnstructured>
              </Citation>
              <Citation ID="CR178">
                <CitationNumber>178.</CitationNumber>
                <BibArticle>
                  <BibAuthorName>
                    <Initials>D</Initials>
                    <FamilyName>Baras</FamilyName>
                  </BibAuthorName>
                  <BibAuthorName>
                    <Initials>R</Initials>
                    <FamilyName>Meir</FamilyName>
                  </BibAuthorName>
                  <Year>2007</Year>
                  <ArticleTitle Language="En">Reinforcement learning, spike time dependent plasticity, and the BCM rule</ArticleTitle>
                  <JournalTitle>Neural Comput</JournalTitle>
                  <VolumeID>19</VolumeID>
                  <IssueID>8</IssueID>
                  <FirstPage>2245</FirstPage>
                  <LastPage>2279</LastPage>
                  <Occurrence Type="AMSID">
                    <Handle>2331047</Handle>
                  </Occurrence>
                  <Occurrence Type="ZLBID">
                    <Handle>1129.92001</Handle>
                  </Occurrence>
                  <Occurrence Type="DOI">
                    <Handle>10.1162/neco.2007.19.8.2245</Handle>
                  </Occurrence>
                </BibArticle>
                <BibUnstructured>Baras D, Meir R (2007) Reinforcement learning, spike time dependent plasticity, and the BCM rule. Neural Comput 19(8):2245–2279</BibUnstructured>
              </Citation>
            </Bibliography>
          </ArticleBackmatter>
        </Article>
      </Issue>
    </Volume>
  </Journal>
</Publisher>
